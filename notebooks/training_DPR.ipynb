{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5303e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - haystack.modeling.model.optimization -  apex not found, won't use it. See https://nvidia.github.io/apex/\n"
     ]
    }
   ],
   "source": [
    "from haystack.nodes import DensePassageRetriever\n",
    "from haystack.utils import fetch_archive_from_http\n",
    "from haystack.document_stores import InMemoryDocumentStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55203f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.nodes import PDFToTextConverter\n",
    "from haystack.document_stores import ElasticsearchDocumentStore\n",
    "from haystack.nodes import DensePassageRetriever\n",
    "from haystack.nodes import FARMReader, ElasticsearchRetriever\n",
    "from haystack.pipelines import ExtractiveQAPipeline, DocumentSearchPipeline\n",
    "from haystack.nodes import PreProcessor\n",
    "from haystack import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e0369112",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d2cd391",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_dir = \"./\"\n",
    "train_filename = \"squad_format_thesis/training_dpr.json\"\n",
    "dev_filename = \"squad_format_thesis/dev_dpr.json\"\n",
    "\n",
    "query_model = \"voidful/dpr-question_encoder-bert-base-multilingual\"\n",
    "passage_model = \"voidful/dpr-ctx_encoder-bert-base-multilingual\"\n",
    "\n",
    "save_dir = \"/models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aba491af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - haystack.modeling.utils -  Using devices: CUDA:0\n",
      "INFO - haystack.modeling.utils -  Number of GPUs: 1\n",
      "INFO - haystack.modeling.utils -  Using devices: CUDA:0\n",
      "INFO - haystack.modeling.utils -  Number of GPUs: 1\n",
      "INFO - haystack.modeling.model.language_model -  LOADING MODEL\n",
      "INFO - haystack.modeling.model.language_model -  =============\n",
      "INFO - haystack.modeling.model.language_model -  Could not find voidful/dpr-question_encoder-bert-base-multilingual locally.\n",
      "INFO - haystack.modeling.model.language_model -  Looking on Transformers Model Hub (in local cache and online)...\n",
      "INFO - haystack.modeling.model.language_model -  Automatically detected language from language model name: multilingual\n",
      "INFO - haystack.modeling.model.language_model -  Loaded voidful/dpr-question_encoder-bert-base-multilingual\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DPRQuestionEncoderTokenizer'. \n",
      "The class this function is called from is 'DPRContextEncoderTokenizerFast'.\n",
      "INFO - haystack.modeling.model.language_model -  LOADING MODEL\n",
      "INFO - haystack.modeling.model.language_model -  =============\n",
      "INFO - haystack.modeling.model.language_model -  Could not find voidful/dpr-ctx_encoder-bert-base-multilingual locally.\n",
      "INFO - haystack.modeling.model.language_model -  Looking on Transformers Model Hub (in local cache and online)...\n",
      "INFO - haystack.modeling.model.language_model -  Automatically detected language from language model name: multilingual\n",
      "INFO - haystack.modeling.model.language_model -  Loaded voidful/dpr-ctx_encoder-bert-base-multilingual\n"
     ]
    }
   ],
   "source": [
    "retriever = DensePassageRetriever(\n",
    "    document_store=InMemoryDocumentStore(),\n",
    "    query_embedding_model=query_model,\n",
    "    passage_embedding_model=passage_model,\n",
    "    max_seq_len_query=64,\n",
    "    max_seq_len_passage=256,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f5037770",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - haystack.modeling.data_handler.data_silo -  \n",
      "Loading data into the data silo ... \n",
      "              ______\n",
      "               |o  |   !\n",
      "   __          |:`_|---'-.\n",
      "  |__|______.-/ _ \\-----.|       \n",
      " (o)(o)------'\\ _ /     ( )      \n",
      " \n",
      "INFO - haystack.modeling.data_handler.data_silo -  LOADING TRAIN DATA\n",
      "INFO - haystack.modeling.data_handler.data_silo -  ==================\n",
      "INFO - haystack.modeling.data_handler.data_silo -  Loading train set from: squad_format_thesis/training_dpr.json \n",
      "INFO - haystack.modeling.data_handler.data_silo -  Got ya 16 parallel workers to convert 101 dictionaries to pytorch datasets (chunksize = 6)...\n",
      "INFO - haystack.modeling.data_handler.data_silo -   0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0  \n",
      "INFO - haystack.modeling.data_handler.data_silo -  /w\\   /|\\  /w\\   /|\\  /|\\  /w\\   /w\\   /w\\   /w\\   /|\\  /w\\   /|\\  /|\\  /w\\   /w\\   /|\\\n",
      "INFO - haystack.modeling.data_handler.data_silo -  / \\   /'\\   / \\   /'\\   /'\\   /'\\   /'\\   / \\   / \\   /'\\   / \\   /'\\   /'\\   /'\\   / \\   /'\\ \n",
      "Preprocessing Dataset squad_format_thesis/training_dpr.json: 100%|█| 101/101 [00\n",
      "INFO - haystack.modeling.data_handler.data_silo -  \n",
      "INFO - haystack.modeling.data_handler.data_silo -  LOADING DEV DATA\n",
      "INFO - haystack.modeling.data_handler.data_silo -  =================\n",
      "INFO - haystack.modeling.data_handler.data_silo -  Loading dev set from: squad_format_thesis/dev_dpr.json\n",
      "INFO - haystack.modeling.data_handler.data_silo -  Got ya 5 parallel workers to convert 25 dictionaries to pytorch datasets (chunksize = 5)...\n",
      "INFO - haystack.modeling.data_handler.data_silo -   0     0     0     0     0  \n",
      "INFO - haystack.modeling.data_handler.data_silo -  /|\\  /w\\   /w\\   /|\\  /w\\ \n",
      "INFO - haystack.modeling.data_handler.data_silo -  /'\\   / \\   / \\   /'\\   / \\ \n",
      "Preprocessing Dataset squad_format_thesis/dev_dpr.json: 100%|█| 25/25 [00:00<00:\n",
      "INFO - haystack.modeling.data_handler.data_silo -  \n",
      "INFO - haystack.modeling.data_handler.data_silo -  LOADING TEST DATA\n",
      "INFO - haystack.modeling.data_handler.data_silo -  =================\n",
      "INFO - haystack.modeling.data_handler.data_silo -  Loading test set from: squad_format_thesis/dev_dpr.json\n",
      "INFO - haystack.modeling.data_handler.data_silo -  Got ya 5 parallel workers to convert 25 dictionaries to pytorch datasets (chunksize = 5)...\n",
      "INFO - haystack.modeling.data_handler.data_silo -   0     0     0     0     0  \n",
      "INFO - haystack.modeling.data_handler.data_silo -  /|\\  /w\\   /w\\   /w\\   /|\\\n",
      "INFO - haystack.modeling.data_handler.data_silo -  /'\\   / \\   /'\\   /'\\   /'\\ \n",
      "Preprocessing Dataset squad_format_thesis/dev_dpr.json: 100%|█| 25/25 [00:00<00:\n",
      "INFO - haystack.modeling.data_handler.data_silo -  \n",
      "INFO - haystack.modeling.data_handler.data_silo -  DATASETS SUMMARY\n",
      "INFO - haystack.modeling.data_handler.data_silo -  ================\n",
      "INFO - haystack.modeling.data_handler.data_silo -  Examples in train: 101\n",
      "INFO - haystack.modeling.data_handler.data_silo -  Examples in dev  : 25\n",
      "INFO - haystack.modeling.data_handler.data_silo -  Examples in test : 25\n",
      "INFO - haystack.modeling.data_handler.data_silo -  Total examples   : 151\n",
      "INFO - haystack.modeling.data_handler.data_silo -  \n",
      "INFO - haystack.modeling.data_handler.data_silo -  Longest query length observed after clipping: 14   - for max_query_len: 64\n",
      "INFO - haystack.modeling.data_handler.data_silo -  Average query length after clipping:          7.7227722772277225\n",
      "INFO - haystack.modeling.data_handler.data_silo -  Proportion queries clipped:                   0.0\n",
      "INFO - haystack.modeling.data_handler.data_silo -  \n",
      "INFO - haystack.modeling.data_handler.data_silo -  Longest passage length observed after clipping: 256.0   - for max_passage_len: 256\n",
      "INFO - haystack.modeling.data_handler.data_silo -  Average passage length after clipping:          206.70792079207922\n",
      "INFO - haystack.modeling.data_handler.data_silo -  Proportion passages clipped:                    0.04950495049504951\n",
      "WARNING - haystack.modeling.logger -  Failed to log params: Changing param values is not allowed. Param with key='batch_size' was already logged with value='16' for run ID='4eebebc1acf14a659da033712115655f'. Attempted logging new value '4'.\n",
      "WARNING - haystack.modeling.logger -  Failed to log params: Changing param values is not allowed. Param with key='num_train_optimization_steps' was already logged with value='0' for run ID='4eebebc1acf14a659da033712115655f'. Attempted logging new value '3'.\n",
      "INFO - haystack.modeling.model.optimization -  Loading optimizer `AdamW`: '{'correct_bias': True, 'weight_decay': 0.0, 'eps': 1e-08, 'lr': 1e-05}'\n",
      "INFO - haystack.modeling.model.optimization -  Using scheduler 'get_linear_schedule_with_warmup'\n",
      "INFO - haystack.modeling.model.optimization -  Loading schedule `get_linear_schedule_with_warmup`: '{'num_warmup_steps': 100, 'num_training_steps': 3}'\n",
      "WARNING - haystack.modeling.logger -  Failed to log params: Changing param values is not allowed. Param with key='num_training_steps' was already logged with value='0' for run ID='4eebebc1acf14a659da033712115655f'. Attempted logging new value '3'.\n",
      "Train epoch 0/0 (Cur. train loss: 0.0000):   0%|         | 0/26 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 7.79 GiB total capacity; 5.10 GiB already allocated; 33.88 MiB free; 5.34 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mretriever\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoc_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_filename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdev_filename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdev_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_filename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdev_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_acc_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluate_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43membed_title\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_positives\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_hard_negatives\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Codes/thesis_v4/env/lib/python3.8/site-packages/haystack/nodes/retriever/dense.py:459\u001b[0m, in \u001b[0;36mDensePassageRetriever.train\u001b[0;34m(self, data_dir, train_filename, dev_filename, test_filename, max_samples, max_processes, multiprocessing_strategy, dev_split, batch_size, embed_title, num_hard_negatives, num_positives, n_epochs, evaluate_every, n_gpu, learning_rate, epsilon, weight_decay, num_warmup_steps, grad_acc_steps, use_amp, optimizer_name, optimizer_correct_bias, save_dir, query_encoder_save_dir, passage_encoder_save_dir)\u001b[0m\n\u001b[1;32m    446\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m    447\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel,\n\u001b[1;32m    448\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39moptimizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    455\u001b[0m     use_amp\u001b[38;5;241m=\u001b[39muse_amp,\n\u001b[1;32m    456\u001b[0m )\n\u001b[1;32m    458\u001b[0m \u001b[38;5;66;03m# 7. Let it grow! Watch the tracked metrics live on the public mlflow server: https://public-mlflow.deepset.ai\u001b[39;00m\n\u001b[0;32m--> 459\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39msave(Path(save_dir), lm1_name\u001b[38;5;241m=\u001b[39mquery_encoder_save_dir, lm2_name\u001b[38;5;241m=\u001b[39mpassage_encoder_save_dir)\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquery_tokenizer\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery_encoder_save_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Codes/thesis_v4/env/lib/python3.8/site-packages/haystack/modeling/training/base.py:289\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# Move batch of samples to device\u001b[39;00m\n\u001b[1;32m    288\u001b[0m batch \u001b[38;5;241m=\u001b[39m {key: batch[key]\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m batch}\n\u001b[0;32m--> 289\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;66;03m# Perform  evaluation\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluate_every \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    294\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluate_every \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlocal_rank \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    297\u001b[0m ):\n",
      "File \u001b[0;32m~/Codes/thesis_v4/env/lib/python3.8/site-packages/haystack/modeling/training/base.py:375\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, batch, step)\u001b[0m\n\u001b[1;32m    373\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbatch)\n\u001b[1;32m    374\u001b[0m per_sample_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mlogits_to_loss(logits\u001b[38;5;241m=\u001b[39mlogits, global_step\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mglobal_step, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbatch)\n\u001b[0;32m--> 375\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward_propagate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mper_sample_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Codes/thesis_v4/env/lib/python3.8/site-packages/haystack/modeling/training/base.py:396\u001b[0m, in \u001b[0;36mTrainer.backward_propagate\u001b[0;34m(self, loss, step)\u001b[0m\n\u001b[1;32m    394\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    395\u001b[0m         torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_grad_norm)\n\u001b[0;32m--> 396\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_schedule:\n",
      "File \u001b[0;32m~/Codes/thesis_v4/env/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:65\u001b[0m, in \u001b[0;36m_LRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m instance\u001b[38;5;241m.\u001b[39m_step_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     64\u001b[0m wrapped \u001b[38;5;241m=\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(instance, \u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Codes/thesis_v4/env/lib/python3.8/site-packages/torch/optim/optimizer.py:88\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Codes/thesis_v4/env/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Codes/thesis_v4/env/lib/python3.8/site-packages/torch/optim/adamw.py:129\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    127\u001b[0m state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexp_avg\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(p, memory_format\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mpreserve_format)\n\u001b[1;32m    128\u001b[0m \u001b[38;5;66;03m# Exponential moving average of squared gradient values\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexp_avg_sq\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreserve_format\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m amsgrad:\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;66;03m# Maintains max of all exp. moving avg. of sq. grad. values\u001b[39;00m\n\u001b[1;32m    132\u001b[0m     state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_exp_avg_sq\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(p, memory_format\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mpreserve_format)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 7.79 GiB total capacity; 5.10 GiB already allocated; 33.88 MiB free; 5.34 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "retriever.train(\n",
    "    data_dir=doc_dir,\n",
    "    train_filename=train_filename,\n",
    "    dev_filename=dev_filename,\n",
    "    test_filename=dev_filename,\n",
    "    n_epochs=1,\n",
    "    batch_size=4,\n",
    "    grad_acc_steps=8,\n",
    "    save_dir=save_dir,\n",
    "    evaluate_every=3000,\n",
    "    embed_title=True,\n",
    "    num_positives=1,\n",
    "    num_hard_negatives=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e8adce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
