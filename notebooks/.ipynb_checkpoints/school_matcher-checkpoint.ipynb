{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5339c4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pdfplumber\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import unidecode\n",
    "import PyPDF2\n",
    "from tika import parser\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy import displacy\n",
    "from progress_bar.progress_bar import printProgressBar\n",
    "\n",
    "class School:\n",
    "    def __init__(self, file_source):\n",
    "        file = open(file_source, \"r\")\n",
    "        file = json.load(file)\n",
    "        temp_list = []\n",
    "        for facultad in file:\n",
    "            temp_list.append(facultad['escuela'])\n",
    "        #print(facultad['escuela'])\n",
    "        self.escuelas = [item for sublist in temp_list for item in sublist] # make the list flat\n",
    "        #have the escuelas with accents in the correct form here\n",
    "        self.escuelas_accent = self.escuelas\n",
    "        \n",
    "        print(self.escuelas)\n",
    "        self.i = 0\n",
    "        self.j = 0\n",
    "        self.k = 0\n",
    "        self.p = 0\n",
    "        \n",
    "    def create_dictionary(self, schools):\n",
    "        myDict = dict((e,i) for i,e in enumerate(schools))\n",
    "        return myDict\n",
    "\n",
    "    def unaccent_list(self, accent_list):\n",
    "        unaccented_schools = []\n",
    "        for sch in accent_list:\n",
    "            unaccented_schools.append(unidecode.unidecode(sch).lower())\n",
    "        return unaccented_schools\n",
    "    \n",
    "    def set_school_to_unaccent(self):\n",
    "        self.escuelas = self.unaccent_list(self.escuelas)\n",
    "        \n",
    "    def create_dicts(self):\n",
    "        #create the dicts only when schools are unaccented\n",
    "        self.escuelas_unaccent_dict = self.create_dictionary(self.escuelas)\n",
    "        self.escuelas_accent_dict = self.create_dictionary(self.escuelas_accent)\n",
    "        \n",
    "    def set_schools_accents(self, row, l):\n",
    "        self.k+= 1\n",
    "        printProgressBar(self.k, l, prefix = 'Progress:', suffix = 'Complete', length = 50)\n",
    "        \n",
    "        index = self.escuelas_unaccent_dict.get(row.lower())\n",
    "        key_list = list(self.escuelas_accent_dict.keys())\n",
    "        val_list = list(self.escuelas_accent_dict.values())\n",
    "        try:\n",
    "            position = val_list.index(index)\n",
    "            return key_list[position]\n",
    "        except:\n",
    "            return None\n",
    "        #return the value of the position, example in dict '{..., Escuela de enfermería: 37, ...}' it will return \n",
    "        #'Escuela de enfermería'\n",
    "    \n",
    "    def clean_spaces_text(self, text):\n",
    "        new_text = \" \".join(text.split())\n",
    "        return(new_text)\n",
    "    \n",
    "    def set_nlp(self, model):\n",
    "        self.nlp_model = spacy.load(model)\n",
    "        \n",
    "    def set_matcher(self):\n",
    "        self.matcher = PhraseMatcher(self.nlp_model.vocab, attr=\"LOWER\")\n",
    "        patterns = [self.nlp_model(name) for name in self.escuelas]\n",
    "        self.matcher.add(\"ESC\", patterns)\n",
    "        \n",
    "    def check_file_tika(self, file_source, l):\n",
    "        self.p+=1\n",
    "        printProgressBar(self.p, l, prefix = 'Progress:', suffix = 'Complete', length = 50)\n",
    "        pages_10 = []\n",
    "        pages_10_l = []\n",
    "        school_name_of_file = \"\"\n",
    "        parsed_pdf = parser.from_file(file_source)\n",
    "        data = parsed_pdf['content']\n",
    "        new_data = clean_spaces_text(data)\n",
    "        #print(new_data[0:90000])\n",
    "        new_data = unidecode.unidecode(new_data).lower()\n",
    "        doc = self.nlp_model(new_data)\n",
    "        for match_id, start, end in self.matcher(doc):\n",
    "            return(doc[start:end])\n",
    "        \n",
    "    def check_file(self, file_source, l):\n",
    "        self.i+=1\n",
    "        printProgressBar(self.i, l, prefix = 'Progress:', suffix = 'Complete', length = 50)\n",
    "        pages_10 = []\n",
    "        pages_10_l = []\n",
    "        school_name_of_file = \"\"\n",
    "        with pdfplumber.open(file_source) as pdf:\n",
    "            for i in range(0,10):\n",
    "                print(pdf.pages[i].extract_text())\n",
    "                pages_10.append(self.clean_spaces_text(pdf.pages[i].extract_text()))\n",
    "            for i in reversed(range(len(pdf.pages)-10,len(pdf.pages))):\n",
    "                pages_10_l.append(self.clean_spaces_text(pdf.pages[i].extract_text()))\n",
    "            #first 10 pages\n",
    "            pages_10_u = self.unaccent_list(pages_10)\n",
    "            for page in pages_10_u:\n",
    "                doc = self.nlp_model(page)\n",
    "                if len(self.matcher(doc)) >=1:\n",
    "                    for match_id, start, end in self.matcher(doc):\n",
    "                        return(doc[start:end]) #returns at the first instance\n",
    "            #last 10 pages\n",
    "            pages_10_l = self.unaccent_list(pages_10_l)\n",
    "            for page in pages_10_l:\n",
    "                doc = self.nlp_model(page)\n",
    "                if len(self.matcher(doc)) >=1:\n",
    "                    for match_id, start, end in self.matcher(doc):\n",
    "                        return(doc[start:end]) #returns at the first instance\n",
    "        return \"No school\"\n",
    "    def create_training_set(self, file_source, l):\n",
    "        self.j+=1\n",
    "        printProgressBar(self.j, l, prefix = 'Progress:', suffix = 'Complete', length = 50)\n",
    "        pages_10 = []\n",
    "        pages_10_l = []\n",
    "        school_name_of_file = \"\"\n",
    "        with pdfplumber.open(file_source) as pdf:\n",
    "            for i in range(0,10):\n",
    "                pages_10.append(self.clean_spaces_text(pdf.pages[i].extract_text()))\n",
    "            for i in reversed(range(len(pdf.pages)-10,len(pdf.pages))):\n",
    "                pages_10_l.append(self.clean_spaces_text(pdf.pages[i].extract_text()))\n",
    "        \n",
    "        #first 10 pages\n",
    "        pages_10_u = self.unaccent_list(pages_10)\n",
    "        for page in pages_10_u:\n",
    "            doc = self.nlp_model(page)\n",
    "            if len(self.matcher(doc)) >=1:\n",
    "                for match_id, start, end in self.matcher(doc):\n",
    "                    aux_training = [page, {\"entities\": (start, end, 'ESC')}]\n",
    "                    return aux_training #returns at the first instance\n",
    "        #last 10 pages\n",
    "        pages_10_l = self.unaccent_list(pages_10_l)\n",
    "        for page in pages_10_l:\n",
    "            doc = self.nlp_model(page)\n",
    "            if len(self.matcher(doc)) >=1:\n",
    "                for match_id, start, end in self.matcher(doc):\n",
    "                    aux_training = [page, {\"entities\": (start, end, 'ESC')}]\n",
    "                    return aux_training #returns at the first instance\n",
    "        return \"No school\"\n",
    "    def print_training_set(self):\n",
    "        print(self.training_set)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720c6d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.prefer_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618a6d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_spaces_text(text):\n",
    "        new_text = \" \".join(text.split())\n",
    "        return(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98031cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "schools = School(\"data/escuelas.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328391d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete accents\n",
    "schools.set_school_to_unaccent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2274bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model set matcher for schools\n",
    "schools.set_nlp('es_core_news_sm')\n",
    "schools.set_matcher()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135bd71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "## get dataframe with text only thesis\n",
    "csv_source = \"data/url_thesis_8211_with_pdf_scan_image.csv\"\n",
    "df = pd.read_csv(csv_source)\n",
    "df = df[df['isScan']==False]\n",
    "df = df.sort_values('isScan', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c355978",
   "metadata": {},
   "outputs": [],
   "source": [
    "#size of data frame\n",
    "l = len(df.index)\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec270b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec902823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# schools.check_file_tika('../'+'thesis_pdf_all/762.pdf',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d851430c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parsed_pdf = parser.from_file('../'+'thesis_pdf_all/200.pdf')\n",
    "  \n",
    "# # saving content of pdf\n",
    "# # you can also bring text only, by parsed_pdf['text'] \n",
    "# # parsed_pdf['content'] returns string \n",
    "# data = parsed_pdf['content'] \n",
    "# #   \n",
    "# # Printing of content\n",
    "# print(type(data))\n",
    "# #print(data)\n",
    "# new_data = clean_spaces_text(data)\n",
    "# #print(new_data[0:90000])\n",
    "# new_data = unidecode.unidecode(new_data).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa3e704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc = self.nlp_model(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2098cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../'+'thesis_pdf_all/200.pdf', mode='rb') as f:\n",
    "#     reader = PyPDF2.PdfFileReader(f)\n",
    "#     page = reader.getPage(1)\n",
    "#     print(clean_spaces_text(page.extractText()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122bdd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# schools.check_file('../'+'thesis_pdf_all/200.pdf',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c75e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make this fucntion a vectorize so it can run in a data frame\n",
    "check_vec = np.vectorize(schools.check_file_tika)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364fe84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ignore this\n",
    "#schools.create_training_set(\"../thesis_pdf/1.pdf\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05dfade5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a list of values with the school column, the l is the total ammount of thesis to check\n",
    "values_c = check_vec(\"../\"+df[\"path\"],l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3fbe2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#append to dataframe the column of the matching school and create a csv\n",
    "df['school_simple'] = values_c.tolist()\n",
    "df.to_csv(\"./data/thesis_7801_with_school.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57660ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "## get dataframe with only schools tag thesis\n",
    "csv_source = \"./data/thesis_7801_with_school.csv\"\n",
    "df = pd.read_csv(csv_source)\n",
    "df = df[df['isScan']==False]\n",
    "df = df.sort_values('isScan', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c63741",
   "metadata": {},
   "outputs": [],
   "source": [
    "#size of data frame\n",
    "l = len(df.index)\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea69dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "schools.create_dicts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117d31a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "schools.escuelas_unaccent_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51086152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the correct data frame first then vectorize\n",
    "check_vec_accents = np.vectorize(schools.set_schools_accents)\n",
    "schools.create_dicts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee321b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['school_simple']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df12e811",
   "metadata": {},
   "outputs": [],
   "source": [
    "schools.create_dicts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bf5626",
   "metadata": {},
   "outputs": [],
   "source": [
    "values_accent = check_vec_accents(df['school_simple'],l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d51543",
   "metadata": {},
   "outputs": [],
   "source": [
    "values_accent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679533ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#append to dataframe the column of the matching school and create a csv with the correct name\n",
    "df['school_complex'] = values_accent.tolist()\n",
    "df.to_csv(\"./data/thesis_7801_with_resumen_school_complex.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad639be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
