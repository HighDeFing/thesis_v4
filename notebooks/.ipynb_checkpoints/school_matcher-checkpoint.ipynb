{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ca68aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pdfplumber\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import unidecode\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy import displacy\n",
    "from progress_bar.progress_bar import printProgressBar\n",
    "\n",
    "class School:\n",
    "    def __init__(self, file_source):\n",
    "        file = open(file_source, \"r\")\n",
    "        file = json.load(file)\n",
    "        temp_list = []\n",
    "        for facultad in file:\n",
    "            temp_list.append(facultad['escuela'])\n",
    "        #print(facultad['escuela'])\n",
    "        self.escuelas = [item for sublist in temp_list for item in sublist] # make the list flat\n",
    "        print(self.escuelas)\n",
    "        self.i = 0\n",
    "        self.j = 0\n",
    "        \n",
    "    def unaccent_list(self, accent_list):\n",
    "        unaccented_schools = []\n",
    "        for sch in accent_list:\n",
    "            unaccented_schools.append(unidecode.unidecode(sch))\n",
    "        return unaccented_schools\n",
    "    \n",
    "    def set_school_to_unaccent(self):\n",
    "        self.escuelas = self.unaccent_list(self.escuelas)\n",
    "    \n",
    "    def clean_spaces_text(self, text):\n",
    "        new_text = \" \".join(text.split())\n",
    "        return(new_text)\n",
    "    \n",
    "    def set_nlp(self, model):\n",
    "        self.nlp_model = spacy.load(model)\n",
    "        \n",
    "    def set_matcher(self):\n",
    "        matcher = PhraseMatcher(self.nlp_model.vocab, attr=\"LOWER\")\n",
    "        patterns = [self.nlp_model(name) for name in self.escuelas]\n",
    "        matcher.add(\"ESC\", patterns)\n",
    "        \n",
    "    def check_file(self, file_source, l):\n",
    "        self.i+=1\n",
    "        printProgressBar(self.i, l, prefix = 'Progress:', suffix = 'Complete', length = 50)\n",
    "        pages_10 = []\n",
    "        pages_10_l = []\n",
    "        school_name_of_file = \"\"\n",
    "        with pdfplumber.open(file_source) as pdf:\n",
    "            for i in range(0,10):\n",
    "                pages_10.append(self.clean_spaces_text(pdf.pages[i].extract_text()))\n",
    "            for i in reversed(range(len(pdf.pages)-10,len(pdf.pages))):\n",
    "                pages_10_l.append(self.clean_spaces_text(pdf.pages[i].extract_text()))\n",
    "        \n",
    "        #first 10 pages\n",
    "        pages_10_u = self.unaccent_list(pages_10)\n",
    "        for page in pages_10_u:\n",
    "            doc = self.nlp_model(page)\n",
    "            if len(matcher(doc)) >=1:\n",
    "                for match_id, start, end in matcher(doc):\n",
    "                    return(doc[start:end]) #returns at the first instance\n",
    "        #last 10 pages\n",
    "        pages_10_l = self.unaccent_list(pages_10_l)\n",
    "        for page in pages_10_l:\n",
    "            doc = self.nlp_model(page)\n",
    "            if len(matcher(doc)) >=1:\n",
    "                for match_id, start, end in matcher(doc):\n",
    "                    return(doc[start:end]) #returns at the first instance\n",
    "        return \"No school\"\n",
    "    def create_training_set(self, file_source, l):\n",
    "        self.j+=1\n",
    "        printProgressBar(self.i, l, prefix = 'Progress:', suffix = 'Complete', length = 50)\n",
    "        self.training_set = []\n",
    "        school_name_of_file = \"\"\n",
    "        with pdfplumber.open(file_source) as pdf:\n",
    "            for i in range(0,10):\n",
    "                pages_10.append(self.clean_spaces_text(pdf.pages[i].extract_text()))\n",
    "            for i in reversed(range(len(pdf.pages)-10,len(pdf.pages))):\n",
    "                pages_10_l.append(self.clean_spaces_text(pdf.pages[i].extract_text()))\n",
    "        \n",
    "        #first 10 pages\n",
    "        pages_10_u = self.unaccent_list(pages_10)\n",
    "        for page in pages_10_u:\n",
    "            doc = self.nlp_model(page)\n",
    "            if len(matcher(doc)) >=1:\n",
    "                for match_id, start, end in matcher(doc):\n",
    "                    aux_training = [page, {\"entities\": [(start, end, 'ESC')]}]\n",
    "                    self.training_set.append(aux_training)\n",
    "                    pass #returns at the first instance\n",
    "        #last 10 pages\n",
    "        pages_10_l = self.unaccent_list(pages_10_l)\n",
    "        for page in pages_10_l:\n",
    "            doc = self.nlp_model(page)\n",
    "            if len(matcher(doc)) >=1:\n",
    "                for match_id, start, end in matcher(doc):\n",
    "                    aux_training = [page, {\"entities\": [(start, end, 'ESC')]}]\n",
    "                    self.training_set.append(aux_training)\n",
    "                    pass #returns at the first instance\n",
    "        return \"No school\"\n",
    "    def print_training_set(self):\n",
    "        print()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fc8089",
   "metadata": {},
   "outputs": [],
   "source": [
    "schools = School(\"data/escuelas.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b60769",
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete accents\n",
    "schools.set_school_to_unaccent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a92330",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model set matcher for schools\n",
    "schools.set_nlp('es_core_news_sm')\n",
    "schools.set_matcher()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254af00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "## get dataframe with text only thesis\n",
    "csv_source = \"data/url_thesis_200_with_scan.csv\"\n",
    "df = pd.read_csv(csv_source)\n",
    "df = df[df['isScan']==False]\n",
    "df = df.sort_values('isScan', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd41515d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#size of data frame\n",
    "l = len(df.index)\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63453267",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make this fucntion a vectorize so it can run in a data frame\n",
    "check_vec = np.vectorize(schools.check_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
