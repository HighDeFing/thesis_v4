Universidad Central de Venezuela Facultad de Ciencias Escuela de ComputaciÃ³n Centro de ComputaciÃ³n GrÃ¡fica Procesamiento Digital de Audio en GPU con CUDA Trabajo Especial de Grado presentado ante la Ilustre Universidad Central De Venezuela por el bachiller Alex Alberto PÃ©rez San ElÃ­as para optar al tÃ­tulo de Licenciado en ComputaciÃ³n Tutor Ernesto Coto Caracas, 28 de Octubre de 2011 Universidad Central de Venezuela Facultad de Ciencias Escuela de ComputaciÃ³n ACTA DEL VEREDICTO Quienes suscriben, miembros del jurado designado por el Consejo de la Escuela de ComputaciÃ³n para examinar el Trabajo Especial de Grado, presentado por el Bachiller Alex Alberto PÃ©rez San ElÃ­as, C.I. 16.461.267, con el tÃ­tulo â€œProcesamiento Digital de Audio en GPU con CUDAâ€, a los fines de cumplir con el requisito legal para optar al tÃ­tulo de Licenciado en ComputaciÃ³n, dejan constancia de lo siguiente: LeÃ­do el trabajo por cada uno de los miembros del jurado, se fijÃ³ el dÃ­a 28 de Octubre de 2011, a las 11:00 a.m., para que su autor lo defendiera en forma pÃºblica, en el Centro de ComputaciÃ³n GrÃ¡fica, lo cual este realizo mediante una exposiciÃ³n oral de su contenido, y luego respondiÃ³ satisfactoriamente a las preguntas que les fueron formuladas por el jurado, todo ello conforme a lo dispuesto en la Ley de Universidades y demÃ¡s normativas vigentes de la Universidad Central de Venezuela. Finalizada la defensa pÃºblica del Trabajo Especial de Grado, el jurado decidiÃ³ APROBARLO. En fe de lo cual se levanta la presente acta, en Caracas el 28 de Octubre de 2011, dejÃ¡ndose tambiÃ©n constancia de que actuÃ³ como coordinador del jurado el Profesor, Ernesto Coto, tutor del trabajo. ___________________________ Prof. Ernesto Coto (Tutor) _____________________________ ___________________________ Prof. Esmitt RamÃ­rez Prof. Robinson Rivas (Jurado Principal) (Jurado Principal) II Resumen Muchos investigadores se han interesado en explorar el gran poder computacional de las Ãºltimas Unidades de Procesamiento GrÃ¡fico (GPU â€“ Graphics Processing Units) en aplicaciones fuera del dominio de los grÃ¡ficos. Esta tendencia al desarrollo del GPU de propÃ³sito general (GPGPU â€“ General Purpose on Graphics Processing Units) se ha intensificado con el lanzamiento de diversos APIs (Application Programming Interfaces) para GPU entre los que se destaca el lenguaje CUDA (Compute Unified Device Architecture) de la empresa NVIDIA. Gracias a estas herramientas de desarrollo el GPU ha sido ampliamente utilizado para resolver problemas de diversas Ã­ndoles, como los de procesamiento de seÃ±ales, que en muchos casos demandan una importante carga de recursos computacionales. En la producciÃ³n musical, son empleados cada vez mÃ¡s algoritmos de gran demanda computacional. Para satisfacer esta demanda de potencia de cÃ¡lculo, los usuarios adquieren hardware de procesamiento de audio sumamente costoso. En este Trabajo Especial de Grado se muestra una alternativa accesible y portable usando el GPU para el procesamiento digital de audio. Este trabajo presenta un mÃ©todo para procesar seÃ±ales digitales de audio utilizando el GPU. Este enfoque explota el paralelismo de los multiprocesadores de las tarjetas de video para alcanzar un mejor rendimiento que en las implementaciones basadas en CPU. La efectividad de este enfoque es demostrada con la implementaciÃ³n en tiempo real y de forma paralela en el GPU, de algunos efectos de audio clÃ¡sicos que son comÃºnmente utilizados en un entorno de producciÃ³n musical. Se analizan dichos algoritmos tanto en CPU como en la Unidad de Procesamiento GrÃ¡fico (GPU), comparando en quÃ© casos es mejor realizar los cÃ¡lculos en el GPU en lugar de en el CPU. Palabras Clave: GPGPU, CUDA, procesamiento digital de audio, efectos de audio, procesamiento de audio en tiempo real. III Tabla de Contenido PÃ¡g. Ãndice de Figuras ...........................................................................................................VI IntroducciÃ³nâ€¦ ............................................................................................................. VIII CapÃ­tulo 1. Planteamiento del TEG ........................................................................... 1 1.1. Planteamiento del problema .................................................................................... 1 1.2. SoluciÃ³n propuesta ................................................................................................... 1 1.3. Objetivo general ........................................................................................................ 1 1.4. Objetivos especÃ­ficos ................................................................................................. 2 1.5. Alcance y limitaciones de este trabajo ..................................................................... 2 CapÃ­tulo 2. Marco TeÃ³rico ........................................................................................... 3 2.1. SeÃ±ales ...................................................................................................................... 3 2.2. SeÃ±ales de audio ....................................................................................................... 4 2.2.1. Sinusoides ........................................................................................................... 5 2.2.2. Exponenciales ..................................................................................................... 6 2.2.3. RelaciÃ³n entre sinusoides y exponenciales ....................................................... 8 2.3. SeÃ±ales analÃ³gicas y digitales ................................................................................ 10 2.3.1. Teorema de muestreo de Nyquist .................................................................... 11 2.4. Series de Fourier..................................................................................................... 12 2.4.1. Transformada de Fourier ................................................................................ 14 2.4.2. Transformada de Fourier discreta .................................................................. 15 2.4.3. Transformada rÃ¡pida de Fourier ..................................................................... 17 2.5. Respuesta al impulso y respuesta de frecuencia ................................................... 20 2.6. ConvoluciÃ³n ............................................................................................................. 21 2.6.1. Teorema de ConvoluciÃ³n .................................................................................. 23 2.7. Filtros digitales ....................................................................................................... 24 2.7.1. EcuaciÃ³n de diferencias ................................................................................... 25 2.7.2. Filtros FIR ........................................................................................................ 26 IV 2.8. Algoritmos de procesamiento digital de audio ...................................................... 29 2.8.1. Over Drive ........................................................................................................ 30 2.8.2. Distortion .......................................................................................................... 30 2.8.3. Ecualizador ....................................................................................................... 30 2.8.4. Vibrato .............................................................................................................. 32 2.8.5. Chorus .............................................................................................................. 32 2.8.6. Ring Modulator ................................................................................................ 32 2.8.7. Tremolo ............................................................................................................. 33 2.8.8. Auto Panner ..................................................................................................... 33 2.8.9. Delay ................................................................................................................. 33 2.9. ComputaciÃ³n paralela en GPU .............................................................................. 34 2.9.1. GPGPU ............................................................................................................. 34 2.9.2. CUDA ................................................................................................................ 38 CapÃ­tulo 3. DiseÃ±o e ImplementaciÃ³n ..................................................................... 46 3.1. Detalles de implementaciÃ³n ................................................................................... 46 3.2. ImplementaciÃ³n general del plugin VST ............................................................... 46 3.3. Estructura de clases del plugin VST ..................................................................... 49 3.3. ImplementaciÃ³n del GUI ........................................................................................ 55 3.4. ImplementaciÃ³n de los efectos de audio ................................................................ 57 3.4.1. Over Drive ........................................................................................................ 57 3.4.2. Distortion .......................................................................................................... 60 3.4.3. Ecualizador ....................................................................................................... 63 3.4.4. Vibrato .............................................................................................................. 66 3.4.5. Chorus .............................................................................................................. 68 3.4.6. Ring Modulator ................................................................................................ 71 3.4.7. Tremolo ............................................................................................................. 73 3.4.8. Auto Panner ..................................................................................................... 76 3.4.9. Delay ................................................................................................................. 79 V CapÃ­tulo 4. Pruebas y Resultados ............................................................................ 82 4.1. DescripciÃ³n del ambiente de pruebas .................................................................... 82 4.2. MediciÃ³n de Tiempo ................................................................................................ 83 4.3. Resultados ............................................................................................................... 84 4.3.1. Over Drive ........................................................................................................ 85 4.3.2. Distortion .......................................................................................................... 86 4.3.3. Ecualizador ....................................................................................................... 87 4.3.4. Vibrato .............................................................................................................. 88 4.3.5. Chorus .............................................................................................................. 89 4.3.6. Ring Modulator ................................................................................................ 90 4.3.7. Tremolo ............................................................................................................. 91 4.3.8. Auto Panner ..................................................................................................... 92 4.3.9. Delay ................................................................................................................. 93 4.3.10. Tiempo de ejecuciÃ³n promedio de un solo efecto .......................................... 94 4.3.11. Nueve efectos de audio simultÃ¡neos ............................................................. 95 CapÃ­tulo 5. Conclusiones y Trabajos Futuros ....................................................... 96 Referencias BibliogrÃ¡ficas .......................................................................................... 98 Glosarioâ€¦â€¦... ................................................................................................................ 100 VI Ãndice de Figuras Figura 2.1: SeÃ±ales y Sistemas ........................................................................................... 3 Figura 2.2: RepresentaciÃ³n de una seÃ±al de audio ............................................................ 5 Figura 2.3: FunciÃ³n exponencial cuando A = 1 y Ï„ = 1 .................................................... 7 Figura 2.4: El plano complejo para la forma cartesiana ................................................... 9 Figura 2.5: ConversiÃ³n de una seÃ±al analÃ³gica a digital ................................................ 11 Figura 2.6: Suma de seÃ±ales armÃ³nicas ........................................................................... 13 Figura 2.7: Cambio de dominio con DFT ......................................................................... 16 Figura 2.8: DescomposiciÃ³n de las muestras de una seÃ±al ............................................. 18 Figura 2.9: SÃ­ntesis de espectros en frecuencia ............................................................... 19 Figura 2.10: MÃ©todo para obtener el espectro final ......................................................... 19 Figura 2.11: Respuesta al impulso ................................................................................... 20 Figura 2.12: Sistema Lineal con una respuesta al impulso h(n) .................................... 22 Figura 2.13: Un filtro como una caja negra ..................................................................... 24 Figura 2.14: Cambio que produce un filtro ...................................................................... 24 Figura 2.15: Operaciones bÃ¡sicas con seÃ±ales digitales .................................................. 26 Figura 2.16: Esquema de implementaciÃ³n de un filtro FIR ............................................ 27 Figura 2.17: Respuesta en frecuencia de algunos filtros ................................................ 31 Figura 2.18: EvoluciÃ³n de los GPU NVIDIA con respecto a los CPU Intel .................... 34 Figura 2.19: Modelo de programaciÃ³n heterogÃ©nea con CUDA ...................................... 38 Figura 2.20: Flujo de la ejecuciÃ³n de un programa hecho en CUDA .............................. 39 Figura 2.21: Tipos de agrupaciÃ³n de hilos con CUDA ..................................................... 40 Figura 2.22: Pasos de compilaciÃ³n del cÃ³digo CUDA ...................................................... 41 Figura 2.23: Manejo de memoria entre el CPU (host) y el GPU (device) ....................... 42 Figura 2.24: Tipos de acceso a memoria por parte de los hilos de CUDA ...................... 43 Figura 3.1: Software Cakewalk SONAR 8.5 (host) con un plugin VST activo ............... 47 Figura 3.2: Diagrama de clases de la clase GPUAFXProcessor ...................................... 49 Figura 3.3: Diagrama de clases de la clase Thread ......................................................... 50 Figura 3.4: Diagrama de clases de la clase CUDA .......................................................... 51 Figura 3.5: Diagrama de clases de la clase KnobParameter ........................................... 52 Figura 3.6: Diagrama de clases de la clase GPUAFXBaseController ............................. 52 Figura 3.7: Diagrama de clases de la clase GPUAFXController ..................................... 53 Figura 3.8: Diagrama de clases del proyecto GPUAFX................................................... 54 Figura 3.9: Contenedor de efectos del plugin GPUAFX .................................................. 56 Figura 3.10: Diagrama de bloques del efecto Over Drive ................................................ 57 Figura 3.11: Interfaz grÃ¡fica del efecto Over Drive ......................................................... 58 Figura 3.12: FunciÃ³n de transferencia del efecto Over Drive ......................................... 59 Figura 3.13: Diagrama de bloques del efecto Distortion ................................................. 60 Figura 3.14: Interfaz grÃ¡fica del efecto Distortion .......................................................... 60 VII Figura 3.15: FunciÃ³n de transferencia del efecto Distortion ........................................... 62 Figura 3.16: Diagrama de bloques del Ecualizador ......................................................... 63 Figura 3.17: Interfaz grÃ¡fica del efecto Ecualizador ....................................................... 64 Figura 3.18: Diagrama de bloques del efecto Vibrato ..................................................... 66 Figura 3.19: Interfaz grÃ¡fica del efecto Vibrato ............................................................... 66 Figura 3.20: Diagrama de bloques del efecto Chorus ...................................................... 68 Figura 3.21: Interfaz grÃ¡fica del efecto Chorus ............................................................... 69 Figura 3.22: Diagrama de bloques del efecto Ring Modulator ....................................... 71 Figura 3.23: Interfaz grÃ¡fica del efecto Ring Modulator ................................................. 71 Figura 3.24: Diagrama de bloques del efecto Tremolo .................................................... 73 Figura 3.25: Interfaz grÃ¡fica del efecto Tremolo ............................................................. 74 Figura 3.26: Una onda sinusoidal modificada por la funciÃ³n tanh(x) ............................ 75 Figura 3.27: Diagrama de bloques del efecto Auto Panner ............................................. 76 Figura 3.28: Interfaz grÃ¡fica del efecto Auto Panner ...................................................... 77 Figura 3.29: Diagrama de bloques del efecto Delay ........................................................ 79 Figura 3.30: Interfaz grÃ¡fica del efecto Delay .................................................................. 80 Figura 4.1: Fragmento de cÃ³digo que mide el tiempo de un algoritmo en CPU ............ 83 Figura 4.2: Fragmento de cÃ³digo que mide el tiempo de un algoritmo en GPU ............ 84 Figura 4.3: ComparaciÃ³n de rendimiento GPU vs. CPU del efecto Over Drive ............. 85 Figura 4.4: ComparaciÃ³n de rendimiento GPU vs. CPU del efecto Distortion .............. 86 Figura 4.5: ComparaciÃ³n de rendimiento GPU vs. CPU del efecto Ecualizador ........... 87 Figura 4.6: ComparaciÃ³n de rendimiento GPU vs. CPU del efecto Vibrato ................... 88 Figura 4.7: ComparaciÃ³n de rendimiento GPU vs. CPU del efecto Chorus ................... 89 Figura 4.8: ComparaciÃ³n de rendimiento GPU vs. CPU del efecto Ring Modulator ..... 90 Figura 4.9: ComparaciÃ³n de rendimiento GPU vs. CPU del efecto Tremolo ................. 91 Figura 4.10: ComparaciÃ³n de rendimiento GPU vs. CPU del efecto Auto Panner ........ 92 Figura 4.11: ComparaciÃ³n de rendimiento GPU vs. CPU del efecto Delay .................... 93 Figura 4.12: ComparaciÃ³n de rendimiento promedio de un efecto en GPU y CPU ....... 94 Figura 4.13: ComparaciÃ³n de rendimiento GPU vs. CPU de 9 efectos simultÃ¡neos ..... 95 VIII IntroducciÃ³n Actualmente en la producciÃ³n musical la mayorÃ­a del audio es manipulado de forma digital, grabado y organizado por pistas. En estas grabaciones el procesamiento digital es aplicado para mejorar el sonido final, ya sea resaltando o atenuando frecuencias, aplicando efectos especiales o eliminando ruidos, entre otros. En los Ãºltimos aÃ±os, el acelerado desarrollo de las tarjetas grÃ¡ficas y sus bajos costos han abierto las puertas del poder de la computaciÃ³n paralela a los consumidores. Igualmente, los programadores han prestado mayor atenciÃ³n a la programabilidad computacional de propÃ³sito general que los procesadores grÃ¡ficos ofrecen desde la reciente apariciÃ³n de lenguajes de programaciÃ³n para GPU (Graphics Processing Units) como CUDA (Compute Unified Device Architecture) y OpenCL (Open Computing Language), Estas condiciones han hecho que este recurso sea utilizado de forma muy novedosa. Trabajos recientes han explotado el paralelismo de la computaciÃ³n de propÃ³sito general en los procesadores grÃ¡ficos, denominado GPGPU (General Purpose computation on Graphics Processing Units). Se han creado aplicaciones de criptografÃ­a, simulaciones fÃ­sicas y procesamiento de seÃ±ales, entre otros. El interÃ©s en el aprovechamiento del GPU (Graphics Processing Unit) ha crecido a raÃ­z de la creaciÃ³n de lenguajes de alto nivel para la programaciÃ³n en hardware grÃ¡fico. Un campo reciente del procesamiento en hardware grÃ¡fico de propÃ³sito general es el procesamiento digital de audio, que ha demostrado en trabajos anteriores [1] [2] [3] [4] ser una herramienta Ãºtil para utilizar el GPU como procesador secundario en ciertas tareas relacionadas con el procesamiento de audio y en algunos casos se ha demostrado la capacidad del GPU de superar al CPU en algunos de estos algoritmos, ademÃ¡s de tener a disposiciÃ³n librerÃ­as de FFT (Fast Fourier Transform) implementadas para GPU que son muy utilizadas en el Ã¡mbito de procesamiento digital de seÃ±ales. Este Trabajo Especial de Grado presenta y desarrolla una novedosa forma de procesamiento digital de audio en GPU como alternativa al procesamiento en CPU, que es el comÃºnmente utilizado para los cÃ¡lculos de los algoritmos de procesamiento de audio. IX Este trabajo estÃ¡ dividido en cuatro capÃ­tulos principales. El primer CapÃ­tulo describe el Planteamiento del Trabajo Especial de Grado, se describe el planteamiento general del problema, su posible soluciÃ³n y los objetivos a realizar en este Trabajo Especial de Grado. El CapÃ­tulo 2 contiene los fundamentos necesarios para entender la teorÃ­a del procesamiento digital de audio asÃ­ como los conceptos bÃ¡sicos de la computaciÃ³n paralela en GPU con CUDA. En el CapÃ­tulo 3 se encuentran los detalles de diseÃ±o e implementaciÃ³n tanto de los algoritmos de audio en GPU y CPU como de la implementaciÃ³n de la interfaz grÃ¡fica para manipular los parÃ¡metros de los efectos de audio. El CapÃ­tulo 4 contiene el anÃ¡lisis y los resultados de las pruebas de rendimiento de los algoritmos implementados descritos en el capÃ­tulo de diseÃ±o e implementaciÃ³n, asÃ­ como se muestran comparativas de velocidad de cÃ¡lculo de cada procesador. Finalmente, en el CapÃ­tulo 5, se presentan las conclusiones y posibles trabajos futuros de esta investigaciÃ³n. 1 CapÃ­tulo 1. Planteamiento del TEG 1.1. Planteamiento del problema Los cÃ¡lculos relacionados con el procesamiento digital de audio generalmente recaen completamente sobre el CPU, lo que a menudo implica que el rendimiento se degrade al utilizar numerosos efectos de audio al mismo tiempo. Para expandir el potencial computacional, permitiendo utilizar mÃ¡s filtros y efectos sobre mÃ¡s pistas de audio de forma concurrente, algunas empresas han creado procesadores dedicados al procesamiento digital de audio, pero son sumamente costosos para el usuario no profesional. Con el poder de cÃ³mputo que actualmente tienen los procesadores grÃ¡ficos, su bajo costo y el alto nivel de programabilidad que se ha logrado en los Ãºltimos aÃ±os se puede crear una soluciÃ³n al problema de la sobrecarga al CPU implementando en el GPU los algoritmos de efectos de audio mÃ¡s utilizados, logrando mejoras en rendimiento, velocidad de cÃ¡lculo y una posible alternativa a los procesadores de audio profesionales dedicados. 1.2. SoluciÃ³n propuesta La soluciÃ³n que se propone estarÃ¡ basada en un plugin VST (Virtual Studio Technology) que contendrÃ¡ un conjunto de efectos de audio procesados en el GPU y contarÃ¡ con una interfaz grÃ¡fica de usuario con controles bÃ¡sicos para la manipulaciÃ³n de los parÃ¡metros de cada efecto de audio. 1.3. Objetivo general Adaptar una serie de algoritmos de procesamiento digital de audio en GPU utilizando CUDA para reducir la carga al CPU cuando se utilizan numerosos plugins de audio en las aplicaciones de ediciÃ³n de audio. 2 1.4. Objetivos especÃ­ficos ï‚§ Implementar una serie de algoritmos de audio en el GPU con CUDA. ï‚§ Crear un plugin VST compatible con la mayorÃ­a de las aplicaciones DAW que existen para el sistema operativo Windows. ï‚§ Hacer procesamiento en tiempo real del audio. ï‚§ Crear una interfaz grÃ¡fica de usuario para la manipulaciÃ³n de los efectos de audio. ï‚§ Hacer pruebas de rendimiento para hacer comparativas GPU vs. CPU. 1.5. Alcance y limitaciones de este trabajo ï‚§ Este trabajo estÃ¡ enfocado en el desarrollo de un plugin de audio VST, aplicando la teorÃ­a bÃ¡sica de procesamiento digital de seÃ±ales en el contexto de programaciÃ³n de audio, aprovechando la capacidad de cÃ¡lculo del GPU para realizar las operaciones involucradas en cada algoritmo de los efectos de audio implementados. ï‚§ Las implementaciones para GPU de los algoritmos serÃ¡n desarrolladas con el lenguaje de programaciÃ³n CUDA, y por lo tanto solo pueden ser ejecutadas en tarjetas de video NVIDIA. ï‚§ Las tarjetas de video NVIDIA deben de tener al menos Compute Capability 1.1, ya que se utiliza mapeo de memoria, caracterÃ­stica que no se encuentra en las tarjetas de video NVIDIA de menor capacidad. ï‚§ Durante las pruebas del prototipo a desarrollar se realizarÃ¡n todas las pruebas con una frecuencia de muestreo de 44.100 muestras por segundo y un tamaÃ±o de bloque de audio de 512 muestras. 3 CapÃ­tulo 2. Marco TeÃ³rico En este capÃ­tulo se presentan los conceptos bÃ¡sicos de la teorÃ­a de seÃ±ales y sistemas, los cuales son fundamentales para entender y estudiar como una seÃ±al de audio puede ser modificada de distintas maneras. TambiÃ©n se muestran los conceptos bÃ¡sicos de la computaciÃ³n paralela en GPU con el API de programaciÃ³n CUDA. 2.1. SeÃ±ales Una seÃ±al fundamentalmente es la representaciÃ³n de la variaciÃ³n de alguna magnitud fÃ­sica que se utiliza para transmitir algÃºn tipo de informaciÃ³n (ver Figura 2.1.a). Todos los tipos de seÃ±ales, sean Ã©stas naturales o artificiales, tienen caracterÃ­sticas comunes que hacen posible su estudio en forma independiente de su naturaleza. Por lo general, las seÃ±ales son procesadas o modificadas por sistemas (ver Figura 2.1.b). En el caso computacional, el computador genera o modifica una seÃ±al acÃºstica digitalizada, por lo tanto se comporta como un sistema. Figura 2.1: a) GrÃ¡fica de una seÃ±al arbitraria. b) RepresentaciÃ³n de un sistema bÃ¡sico [5]. a) b) 4 Una seÃ±al, en forma simplificada, se puede entender como cualquier mecanismo que es utilizado para transmitir informaciÃ³n. Algunos ejemplos de seÃ±ales son: ondas electromagnÃ©ticas enviadas por un radar, ondas de sonido viajando por el aire o las ondas de la actividad del cerebro captadas por un electrocardiograma. Desde un punto de vista matemÃ¡tico, una seÃ±al es una variable de una o mÃ¡s dimensiones que toma valores de acuerdo a otra variable, como por ejemplo el tiempo en el caso del sonido, o el espacio en el caso de imÃ¡genes. MatemÃ¡ticamente, si la dependencia es temporal o espacial carece de importancia, ya que en ambos casos las seÃ±ales se tratan de igual manera y sÃ³lo importa la funciÃ³n que modela su comportamiento [5]. 2.2. SeÃ±ales de audio El audio es un tipo de seÃ±al producida por una perturbaciÃ³n fÃ­sica que se propaga a travÃ©s de un medio (aire, agua, etc.) y que puede variar a lo largo del tiempo. Existen distintos tipos de ondas que difieren en su naturaleza, por ejemplo ondas de radio, de luz, de agua, electromagnÃ©ticas, de sonido o rayos X. TambiÃ©n difieren en su forma de propagaciÃ³n, la que puede ser longitudinal o transversal. En una onda longitudinal, el desplazamiento de las partÃ­culas es paralelo a la direcciÃ³n de propagaciÃ³n de la onda. En una onda transversal, el desplazamiento de las partÃ­culas es perpendicular a la direcciÃ³n de propagaciÃ³n de la onda. Las ondas de sonido son longitudinales. Cuando esta onda alcanza alguna superficie (como el tÃ­mpano del oÃ­do humano o la membrana de un micrÃ³fono), produce una vibraciÃ³n en dicha superficie por simpatÃ­a o resonancia. De esta forma, la energÃ­a acÃºstica es transferida desde la fuente al receptor manteniendo las caracterÃ­sticas de la vibraciÃ³n original. El sonido puede viajar a travÃ©s de objetos sÃ³lidos, lÃ­quidos o gases. Su velocidad es proporcional a la densidad del medio por el cual viaja. A temperatura ambiente, la velocidad del sonido en el aire es de 343 m/s y en el agua, es de 1493 m/s. La velocidad del sonido es independiente de su intensidad y su intensidad decae con la distancia en forma inversamente proporcional. 5 2.2.1. Sinusoides Una seÃ±al de audio puede ser descrita a travÃ©s de las seÃ±ales de tipo sinusoidal. La sinusoide es una de las seÃ±ales mÃ¡s simples e importantes que existen. Las dos funciones matemÃ¡ticas sinusoidales bÃ¡sicas son las funciones seno y coseno, las cuales estÃ¡n derivadas de las funciones trigonomÃ©tricas del mismo nombre. Las seÃ±ales de audio pueden describirse de acuerdo a algunos pocos parÃ¡metros (ver Figura 2.2). Los tres parÃ¡metros mÃ¡s comunes son: la amplitud, que es la altura de la seÃ±al; la frecuencia, que es el nÃºmero de oscilaciones por unidad de tiempo; y la fase de la onda, que es el desplazamiento con respecto al eje del tiempo. Existen algunos otros parÃ¡metros tales como perÃ­odo, espectro o intensidad que pueden derivarse de los parÃ¡metros mencionados anteriormente. Figura 2.2: RepresentaciÃ³n de una seÃ±al de audio y sus parÃ¡metros bÃ¡sicos (amplitud, fase y frecuencia). MatemÃ¡ticamente una funciÃ³n de este tipo se puede escribir como: ğ‘¦(ğ¹) = ï¿½ğ´ğ‘–(ğ¹) sin[2ğœ‹ğ‘“ğ‘–(ğ¹)ğ¹ + ğœƒğ‘–(ğ¹)] ğ‘ ğ‘–=1 (ğ‘¬ğ’„. ğŸ.ğŸ) Donde ğ´ğ‘–(ğ¹), ğ‘“ğ‘–(ğ¹), y ğœƒğ‘–(ğ¹) son el conjunto de amplitudes, frecuencias y fases de las sinusoides, con respecto a un instante de tiempo ğ¹. 6 Las seÃ±ales de voz o mÃºsica, pueden ser descritas de forma bastante precisa como la suma de una gran cantidad de sinusoides de diferentes amplitudes. Otra razÃ³n de la importancia de las sinusoides es que constituyen funciones bÃ¡sicas de los sistemas lineales. Esto significa que cualquier sistema lineal puede ser estudiado a travÃ©s de su respuesta a funciones sinusoidales. En el campo del audio, las sinusoides son importantes para el anÃ¡lisis de filtros tales como reverberadores, ecualizadores y otros tipos de efectos. Desde un punto de vista matemÃ¡tico, las sinusoides constituyen bloques fundamentales que al ser combinados de cierta forma permiten la creaciÃ³n o sÃ­ntesis de cualquier tipo de seÃ±al, por muy compleja que sea. 2.2.2. Exponenciales Existe otra funciÃ³n matemÃ¡tica muy importante, conocida como exponencial. La forma canÃ³nica de una funciÃ³n exponencial es la siguiente: ğ‘“(ğ¹) = ğ´ğ¹âˆ’ğ‘¡/ğœ ğ¹ â‰¥ 0 (ğ‘¬ğ’„. ğŸ.ğŸ) ğ´ corresponde a la amplitud mÃ¡xima de la exponencial y ğœ se conoce como la constante de tiempo de la exponencial. La constante de tiempo es el tiempo que demora la exponencial en decaer 1/ğ¹, es decir: ğ‘“(ğœ) ğ‘“(0) = 1 ğ¹ (ğ‘¬ğ’„. ğŸ.ğŸ‘) La Figura 2.3 muestra el grÃ¡fico de una funciÃ³n exponencial para ğ´ = 1 ğ‘¦ ğœ = 1. En la EcuaciÃ³n 2.4 ğ¹ es el nÃºmero de Euler, el cual tiene el valor irracional aproximado de 2.71828 y constituye la base de los logaritmos naturales. Este nÃºmero es uno de los mÃ¡s importantes de la matemÃ¡tica y puede calcularse como: ğ¹ = ï¿½ 1 ğ‘›! âˆ ğ‘›=0 = lim ğ‘›â†’âˆ ï¿½1 + 1 ğ‘› ï¿½ ğ‘› â‰ˆ 2.71828 (ğ‘¬ğ’„. ğŸ.ğŸ’) 7 Figura 2.3: FunciÃ³n exponencial cuando ğ‘¨ = ğŸ y ğ‰ = ğŸ [5]. El decaimiento exponencial ocurre naturalmente cuando una cantidad estÃ¡ decayendo a una tasa proporcional a lo que aÃºn queda por decaer. En la naturaleza, todos los resonadores lineales, tales como las cuerdas de los instrumentos musicales y los instrumentos de vientos exhiben un decaimiento exponencial en su respuesta a una excitaciÃ³n momentÃ¡nea. La energÃ­a reverberante en una sala de conciertos decae exponencialmente una vez finalizada la emisiÃ³n de sonido. Esencialmente, todas las oscilaciones libres (sin una fuente mantenida en el tiempo) caen exponencialmente siempre que sean lineales e invariantes en el tiempo. Ejemplos de este tipo de oscilaciones incluyen la vibraciÃ³n de un diapasÃ³n, cuerdas pulsadas y barras de marimba o xilÃ³fonos. El crecimiento exponencial ocurre cuando una cantidad crece a una tasa proporcional al incremento actual. El crecimiento exponencial es inestable dado que nada puede crecer para siempre sin llegar a un cierto lÃ­mite. Es necesario notar que en la EcuaciÃ³n 2.2 una constante de tiempo positiva implica a un decaimiento exponencial mientras que una constante de tiempo negativa corresponde a un crecimiento exponencial. En sistemas de audio, un decaimiento de 1/ğ¹ se considera muy pequeÃ±o para aplicaciones prÃ¡cticas, como por ejemplo, para el diseÃ±o acÃºstico de salas de concierto. Por lo general, se utiliza una cantidad tal que asegure que la seÃ±al ha caÃ­do 60 decibelios (ğ‘‘ğ‘‘). Esta cantidad, denotada por ğ¹60, se encuentra resolviendo la ecuaciÃ³n: ğ‘“(ğ¹60) ğ‘“(0) = 10âˆ’60/20 = 0.001 (ğ‘¬ğ’„. ğŸ.ğŸ“) 8 Usando la definiciÃ³n de exponencial de la EcuaciÃ³n 2.2, se tiene entonces que: ğ¹60 = âˆ’ğ¹ğ‘›(0.001)ğœ â‰ˆ 6.91ğœ (ğ‘¬ğ’„. ğŸ.ğŸ”) Esta ecuaciÃ³n nos dice que la constante ğ¹60 es aproximadamente 7 veces la constante de tiempo ğœ. Esto puede verificarse en la Figura 2.3, donde se aprecia la ubicaciÃ³n de dicha cantidad en el eje del tiempo. 2.2.3. RelaciÃ³n entre sinusoides y exponenciales Existen dos relaciones muy importantes para el anÃ¡lisis de seÃ±ales y para el Teorema de Fourier, llamadas ecuaciones de Euler. Estas son: ğ¹ğ‘–ğ‘¥ = ğ‘“ğ‘œğ‘ (ğ‘¥) + ğ¹ ğ‘ ğ¹ğ‘›(ğ‘¥)ğ¹ (ğ‘¬ğ’„. ğŸ.ğŸ•) ğ¹âˆ’ğ‘–ğ‘¥ = ğ‘“ğ‘œğ‘ (ğ‘¥) âˆ’ ğ¹ ğ‘ ğ¹ğ‘›(ğ‘¥)ğ¹ (ğ‘¬ğ’„. ğŸ.ğŸ–) Esta ecuaciÃ³n nos dice que las exponenciales y las sinusoides estÃ¡n Ã­ntimamente relacionadas. En la EcuaciÃ³n 2.7, el nÃºmero ğ¹ representa al nÃºmero imaginario y que estÃ¡ definido como: ğ¹ = âˆšâˆ’1 (ğ‘¬ğ’„. ğŸ.ğŸ—) El nÃºmero imaginario ğ¹ tiene una importancia fundamental en el anÃ¡lisis de frecuencia de una seÃ±al, ya que tal y como se verÃ¡ mÃ¡s adelante, las sinusoides pueden representarse y manejarse en forma mÃ¡s compacta si se utilizan nÃºmeros complejos. Los nÃºmeros complejos estÃ¡n constituidos por un par ordenado de nÃºmeros, uno real y otro imaginario, y usualmente se grafican en lo que se denomina el plano complejo, mostrado en la Figura 2.4, donde el eje de las ordenadas representa los nÃºmeros reales y el eje de las abscisas los nÃºmeros imaginarios. En este plano un nÃºmero complejo es un vector que se puede representar de dos formas: cartesiana y polar. En la forma cartesiana, un nÃºmero complejo ğ‘ se representa como la suma de su parte real con su parte imaginaria o bien ğ‘ = ğ‘¥ + ğ¹ğ‘¦. Pero el mismo nÃºmero se puede representar mediante la longitud del vector y su Ã¡ngulo con respecto al eje de las ordenadas, lo que se denomina forma polar. 9 Figura 2.4: El plano complejo para la forma cartesiana. En este caso se tiene ğ‘ = ğ¹âˆ ğœƒ, donde ğ¹ es el mÃ³dulo o magnitud del nÃºmero complejo y que tambiÃ©n suele representarse como |ğ‘|. Ambas representaciones estÃ¡n relacionadas por las siguientes ecuaciones: ğ¹ = ï¿½ğ‘¥2 + ğ‘¦2 (ğ‘¬ğ’„. ğŸ.ğŸğŸ) ğœƒ = arctan ï¿½ ğ‘¦ ğ‘¥ ï¿½ (ğ‘¬ğ’„. ğŸ.ğŸğŸ) Las Ecuaciones 2.7 y 2.8 pueden utilizarse para demostrar que: ğ‘“ğ‘œğ‘ (ğ‘¥) = ğ¹ğ‘–ğ‘¥ + ğ¹âˆ’ğ‘–ğ‘¥ 2 (ğ‘¬ğ’„. ğŸ.ğŸğŸ) ğ‘ ğ¹ğ‘›(ğ‘¥) = ğ¹ğ‘–ğ‘¥ âˆ’ ğ¹âˆ’ğ‘–ğ‘¥ 2ğ¹ (ğ‘¬ğ’„. ğŸ.ğŸğŸ‘) Multiplicando la EcuaciÃ³n 2.7 por una amplitud ğ´ â‰¥ 0 y utilizando ğ‘¥ = ğ‘¤ğ¹ + ğœ™ se tiene: ğ´ğ¹ğ‘–(ğ‘¤ğ‘¡+ğœ™) = ğ´ğ‘“ğ‘œğ‘ (ğ‘¤ğ¹ + ğœ™) + ğ¹ğ‘‘ ğ‘ ğ¹ğ‘›(ğ‘¤ğ¹ + ğœ™) (ğ‘¬ğ’„. ğŸ.ğŸğŸ’) Esta ecuaciÃ³n describe una sinusoide compleja. Por lo tanto, una sinusoide compleja contiene una parte real coseno y una parte imaginaria seno. 10 De acuerdo a las Ecuaciones 2.12 y 2.13 y dado que ğ¹ğ‘–ğ‘¤ğ‘¡ corresponde a una sinusoide compleja (EcuaciÃ³n 2.14), se tiene entonces que toda sinusoide real estÃ¡ compuesta por una contribuciÃ³n equitativa de frecuencias positivas y negativas. Dicho de otra forma, una sinusoide real consiste de una suma de dos sinusoides complejas, una de frecuencia positiva y la otra de frecuencia negativa. Esto significa que el espectro de frecuencias de una sinusoide o de una funciÃ³n periÃ³dica compuesta por sinusoides es simÃ©trico respecto del origen y contiene tanto frecuencias negativas como positivas. En la prÃ³xima subsecciÃ³n se describe como una seÃ±al se define digitalmente y como puede ser procesada para mejorar o modificar las seÃ±ales de audio. Lo que se entiende como seÃ±al digital comparada con una seÃ±al analÃ³gica se explica tambiÃ©n en la siguiente subsecciÃ³n. 2.3. SeÃ±ales analÃ³gicas y digitales Las seÃ±ales analÃ³gicas como naturalmente ocurren en las seÃ±ales de audio, son seÃ±ales que varÃ­an en tiempo continuo. Definiremos tal seÃ±al como ğ‘¥(ğ¹). Ellas estÃ¡n definidas para cada valor de tiempo en un intervalo continuo. Las seÃ±ales en tiempo discreto estÃ¡n solamente definidas en ciertos valores de tiempo. Cuando se interpretan seÃ±ales en la computadora, se necesita que estÃ©n en forma digital. La seÃ±al analÃ³gica es convertida a digital a travÃ©s de un muestreo. Para digitalizar una seÃ±al analÃ³gica es necesario discretizarla. Es decir, tomar muestras a intervalos regulares de la seÃ±al analÃ³gica original. Este proceso se conoce como digitalizaciÃ³n o muestreo. La calidad de la seÃ±al discreta depende de la frecuencia a la cual se muestrea la seÃ±al original. Esto se ilustra en la Figura 2.5.a. donde cada flecha representa una medida de la muestra en un tiempo dado. La seÃ±al en consecuencia estÃ¡ en tiempo discreto. Una seÃ±al digital puede ser representada solamente en un conjunto limitado de valores, por lo tanto las muestras tomadas tienen que ser cuantizadas al valor mÃ¡s cercano que la pueda representar como se ve en la Figura 2.5.b. La frecuencia de muestreo se refiere al nÃºmero de muestras en una seÃ±al digital por segundo y la resoluciÃ³n se refiere al nÃºmero de niveles de cuantizaciÃ³n. Esto indica que una seÃ±al digital no solamente tiene un tiempo discreto sino un valor de muestra discreto tambiÃ©n. 11 a) Figura 2.5: a) Toma de muestras de una seÃ±al analÃ³gica. b) Su representaciÃ³n de forma digital a travÃ©s de la cuantizaciÃ³n [3]. Usualmente el muestreo ocurre en intervalos de tiempo fijos. Definiremos a la seÃ±al digital como ğ‘¥(ğ‘›) donde ğ‘› es el nÃºmero de Ã­ndice, empezando desde ğ‘› = 0. Teniendo la frecuencia adecuada de muestreo la seÃ±al analÃ³gica podrÃ¡ ser perfectamente representada en forma digital. Esta frecuencia adecuada de muestreo es llamada la frecuencia de Nyquist y es dada por el teorema de muestreo. La frecuencia de muestreo es dada como el nÃºmero de muestras por unidad de tiempo. 2.3.1. Teorema de muestreo de Nyquist Si la frecuencia mÃ¡s alta contenida en una seÃ±al analÃ³gica ğ‘¥ğ‘(ğ¹) es ğ‘“ğ‘šğ‘ğ‘¥ = ğ‘‘ y la seÃ±al es muestreada a la frecuencia ğ¹ğ‘  > 2 ğ‘“ğ‘šğ‘ğ‘¥ â‰¡ 2ğ‘‘, entonces ğ‘¥ğ‘(ğ¹) podrÃ¡ ser recuperada exactamente a partir de los valores de las muestras. Esto significa que la seÃ±al debe ser muestreada con una frecuencia de muestreo de al menos el doble de la frecuencia mÃ¡s alta contenida en la seÃ±al. Esta frecuencia de muestreo es lo que se conoce como la Frecuencia de Muestreo de Nyquist. Para manipular el sonido, la seÃ±al puede ser transformada del dominio del tiempo al dominio de la frecuencia y viceversa. Esto es de suma utilidad para poder manejar de forma mÃ¡s accesible parÃ¡metros de las seÃ±ales tales como la frecuencia o la fase. En la siguiente subsecciÃ³n se explicarÃ¡n los conceptos bÃ¡sicos relacionados a dicha transformaciÃ³n, denominada Transformada de Fourier. b) 12 2.4. Series de Fourier En el aÃ±o 1811, el matemÃ¡tico Jean Baptiste Fourier demostrÃ³ que cualquier seÃ±al periÃ³dica razonable puede expresarse como la suma de una o mÃ¡s sinusoides de distinta frecuencia, fase y amplitud. Se entiende por una seÃ±al razonable, una que posee valores mÃ¡ximos menores que infinito y un nÃºmero finito de saltos en un perÃ­odo. Estas sinusoides estÃ¡n armÃ³nicamente relacionadas entre sÃ­, es decir, sus frecuencias son mÃºltiplos enteros de una frecuencia fundamental. Esta suma ponderada de seÃ±ales sinusoidales se conoce como Serie de Fourier. La Serie de Fourier de una seÃ±al periÃ³dica ğ‘“(ğ‘¥) de perÃ­odo ğ¹0 puede escribirse como: ğ‘“(ğ‘¥) = ğ‘”0 + ï¿½(ğ‘”ğ‘˜ cos(ğ‘“ğ‘¤0ğ‘¥) + ğ‘“ğ‘˜ sin(ğ‘“ğ‘¤0ğ‘¥)) âˆ ğ‘˜=1 (ğ‘¬ğ’„. ğŸ.ğŸğŸ“) Donde ğ‘¤0 es la frecuencia fundamental (ğ‘¤0 = 2ğœ‹ / ğ¹0) y los coeficientes ğ‘”0, ğ‘”ğ‘˜ y ğ‘“ğ‘˜ constituyen un conjunto de coeficientes relacionados unÃ­vocamente con la funciÃ³n ğ‘“(ğ‘¥). Esta forma de escribir la Serie de Fourier se conoce como la forma trigonomÃ©trica. Utilizando las ecuaciones de Euler (Ecuaciones 2.7 y 2.8), se puede reescribir la Serie de Fourier como: ğ‘“(ğ‘¥) = ï¿½ ğ‘‚ğ‘˜ âˆ ğ‘˜=âˆ’âˆ ğ¹ğ‘–ğ‘˜ğ‘¤0ğ‘¥ (ğ‘¬ğ’„. ğŸ.ğŸğŸ”) Esta forma de escribir la ecuaciÃ³n se conoce como forma compleja. La EcuaciÃ³n 2.15 nos dice que para cualquier seÃ±al periÃ³dica ğ‘“(ğ‘¥) pueden encontrarse coeficientes ğ‘”0,ğ‘”1, ğ‘“1,ğ‘”2, ğ‘“2â€¦ tales que multiplicados por sinusoides de frecuencias ğ‘¤0, 2ğ‘¤0, 3ğ‘¤0â€¦ den como resultado la funciÃ³n ğ‘“(ğ‘¥) cuando estas sinusoides se suman. Entonces, toda funciÃ³n periÃ³dica de frecuencia ğ‘¤, cualquiera sea su naturaleza, estÃ¡ compuesta por la suma de varias sinusoides de frecuencias mayores o iguales a ğ‘¤, cada una de las cuales tiene distinta amplitud, frecuencia y fase. Las frecuencias involucradas estÃ¡n armÃ³nicamente relacionadas entre sÃ­. 13 Lo anterior implica dos cosas: 1. Si se toman varias sinusoides de distintas frecuencias, fases y amplitudes y se suman, se obtendrÃ¡ una seÃ±al periÃ³dica. 2. Dada una seÃ±al periÃ³dica ğ‘“(ğ‘¥) cualquiera, Ã©sta siempre podrÃ¡ descomponerse en sus componentes de frecuencia, mediante la determinaciÃ³n de las sinusoides que la conforman. El grÃ¡fico de las frecuencias de estas sinusoides con respecto a la amplitud de cada una de ellas, se conoce como espectro de frecuencias. En la Figura 2.6 se muestran varias sinusoides de frecuencias relacionadas y su suma. AllÃ­ puede observarse que al sumar estas sinusoides se obtiene una nueva forma de onda de frecuencia igual a la frecuencia de la onda fundamental. Esto nos permite comprobar de manera empÃ­rica el descubrimiento de Fourier. Figura 2.6: Suma de seÃ±ales armÃ³nicas [5]. 14 2.4.1. Transformada de Fourier Una forma de extender las series de Fourier a funciones no periÃ³dicas, es truncar las seÃ±ales en un cierto punto y suponer que la zona truncada se repite hasta el infinito desde ahÃ­ hacia adelante. Otra forma es suponer que Ã©stas poseen un perÃ­odo infinito y expandir un tanto las ecuaciones para poder trabajar con este tipo de seÃ±ales. Esto da pie a la Transformada de Fourier. La Transformada de Fourier es una generalizaciÃ³n de las Series de Fourier. En rigor, esta transformada se aplica a funciones continuas y aperiÃ³dicas, pero tambiÃ©n es posible aplicarla a funciones discretas mediante la utilizaciÃ³n de funciones impulso. La Transformada de Fourier es una herramienta clave para traducir seÃ±ales desde los dominios del tiempo a la frecuencia y viceversa. La Transformada de Fourier es una operaciÃ³n que transforma una funciÃ³n descomponiÃ©ndola en varias sinusoides de diferentes frecuencias y amplitudes. La funciÃ³n transformada es la representaciÃ³n en el dominio de la frecuencia de la funciÃ³n original. La Transformada de Fourier se define como: ğ¹(ğ‘¤) = ï¿½ ğ‘“(ğ‘¥)ğ¹âˆ’ğ‘–ğ‘¤ğ‘¥ ğ‘‘ğ‘¥ (ğ‘¬ğ’„. ğŸ.ğŸğŸ•) âˆ âˆ’âˆ Donde ğ‘¤ denota la frecuencia y ğ¹ a la transformada. Esta ecuaciÃ³n corresponde a integrar la funciÃ³n ğ‘“(ğ‘¥) multiplicada por una sinusoide compleja en todo el intervalo de la variable ğ‘¥ (desde âˆ’âˆ hasta âˆ). La transformada inversa, la que permite encontrar ğ‘“(ğ‘¥) cuando se conoce ğ¹(ğ‘¤), estÃ¡ dada por la ecuaciÃ³n: ğ‘“(ğ‘¥) = ï¿½ ğ¹(ğ‘¤)ğ¹ğ‘–ğ‘¤ğ‘¥ ğ‘‘ğ‘¥ (ğ‘¬ğ’„. ğŸ.ğŸğŸ–) âˆ âˆ’âˆ Utilizando estas relaciones, siempre es posible encontrar una correspondencia entre una funciÃ³n y su espectro o viceversa, es decir: ğ‘“(ğ‘¥) â†” ğ¹(ğ‘¤) (ğ‘¬ğ’„. ğŸ.ğŸğŸ—) 15 La Transformada de Fourier estÃ¡ definida para todas las funciones continuas, pero en nuestro caso solo serÃ¡ desarrollada para la forma discreta denominada tambiÃ©n Transformada Discreta de Fourier. Esta transformada nos permite conocer el espectro de frecuencias de cualquier seÃ±al ğ‘“(ğ‘¥), sea Ã©sta periÃ³dica o aperiÃ³dica y cualquiera sea su naturaleza. Es decir, dada una funciÃ³n ğ‘“(ğ‘¥) cualquiera, mediante esta transformada podemos saber que frecuencias estÃ¡n presentes en ella. 2.4.2. Transformada de Fourier discreta La Transformada de Fourier, tal como se presentÃ³ en la secciÃ³n anterior, nos permite encontrar transformadas para seÃ±ales continuas. Sin embargo, esto no es aplicable directamente a seÃ±ales discretas o digitales, que son las que nos interesan tratar. La Transformada de Fourier Discreta, o bien DFT (Discrete Fourier Transform), se emplea para encontrar el contenido de frecuencia de seÃ±ales que son discretas. Esto implica que en el dominio de la frecuencia estas seÃ±ales tambiÃ©n serÃ¡n periÃ³dicas y discretas. La forma compleja de la Transformada de Fourier Discreta se expresa de la siguiente forma: ğ‘‹(ğ‘“) = ï¿½ğ‘¥(ğ‘›)ğ¹âˆ’ ğ‘—2ğœ‹ğ‘›ğ‘˜ ğ‘ ğ‘“ = 0, 1, â€¦ ,ğ‘ âˆ’ 1 (ğ‘¬ğ’„. ğŸ.ğŸğŸ) ğ‘âˆ’1 ğ‘›=0 Donde ğ‘› denota el nÃºmero de muestras de la seÃ±al en el dominio del tiempo ğ‘¥(ğ‘›), ğ‘‹(ğ‘“) el espectro de frecuencias de ğ‘¥(ğ‘›) y ğ¹âˆ’ ğ‘—2ğœ‹ğ‘›ğ‘˜ ğ‘ es la raÃ­z primitiva n-Ã©sima de la unidad en el plano complejo (factor de fase de rotaciÃ³n). Cuando se lleva a cabo la Transformada Discreta de Fourier de una seÃ±al, se obtiene una secuencia de nÃºmeros complejos de longitud ğ‘/2 + 1 (ver Figura 2.7) que consta de dos seÃ±ales a la salida, las cuales contienen informaciÃ³n de las magnitudes de los componentes cosenoidales y senoidales, es decir, la parte real e imaginaria. La Transformada Inversa de Fourier Discreta se calcula mediante la ecuaciÃ³n: ğ‘¥(ğ‘›) = 1 ğ‘ ï¿½ ğ‘‹(ğ‘“)ğ¹ ğ‘—2ğœ‹ğ‘›ğ‘˜ ğ‘ (ğ‘¬ğ’„. ğŸ.ğŸğŸ) ğ‘âˆ’1 ğ‘˜=0 16 Al procedimiento en el cual se transforma una seÃ±al que se encuentra en el dominio del tiempo al dominio de la frecuencia se le llama anÃ¡lisis o forward DFT. Por el contrario, si se tiene una seÃ±al en el dominio de la frecuencia y se lleva al dominio del tiempo, se le llama sÃ­ntesis o inverse DFT. Dominio del Tiempo Dominio de la Frecuencia Figura 2.7: Proceso de cambio de dominio del tiempo al dominio de la frecuencia con la Transformada Discreta de Fourier [6]. Cuando se lleva a cabo una transformada inversa IDFT, es necesario que las muestras en la entrada se encuentren en el orden correcto de tal forma que las frecuencias negativas sean incluidas de manera apropiada, ya que el resultado serÃ­a una seÃ±al diferente a la que se quiere reconstruir. Para comprobar que la simetrÃ­a es la correcta despuÃ©s de hacer la transformaciÃ³n, se verifica que las muestras de la parte imaginaria sean equivalentes a cero. El desarrollo de la DFT histÃ³ricamente se dio en forma paralela al de la transformada de Fourier continua. A pesar de su existencia, la DFT prÃ¡cticamente no se utiliza dado que el cÃ¡lculo de la transformada discreta es un proceso complejo y lento computacionalmente. Lo que se utiliza en la mayorÃ­a de los casos para calcular espectros de seÃ±ales discretas se llama Transformada RÃ¡pida de Fourier, o bien FFT (Fast Fourier Transform), que es un algoritmo desarrollado para obtener la DFT de una forma mÃ¡s rÃ¡pida y eficiente computacionalmente. El tiempo de procesamiento de la FFT es considerablemente mÃ¡s rÃ¡pido que calcular la DFT directamente. X [ ] 0 N-1 N muestras IDFT DFT Real X [ ] Imag. X[ ] 0 N/2 0 N/2 N/2+1 muestras N/2+1 muestras (Amplitudes cosenoidales) (Amplitudes senoidales) 17 2.4.3. Transformada rÃ¡pida de Fourier La FFT (Fast Fourier Transform) es un algoritmo creado por J.W. Cooley y J.W. Tukey [7] para llevar a cabo de manera eficaz la Transformada Discreta de Fourier. En el aÃ±o 1969, se llevÃ³ a cabo un anÃ¡lisis sÃ­smico de 2048 puntos diferentes, el cual tomaba aproximadamente 13 Â½ horas en obtener resultados. Usando la FFT, este mismo cÃ¡lculo tomÃ³ 2.4 segundos en la misma mÃ¡quina. Constituye uno de los mayores desarrollos en la tecnologÃ­a en tratamiento de seÃ±ales. La evoluciÃ³n de la computaciÃ³n, particularmente la del computador personal, ha hecho de la FFT una herramienta de anÃ¡lisis manejable y potente. La rutina de la FFT estÃ¡ basada en la DFT compleja, ya que se utiliza una serie de nÃºmeros complejos de tamaÃ±o ğ‘ en las seÃ±ales de entrada al sistema. Sin embargo, a diferencia de la real se obtendrÃ¡n dos seÃ±ales de tamaÃ±o ğ‘ en la salida, cuyo espectro real estarÃ¡ definido desde 0 hasta ğ‘/2, es decir, los componentes de frecuencia positiva. De esta manera, al incluir las frecuencias negativas, se tratarÃ¡ de un dominio de la frecuencia periÃ³dico. Otro punto a considerar es que el algoritmo de la FFT implementa una DFT real utilizando una DFT compleja, por lo que las muestras correspondientes a la parte imaginaria de la entrada, deberÃ¡n ser iguales a cero. Este algoritmo tiene una complejidad en tiempo de ğ¿(ğ‘› ğ¹ğ‘œğ‘”(ğ‘›)), lo que indica que las convoluciones pueden ser calculadas mÃ¡s rÃ¡pidamente en el dominio de la frecuencia haciendo uso de la FFT en vez de hacer la convoluciÃ³n en el dominio del tiempo. 2.4.3.1. Algoritmo de FFT Para llevar a cabo la Transformada RÃ¡pida de Fourier se requieren cuatro etapas: Primero, se divide una seÃ±al de N muestras en el dominio del tiempo en N seÃ±ales cada una conformada por un solo punto. En la Figura 2.8, se muestra la descomposiciÃ³n en el tiempo de una seÃ±al de 16 muestras: 18 Figura 2.8: DescomposiciÃ³n de las muestras de una seÃ±al de 16 puntos [6]. Al segundo paso del algoritmo se le conoce como Algoritmo de InversiÃ³n de Bits, ya que las ğ‘ muestras son reordenadas contando en binario con los bits rotados de izquierda a derecha. Por ejemplo, la muestra 3 que en binario es â€œ0011â€ serÃ¡ intercambiada por la muestra 12 â€œ1100â€ y asÃ­ sucesivamente. Para este paso se requieren ğ¹ğ‘œğ‘”2ğ‘ etapas. El tercer paso se trata de obtener el espectro de frecuencia correspondiente a cada una de las muestras, el cual es equivalente a su valor actual, ya que debido al paso anterior, se encuentran en el dominio de la frecuencia. Por Ãºltimo, los espectros de frecuencia en la FFT se combinan en pares, duplicÃ¡ndolos y sumÃ¡ndolos de tal manera que ambos espectros coincidan. Esto se logra agregando ceros en el dominio del tiempo a ambas seÃ±ales. En una seÃ±al los puntos impares son ceros mientras que en la otra seÃ±al los puntos pares serÃ¡n ceros, es decir, una de las seÃ±ales serÃ¡ desplazada a la derecha por una muestra. Esto es equivalente a multiplicar su espectro por una seÃ±al sinusoidal, como se muestra en la Figura 2.9. 1 seÃ±al de 16 puntos 2 seÃ±ales de 8 puntos 4 seÃ±ales de 4 puntos 8 seÃ±ales de 2 puntos 16 seÃ±ales de 1 punto 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 0 2 4 6 8 10 12 14 1 3 5 7 9 11 13 15 0 4 8 12 2 6 10 14 1 5 9 13 3 7 11 15 8 4 12 2 10 6 14 1 9 5 13 3 11 7 150 8 4 12 2 10 6 14 1 9 5 13 3 11 7 150 19 Figura 2.9: SÃ­ntesis de espectros en frecuencia [6]. De esta manera, al sumar ambos espectros en el orden mostrado, se obtendrÃ¡ el espectro final a la salida de la FFT (ver Figura 2.10). Figura 2.10: MÃ©todo para obtener el espectro final [6]. a b c d a b c d0 0 0 0 A B C D A B C D A B C D e f g h e f g h0 0 0 0 E F G H F G H E F G H Ã— sinusoid Dominio del Tiempo Dominio de la Frecuencia E ++ + + + + + + Espectro de Frecuencia de 8 puntos Espectro de Frecuencia de 4 puntos - par Sx Sx Sx Sx Espectro de Frecuencia de 4 puntos - impar 20 2.5. Respuesta al impulso y respuesta de frecuencia Para estudiar el comportamiento de un sistema lineal e invariante se utilizan normalmente dos tipos de entradas, un impulso unitario o una seÃ±al constante (ver Figura 2.11). Figura 2.11: Respuesta al impulso [5]. La respuesta al impulso corresponde a la respuesta de un sistema ante un impulso cuando el sistema se encuentra en estado de reposo. Un impulso es una funciÃ³n matemÃ¡tica abstracta que tiene una amplitud infinita y una duraciÃ³n casi nula. Esta seÃ±al es ideal para estudiar sistemas, ya que permite estimar la respuesta de un sistema cualquiera a seÃ±ales con un contenido de frecuencias previamente determinado. Otra respuesta muy utilizada para diseÃ±ar sistemas es la respuesta de frecuencia, que se define como la respuesta del sistema en el dominio de la frecuencia. Esta respuesta puede calcularse como la Transformada de Fourier de la respuesta al impulso, o bien puede medirse o estimarse directamente, si se utilizan como entrada seÃ±ales de tipo sinusoidal. 21 Dado que cualquier seÃ±al puede descomponerse en muchas sinusoides individuales, de acuerdo a la serie o transformada de Fourier, la respuesta total del sistema puede calcularse mediante el principio de superposiciÃ³n, es decir, si un sistema posee varias entradas, pueden analizarse por separado las respuestas del sistema a cada una de las entradas y luego calcular la salida como la suma de las respuestas individuales. En el Ã¡mbito de los sistemas digitales, dado que un impulso es una abstracciÃ³n matemÃ¡tica imposible de representar en un computador, un impulso se implementa como una secuencia de nÃºmeros con valor cero salvo una sola muestra que toma el valor uno. Un tren de impulsos, en cambio, es una secuencia de muestras con valor unitario. La Figura 2.11 muestra en forma grÃ¡fica la relaciÃ³n entre la respuesta al impulso y la respuesta de frecuencia de un sistema digital o filtro. El estudio de la respuesta de frecuencia de un sistema, es clave en la teorÃ­a y diseÃ±o de los filtros digitales. Si se conoce la respuesta al impulso, basta realizar una operaciÃ³n matemÃ¡tica denominada convoluciÃ³n entre una seÃ±al de entrada cualquiera y la respuesta al impulso para obtener la respuesta del sistema a esa entrada. 2.6. ConvoluciÃ³n La convoluciÃ³n es una operaciÃ³n matemÃ¡tica que se utiliza para combinar dos seÃ±ales y formar una tercera seÃ±al. Es la tÃ©cnica mÃ¡s simple e importante en el procesamiento de seÃ±ales. Usando la estrategia de descomposiciÃ³n de un impulso los sistemas pueden ser descritos por una seÃ±al llamada respuesta al impulso (ver glosario). La descomposiciÃ³n en impulsos es importante ya que permite examinar las muestras de una seÃ±al. La convoluciÃ³n es importante porque contiene la relaciÃ³n de tres seÃ±ales de interÃ©s: la seÃ±al de entrada, la de salida y la respuesta al impulso [6]. Mediante la convoluciÃ³n es posible calcular la respuesta de un sistema a una entrada arbitraria. Cuando una seÃ±al de entrada ğ‘¥(ğ‘›) es introducida en un sistema lineal con una respuesta al impulso â„(ğ‘›), el resultado de la convoluciÃ³n de ambas serÃ¡ la seÃ±al ğ‘¦(ğ‘›), cuyo nÃºmero total de muestras serÃ¡ la suma de la entrada con la respuesta al impulso menos uno (ver Figura 2.12). 22 x(n) y(n) Figura 2.12: Sistema Lineal con una respuesta al impulso ğ’‰(ğ’). El operador de convoluciÃ³n se expresa como se muestra en la siguiente ecuaciÃ³n: ğ‘¥(ğ‘›)â¨‚ â„(ğ‘›) = ğ‘¦(ğ‘›) (ğ‘¬ğ’„. ğŸ.ğŸğŸ) La convoluciÃ³n tambiÃ©n puede ser expresada como la suma de los productos escalares de las seÃ±ales del sistema, es decir, la de entrada y la respuesta al impulso: ï¿½ ğ‘¥(ğ‘“)â„(ğ‘› âˆ’ ğ‘“) (ğ‘¬ğ’„. ğŸ.ğŸğŸ‘) ğ‘› ğ‘˜= 0 Hay que considerar que los lÃ­mites de la ecuaciÃ³n anterior comienzan en ğ‘“ = 0, ya que los sistemas en muchos casos no responden a valores de entrada mÃ¡s allÃ¡ del tiempo presente, ya que â„ (ğ‘› âˆ’ ğ‘“) = 0 cuando ğ‘› âˆ’ ğ‘“ < 0. Cuando se trata de hacer procesamiento digital de una seÃ±al no tiene sentido hablar de convoluciones aplicando estrictamente la definiciÃ³n ya que sÃ³lo disponemos de valores en instantes discretos de tiempo. Es necesario para esto realizar una aproximaciÃ³n numÃ©rica. La convoluciÃ³n puede ser calculada en el dominio de la frecuencia (utilizando la FFT), ya que una de sus propiedades es la de ser equivalente a la multiplicaciÃ³n compleja en el dominio de la frecuencia. Una vez ubicados en este dominio se multiplica la entrada ğ‘¥(ğ‘›) por la respuesta al impulso ğ‘“(ğ‘“) y finalmente se realiza la transformada inversa (IFFT) para obtener el resultado del producto hecho en el dominio de la frecuencia. Sistema Lineal h(n) 23 2.6.1. Teorema de ConvoluciÃ³n Sea ğ“• el operador Transformada de Fourier. Entonces, para la funciÃ³n ğ‘¥(ğ‘›), ğ“•(ğ‘¥(ğ‘›)) es la Transformada de Fourier de ğ‘¥(ğ‘›) y ğ“•âˆ’ğŸ(ğ‘¥(ğ‘›)) la Transformada Inversa de Fourier de ğ‘¥(ğ‘›). El Teorema de ConvoluciÃ³n establece si ğ‘¥(ğ‘›) y â„(ğ‘“) son secuencias infinitas: ğ“•(ğ‘¥(ğ‘›) â¨‚ â„(ğ‘“)) = ğ“•(ğ‘¥(ğ‘›)) âˆ™ ğ“•(â„(ğ‘“)) (ğ‘¬ğ’„. ğŸ.ğŸğŸ’) Aplicando la Transformada Inversa de Fourier, la convoluciÃ³n de ğ‘¥(ğ‘›) y â„(ğ‘“), ğ‘¥(ğ‘›) â¨‚ â„(ğ‘“) puede ser calculada como: ğ‘¥(ğ‘›) â¨‚ â„(ğ‘“) = ğ“•âˆ’1(ğ“•(ğ‘¥(ğ‘›)) âˆ™ ğ“•(â„(ğ‘“))) (ğ‘¬ğ’„. ğŸ.ğŸğŸ“) Los conceptos bÃ¡sicos de seÃ±ales y ciertas operaciones que se pueden aplicar en ellas a travÃ©s del procesamiento digital de seÃ±ales han sido introducidos, en la siguiente subsecciÃ³n se explicarÃ¡ cÃ³mo manipular una seÃ±al de audio a travÃ©s de filtros. Esto puede ser de utilidad para reducir ruido o aplicar ecualizaciÃ³n. 24 2.7. Filtros digitales Un filtro es bÃ¡sicamente una caja con una entrada y una salida (ver Figura 2.13). Si la salida es diferente a la entrada, significa que la seÃ±al original ha sido filtrada. Cualquier medio por el cual una seÃ±al de audio pasa, cualquiera sea su forma, puede describirse como un filtro. x(n) y(n) Figura 2.13: Un filtro como una caja negra [5]. Los filtros pueden ser analÃ³gicos, como el caso de un filtro solar de un telescopio, o digitales, como un eliminador de ruido implementado en el computador. Un filtro digital es un sistema de tiempo discreto que deja pasar ciertos componentes de frecuencia de una secuencia de entrada sin distorsiÃ³n y bloquea o atenÃºa otros. Se trata simplemente de un filtro que opera sobre seÃ±ales digitales, tales como las que operan en un computador. Un filtro digital es un cÃ¡lculo que se realiza al recibir una secuencia de datos (la seÃ±al de entrada) y produce una nueva (la seÃ±al de salida). Figura 2.14: Cambio que se produce en una seÃ±al que ha sido pasada a travÃ©s de un filtro [5]. h(n) 25 La Figura 2.14 muestra el cambio que produce un filtro en el dominio de la frecuencia. En el primer caso el espectro de frecuencias se mantiene igual, lo que indica que el filtro no realiza cambio alguno en la seÃ±al de entrada. El segundo caso claramente muestra una modificaciÃ³n en el contenido de frecuencias de la seÃ±al original. Los filtros analÃ³gicos pueden ser usados para cualquier tarea, sin embargo, los filtros digitales pueden alcanzar resultados muy superiores. Un filtro digital puede hacer todo lo que un filtro analÃ³gico es capaz de realizar. 2.7.1. EcuaciÃ³n de diferencias En forma general, cualquier filtro digital puede ser descrito por la siguiente ecuaciÃ³n de diferencias [5]: ğ‘¦(ğ‘›) = ï¿½ğ‘“ğ‘–ğ‘¥(ğ‘› âˆ’ ğ¹) âˆ’ï¿½ğ‘”ğ‘˜ğ‘¦(ğ‘› âˆ’ ğ‘“) ğ‘ ğ‘˜=1 ğ‘€ ğ‘–=0 (ğ‘¬ğ’„. ğŸ.ğŸğŸ”) De esta ecuaciÃ³n se puede concluir que lo Ãºnico que se necesita para implementar un filtro digital son sumas (o restas), multiplicaciones y retrasos, ya que nos dice que la salida del filtro depende de versiones presentes y pasadas de la entrada menos versiones pasadas de la salida. El Ã­ndice ğ‘› es un nÃºmero entero que representa unidades discretas y los nÃºmeros ğ‘”ğ‘˜ y ğ‘“ğ‘– se denominan los coeficientes del filtro. Los coeficientes ğ‘”ğ‘˜ multiplican a valores pasados de la salida y los coeficientes ğ‘“ğ‘– a valores pasados de la entrada. Si todos los coeficientes ğ‘”ğ‘˜ son cero, entonces el filtro sÃ³lo depende de valores anteriores y actuales de la entrada y se trata de un filtro no recursivo o FIR (Finite Impulse Response). Si los valores ğ‘”ğ‘˜ no son todos nulos, entonces se trata de un filtro recursivo o IIR (Infinite Impulse Response). 26 2.7.2. Filtros FIR En un filtro con Respuesta Finita al Impulso o FIR, los coeficientes ğ‘”ğ‘˜ son cero, lo que significa que la respuesta del filtro depende solamente de la entrada y no de valores pasados de la salida. Este tipo de filtros tiene una respuesta finita ya que no exhiben recursiÃ³n. ğ‘¦(ğ‘›) = ï¿½ğ‘“ğ‘–ğ‘¥(ğ‘› âˆ’ ğ¹) (ğ‘¬ğ’„. ğŸ.ğŸğŸ•) ğ‘€ ğ‘–=0 Los filtros pueden ser graficados como un diagrama de bloques, en este tipo de diagramas se pueden definir los elementos que se requieren para implementar el filtro. En la Figura 2.15 se muestran las operaciones bÃ¡sicas que se pueden llevar a cabo para construir un filtro. ï‚§ Adiciones y/o multiplicaciones: Se utilizan para obtener el resultado de una operaciÃ³n (suma o multiplicaciÃ³n) con dos elementos. ï‚§ Retraso unitario: Se utiliza para almacenar un valor anterior al actual para su uso posterior. Sumador: ğ‘“(ğ‘›) = ğ‘”(ğ‘›) + ğ‘“(ğ‘›) a(n) c(n) b(n) Multiplicador: ğ‘“(ğ‘›) = ğ‘”(ğ‘›) âˆ— ğ‘“(ğ‘›) a(n) c(n) b(n) Retraso Unitario: ğ‘“(ğ‘›) = ğ‘”(ğ‘› âˆ’ 1) a(n) b(n) Figura 2.15: Operaciones bÃ¡sicas con seÃ±ales digitales [8]. ğ‘âˆ’1 Î£ 27 La Figura 2.16 muestra la implementaciÃ³n de un filtro FIR. Una de las propiedades mÃ¡s interesantes de este tipo de filtros es la simetrÃ­a de sus coeficientes y el hecho que pueden ser diseÃ±ados de tal forma de exhibir una respuesta de fase lineal. Figura 2.16: Esquema de implementaciÃ³n de un filtro FIR [5]. Los filtros FIR tienen las siguientes propiedades: ï‚§ Respuesta de fase lineal (si se diseÃ±an adecuadamente) ï‚§ Estabilidad con coeficientes cuantizados ï‚§ En general, requieren de un mayor orden que los filtros IIR El diseÃ±o de filtros FIR se basa usualmente en una aproximaciÃ³n directa de la magnitud de la respuesta deseada a la cual se le agrega la condiciÃ³n de una respuesta de fase lineal. 2.7.2.1. DiseÃ±o de filtros FIR Un filtro FIR puede ser diseÃ±ado utilizando distintas tÃ©cnicas tanto en el dominio del tiempo como de la frecuencia. Existe un mÃ©todo Ã³ptimo para producir filtros FIR llamado Algoritmo de Parks McClellan [9] que produce coeficientes Ã³ptimos y fase lineal. Este mÃ©todo es muy popular dado su facilidad de uso, flexibilidad y excelentes resultados. x(n) ğ‘âˆ’1 ğ‘âˆ’1 ğ‘âˆ’1 y(n) b0 b1 b2 bn Î£ Î£ Î£ 28 Las tÃ©cnicas de aplicaciÃ³n de ventanas (windowing), bÃ¡sicamente toman la respuesta al impulso de un filtro ideal con duraciÃ³n infinita y aplican una ventana para truncar y limitar los coeficientes a un nÃºmero especÃ­fico, para luego desplazar la secuencia de manera de garantizar causalidad. Esta tÃ©cnica es simple y produce resultados razonables, pero con distorsiÃ³n respecto a la respuesta ideal. Otras tÃ©cnicas se basan en muestreo en la frecuencia. La idea es muestrear la respuesta de amplitud deseada a intervalos regulares y luego utilizar la DFT inversa para generar una respuesta al impulso de duraciÃ³n finita. El nÃºmero de muestras determina el nivel de precisiÃ³n del filtro a la respuesta ideal. Esta tÃ©cnica es bastante sofisticada y requiere una tasa de muestreo alta. Por Ãºltimo, hay una tÃ©cnica ad-hoc denominada posicionamiento de ceros (ver glosario) [10]. La idea es colocar ceros en el plano Z de forma estratÃ©gica con el fin de aproximarse a la respuesta deseada y realizar el cÃ¡lculo de los coeficientes en forma grÃ¡fica. Mientras mÃ¡s cercano estÃ© el cero al cÃ­rculo unitario, mayor efecto tendrÃ¡ en la respuesta de frecuencia del filtro para ese rango de frecuencias. Esta tÃ©cnica es rÃ¡pida y no muy precisa, pero puede funcionar para aplicaciones simples. 29 2.8. Algoritmos de procesamiento digital de audio Existen distintos tipos de efectos de audio, clasificados comÃºnmente de la siguiente forma [11]: ï‚§ Control de DinÃ¡micas y Ganancia: Este tipo de efectos modifican o alteran de algÃºn modo la dinÃ¡mica (amplitud) del sonido. Entre ellos se encuentra el Compressor, Distortion y Over Drive. ï‚§ Efectos de ModulaciÃ³n: Se encargan de modular la seÃ±al de entrada ya que se altera la seÃ±al de audio de forma no lineal en dos dimensiones, tiempo y frecuencia. Varias seÃ±ales interactÃºan unas con otras para producir nuevas seÃ±ales con frecuencias que no estaban presentes en las seÃ±ales originales. Entre los mÃ¡s comunes se encuentran el Vibrato, Chorus, Flanger, Phaser, Ring Modulator, Tremolo y Auto Panner. ï‚§ Filtros: Son los efectos que se utilizan para modificar el espectro de la frecuencia del sonido. Los filtros mÃ¡s comunes son el filtro paso bajo, filtro paso alto y filtro paso banda. Generalmente estos filtros forman parte de un Ecualizador, el cual se puede parametrizar de una forma mÃ¡s especÃ­fica. ï‚§ Efectos Basados en el Tiempo: Se utilizan para crear una sensaciÃ³n de eco o simular de forma artificial un espacio acÃºstico. El Delay y el Reverb son los mÃ¡s conocidos. A continuaciÃ³n se presenta una descripciÃ³n breve de los efectos de audio implementados en este trabajo. 30 2.8.1. Over Drive Este efecto consiste en saturar la seÃ±al de audio logrando que esta se distorsione ligeramente. Es muy utilizado en la guitarra elÃ©ctrica. El Over Drive ofrece al instrumento un sonido cÃ¡lido con mayor duraciÃ³n de las notas musicales (lo que se conoce como sustain). AsÃ­ mismo adiciona armÃ³nicos (ver glosario) lo cual refuerza las frecuencias altas o agudos. Se puede ajustar el grado de saturaciÃ³n de la seÃ±al, desde un sonido limpio a otro moderadamente saturado. 2.8.2. Distortion Este efecto consiste en saturar fuertemente la seÃ±al de audio logrando que esta se distorsione. Al igual que el Over Drive, es muy utilizado en la guitarra elÃ©ctrica. La distorsiÃ³n le ofrece al instrumento un sonido agresivo y potente asÃ­ como mayor duraciÃ³n a las notas musicales ya que adiciona armÃ³nicos a la seÃ±al lo cual refuerza las frecuencias altas o agudos. Se puede ajustar el grado de distorsiÃ³n de la seÃ±al, desde un sonido suave a otro fuertemente saturado. 2.8.3. Ecualizador Un ecualizador modifica el contenido en frecuencias de la seÃ±al de entrada. Consiste bÃ¡sicamente en una colecciÃ³n de filtros, cada uno centrado en una banda de frecuencias especÃ­ficas que alteran la seÃ±al recibida. Los filtros nos permiten modificar la seÃ±al que reciben, dejando pasar a travÃ©s de ellos una parte de la seÃ±al diferente a la original, todo dependiendo de su funcionamiento. La parte que dejarÃ¡n pasar estarÃ¡ limitada entre la frecuencia de corte inferior y la frecuencia de corte superior. Ambas se corresponden con las frecuencias mÃ­nimas y mÃ¡ximas que pueden pasar a travÃ©s del filtro. 31 Los filtros mÃ¡s comunes que tiene un ecualizador son (ver Figura 2.17): ï‚§ Filtro pasa-bajo: Permite el paso de las frecuencias menores a cierta frecuencia ğœ”ğ‘ denominada frecuencia superior de corte y bloquea las frecuencias superiores a este valor. ï‚§ Filtro pasa-altos: Permite el paso de las frecuencias mayores a cierta frecuencia ğœ”ğ‘ denominada frecuencia inferior de corte y bloquea las frecuencias inferiores a este valor. ï‚§ Filtro pasa-banda: Permite el paso de las frecuencias comprendidas entre dos frecuencias ğœ”1 y ğœ”2 (ğœ”1 < ğœ”2), denominadas frecuencia inferior de corte y frecuencia superior de corte, bloqueando las frecuencias restantes. Filtro rechaza-banda: Bloquea el paso de las frecuencias comprendidas entre dos frecuencias ğœ”1 y ğœ”2 (ğœ”1 < ğœ”2), denominadas frecuencia superior de corte y frecuencia inferior de corte, dejando pasar las frecuencias restantes. a) b) c) d) Figura 2.17: Respuesta en frecuencia de los filtros utilizados en un ecualizador, a) Filtro paso-bajo, b) Filtro paso-alto, c) Filtro pasa-banda, d) Filtro rechaza-banda. ğ‘‘ğ‘‘ ğ‘“ ğœ”ğ‘ ğ‘‘ğ‘‘ ğ‘“ ğœ”ğ‘ ğ‘‘ğ‘‘ ğ‘“ ğœ”1 ğœ”2 ğ‘‘ğ‘‘ ğ‘“ ğœ”1 ğœ”2 32 2.8.4. Vibrato Cuando un carro pasa a una velocidad sumamente rÃ¡pida, se puede apreciar una desviaciÃ³n o desafinaciÃ³n del sonido (lo que se conoce como Efecto Doppler). Esta variaciÃ³n de tono en el sonido es dada por la variaciÃ³n de distancia que existe entre la fuente que emite el sonido y el receptor, esto es equivalente a variar el tiempo de retardo del efecto delay. Al variar periÃ³dicamente el tiempo de retardo se produce una variaciÃ³n periÃ³dica del tono, esto es lo que se logra con el efecto de vibrato. 2.8.5. Chorus El efecto chorus o de coro se obtiene al mezclar una seÃ±al de entrada con vibrato junto con la seÃ±al sin procesar. Para producir el efecto, ya sea natural o artificialmente, deben mezclarse dos o mÃ¡s sonidos individuales con aproximadamente el mismo timbre y alturas levemente disÃ­miles, de manera que sean percibidos como un solo sonido proveniente de una misma fuente. El efecto de coro se enfatiza cuando los sonidos se producen en tiempos y posiciones levemente distintas. Esto es lo que tÃ­picamente sucede en un coro de voces, y es lo que origina el nombre del efecto. 2.8.6. Ring Modulator Se refiere a la modulaciÃ³n de â€œanilloâ€ porque la implementaciÃ³n en circuitos analÃ³gicos de este efecto consiste en posicionar una colecciÃ³n de diodos en forma de anillo. La modulaciÃ³n de anillo estÃ¡ estrechamente relacionada con la modulaciÃ³n de amplitud y la mezcla de frecuencias. Las frecuencias que se generan generalmente son inarmÃ³nicas, lo que hace que se creen sonidos bastante disonantes. Este efecto estÃ¡ diseÃ±ado para generar sonidos con caracterÃ­sticas metÃ¡licas o sonidos similares al sonido de una campana. 33 2.8.7. Tremolo El efecto de tremolo consiste en una modulaciÃ³n de amplitud de la seÃ±al de entrada (volumen) que varÃ­a de forma periÃ³dica durante el tiempo. La seÃ±al portadora serÃ¡ la seÃ±al de audio original, mientras que la seÃ±al moduladora serÃ¡ una seÃ±al periÃ³dica de baja frecuencia generada por un LFO (Low Frequency Oscilator). 2.8.8. Auto Panner El efecto auto panner modifica la posiciÃ³n del sonido en el espacio estereofÃ³nico. Esta posiciÃ³n estereofÃ³nica puede ir desde el extremo izquierdo (valores negativos) al extremo derecho (valores positivos) de los altavoces. Este efecto tiene el mismo principio del tremolo, la diferencia estÃ¡ en que se caracteriza por ser un efecto de tipo estÃ©reo. La seÃ±al de audio se envÃ­a a los altavoces de manera alternada. 2.8.9. Delay Un efecto de delay o retraso consiste en la multiplicaciÃ³n y retraso de una seÃ±al de entrada, que se mezcla con la seÃ±al original, obteniÃ©ndose un efecto de eco artificial. En producciÃ³n musical es utilizado de forma muy creativa. Es utilizado especialmente en grabaciones de instrumentos musicales donde se puede crear una sensaciÃ³n de engrandecimiento del instrumento con delays de corta duraciÃ³n y efectos rÃ­tmicos con delays de larga duraciÃ³n. Cuando se reproduce la copia de la seÃ±al retrasada (Delay Line) con la seÃ±al original, el usuario puede manipular el volumen de dicha copia para generar un efecto de eco alejÃ¡ndose o un sonido perpetuo de la copia. TambiÃ©n es posible definir el tiempo del retraso (desde 1 ms. hasta 1 seg. comÃºnmente). 34 2.9. ComputaciÃ³n paralela en GPU 2.9.1. GPGPU GPGPU son la siglas de General Purpose computation on Graphics Processing Units o ComputaciÃ³n de PropÃ³sito General para Unidades de Procesamiento GrÃ¡fico, tambiÃ©n denominado GPU Computing. Los GPU son procesadores con una gran cantidad de nÃºcleos de procesamiento capaces de tener un alto rendimiento y velocidad en la computaciÃ³n de datos. La evoluciÃ³n a travÃ©s del tiempo de estos procesadores, en particular de los procesadores NVIDIA, se ilustra en la Figura 2.18. Su nombre se deriva de que originalmente se encontraban localizados en la tarjeta grÃ¡fica y se utilizaban solamente para procesamiento grÃ¡fico. Actualmente los GPU son procesadores paralelos de propÃ³sito general con soporte para una programaciÃ³n accesible con lenguajes de programaciÃ³n estÃ¡ndar como C. Los desarrolladores que han traducido sus aplicaciones para ser procesadas por el GPU han tenido muchas veces rendimientos superiores incluso comparables con versiones optimizadas para CPU. Actualmente existe el sitio web GPGPU.org, que se encarga de catalogar el uso histÃ³rico del GPU para la computaciÃ³n de propÃ³sito general ademÃ¡s de proveer recursos para los desarrolladores de GPGPU. Figura 2.18: EvoluciÃ³n de los GPU NVIDIA con respecto a los CPU Intel [12]. GFLOP/s (TeÃ³rico) GPU NVIDIA (PrecisiÃ³n Simple) GPU NVIDIA (PrecisiÃ³n Doble) CPU Intel (PrecisiÃ³n Simple) CPU Intel (PrecisiÃ³n Doble) 1500 1250 1000 750 500 250 0 Sep-01 Ene-03 Jun-04 Oct-05 Mar-07 Jul-08 Dic-09 GeForce GTX 480 GeForce GTX 280 GeForce 8800 GTX GeForce 7800 GTX Tesla C 2050 Tesla C 1060 Woodcrest Pentium 4 Harpertown Bloomfield Westmere GeForce 6800 Ultra GeForce FX 5800 35 El tÃ©rmino GPGPU fue creado por Mark Harris en el aÃ±o 2002 [13], cuando empezÃ³ a notar una tendencia inicial de utilizar el GPU para aplicaciones no grÃ¡ficas. GPGPU.org, por su lado, ha pasado de ser un sitio visitado por unos pocos a ser un sitio sumamente popular para los desarrolladores e investigadores de esta Ã¡rea. Uno de los primeros ejemplos de programaciÃ³n genÃ©rica en GPU fue â€œEl Juego de la Vidaâ€ [14]. Con este ejemplo, Greg James, pretendÃ­a ilustrar las capacidades que tiene la GPU para realizar cÃ¡lculos dentro y tambiÃ©n fuera del Ã¡mbito de la sÃ­ntesis de grÃ¡ficos pura. Este ejemplo estaba orientado a la creaciÃ³n de texturas dinÃ¡micas (procedurales) en tiempo real. Pronto le siguieron otros pequeÃ±os shaders (ver glosario) que ampliaban esas posibilidades hacia mÃ©todos numÃ©ricos aritmÃ©ticamente intensivos en los que la dependencia entre datos era baja, como la resoluciÃ³n de ecuaciones en derivadas parciales para la simulaciÃ³n de fenÃ³menos fÃ­sicos. El campo de la GPGPU tuvo una calurosa aceptaciÃ³n por parte de la comunidad cientÃ­fica, ya que abrÃ­a las puertas a sistemas muy baratos, disponibles en cualquier computador personal, con un potencial de cÃ¡lculo muy alto y un crecimiento exponencial. Algo que no tenÃ­a rival frente a las otras arquitecturas de aquella Ã©poca. Sin embargo, su programaciÃ³n ha sido durante mucho tiempo una tarea especialmente ardua y compleja. Esto debido a que las tarjetas grÃ¡ficas y su pipeline grÃ¡fico estaban Ãºnicamente pensados para realizar cÃ¡lculos para la sÃ­ntesis de grÃ¡ficos geomÃ©tricos, lo que obligÃ³ a sus programadores a adaptar sus algoritmos a la secuencia segmentada del pipeline. Por lo tanto, para lidiar con la tarjeta grÃ¡fica era necesario conocer su arquitectura a muy bajo nivel si se deseaba aprovechar sus caracterÃ­sticas potenciales. AsÃ­ fue creciendo el movimiento GPGPU hasta lo que hoy en dÃ­a es, un campo de desarrollo e investigaciÃ³n muy importante. Inclusive los planes de negocio de compaÃ±Ã­as como NVIDIA, AMD/ATI, IBM e Intel han cambiado debido al potencial que tienen las arquitecturas que se utilizan en procesadores grÃ¡ficos para realizar grandes cantidades de cÃ¡lculos en punto flotante, lo que nos dirige cada vez mÃ¡s rÃ¡pido a los llamados procesadores heterogÃ©neos. Debido a las diferencias fundamentales entre las arquitecturas de la GPU y la CPU, no cualquier problema se puede beneficiar de una implementaciÃ³n en la GPU. En concreto, el acceso a memoria plantea las mayores dificultades. Las CPU estÃ¡n diseÃ±adas para el acceso aleatorio a memoria. Esto favorece la creaciÃ³n de estructuras de datos complejas, con punteros a posiciones arbitrarias en memoria. En cambio, en una GPU, el acceso a memoria estÃ¡ mucho mÃ¡s restringido. 36 Por ejemplo, en un procesador de vÃ©rtices, se favorece el modelo scatter, en el que el programa lee en una posiciÃ³n predeterminada de la memoria, pero escribe en una o varias posiciones arbitrarias. En cambio, un procesador de pÃ­xeles, o fragmentos, favorece el modelo gather, pudiendo el programa leer de varias posiciones arbitrarias, pero escribir en sÃ³lo una posiciÃ³n predeterminada. La tarea del diseÃ±ador de algoritmos GPGPU consiste principalmente en adaptar los accesos a memoria y las estructuras de datos a las caracterÃ­sticas de la GPU. Pese a que cualquier algoritmo que es implementable en una CPU lo es tambiÃ©n en una GPU, esas implementaciones no serÃ¡n igual de eficientes en las dos arquitecturas. En concreto, los algoritmos con un alto grado de paralelismo, sin necesidad de estructuras de datos complejas, y con una alta intensidad aritmÃ©tica, son los que mayores beneficios obtienen de su implementaciÃ³n en la GPU. Tradicionalmente, el desarrollo de software GPGPU se hace en ensamblador, o bien en alguno de los lenguajes especÃ­ficos para aplicaciones grÃ¡ficas usando la GPU, como GLSL (OpenGL Shading Language), Cg (C for Graphics) o HLSL (High Level Shading Language). Sin embargo, recientemente han surgido herramientas para facilitar el desarrollo de aplicaciones GPGPU, al abstraer muchos de los detalles relacionados con los grÃ¡ficos y presentar una interfaz de mÃ¡s alto nivel. Son ejemplos de esto el lenguaje de programaciÃ³n CUDA (Compute Unified Device Architecture) creado por la empresa NVIDIA, o el lenguaje OpenCL (Open Computing Language) creado por Khronos Group para la programaciÃ³n en GPU y CPU de forma paralelizada y con estÃ¡ndar abierto y libre, el cual AMD ha decidido apoyar en lugar de su antigua API Close to Metal. Los GPUs estÃ¡n diseÃ±ados especÃ­ficamente para procesar grÃ¡ficos y son sumamente restrictivos en tÃ©rminos de operacionalidad y programabilidad. Por su naturaleza, los GPUs son solamente efectivos abordando problemas que puedan ser resueltos usando stream processing y el hardware solo puede ser usado de ciertas maneras. Un stream es simplemente un conjunto de registros que requieren de una computaciÃ³n similar. Los streams proporcionan paralelismo de datos. Los kernels son funciones que son aplicadas a cada elemento del stream. En los GPUs, los vÃ©rtices y fragmentos son los elementos en un stream y los vextex y pixel shaders son los kernels que se ejecutan sobre cada uno de los elementos. Los GPUs solo pueden procesar de forma independiente vÃ©rtices y fragmentos, pero pueden procesar muchos de ellos en paralelo. Esto es especÃ­ficamente efectivo cuando el programador necesita procesar varios vÃ©rtices o fragmentos con una operaciÃ³n especifica. En este sentido, los GPUs son stream processors, procesadores que pueden operar en paralelo ejecutando un kernel en varios registros en un sentido a la vez. 37 Ya que los GPUs procesan elementos de forma independiente no hay forma de tener datos en forma estÃ¡tica o compartida. Para cada elemento solo se puede hacer lectura de la entrada, llevar a cabo operaciones en estos elementos y escribir los resultados en la salida. EstÃ¡ permitido tener entradas y salidas mÃºltiples, pero nunca una parte de la memoria puede ser leÃ­da y ademÃ¡s escrita. La intensidad aritmÃ©tica es definida por el nÃºmero de operaciones a llevar a cabo por palabra o bloque de memoria transferida. Es importante para las aplicaciones de GPGPU que exista una alta intensidad aritmÃ©tica, de lo contrario la latencia del acceso a memoria limitarÃ¡ la velocidad de computaciÃ³n de los datos. Las aplicaciones GPGPU ideales son las que tienen gran cantidad de datos, alto paralelismo y mÃ­nima dependencia entre los elementos a procesar. En la siguiente parte de este capÃ­tulo se presentarÃ¡ una visiÃ³n general del lenguaje de programaciÃ³n CUDA para entender como es su arquitectura y modelo de programaciÃ³n. 38 2.9.2. CUDA En Noviembre de 2006 NVIDIA lanzo CUDA, un SDK (Software Development Kit) y API (Application Programming Interface) que les permite a los programadores usar el lenguaje de programaciÃ³n C para implementar algoritmos para su ejecuciÃ³n en las tarjetas de video NVIDIA de la generaciÃ³n GeForce 8 en adelante. El GPU estÃ¡ basado en una arquitectura sumamente paralelizable ya que contiene cientos de nÃºcleos que pueden ejecutar simultÃ¡neamente numerosos hilos de forma concurrente. NVIDIA busca explotar el paralelismo de la GPU mediante un modelo y ambiente de programaciÃ³n paralelo que aproveche al mÃ¡ximo el poder de cÃ³mputo de la unidad de procesamiento grÃ¡fico para programas de propÃ³sito general. Por ello es creada la arquitectura CUDA que permite a los programadores centrarse en el desarrollo de algoritmos paralelos y no en la traducciÃ³n de los algoritmos al API de hardware grÃ¡fico. CUDA es bÃ¡sicamente una extensiÃ³n del lenguaje de programaciÃ³n C, por lo que la implementaciÃ³n de los algoritmos paralelos en CUDA es bastante similar a la que se acostumbra hacer en lenguaje C. AdemÃ¡s de tener una curva rÃ¡pida de aprendizaje en cuanto a la sintaxis y la semÃ¡ntica del lenguaje, no requiere de previo conocimiento del API grÃ¡fico o de programaciÃ³n en GPU. 2.9.2.1. Modelo de programaciÃ³n CUDA permite la programaciÃ³n heterogÃ©nea, esto significa que las aplicaciones pueden usar tanto el CPU como el GPU para los algoritmos implementados (ver Figura 2.19). Figura 2.19: Modelo de programaciÃ³n heterogÃ©nea con CUDA [15]. 39 Tanto el CPU como el GPU tienen sus propios espacios de memoria los cuales pueden ser copiados de uno a otro o mapeados para tener un acceso mÃ¡s eficiente. El CPU es denominado (host), y es el que se encarga de la administraciÃ³n de las tareas y funciones que se van a ejecutar en el GPU (device), que es el que se encarga de hacer los cÃ¡lculos que les fueron asignados desde el host a travÃ©s de los kernels que son las funciones a ser ejecutadas en el device. En otras palabras, un kernel es una funciÃ³n que es llamada por un host y es ejecutada por un device (ver Figura 2.20). SÃ³lo un kernel puede ser ejecutado en un device a la vez y cuando un kernel es ejecutado, son ejecutados varios hilos (CUDA Threads) de forma concurrente. Hay algunas distinciones que deben hacerse entre los hilos de CUDA y los hilos de CPU [12]. Figura 2.20: Flujo de la ejecuciÃ³n de un programa hecho en CUDA, el cÃ³digo en serial se ejecuta en el host mientras que el cÃ³digo en paralelo se ejecuta en el device [12]. 40 Los hilos de CUDA son sumamente ligeros en tÃ©rminos de creaciÃ³n y cambio de contexto de un hilo a otro. Miles de hilos de CUDA pueden ser creados en solo algunos ciclos de reloj y como resultado no hay sobrecarga que tenga que ser amortizada durante la ejecuciÃ³n del kernel. Dado que en CUDA el intercambio de hilos es de bajo costo, las penalizaciones de tiempo asociadas a un intercambio de hilo cuando se encuentra en el estado suspendido o bloqueado son mÃ­nimas. La agrupaciÃ³n de hilos se puede hacer en grids y blocks, con el objetivo de compartir datos y sincronizar la ejecuciÃ³n de dichos hilos. Los blocks son bloques de hilos que pueden ser de una, dos o tres dimensiones y los grids son agrupaciones de bloques de hilos que pueden ser de una o dos dimensiones como se puede apreciar en la Figura 2.21. Figura 2.21: Tipos de agrupaciÃ³n de hilos con CUDA. A la izquierda tenemos un grid de una dimensiÃ³n con cuatro bloques que contienen quince hilos cada uno. A la derecha tenemos un grid de dos dimensiones con seis bloques que contienen doce hilos cada uno [12]. 41 2.9.2.2. CompilaciÃ³n del cÃ³digo CUDA CUDA compila una aplicaciÃ³n C/C++ de CUDA enviÃ¡ndola al compilador NVCC (NVIDIA C Compiler) que provee las herramientas necesarias para la compilaciÃ³n en GPU. En el NVCC se generan dos cÃ³digos objetos, uno para el CPU (CPU Code) y otro para el GPU. El cÃ³digo que va hacia el GPU es pasado previamente por el lenguaje intermedio PTX (Parallel Thread Execution) independiente de la plataforma, el cual transforma el cÃ³digo al momento de la ejecuciÃ³n a un binario que puede ser ejecutado por la tarjeta grÃ¡fica (ver Figura 2.22). Figura 2.22: Pasos de compilaciÃ³n del cÃ³digo CUDA. 42 2.9.2.3. Modelo de memoria El CPU y el GPU tienen espacios de memoria separados, el cÃ³digo del host (CPU) maneja la memoria del device (GPU). Entre las operaciones bÃ¡sicas de manejo de memoria se encuentran la asignaciÃ³n y liberaciÃ³n de memoria del GPU, la copia de datos desde el host al device y viceversa y la copia de memoria de un device a otro device. Estas operaciones aplican para la memoria global del (de los) device (s). Cada multiprocesador posee un pequeÃ±o espacio de memoria (Registers) asÃ­ como memoria compartida (Shared Memory) para habilitar la comunicaciÃ³n y colaboraciÃ³n entre los hilos. AdemÃ¡s de los espacios de memoria de cada multiprocesador dentro del device, Ã©ste tambiÃ©n posee un espacio de memoria (DRAM) el cual se encuentra dividido en memoria local y global. El host y el device se comunican a travÃ©s de la memoria global, la cual puede ser accedida por el host para lectura y escritura, tal como se muestra en la Figura 2.23. Figura 2.23: Manejo de memoria entre el CPU (host) y el GPU (device). 43 Los hilos en CUDA pueden acceder a los espacios de memoria mÃºltiple durante su ejecuciÃ³n, como se ilustra en la Figura 2.24. Cada hilo tiene una memoria privada local. Cada bloque de hilos tiene una memoria compartida que es visible entre todos los hilos de dicho bloque. Todos los hilos tienen acceso a la misma memoria global. Figura 2.24: Tipos de acceso a memoria por parte de los hilos de CUDA [12]. 44 2.9.2.4. CUDA Framework CUDA estÃ¡ compuesto en su totalidad por los siguientes componentes: Driver NVIDIA CUDA: Habilita el acceso al hardware y asÃ­ poder ejecutar los cÃ¡lculos en el GPU con CUDA. CUDA Toolkit: Contiene un conjunto de herramientas necesarias para la creaciÃ³n de programas con CUDA. Entre las mÃ¡s importantes se encuentran: ï‚§ Compilador NVCC para C/C++ ï‚§ LibrerÃ­a CUBLAS (CUDA Basic Linear Algebra Subprograms) ï‚§ LibrerÃ­a CUFF (CUDA Fast Fourier Transform) ï‚§ LibrerÃ­a para operaciones con Matrices Dispersas ï‚§ LibrerÃ­a para generaciÃ³n de nÃºmeros aleatorios ï‚§ Herramientas adicionales y documentaciÃ³n CUDA SDK: El Kit de desarrollo de CUDA que contiene ejemplos para su utilizaciÃ³n y documentaciÃ³n adicional. En la prÃ³xima secciÃ³n se hablara un poco acerca de la librerÃ­a CUFFT de CUDA, la cual es de suma importancia para el procesamiento digital de seÃ±ales. 45 2.9.2.5. CUFFT CUFFT (CUDA Fast Fourier Transform) es una librerÃ­a que forma parte de las herramientas de desarrollo de CUDA para calcular de forma eficiente la Transformada Discreta de Fourier (ver Capitulo 2, secciÃ³n 2.4.2). La implementaciÃ³n del algoritmo de FFT estÃ¡ basado en el paradigma del algoritmo â€œDivide y VencerÃ¡sâ€ y es uno de los algoritmos mÃ¡s importantes y ampliamente usados de los algoritmos numÃ©ricos, mÃ¡s que todo en el Ã¡rea de fÃ­sica y procesamiento digital de seÃ±ales. La librerÃ­a CUFTT proporciona una interfaz simple para calcular la FFT en forma paralela en los GPUs NVIDIA, ademÃ¡s de permitir a los usuarios tener el poder de procesar nÃºmeros en punto flotante y aprovechar el paralelismo del GPU sin tener que crear una implementaciÃ³n personalizada del algoritmo FFT [16]. Las librerÃ­as de FFT varÃ­an en tÃ©rminos de tamaÃ±os de la transformaciÃ³n y del tipo de datos. Por ejemplo, algunas librerÃ­as solo implementan FFTs Radix-2, restringiendo la transformaciÃ³n a tamaÃ±os de solo potencias de dos, mientras que otras implementaciones soportan implementaciones de transformaciones de tamaÃ±o arbitrario. La librerÃ­a CUFFT soporta las siguientes caracterÃ­sticas: ï‚§ Transformaciones 1D, 2D y 3D de datos complejos y reales. ï‚§ EjecuciÃ³n por lotes para hacer mÃºltiples transformaciones de cualquier dimensiÃ³n en paralelo. ï‚§ Transformaciones en 2D y 3D de tamaÃ±os en el rango de 2 a 16384 en cualquier dimensiÃ³n. ï‚§ Transformaciones 1D de hasta 8 millones de elementos. ï‚§ Transformaciones de nÃºmeros en punto flotante de doble precisiÃ³n para hardware compatible (GPUs GT200 y superiores). ï‚§ Soporte para ejecuciÃ³n en streaming, habilitando cÃ¡lculos simultÃ¡neos a la vez que se hacen movimientos de datos. 46 CapÃ­tulo 3. DiseÃ±o e ImplementaciÃ³n 3.1. Detalles de implementaciÃ³n Para la implementaciÃ³n de la aplicaciÃ³n se decidiÃ³ utilizar el lenguaje de programaciÃ³n C++ sobre el entorno de desarrollo Microsoft Visual Studio 2010, la arquitectura CUDA es usada para realizar las operaciones de los algoritmos que se ejecutan de forma paralela en el GPU y el Kit de Desarrollo Steinberg VST SDK 3.5 para la creaciÃ³n del plugin de audio. Para la interfaz del programa se empleÃ³ VSTGUI, la cual es una biblioteca multiplataforma para desarrollar interfaces grÃ¡ficas de plugins de audio. Se empleÃ³ el software DAW Cakewalk SONAR 8.5 compatible con el plugin VST para hacer las pruebas necesarias de su funcionamiento. En cuanto a la metodologÃ­a de programaciÃ³n, se utilizÃ³ la programaciÃ³n Orientada a Objetos para la creaciÃ³n de la interfaz de usuario. Para la implementaciÃ³n de los algoritmos de audio en GPU se utilizÃ³ la programaciÃ³n paralela. Una de las limitaciones que podemos encontrar al utilizar CUDA es la utilizaciÃ³n de un hardware especÃ­fico, y es que para poder utilizar esta herramienta es necesario poseer una tarjeta grÃ¡fica NVIDIA compatible con CUDA. Dado que la implementaciÃ³n requiere algunas operaciones especiales en CUDA (mapeo de memoria), es imprescindible que posea Compute Capability versiÃ³n 1.1. En las siguientes secciones se explica de forma mÃ¡s especÃ­fica la implementaciÃ³n de cada una de las partes que componen el plugin VST. 3.2. ImplementaciÃ³n general del plugin VST Virtual Studio Technology (TecnologÃ­a de Estudio Virtual) o VST es una interfaz estÃ¡ndar desarrollada por la empresa alemana Steinberg para conectar sintetizadores de audio y plugins de efectos a editores de audio y sistemas de grabaciÃ³n [17]. Para poder programar plugins para las aplicaciones de audio, la empresa Steinberg creÃ³ un kit de desarrollo para plugins VST llamado VST SDK. Es un paquete para programadores, con documentaciÃ³n, ejemplos y un conjunto amplio de librerÃ­as para desarrollar plugins VST. De forma mÃ¡s especÃ­fica el VST SDK es un conjunto de clases hechas en el lenguaje de programaciÃ³n C++. Este SDK puede ser descargado desde la web de Steinberg [18]. 47 Un plugin VST es un proceso de audio que debe ser ejecutado mediante una aplicaciÃ³n que soporte esta tecnologÃ­a. A esta aplicaciÃ³n se le llama VST Host, que maneja el flujo de audio y hace uso del proceso del plugin VST como un complemento, en este caso se utiliza el software DAW Cakewalk SONAR 8.5 (ver Figura 3.1). De forma general, el plugin VST puede tomar un flujo de datos de audio, aplicarle un procesamiento y enviar el resultado a la aplicaciÃ³n host. Los plugins VST normalmente hacen su procesamiento usando el CPU de la computadora, sin embargo en este caso se hace uso del GPU para realizar dicho procesamiento. Desde el punto de vista de la aplicaciÃ³n host el plugin VST es una caja negra con una cantidad arbitraria de entradas y salidas, ya sean MIDI (ver glosario) o de audio y sus parÃ¡metros asociados. La aplicaciÃ³n host no necesita saber de quÃ© forma el plugin procesa los datos para saber si puede usar el plugin o no. Figura 3.1: Software Cakewalk SONAR 8.5 (host) con un plugin VST activo. El proceso que hace el plugin tiene una cantidad determinada de parÃ¡metros que pueden ser mostrados a travÃ©s de una interfaz grÃ¡fica contenida en una ventana de la aplicaciÃ³n host. Estos parÃ¡metros se pueden manipular de forma manual o automÃ¡tica si la aplicaciÃ³n host lo permite. 48 La tecnologÃ­a VST es compatible con los sistemas operativos Windows y Mac OS. En este caso (Windows) el plugin VST es un archivo DLL (Dynamic Link Library) capaz de ejecutarse en la mayorÃ­a de las aplicaciones DAW. Como estos archivos binarios son dependientes de la plataforma donde se ejecutan, un plugin VST compilado para Mac OS no funcionarÃ¡ en Windows y recÃ­procamente. El sistema de procesamiento de audio utilizado en la mayorÃ­a de las aplicaciones DAW es de procesamiento por bloques. La forma de procesamiento de un plugin VST es un ejemplo de ello. El flujo de audio es segmentado en una serie de bloques donde la aplicaciÃ³n host le provee al plugin VST los bloques de forma secuencial. En el programa host se determina de quÃ© tamaÃ±o van a ser los bloques de audio a procesar. Cada muestra de audio es acumulada hasta obtener la cantidad definida por bloque de procesamiento que generalmente va del rango de 64 muestras (baja latencia) a 8192 muestras (alta latencia), comÃºnmente siendo potencia de 2, para luego ser pasada a una funciÃ³n predefinida en el plugin VST que se encarga de hacer los cÃ¡lculos correspondientes. Para que este sistema de procesamiento de audio en tiempo real funcione correctamente el cÃ¡lculo de los datos tiene que ser rÃ¡pido. Por ejemplo, un bloque de datos de 512 muestras con una frecuencia de muestreo de 44.100 muestras por segundo tiene que ser procesado a lo sumo en 11,6 milisegundos. Este estimado de tiempo se calcula del siguiente modo: 1 ğ‘“ğ‘  â‹… ğ‘‘ğ¹ğ‘“ğµğ‘§ â‹… 1.000 ğ‘šğ‘ . (ğ‘¬ğ’„.ğŸ‘.ğŸ) donde ğ‘“ğ‘  es la frecuencia de muestreo y ğ‘‘ğ¹ğ‘“ğµğ‘§ el nÃºmero de muestras del bloque de datos de audio a procesar. Si el procesamiento requiere mÃ¡s tiempo que este el sistema se cargarÃ­a a su mÃ¡xima capacidad (exceso de uso de CPU) y ocurrirÃ­an fallas de procesamiento o suspensiÃ³n del sistema de reproducciÃ³n de audio. Los datos de audio estÃ¡n dados como un arreglo de nÃºmeros de punto flotante, con magnitudes en el rango de -1.0 a +1.0. Cada nÃºmero representa la amplitud de una muestra de seÃ±al de audio. En la prÃ³xima secciÃ³n se explica con detalle cÃ³mo estÃ¡ conformado el plugin VST a nivel de cÃ³digo, su estructura de clases y mÃ©todos asociados. 49 3.3. Estructura de clases del plugin VST Para desarrollar el plugin, este debe ser implementado como un conjunto de clases. Ya que el VST SDK provee un conjunto de clases predefinidas no es necesario construir todas las estructuras desde el principio. La estructura interna del plugin VST estÃ¡ dividida en varias secciones como se muestra a continuaciÃ³n. Clase GPUAFXProcessor: Hereda de la clase AudioEffect definida en el VST SDK para crear un nuevo efecto de audio (ver Figura 3.2 y Figura 3.8). Es donde se inicializan y ajustan los parÃ¡metros principales del plugin VST, como la inicializaciÃ³n del bÃºfer de audio, el tamaÃ±o de bloque de audio, la tasa de muestreo, el mÃ©todo en donde se realiza el procesamiento del audio, etc. Ademas de esto, instancia un objeto de la clase CUDA que es un thread de Windows se encarga del pase de parÃ¡metros y la comunicaciÃ³n con el GPU a travÃ©s de CUDA. Figura 3.2: Diagrama de clases de la clase GPUAFXProcessor. Esta clase tiene constructor y destructor; el mÃ©todo createInstance() se encarga de instanciar el efecto de audio y el mÃ©todo initialize() inicializa las variables que se vayan a utilizar en la instancia del efecto de audio. El mÃ©todo setBusArrangements() se encarga de definir cuÃ¡ntos canales de audio van a ser procesados (mono o estÃ©reo). El mÃ©todo setupProcessing() inicializa los bÃºferes de audio, tamaÃ±o de bloque y frecuencia de muestreo segÃºn como se hayan definido en la aplicaciÃ³n host. GPUAFXProcessor +currentProcessMode: int32 +init: bool +blockbytes: int +paramsValues: float* <<create>>-GPUAFXProcessor() <<destroy>>-GPUAFXProcessor() +initialize(context: FUnknown*): tresult PLUGIN_API +setActive(state: TBool): tresult PLUGIN_API +setBusArrangements(inputs: SpeakerArrangement*, numIns: int32, outputs: SpeakerArrangement*, numOuts: int32): tresult PLUGIN_API +process(data: ProcessData&): tresult PLUGIN_API +setupProcessing(newSetup: ProcessSetup&): tresult PLUGIN_API +createInstance(: void*): FUnknown* AudioEffect 50 Finalmente, el mÃ©todo process() se encarga de ejecutar todos los cÃ¡lculos que sean definidos para transformar los bÃºferes de audio y eviarlos de vuelta a la aplicaciÃ³n host. Clase Thread: Se utiliza para crear un thread de Windows para procesar el audio con CUDA (ver Figura 3.3 y Figura 3.8). Cada vez que el Sistema Operativo solicita el CPU para cederselo a un hilo de otro proceso, los registros y la pila (es decir, el contexto del hilo) contienen unos valores determinados. El Sistema Operativo guarda todos esos datos en cada cambio, de modo que al volver a conmutar el hilo inicial, pueda restaurar el contexto en el que quedo anteriormente. Si el procesamiento no se hace de este modo, los contextos se destruirÃ­an al terminarse cada llamada a la funciÃ³n audioProcessing() de CUDA, ya que el tiempo de vida de una funciÃ³n es solo durante su llamada y no a lo largo de toda la ejecuciÃ³n del proceso. En cambio con la creaciÃ³n del thread mantenemos todas las asignaciones de memoria y estados de CUDA. Figura 3.3: Diagrama de clases de la clase Thread. Esta clase tiene su constructor y destructor, el mÃ©todo create() se encarga de instanciar un thread de Windows, el mÃ©todo run() ejecuta las operaciones definidas en el thread (llamadas a funciones, etc.). Finalmente el mÃ©todo start() inicia el thread una vez creado. Thread -m_hThread: HANDLE -m_threadId: unsigned int -m_canRun: bool <<create>>-Thread() <<destroy>>-Thread() +create(stackSize: unsigned int): bool +start(): void +run(): void 51 Clase CUDA: Hereda de la clase Thread y es instanciada por la clase GPUAFXProcessor. Es donde se mandan a ejecutar todos los cÃ¡lculos de los algoritmos de audio (ver Figura 3.4 y Figura 3.8). Para ello se han creado kernels para cada efecto de audio. En esta clase tambiÃ©n se hace el manejo del orden de los efectos y la sincronizaciÃ³n con el GPU. Figura 3.4: Diagrama de clases de la clase CUDA. Esta clase tiene constructor y destructor; el mÃ©todo initDevice() se encarga de inicializar los parÃ¡metros del device de CUDA, como por ejemplo el identificador del device, las banderas de estados como fijar el mapeo de memoria, etc. El mÃ©todo setDelayBufer() se encarga de inicializar en el device el bÃºfer de memoria para el efecto delay; setDevice() determina la cantidad de bloques e hilos a utilizar por los kernels de CUDA; ademÃ¡s de esto, inicializa todos los bÃºferes de audio a utilizar por el GPU. El mÃ©todo run() heredado de la clase Thread contiene la llamada al mÃ©todo audioProcessing() que es el que se encarga de ejecutar los kernels de los efectos de audio, asÃ­ como administrar los pases de parÃ¡metros a los kernels y la activaciÃ³n o desactivaciÃ³n de los efectos. Thread -m_hThread: HANDLE -m_threadId: unsigned int -m_canRun: bool <<create>>-Thread() <<destroy>>-Thread() +create(stackSize: unsigned int): bool +start(): void +run(): void CUDA <<create>>-CUDA() <<destroy>>-CUDA() +InitDevice(): void +SetDevice(): void +SetDelayBuffer(): void +AudioCUDA(): void -run(): void 52 Clase KnobParameter: Hereda de la clase Parameter del VST SDK y es instanciada pro la clase GPUAFXBaseController (ver Figura 3.5 y Figura 3.8). En esta clase se definen los parÃ¡metros (perillas y switches) de cada efecto para poder manipularlos de forma independiente. Figura 3.5: Diagrama de clases de la clase KnobParameter. Esta clase tiene un constructor para definir sus parÃ¡metros por defecto (identificador, nombre, valor por defecto). El mÃ©todo toString() se encarga de enviar una cadena de texto que identifica a cada parÃ¡metro con su rango de valores especÃ­fico. Clase GPUAFXBaseController: Hereda de la clase EditController y VST3EditDelegate que forman parte del VST SDK. En esta clase se asocian los identificadores de cada parÃ¡metro con su valor respectivo instanciando a la clase KnobParameter por la cantidad de parÃ¡metros preestablecidos (ver Figura 3.6 y Figura 3.8). TambiÃ©n gestiona la forma en que se van a desplegar los elementos de la interfaz grÃ¡fica creada. Figura 3.6: Diagrama de clases de la clase GPUAFXBaseController. KnobParameter <<create>>-KnobParameter(flags: int32, id: int32, title: TChar*) +toString(normValue: ParamValue, string: String128): void Parameter GPUAFXBaseController +initialize(context: FUnknown*): tresult PLUGIN_API +setComponentState(state: IBStream*): tresult PLUGIN_API +setState(state: IBStream*): tresult PLUGIN_API +getState(state: IBStream*): tresult PLUGIN_API VST3EditorDelegate EditController 53 El mÃ©todo initialize() se encarga de instanciar los parÃ¡metros que se crean de la clase KnobParameter y guardar dichas instancias en un arreglo de parÃ¡metros para ser posteriormente utilizados por la clase GPUAFXProcessor (ver Figura 3.8). El mÃ©todo setComponentState() se encarga de guardar el estado de los parÃ¡metros instanciados en un preset definido por el usuario desde la aplicaciÃ³n host. El mÃ©todo setState() guarda los valores asociados a los parÃ¡metros instanciados y getState() carga dichos valores. Clase GPUAFXController: Hereda de la clase GPUAFXBaseController. Esta clase (ver Figura 3.7 y Figura 3.8) se encarga de crear las vistas de la interfaz grÃ¡fica de usuario asÃ­ como crear subcontroladores que se pueden asignar a los parÃ¡metros para un comportamiento personalizado. Figura 3.7: Diagrama de clases de la clase GPUAFXController. El mÃ©todo initialize() se encarga de inicializar los parÃ¡metros que pueden ser usados en el mÃ©todo createView() o createSubController() (ver Figura 3.8). El mÃ©todo createView() define la vista general de la interfaz grÃ¡fica de usuario, en este mÃ©todo se hace la carga del archivo XML que contiene la estructura y parÃ¡metros de la vista. El mÃ©todo createSubController() sirve para dar un comportamiento personalizado (tooltips, cambios de texturas, color, tipo de fuente de texto, etc.) a los parÃ¡metros que se le asignen el uso de esta funciÃ³n. GPUAFXBaseController +initialize(context: FUnknown*): tresult PLUGIN_API +setComponentState(state: IBStream*): tresult PLUGIN_API +setState(state: IBStream*): tresult PLUGIN_API +getState(state: IBStream*): tresult PLUGIN_API GPUAFXController <<create>>-GPUAFXController() +initialize(context: FUnknown*): tresult PLUGIN_API +createView(name: FIDString): IPlugView* PLUGIN_API +createSubController(name: char, description: IUIDescription, editor: VST3Editor): IController* 54 Figura 3.8: Diagrama de clases del proyecto GPUAFX. CUDA +threads: int +blocks: int +blockbytes: int +sampleRate: float +audioStream: cudaStream_t +strt: cudaEvent_t +stp: cudaEvent_t +time: float +timer1Total: float +timer2Total: float +IOBufferLGPU: float +IOBufferRGPU: float +IOBufferLVibGPU: float +IOBufferRVibGPU: float +delayBufferL: float +delayBufferR: float +plan: cufftHandle +plan2: cufftHandle +IOBufferZcpy: float +IOBufferZGPU: float +IOBufferLVibcpyEQ: float +IOBufferLVibGPUEQ: float +complexData: cufftComplex +complexSize: int +FFTNorm: float +IOBufferLcpy: float +IOBufferRcpy: float +IOBufferLVibcpy: float +IOBufferRVibcpy: float +lfoCounter: unsigned int +lfoCounter2: unsigned int +hEvent: HANDLE +StartEvent: HANDLE +cudaEvent: HANDLE +IOBufferLout: float +IOBufferRout: float +sampleFramesParam: int +paramValues: float +init: bool <<create>>-CUDA() <<destroy>>-CUDA() +InitDevice(): void +SetDevice(): void +SetDelayBuffer(): void +AudioCUDA(): void -run(): void GPUAFXBaseController +initialize(context: FUnknown*): tresult PLUGIN_API +setComponentState(state: IBStream*): tresult PLUGIN_API +setState(state: IBStream*): tresult PLUGIN_API +getState(state: IBStream*): tresult PLUGIN_API GPUAFXController <<create>>-GPUAFXController() +initialize(context: FUnknown*): tresult PLUGIN_API +createView(name: FIDString): IPlugView* PLUGIN_API +createSubController(name: char, description: IUIDescription, editor: VST3Editor): IController* KnobParameter <<create>>-KnobParameter(flags: int32, id: int32, title: TChar*) +toString(normValue: ParamValue, string: String128): void Thread -m_hThread: HANDLE -m_threadId: unsigned int -m_canRun: bool <<create>>-Thread() <<destroy>>-Thread() +create(stackSize: unsigned int): bool +start(): void +run(): void GPUAFXProcessor +currentProcessMode: int32 +init: bool +blockbytes: int +paramsValues: float* <<create>>-GPUAFXProcessor() <<destroy>>-GPUAFXProcessor() +initialize(context: FUnknown*): tresult PLUGIN_API +setActive(state: TBool): tresult PLUGIN_API +setBusArrangements(inputs: SpeakerArrangement*, numIns: int32, outputs: SpeakerArrangement*, numOuts: int32): tresult PLUGIN_API +process(data: ProcessData&): tresult PLUGIN_API +setupProcessing(newSetup: ProcessSetup&): tresult PLUGIN_API +createInstance(: void*): FUnknown* AudioEffect ParameterVST3EditorDelegate EditController 11 audioThread 1 1 1 * 55 En la siguiente secciÃ³n se explica cÃ³mo se realizÃ³ la implementaciÃ³n de la interfaz grÃ¡fica de usuario con la librerÃ­a VSTGUI. 3.3. ImplementaciÃ³n del GUI VSTGUI es una librerÃ­a multi-plataforma para C++ diseÃ±ada principalmente para crear interfaces grÃ¡ficas de usuario para plugins de audio. Esta herramienta grÃ¡fica proporciona manejo de ventanas y gestiÃ³n de eventos de usuario para poder interactuar con la interfaz creada. Funciona en los sistemas operativos Windows y Mac OSX. Entre los objetos mÃ¡s comunes de esta librerÃ­a estÃ¡n los contenedores de vista, perillas, sliders y switches. Estos crean la sensaciÃ³n de estar interactuando con interfaces de audio reales logrando que el trabajo sea mÃ¡s natural para el usuario. Las interfaces se pueden construir con implementaciones especÃ­ficas de los objetos de la librerÃ­a en C++ o describiendo la estructura de la interfaz de audio en archivos XML. Para la implementaciÃ³n del GUI, se creÃ³ un contenedor de efectos ordenados, con una barra de desplazamiento vertical para poder desplazarse entre los efectos. La disposiciÃ³n de los objetos dentro del GUI estÃ¡ definida en un archivo XML. En este archivo se declaran los objetos con su etiqueta correspondiente y de forma jerÃ¡rquica. Adicionalmente el VSTGUI tiene la opciÃ³n de habilitar un editor visual para poder realizar cambios de forma mÃ¡s sencilla. Para la creaciÃ³n de las interfaces, se utilizaron las siguientes clases de la librerÃ­a: CAnimKnob: Es la clase con la cual se implementan las perillas de cada efecto, con las cuales se pueden ajustar los parÃ¡metros de dichos efectos y asÃ­ percibir los cambios auditivamente. Su funciÃ³n es intercambiar una serie de mapas de bits en secuencia para crear un efecto de perilla animada al momento de posicionar el mouse encima, tambiÃ©n envÃ­a a la clase GPUAFXProcessor el valor del parÃ¡metro actual el cual esta normalizado entre 0 y 1. COnOffButton: Es la clase con la cual se implementa el encendido o apagado de cada efecto. Su funciÃ³n es intercambiar dos mapas de bits segÃºn el estado que tenga al hacer click, asÃ­ como su valor que puede ser de 0 o 1. 56 CViewContainer: Es la clase que contiene los objetos con los cuales el usuario interactÃºa (perillas, switches, sliders, etc.). Esta clase puede tener un mapa de bits de fondo. CScrollView: Esta clase hereda de la clase CViewContainer y su funciÃ³n es ser un contenedor pero ademÃ¡s tener un slider para poder navegar en dicho contenedor. Es sumamente Ãºtil cuando la cantidad de objetos es considerable y se necesita navegar a travÃ©s de ellos. Para cada efecto se implementÃ³ el uso de CAnimKnob para el manejo de las perillas y un COnOffButton para activar o desactivar el efecto. Estos objetos estÃ¡n contenidos en un objeto CScrollView que hereda de la clase CViewContainer (ver Figura 3.9). Figura 3.9: Contenedor de efectos del plugin GPUAFX y sus clases de GUI asociadas. En la siguiente secciÃ³n se muestran los detalles de implementaciÃ³n de cada uno de los efectos propuestos para la soluciÃ³n, los parÃ¡metros de cada efecto y su algoritmo en pseudocÃ³digo. CAnimKnob COnOffButton CScrollView CViewContainer 57 3.4. ImplementaciÃ³n de los efectos de audio Las implementaciones en CPU se hicieron de forma secuencial mientras que en GPU se hicieron de forma paralela. En algunos casos, para adaptar algunos algoritmos de forma paralela se tuvo que hacer ajustes en la forma en que se procesan, incluso haciendo uso de mÃ¡s de un kernel de CUDA para llevar a cabo la implementaciÃ³n. 3.4.1. Over Drive El algoritmo consiste en hacer pasar la seÃ±al por una funciÃ³n de transferencia no lineal (ver Figura 3.10), lo cual cambia la forma de la seÃ±al, es decir, distorsionarÃ¡ la seÃ±al de audio. La nueva forma de la seÃ±al dependerÃ¡ de la forma de la funciÃ³n de transferencia: ğ‘“(ğ‘¥) = ï¿½âˆ’âˆšâˆ’ğ‘¥, ğ‘¥ â‰¤ 0 âˆšğ‘¥, ğ‘¥ > 0 (ğ‘¬ğ’„.ğŸ‘.ğŸ) ğ‘¥(ğ‘›) ğ‘¦(ğ‘›) Figura 3.10: Diagrama de bloques del efecto Over Drive. Los parÃ¡metros de este efecto son (ver Figura 3.11): ï‚§ Ganancia (Gain): Es la cantidad de saturaciÃ³n que se le aplica a la seÃ±al de entrada. ï‚§ Nivel de salida (Level): Se define como la cantidad de volumen que tendrÃ¡ el efecto. ğ‘“(ğ‘¥(ğ‘›)) FunciÃ³n de Transferencia 58 Figura 3.11: Interfaz grÃ¡fica del efecto Over Drive. El Algoritmo 1 corresponde a la versiÃ³n secuencial para CPU del efecto Over Drive. Para la implementaciÃ³n en CUDA se elimina el ciclo para, ya que todo el cÃ³digo de la funciÃ³n tiene que ser ejecutado de forma paralela por cada hilo asignado al kernel de CUDA de este efecto. Algoritmo 1 Algoritmo del efecto Over Drive 1: funciÃ³n OverDrive(array in, array out, int N, float gain, float lvl) 2: 3: float TFResult 4: 5: para int i 0 hasta i < N hacer 6: 7: si in[i] > 0 entonces 8: TFResult sqrtf(in[i]) 9: si no 10: TFResult -sqrtf(-in[i]) 11: fin si 12: 13: out[i] (lvl â‹… 2) â‹… (gain â‹… (TFResult - in[i]) + in[i]) 14: 15: fin para 16: fin funciÃ³n 59 En el Algoritmo 1, lÃ­nea 3, se encuentra la variable TFResult, la cual va a contener el resultado del cÃ¡lculo de la funciÃ³n de transferencia (ver Figura 3.12) aplicada a la seÃ±al de entrada. Luego en las lÃ­neas 7-11 para cada elemento del arreglo de entrada in se calcula la relaciÃ³n que tiene su valor con respecto a la funciÃ³n de transferencia âˆšğ‘¥ siempre y cuando su valor sea mayor a 0, de lo contrario se calcula con la funciÃ³n de transferencia âˆ’âˆšâˆ’ğ‘¥. Una vez obtenido el resultado, en la lÃ­nea 13 se procede a determinar la proporciÃ³n de mezcla entre la seÃ±al original y la seÃ±al procesada por la funciÃ³n de transferencia con la variable gain. Con la variable lvl se define la cantidad de volumen del efecto. Figura 3.12: FunciÃ³n de transferencia del efecto Over Drive. 60 3.4.2. Distortion Al igual que el Over Drive, el algoritmo consiste en hacer pasar la seÃ±al por una funciÃ³n de transferencia no lineal (ver Figura 3.13), pero en este caso la forma de la seÃ±al cambiarÃ¡ de un modo mÃ¡s brusco. La nueva forma de la seÃ±al dependerÃ¡ de la forma de la funciÃ³n de transferencia tangente hiperbÃ³lica (tanh(ğ‘¥)). ğ‘¥(ğ‘›) ğ‘¦(ğ‘›) Figura 3.13: Diagrama de bloques del efecto Distortion. Los parÃ¡metros generales de este efecto son (ver Figura 3.14): ï‚§ Ganancia (Gain): Es la cantidad de saturaciÃ³n que se le aplica a la seÃ±al de entrada. ï‚§ Nivel de salida (Level): Se define como la cantidad de volumen que tendrÃ¡ el efecto. Figura 3.14: Interfaz grÃ¡fica del efecto Distortion. ğ‘“(ğ‘¥(ğ‘›)) FunciÃ³n de Transferencia 61 El Algoritmo 2 corresponde a la versiÃ³n secuencial para CPU del efecto Distortion. Para la implementaciÃ³n en CUDA se elimina el ciclo para, ya que todo el cÃ³digo de la funciÃ³n tiene que ser ejecutado de forma paralela por cada hilo asignado al kernel de CUDA de este efecto. Algoritmo 2 Algoritmo del efecto Distortion 1: funciÃ³n Distortion(array in, array out, int N, float gain, float lvl) 2: 3: float TFResult 4: 5: para int i 0 hasta i < N hacer 6: 7: TFResult 0.8 â‹… tanh((gain â‹… 1023 + 1) â‹… in[i]) 8: 9: out[i] (lvl â‹… 2) â‹… (gain â‹… (TFResult - in[i]) + in[i]) 10: 11: fin para 12: fin funciÃ³n En el Algoritmo 2, lÃ­nea 3, se encuentra la variable TFResult, la cual va a contener el resultado del cÃ¡lculo de la funciÃ³n de transferencia aplicada a la seÃ±al de entrada. Luego para cada elemento del arreglo de entrada in se calcula la relaciÃ³n que tiene su valor con respecto a la funciÃ³n de transferencia tanh(x). La variable gain define la forma de la funciÃ³n de transferencia, a medida que incrementa su valor, la pendiente de la recta tangente a la funciÃ³n en el punto cero tiende hacia el infinito (ver Figura 3.15). Una vez obtenido el resultado, en la lÃ­nea 9 se procede a determinar la proporciÃ³n de mezcla entre la seÃ±al original y la seÃ±al procesada por la funciÃ³n de transferencia con la variable gain. Con la variable lvl se define la cantidad de volumen del efecto. 62 Figura 3.15: FunciÃ³n de transferencia del efecto Distortion. 63 3.4.3. Ecualizador El algoritmo del ecualizador consiste en tomar la seÃ±al y aplicarle la Transformada de Fourier, una vez en el dominio de la frecuencia se puede multiplicar por un conjunto de filtros con sus niveles de ganancia respectivos y asÃ­ determinar la nueva forma de la seÃ±al. Luego de esto se procede a hacer la Transformada Inversa de Fourier de la seÃ±al modificada para regresarla al dominio del tiempo (ver Figura 3.16). Figura 3.16: Diagrama de bloques del Ecualizador. Los parÃ¡metros que se pueden modificar en un ecualizador son (ver Figura 3.17): ï‚§ Frecuencias Bajas (Low): Es el parÃ¡metro que modifica el rango de ganancia de las frecuencias bajas. ï‚§ Frecuencias Medias (Mid): Es el parÃ¡metro que modifica el rango de ganancia de las frecuencias medias. ï‚§ Frecuencias Altas (High): Es el parÃ¡metro que modifica el rango de ganancia de las frecuencias altas. ğ¿ğ¿ ğ¹ğ¹ğ¹ğ¹ğ¹ğ¹ ğ‘”ğ‘”ğ¹ğ‘› ğ‘‘ğ¿/ğ‘‘ğµ ğ¹ğ¹ğ¹ğ¹ğ¹ğ¹ ğ‘”ğ‘”ğ¹ğ‘› ğ»ğ¿ ğ¹ğ¹ğ¹ğ¹ğ¹ğ¹ ğ‘”ğ‘”ğ¹ğ‘› ğ‘¥(ğ‘›) ğ‘¦(ğ‘›) ğ¹ğ¹ğ¹ ğ¼ğ¹ğ¹ğ¹ 64 Figura 3.17: Interfaz grÃ¡fica del efecto Ecualizador. El Algoritmo 3 corresponde a la versiÃ³n secuencial para CPU del efecto Ecualizador. Para la implementaciÃ³n en CUDA se elimina el ciclo para, ya que todo el cÃ³digo de la funciÃ³n tiene que ser ejecutado de forma paralela por cada hilo asignado al kernel de CUDA de este efecto. Algoritmo 3 Algoritmo del efecto Ecualizador 1: funciÃ³n EQ(array2D inComplex, array2D outComplex, int N, float scale, 2: float low, float mid, float high) 3: 4: float mag, magin, phi, p, q 5: float dB2lin 0,1155 6: 7: low exp(((low â‹… 48) â€“ 24) â‹… dB2lin) 8: mid exp(((mid â‹… 48) â€“ 24) â‹… dB2lin) 9: high exp(((high â‹… 48) â€“ 24) â‹… dB2lin) 10: 11: para int i 0 hasta i < N hacer 12: magin sqrt(inComplex.x â‹… inComplex.x + inComplex.y â‹… inComplex.y) 13: 14: si i < (N/128) entonces 15: mag low â‹… magin 16: si i >= (N/128) Y i < (N/16) entonces 17: q (mid-low) / (N/16 â€“ N/128) 18: p low + (i â€“ N/128) â‹… q 19: mag p â‹… magin 20: si i >= (N/16) Y i < (N/4) entonces 21: q (high-mid) / (N/4 â€“ N/16) 22: p mid + (i â€“ N/16) â‹… q 65 23: mag p â‹… magin 24: si no 25: mag high â‹… magin 26: fin si 27: 28: phi atan2(inComplex.y, inComplex.x) 29: 30: outComplex.x scale â‹… (mag â‹… cos(phi)) 31: outComplex.y scale â‹… (mag â‹… sin(phi)) 32: 33: fin para 34: fin funciÃ³n Algoritmo 4 EjecuciÃ³n del algoritmo del efecto Ecualizador 1: FFT(in, inComplex, N) 2: EQ(inComplex, outComplex, N, scale, low, mid, high) 3: IFFT(outComplex, out, N) En el Algoritmo 3, lÃ­neas 7-9, los parÃ¡metros low, mid y high que estÃ¡n en el rango de -24 a +24 dB son transformados de escala en decibeles a escala lineal con la fÃ³rmula ğ¹ğ‘¥â‹…0,1155 [3]. Luego de esto se procede a manipular cada muestra de la entrada. En la lÃ­nea 12 se calcula la magnitud de cada muestra con la fÃ³rmula ï¿½ğ‘¥2 + ğ‘¦2. Seguidamente en las lÃ­neas 14-26 se calcula el valor de magnitud de cada banda de frecuencia con una interpolaciÃ³n lineal entre los puntos centrales definidos para cada banda, estos se multiplican por la magnitud de la entrada para obtener la nueva respuesta en frecuencia de cada muestra. Posteriormente en la lÃ­nea 28 se determina la fase de cada elemento con la fÃ³rmula ğ‘”ğ¹ğ‘”ğ‘›2(ğ‘¦, ğ‘¥), que es el cÃ¡lculo del arcotangente en el plano complejo. Luego en la lÃ­nea 30 y 31 se convierten los datos de forma polar a forma rectangular normalizados con el parÃ¡metro scale que es la unidad dividida entre el nÃºmero de elementos a procesar N. El Algoritmo 4 comienza convirtiendo la seÃ±al de entrada in del dominio del tiempo al dominio de la frecuencia con la Transformada RÃ¡pida de Fourier (FFT). Esto da como resultado un arreglo de nÃºmeros complejos inComplex el cual se utiliza para realizar los cÃ¡lculos correspondientes de ecualizaciÃ³n. Se llama a la funciÃ³n EQ descrita en el Algoritmo 3 y finalmente se hace la Transformada Inversa de Fourier para regresar los datos resultantes al dominio del tiempo y envÃ­alos al bÃºfer de salida. Para realizar la FFT en CPU se usÃ³ la librerÃ­a FFTW [19] y para GPU se utilizÃ³ la librerÃ­a CUFFT [16] que forma parte de CUDA. 66 3.4.4. Vibrato Para implementar este efecto se necesita controlar la cantidad de retardo de lÃ­nea (generalmente entre 5 ms. y 10 ms. de delay) mediante un LFO (siglas de Low Frequency Oscilator), del cual se tiene control de su amplitud y frecuencia. La amplitud del Vibrato determinarÃ¡ cuÃ¡l es el retardo inicial sobre el cual oscilarÃ¡ la seÃ±al sinusoidal (ver Figura 3.18). ğ‘¥(ğ‘›) ğ‘¦(ğ‘›) Figura 3.18: Diagrama de bloques del efecto Vibrato. Los parÃ¡metros del efecto Vibrato son (ver Figura 3.19): ï‚§ Frecuencia (Rate): Es la velocidad con la cual se hace la variaciÃ³n de tono del Vibrato. Generalmente la frecuencia del LFO oscila entre 0 Hz. y 10 Hz. ï‚§ Profundidad (Depth): Se define como la intensidad de Vibrato que se le aplica a la seÃ±al. ï‚§ ProporciÃ³n de Mezcla (Mix): Es la cantidad de efecto Vibrato que se le aplica a la seÃ±al original. Figura 3.19: Interfaz grÃ¡fica del efecto Vibrato. ğ·ğ¹ğ¹ğ‘”ğ‘¦ ğ¹ğ¹ğ‘›ğ¹ ğ¿ğ¹ğ¿ ğ¾ 67 El Algoritmo 5 corresponde a la versiÃ³n secuencial para CPU del efecto Vibrato. Para la implementaciÃ³n en CUDA se elimina el ciclo para, ya que todo el cÃ³digo de la funciÃ³n tiene que ser ejecutado de forma paralela por cada hilo asignado al kernel de CUDA de este efecto. Algoritmo 5 Algoritmo del efecto Vibrato 1: funciÃ³n Vibrato(array in, array out, array oldIn, int N, int counter, 2: int sampleRate, float rate, float depth, float mix) 3: 4: float TWOPI 6,283185307 5: float modifier, index, frac, next 6: int indexInt 7: 8: rate rate â‹… 10 9: 10: float t (TWOPI â‹… rate) / sampleRate 11: 12: para int i 0 hasta i < N hacer 13: 14: modifier depth â‹… sin(t â‹… (N â‹… counter + i)) 15: index i + (modifier â‹… N/2 + N/2) 16: indexInt index 17: frac index â€“ indexInt 18: next oldIn[i+1] 19: 20: out[i] (1 - mix) â‹… in[i] + mix â‹… (oldIn[i] + frac â‹… (next â€“ oldIn[i])) 21: 22: fin para 23: fin funciÃ³n 24: 25: counter counter + 1 En el Algoritmo 5, lÃ­nea 8, se encuentra la variable rate que es multiplicada por 10, ya que las oscilaciones del modificador de este efecto es de mÃ¡ximo 10 Hz. La variable t es la proporciÃ³n de cambio de fase de la onda senoidal que se va a generar para manipular el efecto. En la lÃ­nea 14, en la variable modifier, se calcula el valor de la onda seno que se va a utilizar para recorrer el Ã­ndice del arreglo oldIn, el cual es un arreglo de tamaÃ±o 2 â‹… ğ‘ que contiene el bloque de audio anterior y actual. Como el Ã­ndice de la onda seno tiene que estar dentro del rango del tamaÃ±o del arreglo oldIn en la lÃ­nea 15 se calcula en la variable index un valor que estÃ© dentro de este rango. 68 En la lÃ­nea 16 se calcula la parte entera del Ã­ndice y en la siguiente lÃ­nea el resto que queda de la variable index. Se calcula la siguiente posiciÃ³n en el Ã­ndice del arreglo oldIn que se utilizarÃ¡ para calcular una interpolaciÃ³n lineal entre la posiciÃ³n actual del Ã­ndice y la siguiente posiciÃ³n. En la lÃ­nea 20 se calcula la proporciÃ³n de mezcla de la seÃ±al original con respecto a la seÃ±al modificada por el efecto. Finalmente al terminar la funciÃ³n en la lÃ­nea 25 se suma 1 a la variable counter, con el propÃ³sito de poder seguir la forma de onda del modificador para el siguiente bloque de audio a procesar. 3.4.5. Chorus Este efecto se consigue mezclando la seÃ±al original con una copia ligeramente retardada y modulada de la misma (entre 10 ms. y 30 ms. de delay) donde el tiempo del retardo varÃ­a de forma constante a travÃ©s de un LFO (ver Figura 3.20). Este retraso es muy corto asÃ­ como su modulaciÃ³n. Ambos crean el efecto de coro que logra a su vez un efecto de engrandecimiento en el sonido. ğ‘¥(ğ‘›) ğ‘¦(ğ‘›) Figura 3.20: Diagrama de bloques del efecto Chorus. Los parÃ¡metros del efecto Chorus son (ver Figura 3.21): ï‚§ Frecuencia (Rate): Es la velocidad con la cual se hace la variaciÃ³n de tono del Chorus. Generalmente la frecuencia del LFO oscila entre 0 Hz. y 2 Hz. ï‚§ Profundidad (Depth): Este parÃ¡metro modifica el tiempo total de retraso que varÃ­a durante el tiempo. ï‚§ ProporciÃ³n de Mezcla (Mix): Es la cantidad de efecto Chorus que se le aplica a la seÃ±al original. ğ¿ğ¹ğ¿ ğ·ğ¹ğ¹ğ‘”ğ‘¦ ğ¹ğ¹ğ‘›ğ¹ ğ¾ + ğ·ğ¹ğ‘¦/ğ‘Šğ¹ğ¹ ğ‘€ğ¹ğ‘¥ 69 Figura 3.21: Interfaz grÃ¡fica del efecto Chorus. El Algoritmo 6 corresponde a la versiÃ³n secuencial para CPU del efecto Chorus, que es muy similar al Algoritmo 5 del efecto Vibrato, ya que es un efecto derivado de este. La diferencia con respecto al efecto Vibrato es el rango de las oscilaciones por segundo que tiene el efecto Chorus y la suma de la seÃ±al original con la seÃ±al procesada en la mezcla final. Para la implementaciÃ³n en CUDA se elimina el ciclo para, ya que toda la funciÃ³n tiene que ser ejecutada de forma paralela por cada hilo asignado al kernel de CUDA de este efecto. Algoritmo 6 Algoritmo del efecto Chorus 1: funciÃ³n Chorus(array in, array out, array oldIn, int N, int counter, 2: int sampleRate, float rate, float depth, float mix) 3: 4: float TWOPI 6,283185307 5: float modifier, index, frac, next 6: int indexInt 7: 8: rate rate â‹… 2 9: 10: float t (TWOPI â‹… rate) / sampleRate 11: 12: para int i 0 hasta i < N hacer 13: 14: modifier depth â‹… sin(t â‹… (N â‹… counter + i)) 15: index i + (modifier â‹… N/2 + N/2) 16: indexInt index 17: frac index â€“ indexInt 18: next oldIn[i+1] 70 19: 20: out[i] (1 - mix) â‹… in[i] + mix â‹… (in[i] + oldIn[i] + frac â‹… (next â€“ oldIn[i])) 21: 22: fin para 23: fin funciÃ³n 24: 25: counter counter + 1 En el Algoritmo 6, lÃ­nea 8, se encuentra la variable rate que es multiplicada por 2, ya que el nÃºmero de oscilaciones del modificador de este efecto es de mÃ¡ximo 2 Hz. La variable t indica la proporciÃ³n de cambio de fase de la onda senoidal que se va a generar para manipular el efecto. En la lÃ­nea 14 en la variable modifier se calcula el valor de la onda seno que se va a utilizar para recorrer el Ã­ndice del arreglo oldIn, el cual es un arreglo de tamaÃ±o 2 â‹… ğ‘ que contiene el bloque de audio anterior y el bloque de audio actual. Como el Ã­ndice de la onda seno tiene que estar dentro del rango del tamaÃ±o del arreglo oldIn, en la lÃ­nea 15 se calcula en la variable index un valor que este dentro de este rango. En la lÃ­nea 16 se calcula la parte entera del Ã­ndice, y en la siguiente lÃ­nea el resto que queda de la variable index. Se calcula la siguiente posiciÃ³n en el Ã­ndice del arreglo oldIn que se utilizarÃ¡ para calcular una interpolaciÃ³n lineal entre la posiciÃ³n actual del Ã­ndice y la siguiente posiciÃ³n. En la lÃ­nea 20 se calcula la proporciÃ³n de mezcla de la seÃ±al original con respecto a la suma de la seÃ±al original y la seÃ±al modificada por el efecto. Finalmente al terminar la funciÃ³n en la lÃ­nea 25 se suma 1 a la variable counter, con el propÃ³sito de poder seguir la forma de onda del modificador para el siguiente bloque de audio a procesar. 71 3.4.6. Ring Modulator El efecto de Ring Modulator consiste en multiplicar en el dominio del tiempo la seÃ±al de entrada con una seÃ±al portadora como por ejemplo una seÃ±al sinusoidal (ver Figura 3.22). El principio de esto radica en que la multiplicaciÃ³n de cualquier seÃ±al por otra seÃ±al genera una suma y diferencia de frecuencias que modifican la seÃ±al final. ğ‘¦(ğ‘›) ğ‘¥(ğ‘›) Figura 3.22: Diagrama de bloques del efecto Ring Modulator. Los parÃ¡metros generales de este efecto son (ver Figura 3.23): ï‚§ Frecuencia (Rate): Es la velocidad con la cual se repetirÃ¡n los ciclos de la seÃ±al sinusoidal portadora (de 20 Hz a 4 KHz). ï‚§ ProporciÃ³n de Mezcla (Mix): Se define como la intensidad del efecto que se le aplica a la seÃ±al original. Figura 3.23: Interfaz grÃ¡fica del efecto Ring Modulator. x ğ¿ğµğ‘‚ 72 El Algoritmo 7 corresponde a la versiÃ³n secuencial para CPU del efecto Ring Modulator. Para la implementaciÃ³n en CUDA se elimina el ciclo para, ya que todo el cÃ³digo de la funciÃ³n tiene que ser ejecutado de forma paralela por cada hilo asignado al kernel de CUDA de este efecto. Algoritmo 7 Algoritmo del efecto Ring Modulator 1: funciÃ³n RingMod(array in, array out, int N, int counter, int sampleRate, 2: float rate, float mix) 3: 4: float TWOPI 6,283185307 5: float modifier 6: 7: rate (rate â‹… 3980) + 20 8: 9: float t (TWOPI â‹… rate) / sampleRate 10: 11: para int i 0 hasta i < N hacer 12: 13: modifier sin(t â‹… (N â‹… counter + i)) 14: 15: out[i] (1 - mix) â‹… in[i] + mix â‹… (in[i] â‹… modifier) 16: 17: fin para 18: fin funciÃ³n 19: 20: counter counter + 1 En el Algoritmo 7, lÃ­nea 7, se encuentra la variable rate que estÃ¡ definida para el rango entre 20 Hz y 4000 Hz. La variable t indica la proporciÃ³n de cambio de fase de la onda senoidal que se va a generar para manipular el efecto. En la lÃ­nea 13 se encuentra la variable modifier en la cual se calcula el valor de la onda seno que se utiliza para multiplicar la muestra actual de la seÃ±al original. En la lÃ­nea 15 se calcula la proporciÃ³n de mezcla de la seÃ±al original con respecto a la seÃ±al modificada por el efecto. Finalmente al terminar la funciÃ³n en la lÃ­nea 20 se suma 1 a la variable counter, con el propÃ³sito de poder seguir la forma de onda del modificador para el siguiente bloque de audio a procesar. 73 3.4.7. Tremolo Este efecto consiste en enviar la seÃ±al a los altavoces a una frecuencia determinada. El algoritmo utilizado consiste en implementar en el programa una sinusoide de amplitud unitaria y de una determinada frecuencia. Luego se detecta si el valor del seno es positivo o negativo. Cuando es positivo se enviarÃ¡ al parlante el audio multiplicado por los valores positivos de la sinusoide y cuando es negativo simplemente se inhibirÃ¡ el envÃ­o de seÃ±al (ver Figura 3.24). La cantidad de efecto aplicado a la seÃ±al es controlado por la Profundidad y la velocidad de las oscilaciones (Frecuencia) serÃ¡ determinada por los ciclos del LFO. ğ‘¥(ğ‘›) ğ‘¦(ğ‘›) Figura 3.24: Diagrama de bloques del efecto Tremolo. Los parÃ¡metros de este efecto son (ver Figura 3.25): ï‚§ Frecuencia (Rate): Es la velocidad con la cual se hace la variaciÃ³n de volumen en los altavoces (generalmente entre 0,1 Hz y 10 Hz.). ï‚§ Profundidad (Depth): Se define como la intensidad de cambio de volumen del efecto Tremolo. ï‚§ ProporciÃ³n de Mezcla (Mix): Es la cantidad de efecto Tremolo que se mezcla con la seÃ±al original. ğ¿ğ¹ğ¿ 74 Figura 3.25: Interfaz grÃ¡fica del efecto Tremolo. El Algoritmo 8 corresponde a la versiÃ³n secuencial para CPU del efecto Tremolo. Para la implementaciÃ³n en CUDA se elimina el ciclo para, ya que todo el cÃ³digo de la funciÃ³n tiene que ser ejecutado de forma paralela por cada hilo asignado al kernel de CUDA de este efecto. Algoritmo 8 Algoritmo del efecto Tremolo 1: funciÃ³n Tremolo(array in, array out, int N, int counter,int sampleRate, 2: float rate, float depth, float mix) 3: 4: float TWOPI 6,283185307 5: float modifier 6: 7: rate (rate â‹… 9,9) + 0,1 8: depth (depth â‹… 9) + 1 9: 10: float t rate / sampleRate 11: 12: para int i 0 hasta i < N hacer 13: 14: modifier 0,5 â‹… tanh(depth â‹… sin(TWOPI â‹… t â‹… (N â‹… counter + i))) + 0,5 15: 16: out[i] (1 - mix) â‹… in[i] + mix â‹… (in[i] â‹… modifier) 17: 18: fin para 19: fin funciÃ³n 20: 21: counter counter + 1 75 En el Algoritmo 8, lÃ­nea 7, se encuentra la variable rate que estÃ¡ definida para el rango entre 0,1 Hz y 10 Hz. En la lÃ­nea 8 en la variable depth se define la curvatura de la seÃ±al senoidal en un rango de 1 a 10, mientras mayor sea el valor, la onda senoidal tendrÃ¡ una forma mÃ¡s cuadrada (ver Figura 3.26). La variable t indica la proporciÃ³n de cambio de fase de la onda senoidal que se va a generar para manipular el efecto. En la lÃ­nea 14 en la variable modifier se calcula el valor de la onda seno que se utiliza para multiplicar con la muestra actual de la seÃ±al original. En la lÃ­nea 16 se calcula la proporciÃ³n de mezcla de la seÃ±al original con respecto a la seÃ±al modificada por el efecto. Finalmente al terminar la funciÃ³n en la lÃ­nea 21 se suma 1 a la variable counter, con el propÃ³sito de poder seguir la forma de onda del modificador para el siguiente bloque de audio a procesar. Figura 3.26: Una onda sinusoidal modificada por la funciÃ³n tanh(x). A medida que el multiplicador dentro de la funciÃ³n sea mayor, la onda sinusoidal tendrÃ¡ una tendencia a ser cada vez mÃ¡s cuadrada. 76 3.4.8. Auto Panner El algoritmo consiste en implementar una sinusoide de amplitud unitaria y frecuencia determinada, generalmente un LFO (ver Figura 3.27). Si se detecta que el valor actual del seno es positivo entonces se envÃ­a al parlante derecho la seÃ±al de audio multiplicada por el valor positivo del seno, en caso contrario, si se detecta que el valor del seno es negativo se enviarÃ¡ al parlante izquierdo la seÃ±al de audio multiplicada por el valor actual del seno. En resumen, mientras un parlante emite seÃ±al el otro no emite seÃ±al alguna, luego se invierten los estados. Esto se repite en un bucle infinito hasta que el efecto sea desactivado por el usuario. La amplitud del LFO permite determinar las posiciones mÃ­nimas y mÃ¡ximas de la oscilaciÃ³n. La frecuencia del LFO determinarÃ¡ el perÃ­odo de oscilaciÃ³n. ğ‘¥(ğ‘›) ğ‘¦1(ğ‘›) Figura 3.27: Diagrama de bloques del efecto Auto Panner. Los parÃ¡metros de este efecto son (ver Figura 3.28): ï‚§ Frecuencia (Rate): Es la velocidad con la cual se hace la variaciÃ³n de volumen de izquierda a derecha entre los altavoces (generalmente entre 0,1 Hz. y 5 Hz.). ï‚§ Profundidad (Depth): Se define como la intensidad de cambio de volumen del efecto Auto Panner. ï‚§ ProporciÃ³n de Mezcla (Mix): Es la cantidad de efecto que se mezcla con la seÃ±al original. ğ¿ğ¹ğ¿ âˆ’1 ğ‘¦2(ğ‘›) x 77 Figura 3.28: Interfaz grÃ¡fica del efecto Auto Panner. El Algoritmo 9 corresponde a la versiÃ³n secuencial para CPU del efecto Auto Panner. Este efecto es muy similar al Algoritmo 8 del efecto Tremolo ya que es un efecto derivado de este. La diferencia con respecto al efecto Tremolo es el rango de las oscilaciones por segundo que tiene el efecto Auto Panner y que se utilizan 2 canales de audio ya que es un efecto estÃ©reo. Para la implementaciÃ³n en CUDA se elimina el ciclo para, ya que todo el cÃ³digo de la funciÃ³n tiene que ser ejecutado de forma paralela por cada hilo asignado al kernel de CUDA de este efecto. Algoritmo 9 Algoritmo del efecto Auto Panner 1: funciÃ³n AutoPannner(array inL, array inR, array outL, array outR, int N, 2: int counter, int sampleRate, float rate, float depth, float mix) 3: 4: float TWOPI 6,283185307 5: float modifier 6: 7: rate (rate â‹… 4,9) + 0,1 8: depth (depth â‹… 9) + 1 9: 10: float t rate / sampleRate 11: 12: para int i 0 hasta i < N hacer 13: 14: modifier 0,5 â‹… tanh(depth â‹… sin(TWOPI â‹… t â‹… (N â‹… counter + i))) 15: 16: outL[i] (1 - mix) â‹… inL[i] + mix â‹… (inL[i] â‹… (modifier + 0,5)) 17: outR[i] (1 - mix) â‹… inR[i] + mix â‹… (inR[i] â‹… (-modifier + 0,5)) 78 18: 19: fin para 20: fin funciÃ³n 21: 22: counter counter + 1 En el Algoritmo 9, lÃ­nea 7, se encuentra la variable rate que estÃ¡ definida para el rango entre 0,1 Hz y 5 Hz. En la lÃ­nea 8 en la variable depth se define la curvatura de la seÃ±al senoidal en un rango de 1 a 10; mientras mayor sea el valor, la onda senoidal tendrÃ¡ una forma mÃ¡s cuadrada (ver Figura 3.26). La variable t indica la proporciÃ³n de cambio de fase de la onda senoidal que se va a generar para manipular el efecto. En la lÃ­nea 14, en la variable modifier, se calcula el valor de la onda seno que se utiliza para multiplicar la muestra actual de la seÃ±al original. En las lÃ­neas 16 y 17 se calcula la proporciÃ³n de mezcla de la seÃ±al original con respecto a la seÃ±al modificada por el efecto; en la salida de audio izquierda outL se multiplica la seÃ±al por la variable modifier y en la salida de audio derecha se multiplica la seÃ±al por -modifier para tener la respuesta contraria a la salida izquierda y generar el efecto buscado. Finalmente al terminar la funciÃ³n en la lÃ­nea 22 se suma 1 a la variable counter, con el propÃ³sito de poder seguir la forma de onda del modificador para el siguiente bloque de audio a procesar. 79 3.4.9. Delay La estructura general para implementar el efecto Delay es descrita por la ecuaciÃ³n: ğ‘¦(ğ‘›) = ğ‘¥(ğ‘›) + (ğ‘¦(ğ‘› âˆ’ ğ¾) âˆ— ğ‘“ğ¹ğ¹ğ‘‘ğ‘“ğ‘”ğ‘“ğ‘“) âˆ— ğ‘‘ğ¹ğ‘¦/ğ‘¤ğ¹ğ¹ (ğ‘¬ğ’„. ğŸ‘.ğŸ‘) donde ğ¾ es el tamaÃ±o del retraso (medido en muestras), ğ‘“ğ¹ğ¹ğ‘‘ğ‘“ğ‘”ğ‘“ğ‘“ es un factor de amplificaciÃ³n aplicado a la seÃ±al retrasada y ğ‘‘ğ¹ğ‘¦/ğ‘¤ğ¹ğ¹ es un factor de mezcla de la seÃ±al original con la seÃ±al retrasada. En la Figura 3.29 se puede observar la implementaciÃ³n del efecto en forma de diagrama de bloques. ğ‘¥(ğ‘›) ğ‘¦(ğ‘›) Figura 3.29: Diagrama de bloques del efecto Delay. Los parÃ¡metros de un delay son (ver Figura 3.30): ï‚§ Tiempo de retraso (Time): Corresponde al tiempo que tarda en producirse un eco (ğ¾) y es ajustado en milisegundos. ï‚§ RetroalimentaciÃ³n (Feedback): Define la cantidad de veces que se repite la seÃ±al sonora. ï‚§ ProporciÃ³n de mezcla (Dry/Wet): Es la cantidad de sonido retrasado que se mezcla con la seÃ±al original. ğ·ğ¹ğ¹ğ‘”ğ‘¦ ğ¹ğ¹ğ‘›ğ¹ + ğ‘“ğ¹ğ¹ğ‘‘ğ‘“ğ‘”ğ‘“ğ‘“ ğ¾ + ğ·ğ¹ğ‘¦/ğ‘Šğ¹ğ¹ ğ‘€ğ¹ğ‘¥ 80 Figura 3.30: Interfaz grÃ¡fica del efecto Delay. El Algoritmo 10 corresponde a la versiÃ³n secuencial para CPU del efecto Delay Para la implementaciÃ³n en CUDA se elimina el ciclo para, ya que todo el cÃ³digo de la funciÃ³n tiene que ser ejecutado de forma paralela por cada hilo asignado al kernel de CUDA de este efecto. Algoritmo 10 Algoritmo del efecto Delay 1: funciÃ³n Delay(array in, array out, array delayBuffer, int N, int counter, 2: int sampleRate, float time, float feedback, float drywet) 3: 4: para int i 0 hasta i < N hacer 5: 6: cursor N â‹… counter + i 7: 8: delayBuffer[cursor] (delayBufferL[cursor] â‹… feedback) + in[i] 9: 10: in[i] (1 - drywet) â‹… in[i] + (drywet â‹… delayBuffer[cursor]) 11: 12: fin para 13: fin funciÃ³n 14: 15: counter counter + 1 16: 17: si counter â‹… N > time â‹… sampleRate entonces 18: counter 0 19: fin si 81 En el Algoritmo 10, lÃ­nea 6, se encuentra la variable cursor la cual va a contener el valor del Ã­ndice con el cual se va a recorrer el arreglo delayBuffer, el cual es el arreglo donde se van a almacenar las muestras anteriores del bÃºfer de entrada. En la lÃ­nea 8 a cada posiciÃ³n del arreglo delayBuffer se le asigna un valor de la muestra actual mÃ¡s la muestra que tenga almacenada previamente por un factor entre 0 y 1 asignado a la variable feedback que hace decaer en volumen a las muestras anteriores almacenadas en el arreglo delayBuffer tras sucesivas iteraciones. En la lÃ­nea 10 se calcula la proporciÃ³n de mezcla de la seÃ±al original con respecto a la seÃ±al modificada por el efecto. Finalmente al terminar la funciÃ³n, en la lÃ­nea 15, se suma 1 a la variable counter, con el propÃ³sito de poder tener un acumulador para la variable cursor del efecto Delay. En las lÃ­neas 17-19 se verifica si la variable counter multiplicada por N es mayor que el tiempo en nÃºmero de muestras, de ser asÃ­, se reinicia el contador a cero. En la siguiente secciÃ³n se describe todo el proceso de las pruebas de rendimiento para diversos CPUs y GPUs, de las distintas plataformas de hardware seleccionadas. 82 CapÃ­tulo 4. Pruebas y Resultados Una vez finalizada la etapa de diseÃ±o y desarrollo se puso a prueba el sistema para evaluar el rendimiento y eficiencia y asÃ­ determinar si se alcanzan los objetivos planteados. En este capÃ­tulo se presentarÃ¡n las pruebas realizadas para el procesamiento digital de audio de los efectos anteriormente mencionados tanto en su implementaciÃ³n en GPU como en CPU. 4.1. DescripciÃ³n del ambiente de pruebas Para las pruebas se requiriÃ³ de hardware en particular para poder ejecutar los algoritmos. A continuaciÃ³n se muestran las arquitecturas utilizadas: Plataforma de Hardware 1 (Laptop ASUS M50VM-B2): ï‚§ Computador basado en procesador Intel Core 2 Duo T9400 2,53 GHz. ï‚§ 4 GB de memoria RAM DDR2 800 MHz. ï‚§ Tarjeta de video NVIDIA GeForce 9600M GS con 1024 MB de VRAM DDR2. Plataforma de Hardware 2 (Laptop MacBookPro MB470xx/A): ï‚§ Computador basado en procesador Intel Core 2 Duo P8600 2,4 GHz. ï‚§ 4 GB de memoria RAM DDR3 1066 MHz. ï‚§ Tarjeta de video NVIDIA GeForce 9600M GT con 512 MB de VRAM GDDR3. Plataforma de Hardware 3 (Desktop Personalizada): ï‚§ Computador basado en procesador Intel Core i7 920 2,66 GHz. ï‚§ 6 GB de memoria RAM DDR3 1066 MHz. ï‚§ Tarjeta de video NVIDIA GeForce 9800M GT con 1024 MB de VRAM GDDR3. Plataforma de Hardware 4 (Desktop Personalizada): ï‚§ Computador basado en procesador Intel Core i3 540 3,06 GHz. ï‚§ 4 GB de memoria RAM DDR3 1066 MHz. ï‚§ Tarjeta de video NVIDIA GeForce GTX 470 con 1280 MB de VRAM GDDR5. El sistema fue evaluado en la plataforma del Sistema Operativo Microsoft Windows 7. Se empleÃ³ el software DAW Cakewalk Sonar 8.5 que es compatible con el plugin VST para hacer las pruebas necesarias de su funcionamiento. 83 Para todas las pruebas se utilizÃ³ la tarjeta de sonido integrada de cada mÃ¡quina con un software de emulaciÃ³n ASIO (especificaciÃ³n Audio Stream Input/Output) que habilita la grabaciÃ³n y reproducciÃ³n de audio a baja latencia. Para todas las pruebas se utilizÃ³ un tamaÃ±o de bloque de audio de 512 muestras, que es un arreglo de nÃºmeros en punto flotante de 32 bits a una tasa de muestreo de 44.100 muestras por segundo. Usando la EcuaciÃ³n 3.1 tenemos que: 1 44100 â‹… 512 â‹… 1000 = 11,6 ğ‘šğ‘ . Este tamaÃ±o de bloque es lo suficientemente aceptable para no percibir la latencia que genera el procesamiento de audio por bloques, asÃ­ como la tasa de muestreo que es la utilizada de forma estÃ¡ndar para el audio contenido en un CD. 4.2. MediciÃ³n de Tiempo Para medir el tiempo en las pruebas de CPU se utilizÃ³ el contador de tiempo de alta resoluciÃ³n QueryPerformanceCounter, ya que el orden de las magnitudes a evaluar son en milisegundos y la funciÃ³n clock() o GetTickCount() de Windows no ofrecen la resoluciÃ³n adecuada para estos casos. QueryPerformanceCounter se utiliza para medir el tiempo transcurrido de un fragmento de cÃ³digo tomando un tiempo inicial y final para luego calcular la resta entre estos dos tiempos y obtener el tiempo transcurrido del algoritmo. En la Figura 4.1 se muestra la implementaciÃ³n del cÃ¡lculo del tiempo en CPU. __int64 ctr1, ctr2, freq; // Variables para medir el tiempo float resultado; // Variable para guardar resultado final QueryPerformanceCounter((LARGE_INTEGER *)&ctr1); // Tiempo Inicial // SecciÃ³n de cÃ³digo a evaluar QueryPerformanceCounter((LARGE_INTEGER *)&ctr2); // Tiempo Final QueryPerformanceFrequency((LARGE_INTEGER *)&freq); // Tics por segundo del CPU resultado = ((ctr2-ctr1)/(float)freq * 1000.0f); // Calculo final del tiempo printf("Tiempo Total: %f (ms)", resultado); // Despliegue por pantalla // del resultado final en ms. Figura 4.1: Fragmento de cÃ³digo que mide el tiempo de un algoritmo en CPU. 84 Por otro lado para la mediciÃ³n de tiempo en GPU usamos las funciones propias de CUDA: Se utilizÃ³ cudaEventRecord() que graba un evento en CUDA. Dado que la operaciÃ³n es asÃ­ncrona la funciÃ³n cudaEventSynchronize() es usada para determinar cuÃ¡ndo un evento ha sido realmente grabado. En la Figura 4.2 se muestra la implementaciÃ³n del cÃ¡lculo del tiempo en GPU. cudaEvent_t start, stop; // Variables para medir el tiempo float resultado; // Variable para guardar resultado final cudaEventCreate(&start); // CreaciÃ³n de evento start para medir tiempo cudaEventCreate(&stop); // CreaciÃ³n de evento stop para medir tiempo cudaEventRecord(start, 0); // Tiempo Inicial // SecciÃ³n de cÃ³digo a evaluar cudaEventRecord(stop, 0); // Tiempo Final cudaEventSynchronize(stop); // Parada de la toma de muestra del tiempo cudaEventElapsedTime(&resultado, start, stop) ; // Calculo final del tiempo printf("Tiempo Total: %f (ms)", resultado); // Despliegue por pantalla // del resultado final en ms. Figura 4.2: Fragmento de cÃ³digo que mide el tiempo de un algoritmo en GPU. Se hizo la toma de tiempo de los algoritmos en CPU a las funciones que calculan las operaciones aritmÃ©ticas del procesamiento digital del audio. En GPU se toma el tiempo en dos partes; un cÃ¡lculo del tiempo tomado en procesar el o los kernels de CUDA y un tiempo para la sincronizaciÃ³n con el GPU. 4.3. Resultados Para las pruebas de rendimiento los algoritmos se ejecutaron durante 20 segundos aproximadamente. Como se estima que cada algoritmo se ejecuta unas 100 veces por segundo, se estima que para cada prueba cada algoritmo se ejecutÃ³ unas 2.000 veces. 85 Las primeras cuatro columnas de las grÃ¡ficas representan los tiempos de ejecuciÃ³n de cada CPU, las otras cuatro columnas representan el tiempo de ejecuciÃ³n y sincronizaciÃ³n de cada GPU de las plataformas de hardware anteriormente definidas. 4.3.1. Over Drive La grÃ¡fica de la Figura 4.3 muestra los tiempos de ejecuciÃ³n de la implementaciÃ³n del efecto de audio Over Drive. En la implementaciÃ³n para CPU se puede apreciar que el procesador Intel Core i7 2,66 GHz es aproximadamente 13 veces mÃ¡s rÃ¡pido el GPU GeForce 9600M GS. Los tiempos de sincronizaciÃ³n penalizan duramente el rÃ¡pido tiempo de cÃ¡lculo ofrecido por el GPU. La implementaciÃ³n de CPU ejecutada en el procesador Intel Core i3 3,06 GHz es 2,9 veces mÃ¡s rÃ¡pida que la tarjeta de video GeForce GTX 470. Se puede notar un retardo de tiempo del GPU GeForce 9600M GS debido a que la sincronizaciÃ³n es mucho mÃ¡s lenta que la del resto de los GPUs. Para este efecto, el procesador Intel Core i7 2,66 GHz es mÃ¡s rÃ¡pido que el resto de los GPUs y CPUs. Figura 4.3: ComparaciÃ³n de rendimiento en tiempo para las implementaciones en CPU y GPU del efecto Over Drive. 0 0,05 0,1 0,15 0,2 0,25 0,3 C2D 2,53 GHz C2D 2,4 GHz i3 3,06 GHz i7 2,66 GHz 9600M GS 9600M GT 9800 GT GTX470 0,021 0,024 0,026 0,018 0,036 0,013 0,011 0,008 0,204 0,086 0,071 0,068 0,240 0,099 0,083 0,076 Ti em po ( m s. ) Procesador Efecto de Audio: Over Drive Tiempo de SincronizaciÃ³n Tiempo de Procesamiento 86 4.3.2. Distortion La grÃ¡fica de la Figura 4.4 muestra los tiempos de ejecuciÃ³n de la implementaciÃ³n del efecto de audio Distortion en varias arquitecturas. En la implementaciÃ³n para CPU se puede apreciar que el procesador Intel Core i7 2,66 GHz es aproximadamente 1,4 veces mÃ¡s rÃ¡pido que el CPU Intel Core i3 3,06 GHz y 2,6 veces mÃ¡s rÃ¡pido el GPU GeForce 9600M GS. Sin embargo la implementaciÃ³n de GPU ejecutada en la tarjeta de video GeForce GTX 470 es mÃ¡s rÃ¡pida que el resto de los CPUs y GPUs. Esto es debido a que la velocidad de cÃ¡lculo de la funciÃ³n de transferencia ğ¹ğ‘”ğ‘›â„(ğ‘¥) es realizada de forma mÃ¡s eficiente en GPU que en CPU, incluso tomando en cuenta el tiempo de sincronizaciÃ³n con el GPU. Se puede notar una gran diferencia de tiempo entre el resto de los GPUs y el GPU GeForce 9600M GS debido a que su tiempo de sincronizaciÃ³n es mucho mÃ¡s lento. Para este efecto, los GPUs GeForce GTX 470, 9800GT son mÃ¡s rÃ¡pidos que todos los CPUs evaluados. Figura 4.4: ComparaciÃ³n de rendimiento en tiempo para las implementaciones en CPU y GPU del efecto Distortion. 0 0,05 0,1 0,15 0,2 0,25 0,3 C2D 2,53 GHz C2D 2,4 GHz i3 3,06 GHz i7 2,66 GHz 9600M GS 9600M GT 9800 GT GTX470 0,128 0,129 0,136 0,091 0,033 0,014 0,013 0,008 0,206 0,086 0,072 0,067 0,239 0,101 0,085 0,075 Ti em po (m s. ) Procesador Efecto de Audio: Distortion Tiempo de SincronizaciÃ³n Tiempo de Procesamiento 87 4.3.3. Ecualizador La grÃ¡fica de la Figura 4.5 muestra los tiempos de ejecuciÃ³n de la implementaciÃ³n del efecto Ecualizador en varias arquitecturas. En la implementaciÃ³n para CPU se puede apreciar que el procesador Intel Core i7 2,66 GHz es aproximadamente 1,4 veces mÃ¡s rÃ¡pido que el CPU Intel Core 2 Duo 2,4 GHz y 1,9 veces mÃ¡s rÃ¡pido que el GPU GeForce 9600M GS. Sin embargo la tarjeta de video GeForce GTX 470 es 2,5 veces mÃ¡s rÃ¡pida que el CPU Intel Core i7 2,66 GHz y 3,6 veces mas rÃ¡pida que el procesador Intel Core 2 Duo 2,4 GHz. Cabe acotar que la tarjeta de video GeForce GTX 470 es mÃ¡s rÃ¡pida que el resto de los CPUs y GPUs, esto es debido a que la velocidad de cÃ¡lculo de la Transformada de Fourier es mucho mÃ¡s eficiente en GPU que en CPU. Se puede notar una demora de tiempo del GPU GeForce 9600M GS con respecto a los otros GPUs debido a que su tiempo de sincronizaciÃ³n es mucho mÃ¡s lento. Para este efecto, la tarjeta de video GeForce GTX 470 es mÃ¡s rÃ¡pida que el resto de los GPUs y CPUs. Figura 4.5: ComparaciÃ³n de rendimiento en tiempo para las implementaciones en CPU y GPU del Ecualizador. 0 0,1 0,2 0,3 0,4 0,5 0,6 0,7 0,8 0,9 1 C2D 2,53 GHz C2D 2,4 GHz i3 3,06 GHz i7 2,66 GHz 9600M GS 9600M GT 9800 GT GTX470 0,589 0,678 0,465 0,464 0,689 0,304 0,276 0,124 0,206 0,106 0,079 0,064 0,895 0,410 0,355 0,187 T ie m po (m s. ) Procesador Efecto de Audio: Ecualizador Tiempo de SincronizaciÃ³n Tiempo de Procesamiento 88 4.3.4. Vibrato La grÃ¡fica de la Figura 4.6 muestra los tiempos de ejecuciÃ³n de la implementaciÃ³n del efecto de audio Vibrato en varias arquitecturas. En la implementaciÃ³n para CPU se puede apreciar que el procesador Intel Core i7 2,66 GHz es aproximadamente 1,4 veces mÃ¡s rÃ¡pido que el CPU Intel Core i3 3,06 GHz y 7,1 veces mÃ¡s rÃ¡pido que el GPU GeForce 9600M GS. Los tiempos de sincronizaciÃ³n penalizan duramente el rÃ¡pido tiempo de cÃ¡lculo ofrecido por el GPU. Sin embargo la implementaciÃ³n de CPU ejecutada en el procesador Intel Core i3 3,06 GHz es solo 1,4 veces mÃ¡s rÃ¡pida que la tarjeta de video GeForce GTX 470. Se puede notar un retardo de tiempo del GPU GeForce 9600M GS debido a que su sincronizaciÃ³n es mucho mÃ¡s lenta que la del resto de los GPUs. Para este efecto, los CPUs son mÃ¡s rÃ¡pidos que todos los GPUs, sin embargo, el procesador Intel Core i3 3,06 GHz es solamente un poco mÃ¡s rÃ¡pido que la tarjeta de video GeForce GTX 470. Figura 4.6: ComparaciÃ³n de rendimiento en tiempo para las implementaciones en CPU y GPU del efecto Vibrato. 0 0,05 0,1 0,15 0,2 0,25 0,3 C2D 2,53 GHz C2D 2,4 GHz i3 3,06 GHz i7 2,66 GHz 9600M GS 9600M GT 9800 GT GTX470 0,044 0,047 0,055 0,038 0,064 0,066 0,138 0,010 0,207 0,086 0,079 0,068 0,271 0,152 0,217 0,078 Ti em po (m s. ) Procesador Efecto de Audio: Vibrato Tiempo de SincronizaciÃ³n Tiempo de Procesamiento 89 4.3.5. Chorus La grÃ¡fica de la Figura 4.7 muestra los tiempos de ejecuciÃ³n de la implementaciÃ³n del efecto de audio Chorus en varias arquitecturas. Los resultados son muy similares al del efecto Vibrato, ya que el Chorus es un derivado de este efecto. En la implementaciÃ³n para CPU se puede apreciar que el procesador Intel Core i7 2,66 GHz es aproximadamente 1,4 veces mÃ¡s rÃ¡pido que el CPU Intel Core i3 3,06 GHz y 6,7 veces mÃ¡s rÃ¡pido que el GPU GeForce 9600M GS. Los tiempos de sincronizaciÃ³n penalizan duramente el rÃ¡pido tiempo de cÃ¡lculo ofrecido por el GPU. Sin embargo la implementaciÃ³n de CPU ejecutada en el procesador Intel Core i3 3,06 GHz es solo 1,3 veces mÃ¡s rÃ¡pida que la tarjeta de video GeForce GTX 470. Se puede notar un retardo de tiempo del GPU GeForce 9600M GS debido a que su sincronizaciÃ³n es mucho mÃ¡s lenta que la del resto de los GPUs. Para este efecto, los CPUs son mÃ¡s rÃ¡pidos que todos los GPUs, sin embargo, el procesador Intel Core i3 3,06 GHz es solamente un poco mÃ¡s rÃ¡pido que la tarjeta de video GeForce GTX 470. Figura 4.7: ComparaciÃ³n de rendimiento en tiempo para las implementaciones en CPU y GPU del efecto Chorus. 0 0,05 0,1 0,15 0,2 0,25 0,3 C2D 2,53 GHz C2D 2,4 GHz i3 3,06 GHz i7 2,66 GHz 9600M GS 9600M GT 9800 GT GTX470 0,045 0,049 0,059 0,040 0,063 0,064 0,131 0,010 0,205 0,088 0,079 0,067 0,268 0,152 0,210 0,077 Ti em po (m s. ) Procesador Efecto de Audio: Chorus Tiempo de SincronizaciÃ³n Tiempo de Procesamiento 90 4.3.6. Ring Modulator La grÃ¡fica de la Figura 4.8 muestra los tiempos de ejecuciÃ³n de la implementaciÃ³n del efecto de audio Ring Modulator en varias arquitecturas. En la implementaciÃ³n para CPU se puede apreciar que el procesador Intel Core i7 2,66 GHz es aproximadamente 1,4 veces mÃ¡s rÃ¡pido que el CPU Intel Core i3 3,06 GHz y 5 veces mÃ¡s rÃ¡pido que el GPU GeForce 9600M GS. Los tiempos de sincronizaciÃ³n penalizan duramente el rÃ¡pido tiempo de cÃ¡lculo ofrecido por el GPU. Sin embargo el procesador Intel Core i3 3,06 GHz es apenas 1,1 veces mas rÃ¡pido que la tarjeta de video GeForce GTX 470. Se puede notar un retardo de tiempo del GPU GeForce 9600M GS debido a que su sincronizaciÃ³n es mucho mÃ¡s lenta que la del resto de los GPUs. Para este efecto, los CPUs son mÃ¡s rÃ¡pidos que todos los GPUs, sin embargo, el procesador Intel Core i3 3,06 GHz es solamente un poco mÃ¡s rÃ¡pido que la tarjeta de video GeForce GTX 470. Figura 4.8: ComparaciÃ³n de rendimiento en tiempo para las implementaciones en CPU y GPU del efecto Ring Modulator. 0 0,05 0,1 0,15 0,2 0,25 0,3 C2D 2,53 GHz C2D 2,4 GHz i3 3,06 GHz i7 2,66 GHz 9600M GS 9600M GT 9800 GT GTX470 0,044 0,048 0,066 0,045 0,021 0,011 0,010 0,007 0,206 0,087 0,079 0,066 0,227 0,098 0,089 0,073 Ti em po (m s. ) Procesador Efecto de Audio: Ring Modulator Tiempo de SincronizaciÃ³n Tiempo de Procesamiento 91 4.3.7. Tremolo La grÃ¡fica de la Figura 4.9 muestra los tiempos de ejecuciÃ³n de la implementaciÃ³n del efecto de audio Tremolo en varias arquitecturas. En la implementaciÃ³n para CPU se puede apreciar que el procesador Intel Core i7 2,66 GHz es aproximadamente 1,4 veces mÃ¡s rÃ¡pido que el CPU Intel Core i3 3,06 GHz y 3 veces mÃ¡s rÃ¡pido que el GPU GeForce 9600M GS. Los tiempos de sincronizaciÃ³n penalizan duramente el rÃ¡pido tiempo de cÃ¡lculo ofrecido por el GPU. Sin embargo la implementaciÃ³n de GPU ejecutada en el resto de las tarjetas de video es ligeramente mÃ¡s rÃ¡pida que la ejecutada en el resto de los CPUs. Se puede notar un retardo de tiempo del GPU GeForce 9600M GS debido a que su sincronizaciÃ³n es mucho mÃ¡s lenta que la del resto de los GPUs. Para este efecto, los GPUs y CPUs tienen un tiempo de procesamiento muy similar entre ellos, excepto el GPU de la tarjeta de video GeForce 9600M GS. Figura 4.9: ComparaciÃ³n de rendimiento en tiempo para las implementaciones en CPU y GPU del efecto Tremolo. 0 0,05 0,1 0,15 0,2 0,25 0,3 C2D 2,53 GHz C2D 2,4 GHz i3 3,06 GHz i7 2,66 GHz 9600M GS 9600M GT 9800 GT GTX470 0,093 0,100 0,108 0,077 0,022 0,011 0,010 0,007 0,208 0,086 0,075 0,067 0,230 0,097 0,085 0,073 Ti em po (m s. ) Procesador Efecto de Audio: Tremolo Tiempo de SincronizaciÃ³n Tiempo de Procesamiento 92 4.3.8. Auto Panner La grÃ¡fica de la Figura 4.10 muestra los tiempos de ejecuciÃ³n de la implementaciÃ³n del efecto de audio Auto Panner en varias arquitecturas. Los resultados son muy similares al del efecto Tremolo, ya que el Auto Panner es un derivado de este efecto. En la implementaciÃ³n para CPU se puede apreciar que el procesador Intel Core i7 2,66 GHz es aproximadamente 1,4 veces mÃ¡s rÃ¡pido que el CPU Intel Core i3 3,06 GHz y 3 veces mÃ¡s rÃ¡pido que el GPU GeForce 9600M GS. Los tiempos de sincronizaciÃ³n penalizan duramente el rÃ¡pido tiempo de cÃ¡lculo ofrecido por el GPU. Sin embargo la implementaciÃ³n de GPU ejecutada en el resto de las tarjetas de video es ligeramente mÃ¡s rÃ¡pida que la ejecutada en el resto de los CPUs. Se puede notar un retardo de tiempo del GPU GeForce 9600M GS debido a que su sincronizaciÃ³n es mucho mÃ¡s lenta que la del resto de los GPUs. Para este efecto, los GPUs y CPUs tienen un tiempo de procesamiento muy similar entre ellos, excepto el GPU de la tarjeta de video GeForce 9600M GS. Figura 4.10: ComparaciÃ³n de rendimiento en tiempo para las implementaciones en CPU y GPU del efecto Auto Panner. 0 0,05 0,1 0,15 0,2 0,25 0,3 C2D 2,53 GHz C2D 2,4 GHz i3 3,06 GHz i7 2,66 GHz 9600M GS 9600M GT 9800 GT GTX470 0,094 0,101 0,110 0,076 0,022 0,011 0,010 0,007 0,205 0,088 0,079 0,066 0,227 0,100 0,089 0,073 Ti em po (m s. ) Procesador Efecto de Audio: Auto Panner Tiempo de SincronizaciÃ³n Tiempo de Procesamiento 93 4.3.9. Delay La grÃ¡fica de la Figura 4.11 muestra los tiempos de ejecuciÃ³n de la implementaciÃ³n del efecto de audio Delay en varias arquitecturas. En la implementaciÃ³n para CPU se puede apreciar que el procesador Intel Core i7 2,66 GHz es 1,6 veces mÃ¡s rÃ¡pido que el CPU Intel Core 2 Duo 2,4 GHz y 46 veces mÃ¡s rÃ¡pido el GPU GeForce 9600M GS. La diferencia entre los resultados de las ejecuciones en CPU y en GPU son arrasantes. No hay lugar a duda de que la implementaciÃ³n para GPU es la menos indicada para conseguir un buen rendimiento en este efecto. Se puede notar un retardo de tiempo del GPU GeForce 9600M GS debido a que su sincronizaciÃ³n es mucho mÃ¡s lenta que la del resto de los GPUs. Para este efecto, todos los CPUs son mÃ¡s rÃ¡pidos que los GPUs aquÃ­ evaluados. Figura 4.11: ComparaciÃ³n de rendimiento en tiempo para las implementaciones en CPU y GPU del efecto Delay. 0 0,05 0,1 0,15 0,2 0,25 0,3 C2D 2,53 GHz C2D 2,4 GHz i3 3,06 GHz i7 2,66 GHz 9600M GS 9600M GT 9800 GT GTX470 0,007 0,008 0,006 0,005 0,025 0,012 0,011 0,007 0,205 0,087 0,078 0,066 0,230 0,098 0,088 0,072 Ti em po (m s. ) Procesador Efecto de Audio: Delay Tiempo de SincronizaciÃ³n Tiempo de Procesamiento 94 4.3.10. Tiempo de ejecuciÃ³n promedio de un solo efecto La grÃ¡fica de la Figura 4.12 muestra el tiempo promedio de ejecuciÃ³n de un solo efecto en varias arquitecturas. Se puede notar que el procesador Intel Core i7 2,66 GHz es mas rÃ¡pido que el resto de los CPUs. Sin embargo, el GPU GeForce GTX 470 es ligeramente mÃ¡s rÃ¡pido que el CPU Intel Core i7 2,66 GHz. Esto es debido a que este GPU tiene una gran ventaja en el efecto Ecualizador realizando el cÃ¡lculo de la Transformada RÃ¡pida de Fourier. TambiÃ©n se puede observar que los tiempos entre el procesador Core 2 Duo 2,4 Ghz y el GPU GeForce 9800M GT son muy similares. El GPU GeForce 9600M GS es el mÃ¡s lento con respecto al resto de los GPUs y CPUs. En promedio, la tarjeta de video GeForce GTX 470 es mÃ¡s rÃ¡pida que el resto de los GPUs y CPUs. Figura 4.12: ComparaciÃ³n del tiempo de ejecuciÃ³n promedio de los efectos en CPU y GPU. 0 0,05 0,1 0,15 0,2 0,25 0,3 0,35 C2D 2,53 GHz C2D 2,4 GHz i3 3,06 GHz i7 2,66 GHz 9600M GS 9600M GT 9800 GT GTX470 0,118 0,131 0,115 0,095 0,108 0,056 0,068 0,021 0,206 0,089 0,077 0,066 0,314 0,145 0,144 0,087 Ti em po (m s. ) Procesador Tiempo promedio de los efectos Tiempo de SincronizaciÃ³n Tiempo de Procesamiento 95 4.3.11. Nueve efectos de audio simultÃ¡neos La grÃ¡fica de la Figura 4.13 muestra los tiempos de ejecuciÃ³n de 9 efectos activados de forma simultÃ¡nea en varias arquitecturas. Los efectos activados son todos los aquÃ­ presentados. Se puede notar que se puede aprovechar el potencial del GPU al realizar bastantes cÃ¡lculos en Ã©l. En la implementaciÃ³n para GPU se puede apreciar que la tarjeta de video GeForce GTX 470 es 3 veces mÃ¡s rÃ¡pida que el CPU Intel Core i7 2.66 GHz y 4,3 veces mÃ¡s rÃ¡pido que el procesador Intel Core 2 Duo 2,4 GHz. En este caso el tiempo de sincronizaciÃ³n con el GPU se equilibra con el rÃ¡pido tiempo de cÃ¡lculo que ofrecen los GPUs. El procesador Intel Core i7 2.66 GHz es 1,4 veces mÃ¡s rÃ¡pido que el GPU GeForce 9600M GS, sin embargo el resto de las ejecuciones hechas en los GPUs son mÃ¡s rÃ¡pidas que las ejecuciones hechas en CPU. Figura 4.13: ComparaciÃ³n de rendimiento en tiempo de 9 efectos implementados ejecutÃ¡ndose de forma simultanea (CPU y GPU). 0 0,2 0,4 0,6 0,8 1 1,2 1,4 C2D 2,53 GHz C2D 2,4 GHz i3 3,06 GHz i7 2,66 GHz 9600M GS 9600M GT 9800 GT GTX470 0,998 1,121 1,021 0,767 0,922 0,481 0,680 0,178 0,207 0,087 0,081 0,081 1,129 0,568 0,761 0,260 Ti em po (m s. ) Procesador 9 Efectos de Audio SimultÃ¡neos Tiempo de SincronizaciÃ³n Tiempo de Procesamiento 96 CapÃ­tulo 5. Conclusiones y Trabajos Futuros Con la propuesta de este proyecto, se logrÃ³ entrar en un Ã¡rea no explorada a lo largo de la carrera como lo es el audio digital, puesto que el pensum actual comprende algunos apectos de seÃ±ales de manera generalizada o bien enfocada al procesamiento digital de imÃ¡genes. En este trabajo se ha presentado la implementaciÃ³n de un sistema de procesamiento digital de audio en tiempo real en GPU logrando los objetivos planteados, demostrando que es factible procesar dichos efectos de audio en GPU y de forma eficiente. Se observÃ³ un incremento en la velocidad de procesamiento de los algoritmos de audio en el GPU con respecto al CPU cuando son activados varios efectos. Se notÃ³ que al activar solo un efecto no se aprovecha la capacidad del GPU ya que se estÃ¡ subutilizando y la sincronizaciÃ³n penaliza los tiempos esperados. En el caso del efecto Delay la diferencia de tiempos es muy marcada. Se apreciÃ³ una particularidad en el efecto Distortion en cuanto al tiempo de procesamiento, ya que el cÃ¡lculo de la funciÃ³n de transferencia para este efecto (ğ¹ğ‘”ğ‘›â„(ğ‘¥)) es mÃ¡s costoso en CPU que en la implementaciÃ³n para GPU. El efecto Ecualizador implementado para GPU conlleva una gran ventaja para las tarjetas de video mÃ¡s recientes, ya que el cÃ¡lculo de la Transformada RÃ¡pida de Fourier se realiza de forma muy eficiente con respecto a la implementaciÃ³n en CPU. Se demostrÃ³ que el poder de cÃ¡lculo de esta Ãºltima generaciÃ³n de GPUs puede ofrecer un aumento de velocidad significativo en los cÃ¡lculos, sin embargo, se espera que en las prÃ³ximas generaciones de GPUs existan mejoras en cuanto a la latencia que existe en la comunicaciÃ³n y transferencia de datos entre el CPU y el GPU. Con este proyecto se logra un beneficio econÃ³mico para los usuarios no profesionales al tener la posibilidad de utilizar un procesador digital de audio con una tarjeta de video NVIDIA de precio accesible. 97 Como trabajo futuro se puede plantear la utilizaciÃ³n del lenguaje OpenCL como alternativa para obtener mayor portabilidad entre las tarjetas de video, ya que este trabajo tiene como limitaciÃ³n el uso de la arquitectura CUDA que es solo compatible con las tarjetas de video NVIDIA. Al implementar el cÃ³digo en OpenCL se lograrÃ­a conseguir un rango mayor de compatibilidad con las tarjetas de video que soporten este lenguaje. Igualmente en el futuro se pueden hacer pruebas en interfaces de audio que puedan manejar una mayor frecuencia de muestreo, ya que con bloques de datos de mayor tamaÃ±o se podrÃ­a explotar aÃºn mÃ¡s la capacidad de los GPU para procesar datos en paralelo. Una dura prueba serÃ­a el procesamiento de bloques de audio de 11.6 ms. a una tasa de muestreo de 172.400 muestras por segundo, ya que esto implicarÃ­a un procesamiento de 4 veces mÃ¡s datos que las pruebas hechas en este trabajo. TambiÃ©n, se puede intentar hacer pruebas en pistas de audio con canales Surround 7.1, ya que explotarÃ­a la capacidad de paralelismo del GPU no solo procesando 2 canales en estÃ©reo sino hasta 8 canales de forma simultÃ¡nea. De ser factible, tambiÃ©n se podrÃ­a implementar otros efectos como el reverb de convoluciÃ³n, compressor (ver glosario) o algÃºn efecto que tenga que ser procesado en el dominio de la frecuencia (e.g., reducciÃ³n de ruido o pitch shifting). Finalmente se recomienda implementar mejoras en la GUI, para poder obtener mÃ¡s flexibilidad al manipular los efectos, como por ejemplo cambiar el orden en que se procesan los efectos, agregar o quitar efectos, etc. 98 Referencias BibliogrÃ¡ficas [1] WHALEN, S. Audio and the Graphics Processing Unit. [En lÃ­nea] Disponible en: <http://www.extalin.com/gpudsp/gpuaudio.pdf>. [2] JEDRZEJEWSKI, M.; MARASEK, K. "Computation of Room Acoustics Using Programmable Video Hardware." 2004. Computer Vision and Graphics International Conference, ICCVG 2004, Warsaw, Polonia, Septiembre de 2004. [3] FABRITIUS, F. Audio Processing Algorithms on the GPU. Technical University of Denmark. 2009. Tesis de MaestrÃ­a. [4] GALLO, E.; TSINGOS, N. "Efficient 3D Audio Processing with the GPU." 2004. Proceedings of the ACM Workshop on General Purpose Computing on Graphics Processors. pÃ¡g. C-42. [5] CADIZ, R. IntroducciÃ³n a la MÃºsica Computacional. Centro de InvestigaciÃ³n en TecnologÃ­as de Audio, Instituto de MÃºsica, Pontificia Universidad CatÃ³lica de Chile, 2008. [6] SMITH, S. W. The Scientist and Engineer's Guide to Digital Signal Processing. 2da. EdiciÃ³n. San Diego, California. California Technical Publishing, 1999. [7] COOLEY, J.W.; TUKEY, J.W. "An Algorithm for the Machine Calculation of Complex Fourier Series." Mathematics of Computation. 1965, vol. 19, pÃ¡gs. 297-301. [8] BORES SIGNAL PROCESSING. Introduction to DSP - filtering: digital filter equation. [En lÃ­nea] Disponible en: <http://bores.com/courses/intro/filters/4_eq.htm>. [9] Parks-McClellan filter design algorithm. [En lÃ­nea] Disponible en: <http://en.wikipedia.org/wiki/Parks-McClellan_filter_design_algorithm>. [10] CONNEXIONS. DiseÃ±o de Filtros usando la GrÃ¡fica de Polos y Ceros de la Transformada-Z. [En lÃ­nea] Disponible en: <http://cnx.org/content/m12967/latest/>. [11] ZÃ–LZER, U. DAFX - Digital Audio Effects. Wiley, 2002. [12] CUDA Programming Guide. VersiÃ³n 3.1. NVIDIA Corporation, 2010. [13] GPGPU. [En lÃ­nea] [Citado el: 14 de Noviembre de 2010.] Disponible en: <http://www.gpgpu.org>. 99 [14] JAMES, G., "Operations for Hardware-Accellerated Procedural Texture Animation." Ed. M. Deloura. Game Programming Gems 2. Charles River Media, 2001, pÃ¡gs. 497-509. [15] MARTINEZ, M. ComputaciÃ³n en GPU con NVIDIA CUDA. Tutorial dentro del II Workshop en Aplicaciones de Nuevas Arquitecturas de Consumo y Altas Prestaciones (ANACAP 2009), EspaÃ±a, Noviembre de 2009. [16] CUDA CUFFT Library. VersiÃ³n 3.1. NVIDIA Corporation, 2010. [17] Virtual Studio Technology. [En lÃ­nea] Disponible en: <http://en.wikipedia.org/wiki/Virtual_Studio_Technology>. [18] STEINBERG. 3rd Party Developer Area. [En lÃ­nea] Disponible en: <http://www.steinberg.net/en/company/developer.html>. [19] FFTW. [En lÃ­nea] [Citado el: 5 de Octubre de 2011.] Disponible en: <http://www.fftw.org>. [20] GÃ“MEZ, E. IntroducciÃ³n al Filtrado Digital. Departamento de SonologÃ­a, Escuela Superior de MÃºsica de Catalunya. Barcelona, EspaÃ±a, 2009. [21] GÃ“MEZ, E. Efectos Digitales BÃ¡sicos. Departamento de SonologÃ­a, Escuela Superior de MÃºsica de Catalunya. Barcelona, EspaÃ±a, 2009. [22] MICEA, M.; STRATULAR M.; ARDELEAN, D.; AIOANEI, D. Implementing Professional Audio Effects with DSPs. Software and Computer Engineering Department, Politehnica University of Timisoara, Rumania, 2001. 100 Glosario Altura tonal: En psicoacÃºstica, la altura es un parÃ¡metro utilizado para determinar la percepciÃ³n del tono (frecuencia) de un sonido. El que un sonido sea agudo o grave depende de su frecuencia. ArmÃ³nicos: Son los componentes de frecuencia de una seÃ±al periÃ³dica. Consiste siempre en mÃºltiplos enteros de la frecuencia fundamental. La frecuencia fundamental es el primer armÃ³nico. ASIO (Audio Stream Input/Output): Es un protocolo de comunicaciÃ³n de hardware para audio digital de la empresa Steinberg, que provee una baja latencia y una interfaz de alta fidelidad entre la aplicaciÃ³n de audio, el hardware y la tarjeta de sonido. AU: Siglas de Audio Units. Es un formato de plugin desarrollado por Apple Computer para manipular audio en tiempo real, usado por el software DAW LogicPro. Cero: En el contexto matemÃ¡tico de plano complejo, un cero de una funciÃ³n ğ‘“(ğ‘§) es un punto ğ‘§ = ğ‘” tal que ğ‘“(ğ‘§) se hace cero en dicho punto ğ‘”. Compressor: Es un efecto de audio que tiene un sistema automÃ¡tico de control de ganancia que constantemente monitorea la seÃ±al y ajusta la ganancia para maximizar la razÃ³n seÃ±al a ruido sin producir distorsiÃ³n. Es importante no confundir a este tipo de compresores de amplitud de audio con la compresiÃ³n de informaciÃ³n digital o perceptual. DAW: Son las siglas de Digital Audio Workstation. Es un sistema electrÃ³nico dedicado a la grabaciÃ³n y ediciÃ³n de audio digital por medio de un software de ediciÃ³n de audio. Decibelio (ğ’…ğ‘©): El decibelio es la unidad de medida utilizada para el nivel de potencia de la seÃ±al y el nivel de intensidad del ruido. El decibelio, es una unidad logarÃ­tmica. Device: En el contexto de CUDA, es la unidad de cÃ¡lculo (GPU) en la cual se ejecutan los kernels. DSP: Son las siglas de Digital Signal Processing. Es el anÃ¡lisis y manipulaciÃ³n de los datos en forma digital. En este trabajo se refiere al procesamiento digital de los datos de audio. Frecuencia de Muestreo: Es el nÃºmero de muestras por unidad de tiempo que se toman de una seÃ±al continua para producir una seÃ±al discreta, durante el proceso necesario para convertirla de analÃ³gica a digital. 101 FunciÃ³n Delta: Es un impulso normalizado. La funciÃ³n discreta delta es una seÃ±al compuesta por ceros, excepto la muestra cero que tiene valor unitario. Host: En el contexto de CUDA, es el elemento que interactÃºa con el device para ejecutar y administrar las tareas y funciones asignadas a procesar en el GPU. Impulso: Es una seÃ±al compuesta por una colecciÃ³n de muestras con valor cero, excepto para algunos pulsos muy breves. Impulso Unitario: Otro nombre para la FunciÃ³n Delta. Kernel: Es una funciÃ³n declarada en el cÃ³digo del programa a ser ejecutado en el GPU. Kernel de ConvoluciÃ³n: Es la respuesta al impulso de un filtro implementado por convoluciÃ³n. TambiÃ©n es conocido como Kernel del Filtro. LFO (Low Frequency Oscilator): (Traducido como "Oscilador de Baja Frecuencia"), se refiere a una seÃ±al de audio normalmente por debajo de 20 Hz que crea un ritmo palpitante en vez de un tono audible. MIDI: Siglas de (Musical Instrument Digital Interface). Es un protocolo de comunicaciÃ³n serial que permite a los computadores, sintetizadores, secuenciadores, controladores y otros dispositivos musicales electrÃ³nicos comunicarse y compartir informaciÃ³n. Octava: Es un factor de dos en la frecuencia de una seÃ±al. OSC (Oscilador): Es un circuito que produce una seÃ±al electrÃ³nica repetitiva, a menudo una onda sinusoidal o una onda cuadrada. Polo: En el contexto matemÃ¡tico de plano complejo, un polo de una funciÃ³n ğ‘“(ğ‘§) es un punto ğ‘§ = ğ‘” tal que ğ‘“(ğ‘§) tiende a infinito a medida que ğ‘§ tiende a ğ‘”. Respuesta al Impulso: Es la salida de un sistema cuando la entrada es un impulso normalizado (funciÃ³n delta). ReverberaciÃ³n (reverb): Es un fenÃ³meno derivado de la reflexiÃ³n de sonido consistente en una ligera permanencia del sonido una vez que se ha extinguido el original, debido a las ondas reflejadas. RTAS: Siglas de Real-Time Audio Suite, es un formato de plugin desarrollado por la empresa Avid Technology para su gama de programas DAW ProTools. SDK: Son las siglas de Software Development Kit. Es un paquete que generalmente le permite a los desarrolladores crear software para alguna plataforma en particular. 102 Sesgo: Es el nivel de error sistemÃ¡tico en un proceso que causa que todos los valores medidos se desvÃ­en a valores mayores o menores a los valores esperados. Shader: Es un conjunto de instrucciones que son usadas generalmente para calcular efectos de rendering en hardware grÃ¡fico con un gran nivel de flexibilidad en cuanto a programaciÃ³n. Resumen Tabla de Contenido PÃ¡g. Ãndice de Figuras IntroducciÃ³n 1.1. Planteamiento del problema 1.2. SoluciÃ³n propuesta 1.3. Objetivo general 1.4. Objetivos especÃ­ficos 1.5. Alcance y limitaciones de este trabajo CapÃ­tulo 2. Marco TeÃ³rico 2.1. SeÃ±ales 2.2. SeÃ±ales de audio 2.2.1. Sinusoides 2.2.2. Exponenciales 2.2.3. RelaciÃ³n entre sinusoides y exponenciales 2.3. SeÃ±ales analÃ³gicas y digitales 2.3.1. Teorema de muestreo de Nyquist 2.4. Series de Fourier 2.4.1. Transformada de Fourier 2.4.2. Transformada de Fourier discreta 2.4.3. Transformada rÃ¡pida de Fourier 2.4.3.1. Algoritmo de FFT 2.5. Respuesta al impulso y respuesta de frecuencia 2.6. ConvoluciÃ³n 2.6.1. Teorema de ConvoluciÃ³n 2.7. Filtros digitales 2.7.1. EcuaciÃ³n de diferencias 2.7.2. Filtros FIR 2.7.2.1. DiseÃ±o de filtros FIR 2.8. Algoritmos de procesamiento digital de audio 2.8.1. Over Drive 2.8.2. Distortion 2.8.3. Ecualizador 2.8.4. Vibrato 2.8.5. Chorus 2.8.6. Ring Modulator 2.8.7. Tremolo 2.8.8. Auto Panner 2.8.9. Delay 2.9. ComputaciÃ³n paralela en GPU 2.9.1. GPGPU 2.9.2. CUDA 2.9.2.1. Modelo de programaciÃ³n 2.9.2.2. CompilaciÃ³n del cÃ³digo CUDA 2.9.2.3. Modelo de memoria 2.9.2.4. CUDA Framework 2.9.2.5. CUFFT CapÃ­tulo 3. DiseÃ±o e ImplementaciÃ³n 3.1. Detalles de implementaciÃ³n 3.2. ImplementaciÃ³n general del plugin VST 3.3. Estructura de clases del plugin VST 3.3. ImplementaciÃ³n del GUI 3.4. ImplementaciÃ³n de los efectos de audio 3.4.1. Over Drive 3.4.2. Distortion 3.4.3. Ecualizador 3.4.4. Vibrato 3.4.5. Chorus 3.4.6. Ring Modulator 3.4.7. Tremolo 3.4.8. Auto Panner 3.4.9. Delay CapÃ­tulo 4. Pruebas y Resultados 4.1. DescripciÃ³n del ambiente de pruebas 4.2. MediciÃ³n de Tiempo 4.3. Resultados 4.3.1. Over Drive 4.3.2. Distortion 4.3.3. Ecualizador 4.3.4. Vibrato 4.3.5. Chorus 4.3.6. Ring Modulator 4.3.7. Tremolo 4.3.8. Auto Panner 4.3.9. Delay 4.3.10. Tiempo de ejecuciÃ³n promedio de un solo efecto 4.3.11. Nueve efectos de audio simultÃ¡neos CapÃ­tulo 5. Conclusiones y Trabajos Futuros Referencias BibliogrÃ¡ficas Glosario