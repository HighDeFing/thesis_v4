Universidad Central de Venezuela Facultad de Ciencias Escuela de Computación Laboratorio de Redes Móviles e Inalámbricas (ICARO) Centro de Computación Gráfica (CCG) Diseño e implementación de una arquitectura de software genérica para el control de robots móviles basada en realidad aumentada Trabajo especial de grado presentado ante la Ilustre Universidad Central de Venezuela Por el Bachiller Miguel Angel Astor Romero Tutores: Prof. David Pérez Abreu y Prof. Walter Hernández Caracas, septiembre de 2014 ii Universidad Central de Venezuela Facultad de Ciencias Escuela de Computación Laboratorio de Redes Móviles e Inalámbricas (ICARO) Centro de Computación Gráfica Acta del veredicto Quienes suscriben, Miembros del Jurado designado por el Consejo de la Escuela de Compu- tación para examinar el Trabajo Especial de Grado, presentado por el Bachiller Miguel Ángel Astor Romero C.I.: 18810993, con el t́ıtulo “Diseño e implementación de una arquitectura de software genérica para el control de robots móviles basada en realidad aumentada”, a los fines de cumplir con el requisito legal para optar al t́ıtulo de Licenciado en Computación, dejan constancia de lo siguiente: Léıdo el trabajo por cada uno de los Miembros del Jurado, se fijó el d́ıa 19 de septiembre de 2014, a las 11:00 am, para que su autor lo defendiera en forma pública en el Centro de Computación Gráfica, lo cual este realizó mediante una exposición oral de su contenido, y luego respondió satisfactoriamente a las preguntas que les fueron formuladas por el Jurado, todo ello conforme a lo dispuesto en la Ley de Universidades y demás normativas vigentes de la Universidad Central de Venezuela. Finalizada la defensa pública del Trabajo Especial de Grado, el jurado decidió aprobarlo. En fe de lo cual se levanta la presente acta, en Caracas el 19 de septiembre de 2014, dejándose también constancia de que actuó como Coordinador del Jurado el Profesor Tutor Walter Hernandez. Prof. Walter Hernández (Tutor) Profa. Zenaida Castillo Prof. David Perez Abreu Prof. Hector Navarro (Jurado principal) (Tutor) (Jurado principal) iv Agradecimientos Gracias a la profesora Karima Velazquez por su inestimable ayuda para determinar el código de bloques utilizado durante el desarrollo de este proyecto. Gracias a Teresa Tavernelli, Francisco Sans, Fernando Crema, Audel Dugarte y Deyban Perez por su ayuda en la realización de las pruebas de usuario. Dedicado a Elba Salazar, a mis padres y a mi hermano. v Resumen T́ıtulo: Diseño e implementación de una arquitectura de software genérica para el control de robots móviles basada en realidad aumentada Autor: Miguel Ángel Astor Romero Tutores: Prof. David Pérez Abreu, Prof. Walter Hernández Hoy en d́ıa la proliferación de los dispositivos móviles de alta potencia para consumidores ha tráıdo con sigo la posibilidad de difundir tecnoloǵıas como la realidad aumentada, algo que en tiempos anteriores era sumamente engorroso e incluso inviable. Aśı mismo, el campo de la robótica se ha beneficiado de la aceptación por parte del público general de los robots para entornos caseros y usuarios aficionados, como los distribuidos por la empresa LEGO. Con base en la amplia disponibilidad de estos dispositivos se hace factible el desarrollo de arquitecturas complejas que combinen ambas tecnoloǵıas manteniendo un bajo costo económico. Por lo expuesto anteriormente, el presente trabajo de investigación plantea el diseño y desarrollo de una arquitectura de hardware y software para el control de robots móviles, la cual hace uso de las posibilidades provistas por la realidad aumentada para crear experiencias de control enriquecidas por interacciones virtuales. Adicionalmente se presenta una implementación de esta arquitectura para el sistema operativo Android y las consolas de videojuegos OUYA, utilizando robots LEGO Mindstorms NXT TM . Palabras clave: Robótica, realidad aumentada, robots móviles, LEGO Mindstorms, Android, OUYA vi Índice general Índice de figuras IX Índice de tablas XI 1. Introducción 1 1.1. Planteamiento del problema . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 1.2. Objetivo general . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 1.3. Objetivos espećıficos . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 1.4. Justificación . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 1.5. Distribución del documento . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 2. Marco Teórico 4 2.1. Realidad Aumentada . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 2.1.1. Realidad Mixta y el contexto de la realidad aumentada . . . . . . . . 4 2.1.2. El continuo Realidad-Virtualidad . . . . . . . . . . . . . . . . . . . . 5 2.1.3. Aplicaciones de la realidad aumentada . . . . . . . . . . . . . . . . . 7 2.2. Robótica . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 2.2.1. Definición de robótica . . . . . . . . . . . . . . . . . . . . . . . . . . 12 2.2.2. Tipos de robots . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 2.2.3. Mecanismos de movimiento para robots móviles terrestres . . . . . . 16 2.2.4. Aplicaciones de la robótica . . . . . . . . . . . . . . . . . . . . . . . . 18 2.3. La realidad aumentada y el control de robots móviles . . . . . . . . . . . . . 21 3. Método de investigación y herramientas utilizadas 24 3.1. Metodoloǵıa . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 3.2. Herramientas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 3.2.1. Herramientas de hardware . . . . . . . . . . . . . . . . . . . . . . . . 26 3.2.2. Herramientas de software . . . . . . . . . . . . . . . . . . . . . . . . . 27 4. Descripción de la solución implementada 31 4.1. Diseño arquitectónico de la solución . . . . . . . . . . . . . . . . . . . . . . . 31 4.1.1. Diseño de casos de uso . . . . . . . . . . . . . . . . . . . . . . . . . . 32 4.1.2. Control del robot . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33 vii 4.1.3. Escenarios jugables y despliegue de objetos virtuales. . . . . . . . . . 34 4.2. Descripción de la implementación de referencia . . . . . . . . . . . . . . . . . 35 4.2.1. Diseño de clases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35 4.2.2. Detalles del desarrollo de los módulos del robot y la cámara . . . . . 37 4.2.3. Patrones de diseño utilizados en la aplicación de control . . . . . . . . 40 4.2.4. Detección de marcadores . . . . . . . . . . . . . . . . . . . . . . . . . 45 4.2.5. Bomb Game, el escenario de demostración . . . . . . . . . . . . . . . 49 5. Pruebas y resultados 51 5.1. Medición de distancias óptimas para reconocimiento de marcadores . . . . . 51 5.2. Perfilado del procesamiento en el estado de juego . . . . . . . . . . . . . . . 52 5.2.1. Resultados y análisis . . . . . . . . . . . . . . . . . . . . . . . . . . . 52 5.3. Tasas de recepción de video en el dispositivo de control . . . . . . . . . . . . 53 5.3.1. Resultados y análisis . . . . . . . . . . . . . . . . . . . . . . . . . . . 53 5.4. Determinación del punto de saturación del video . . . . . . . . . . . . . . . . 55 5.4.1. Resultados y análisis . . . . . . . . . . . . . . . . . . . . . . . . . . . 55 5.5. Pruebas de usuario . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56 5.5.1. Resultados y análisis . . . . . . . . . . . . . . . . . . . . . . . . . . . 57 6. Conclusiones 62 6.1. Contribuciones . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63 6.2. Limitaciones . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63 6.3. Trabajos futuros . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63 Anexos 65 A. El robot LEGO Mindstorms TM 66 A.1. El MIT Programmable Brick . . . . . . . . . . . . . . . . . . . . . . . . . . . 66 A.2. Evolución del LEGO Mindstorms . . . . . . . . . . . . . . . . . . . . . . . . 67 A.2.1. LEGO Mindstorms RCX . . . . . . . . . . . . . . . . . . . . . . . . . 67 A.2.2. LEGO Mindstorms NXT . . . . . . . . . . . . . . . . . . . . . . . . . 68 A.2.3. LEGO Mindstorms EV3 . . . . . . . . . . . . . . . . . . . . . . . . . 70 B. Listado del código de generación de marcadores 72 C. Cuestionario utilizado en la prueba de usuario 76 D. Datos y gráficos adicionales de las pruebas realizadas 78 Referencias 81 viii Índice de figuras 2.1. El continuo Realidad-Virtualidad. . . . . . . . . . . . . . . . . . . . . . . . . 6 2.2. Despliegue de ultrasonido sobre una paciente. . . . . . . . . . . . . . . . . . 8 2.3. AR Games en el Nintendo 3DS TM . . . . . . . . . . . . . . . . . . . . . . . . . 10 2.4. ARQuake en ejecución visto a través de un HMD. . . . . . . . . . . . . . . . 11 2.5. Visualización en el sistema ARTHUR. . . . . . . . . . . . . . . . . . . . . . . 12 2.6. Diagrama de un manipulador industrial t́ıpico. . . . . . . . . . . . . . . . . . 14 2.7. Un robot móvil para exploración espacial. . . . . . . . . . . . . . . . . . . . 15 2.8. Los robots ASIMO y P3 de Honda. . . . . . . . . . . . . . . . . . . . . . . . 16 2.9. Fotograf́ıa de Titán tomada por la sonda Huygens. . . . . . . . . . . . . . . 20 2.10. El sistema de control presentado por Milgram. . . . . . . . . . . . . . . . . . 22 2.11. El juego Augmented Colliseum. . . . . . . . . . . . . . . . . . . . . . . . . . 23 3.1. El successfull Git branching model . . . . . . . . . . . . . . . . . . . . . . . . 25 3.2. La consola de videojuegos OUYA. . . . . . . . . . . . . . . . . . . . . . . . . 27 3.3. El Java Universal Tween Engine. . . . . . . . . . . . . . . . . . . . . . . . . 30 4.1. Diseño general de la arquitectura. . . . . . . . . . . . . . . . . . . . . . . . . 32 4.2. Casos de uso de nivel 0. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32 4.3. Casos de uso de nivel 1. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33 4.4. Movimiento del brazo virtual. . . . . . . . . . . . . . . . . . . . . . . . . . . 34 4.5. Icono de la implementación de referencia. . . . . . . . . . . . . . . . . . . . . 35 4.6. Diagrama de clases de NxtAR-core. . . . . . . . . . . . . . . . . . . . . . . . 36 4.7. Enlaces de red en NxtAR. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39 4.8. Los estados principales de NxtAR-core. . . . . . . . . . . . . . . . . . . . . . 41 4.9. Patrón de calibración de la cámara. . . . . . . . . . . . . . . . . . . . . . . . 42 4.10. El brazo virtual implementado. . . . . . . . . . . . . . . . . . . . . . . . . . 43 4.11. Composición de entidades. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44 4.12. Uno de los marcadores utilizados. . . . . . . . . . . . . . . . . . . . . . . . . 46 4.13. Pasos del algoritmo de detección de marcadores. . . . . . . . . . . . . . . . . 47 4.14. Obtención de las transformaciones de un marcador. . . . . . . . . . . . . . . 49 4.15. Las bombas del juego Bomb Game. . . . . . . . . . . . . . . . . . . . . . . . 49 5.1. Tiempo promedio de procesamiento por cuadro. . . . . . . . . . . . . . . . . 53 ix 5.2. Porcentaje de tiempo consumido por distintos procesamientos. . . . . . . . . 54 5.3. Tasas de transmisión de video. . . . . . . . . . . . . . . . . . . . . . . . . . . 54 5.4. Comportamiento de la recepción de video. . . . . . . . . . . . . . . . . . . . 56 5.5. Preferencia por dispositivos de control. . . . . . . . . . . . . . . . . . . . . . 57 5.6. Preferencia por mecanismo de control. . . . . . . . . . . . . . . . . . . . . . 58 5.7. Apreciación del tiempo de respuesta de la pantalla táctil. . . . . . . . . . . . 59 5.8. Apreciación del tiempo de respuesta del gamepad. . . . . . . . . . . . . . . . 59 5.9. Apreciación de la dificultad de controlar al robot. . . . . . . . . . . . . . . . 60 5.10. Apreciación de la interfaz en la tablet. . . . . . . . . . . . . . . . . . . . . . . 60 5.11. Utilidad de la pantalla de ayuda. . . . . . . . . . . . . . . . . . . . . . . . . 61 5.12. Dificultad de controlar el brazo virtual del robot. . . . . . . . . . . . . . . . 61 A.1. El MIT Programmable Brick . . . . . . . . . . . . . . . . . . . . . . . . . . . 67 A.2. El brick del Mindstorms RCX 2.0. . . . . . . . . . . . . . . . . . . . . . . . . 68 A.3. El brick del Mindstorms NXT. . . . . . . . . . . . . . . . . . . . . . . . . . . 69 A.4. El brick del Mindstorms EV3. . . . . . . . . . . . . . . . . . . . . . . . . . . 70 D.1. Distribución del tiempo promedio de procesamiento por cuadro. . . . . . . . 78 D.2. Distribución de las tasas de recepción de video. . . . . . . . . . . . . . . . . 79 D.3. Distribución del comportamiento de la recepción de video. . . . . . . . . . . 80 x Índice de tablas 2.1. Clases de dispositivos de Realidad Mixta. . . . . . . . . . . . . . . . . . . . . 7 3.1. Dispositivos móviles utilizados. . . . . . . . . . . . . . . . . . . . . . . . . . 26 5.1. Distancias de reconocimiento de marcadores. . . . . . . . . . . . . . . . . . . 52 D.1. Datos estad́ısticos del tiempo de procesamiento por cuadro. . . . . . . . . . . 79 D.2. Datos estad́ısticos de la recepción de video. . . . . . . . . . . . . . . . . . . . 79 D.3. Datos estad́ısticos del comportamiento del video. . . . . . . . . . . . . . . . . 80 xi xii Caṕıtulo 1 Introducción Hoy en d́ıa la alta proliferación de dispositivos móviles programables de alto desempeño coloca en las manos de los usuarios y los desarrolladores la posibilidad de investigar y desa- rrollar a plenitud el uso de tecnoloǵıas que dadas su naturaleza se benefician ampliamente de las prestaciones de estos dispositivos. Un ejemplo de esto es la realidad aumentada, la cual a pesar de no ser una tecnoloǵıa reciente ha presentado dificultad en su difusión dado el engorro que representan todos los equipos y dispositivos necesarios para obtener movilidad con buen desempeño, esto antes de la llegada de los smartphones y tablets. Por otro lado, la robótica de bajo costo también ha disfrutado de un gran empuje en tiempos recientes gracias a proyectos enfocados a llevar esta tecnoloǵıa a las masas. Ejemplos de estos son los proyectos Arduino1, Raspberry Pi2 y LEGO Mindstorms. Estos proyectos permiten a los usuarios profesionales y aficionados realizar desarrollos que involucren robots y sistemas de sensores por muy bajo costo y con una gran cantidad de herramientas de apoyo. La disponibilidad de estas tecnoloǵıas mencionadas permite entonces el desarrollo de proyectos de investigación que las combinen de distintas maneras para obtener sistemas complejos. Una categoŕıa de estos sistemas son los sistemas de control, los cuales permiten a un operador humano el manipular a uno o más robots para lograr un objetivo. Y es en esta área donde se ubica el presente trabajo de investigación. 1.1. Planteamiento del problema Actualmente han sido desarrollados múltiples sistemas de control para robots móviles que hacen uso de la realidad aumentada para enriquecer la simulación presentada al usuario. Sin embargo, todos estos trabajos hacen uso ya sea de herramientas ad hoc diseñadas para el problema en cuestión o de dispositivos comerciales para propósitos espećıficos. Esto limita a quienes desean realizar investigaciones en esta área dado que dichas soluciones, si es que pueden ser adquiridas en primer lugar, suelen implicar un desembolso económico muy elevado 1http://www.arduino.cc 2http://www.raspberrypi.org 1 http://www.arduino.cc http://www.raspberrypi.org que fácilmente puede estar fuera del alcance de instituciones de investigación modestas. Esto nos lleva preguntarnos si será posible aprovechar las capacidades provistas por los dispositivos móviles para consumidores, verbigracia los smartphones y las tablets, junto a robots modulares de bajo costo para desarrollar un sistema de control por realidad aumentada con caracteŕısticas sofisticadas. 1.2. Objetivo general El objetivo principal del presente trabajo es diseñar y desarrollar un sistema de realidad aumentada para tablets y smartphones, el cual debe involucrar un robot móvil controlado por teleoperación. 1.3. Objetivos espećıficos Definir y diseñar una arquitectura de hardware y software. Desarrollar un módulo que permita capturar imágenes con la cámara del dispositivo las cuales serán transmitidas v́ıa WiFi a otro dispositivo para su procesamiento. Desarrollar un módulo el cual reciba las imágenes capturadas para incorporarles una escena virtual. Modificar el robot diseñado en trabajos previos [76], de forma que sea posible incorpo- rarle un smartphone que pueda utilizar como sistema de visión. Diseñar e implementar un protocolo de capa de aplicación que permita que los progra- mas desarrollados puedan comunicarse entre śı correctamente. Diseñar e implementar el conjunto de reglas y objetos que componen un videojuego el cual formará parte de la escena aumentada de la segunda aplicación mencionada. Implementar el protocolo necesario para comunicarse directamente con el robot móvil v́ıa Bluetooth. 1.4. Justificación Este trabajo se justifica en las posibilidades de investigación y desarrollo que se abren al poseer una arquitectura que permita controlar robots móviles de forma remota, presentando a los usuarios con un entorno virtual con el cual es posible simular distintos escenarios que en la realidad pueden ser muy costosos o peligrosos. Un ejemplo claro de esto es una simulación de un campo minado, con la cual se pueden entrenar operadores de robots móviles teleoperados especializados en la detección y desactivación de dichos armamentos, esto sin la necesidad de poner en riesgo ni al robot o al operador del mismo. Aśı mismo muchas 2 otras aplicaciones pueden desarrollarse a partir de la solución mostrada en esta propuesta, pasando por campos tan diversos como la exploración, el entretenimiento, el entrenamiento, entre otros. 1.5. Distribución del documento El presente documento describe la investigación y el desarrollo llevados a cabo para cumplir con los objetivos antes descritos. El documento está organizado en los siguientes 6 caṕıtulos: El presente Caṕıtulo 1 contiene el planteamiento del problema y su justificación, presentando además los objetivos propuestos para resolverlo. El Caṕıtulo 2 detalla los ba- samentos teóricos necesarios para comprender el trabajo realizado, incluyendo un resumen de los antecedentes y los trabajos relacionados con esta investigación. En el Caṕıtulo 3 se describen las caracteŕısticas y prestaciones de las distintas herramientas de hardware y soft- ware utilizadas durante el desarrollo del trabajo, presentadas de forma independiente del contexto en el que fueron utilizadas. El Caṕıtulo 4 contiene la descripción detallada del tra- bajo realizado para cumplir los objetivos propuestos, haciendo especial énfasis en los detalles del desarrollo de los módulos propuestos. El Caṕıtulo 5 se muestran los resultados de cinco pruebas realizadas a las aplicaciones desarrolladas. Finalmente, en el Caṕıtulo 6 se presentan las conclusiones del trabajo realizado, describiendo los aportes realizados y las limitaciones encontradas, junto al planteamiento de posibles trabajos futuros. 3 Caṕıtulo 2 Marco Teórico En este caṕıtulo se presenta de manera concisa los fundamentos teóricos necesarios para comprender y utilizar las distintas tecnoloǵıas involucradas en este trabajo de investigación. Primero se describen las bases teóricas de la realidad aumentada aśı como los trabajos más representativos desarrollados en el área. Luego se da una breve introducción al campo de la robótica, haciendo especial énfasis en los distintos tipos de robots y sus sistemas de control y locomoción. Finalmente se presenta un estado del arte acerca de sistemas de control para robots que incorporan componentes de realidad virtual o aumentada para mejorar o extender la experiencia del usuario. 2.1. Realidad Aumentada En esta sección se describen los conceptos básicos que definen a la realidad aumentada como campo de investigación y desarrollo. El caṕıtulo comienza con una introducción al con- cepto de realidad aumentada, ubicándola dentro del Continuo Realidad-Virtualidad definido por Milgram y Kishino en [49], con el objetivo de colocar a la Realidad Aumentada dentro de un marco de referencia, comparándola y contrastándola con la Virtualidad Aumentada y la Realidad Virtual. Posteriormente se describen distintos campos de estudio en los cuales se han realizado investigaciones sobre Realidad Aumentada, realizando un breve resumen de estudios significativos en cada área. 2.1.1. Realidad Mixta y el contexto de la realidad aumentada Azuma define en [7] a la realidad aumentada como una variante de la Realidad Virtual, la cual en lugar de intentar suplantar completamente al mundo real, se dedica a complementar la percepción que el usuario tiene de este, agregando elementos virtuales con la intención de presentar al usuario con información que puede percibir directamente con sus sentidos. Un sistema tiene que poseer las siguientes caracteŕısticas para ser considerado dentro del esquema de la realidad aumentada: Combinar objetos virtuales con el entorno real. 4 Poseer interactividad en tiempo real. Realizar registro (alineación) de objetos en 3D. Para poder cumplir con estas caracteŕısticas, los sistemas de realidad aumentada deben hacer uso de múltiples tecnoloǵıas y dispositivos. Como mı́nimo se necesita disponer de un sistema de despliegue para poder mostrar la escena aumentada sobre la escena real y de un sistema de sensores para captar datos de entrada provenientes del ambiente real y del usuario. La naturaleza de los sensores y del sistema de despliegue depende del tipo de realidad aumentada que se esté utilizando, sea visual, auditiva o háptica (lease táctil). Esta lista de caracteŕısticas permite un amplio abanico de tecnoloǵıas de sensores y despliegue, pasando por los HMD (Head-Mounted Displays - Visualizadores Montados en la Cabeza), los monitores monoscópicos o estereoscópicos, y los VRD (Virtual Retinal Displays - Visualizadores Virtuales en la Retina), entre otros. Quedan fuera de esta definición medios como el cine de efectos especiales (por ejemplo Jurassic Park) debido a la falta de interacción; aśı como los videojuegos en su definición clásica por la falta de registro de objetos reales y virtuales, y los sistemas que realizan superposición de elementos 2D sobre el entorno real visible. En [6] Azuma y colaboradores modifican la lista de caracteŕısticas anterior eliminando el requisito de que la alineación de objetos se realice en 3 dimensiones, reescribiendo este punto como sigue: Alinea objetos reales y virtuales entre si. Esto permite la inclusión de los sistemas que realizan superposición interactiva de elemen- tos 2-D sobre el entorno real dentro del esquema de la realidad aumentada. Adicionalmente esta nueva definición también extiende el ámbito de percepción de la realidad aumentada más allá del sentido de la vista, incluyendo a los sistemas que proveen al usuario con informa- ción táctil utilizando dispositivos hápticos e información auditiva utilizando sonido surround, entre otros [47] [66] [22]. 2.1.2. El continuo Realidad-Virtualidad En [49] y [50] Paul Milgram, Fumio Kishino y colaboradores definen al continuo Realidad- Virtualidad como una agrupación de tecnoloǵıas de despliegue que involucran la represen- tación de una realidad, organizadas según la virtualidad de los elementos que la conforman. Las tecnoloǵıas que caen dentro de este continuo son llamadas tecnoloǵıas de Realidad Mix- ta. En un extremo se tiene a la realidad tal cual la percibimos con nuestros sentidos, sea en persona o de manera indirecta (por ejemplo, por medio de un v́ıdeo). En el extremo opuesto del continuo se tienen los llamados ambientes virtuales que corresponden con la Realidad Virtual, donde todos los objetos que percibe el usuario son completamente sintéticos. Este continuo puede observarse en la Figura 2.1. El objetivo de este continuo es poder clasificar los distintos sistemas de Realidad Mix- ta, atendiendo principalmente a la proporción de objetos reales contra virtuales presentes 5 Figura 2.1: El continuo Realidad-Virtualidad. Figura recuperada de [49]. en el sistema de despliegue. Estos sistemas pueden clasificarse como sistemas de realidad aumentada, donde una escena predominantemente real se complementa con algunos objetos virtuales; sistemas de Virtualidad Aumentada, en los cuales una escena sintética se comple- menta con algunos objetos extráıdos del entorno real del usuario; o sistemas de Realidad Virtual, donde todos los objetos que puede percibir el usuario son completamente sintéticos. Cabe resaltar que Milgram y Kishino no plantean una escala exacta para determinar a partir de que proporción de objetos reales contra virtuales se considera que un sistema es de reali- dad aumentada o de Virtualidad Aumentada, quedando esto a criterio de los diseñadores del sistema en cuestión. De acuerdo con Milgram, Kishino y colaboradores los dispositivos de Realidad Mixta, entendiéndose por estos a los sistemas de realidad aumentada, Virtualidad Aumentada y Realidad Virtual, se pueden categorizar dentro de 7 clases [50] visibles en el Cuadro 2.1. Estas clases se definen en base a cuatro parámetros que determinan la posición de dichas clases en el continuo Realidad-Virtualidad. Los cuatro parámetros son la virtualidad del mundo circundante, principalmente real o principalmente virtual; el esquema de visión, que es directo cuando el usuario puede percibir de manera inmediata al mundo que le rodea con sus sentidos o indirecto en caso contrario; el marco de referencia que se refiere a si el usuario se encuentra dentro del mundo a medida que lo percibe (egocéntrico), o si se encuentra fuera de este (exocéntrico); y la representación conforme, que se refiere a la necesidad de realizar un registro exacto de los objetos aumentados con los objetos que pertenecen intŕınsecamente al mundo [78], sean estos los objetos reales o los objetos virtuales. Cabe resaltar que el cuadro 2.1 se refiere únicamente a los sistemas de Realidad Mixta que se enfocan en el aspecto visual. Como se mencionó, el objetivo principal de la realidad aumentada es el presentar al usua- rio con información complementaria al mundo real que no puede ser percibida directamente con los sentidos. La finalidad es que esta información adicional ayude al usuario a desem- peñar tareas con mayor facilidad dentro de un dominio espećıfico. En este sentido, la realidad aumentada cabe dentro de la llamada Amplificación de la Inteligencia definida por Frederick Brooks en [15], que se refiere al uso de computadoras como herramientas para hacer que determinadas tareas sean más fáciles de realizar. 6 Clase Mundo Esquema Marco de Representación de Sistema de Visión Referencia Conforme Vı́deo basado en monitores con Real Indirecta Exocéntrico No superposición virtual Vı́deo basado en HMD con Real Indirecta Egocéntrico No superposición virtual Óptica basada en HMD con Real Directa Egocéntrico Si superposición virtual Vı́deo basado en HMD con Real Indirecta Egocéntrico Si superposición virtual Mundo virtual por monitor Virtual Indirecta Exocéntrico No con superposición de v́ıdeo. Mundo virtual por HMD con Virtual Indirecta Egocéntrico No superposición de v́ıdeo. Mundo virtual con intervención Virtual Directa o Egocéntrico Si de objetos indirecta reales. Tabla 2.1: Clases de dispositivos de Realidad Mixta. Tabla recuperada de [50]. 2.1.3. Aplicaciones de la realidad aumentada Desde que se despertó el interés por la realidad aumentada durante la década de los 90s como se menciona en [6], estas tecnoloǵıas se han utilizado en una amplia variedad de campos de trabajo e investigación. En [77] Van Krevelen y Poelman, al igual que Azuma y colaboradores en [7] y [6], describen la aplicabilidad de la realidad aumentada en los siguientes campos: medicina, industria, aviación, entretenimiento, videojuegos y entornos de trabajo colaborativos enfocados en ambientes de oficina. Es por esto que a continuación se abordarán las investigaciones y trabajos más relevantes en cada una de estas áreas. 7 Medicina La realidad aumentada ha sido utilizada en aplicaciones médicas en ámbitos que van desde el entrenamiento de personal médico hasta la presentación de datos volumétricos ob- tenidos directamente del paciente sobre el mismo en tiempo real [7] [77]. Estas aplicaciones tienen requisitos técnicos muy altos en lo referente a velocidad y precisión de la información presentada, la cual dependiendo de la aplicación final debe ser obtenida de bases de datos (por ejemplo un sistema que muestra historias médicas sobre el paciente) o directamente del paciente utilizando dispositivos de ultrasonido, rayos X, o tomograf́ıas, como se observa en la Figura 2.2. Figura 2.2: Despliegue de ultrasonido sobre una paciente. Imagen recuperada de [17]. Los dispositivos de realidad aumentada, principalmente basados en See-Through HMD han sido utilizados exitosamente en diferentes áreas de la medicina como lo son la ciruǵıa laparoscópica [26], la obstetricia [17] [10] o el entrenamiento [45]. Industria En la industria la realidad aumentada ha sido utilizada exitosamente en áreas como la manufactura y ensamblado de componentes [16] [40] [70], el entrenamiento de personal técnico, reparación de dispositivos [25] e incluso la planificación de rutas para robots ma- nipuladores industriales [51] [21] (véase la sección 2.2.2). En estas aplicaciones el uso de la realidad aumentada suele estar ligado a dispositivos HMD, sean See-Through o de video, y usualmente toman la forma de gráficos 3-D superpuestos sobre la maquinaria o las piezas que se estén manipulando. Estos gráficos son tomados de planos o esquemas técnicos de la maquinaria en cuestión. 8 Aviación y usos militares La aviación militar ha hecho uso de dispositivos See-Through para proveer a los pilotos con información referente a los parámetros actuales de su aeroplano, aśı como identificación de otros aviones en vuelo o “mirillas” automáticas para los distintos armamentos del avión [80] [2]. Suelen hacer uso de tecnoloǵıas llamadas HUD (Heads Up Displays - Pantalla de Información) similares en concepto a los HMD. Los HUD permiten mostrar la información aumentada sobre una lámina de vidrio, utilizando en este caso gráficos vectoriales [7]. Estos dispositivos toman la forma de un panel de control colocado en la cabina del avión, o de un HMD incorporado al casco del piloto. Entretenimiento La realidad aumentada ha sido aplicada a diversos sistemas de entretenimiento, no li- mitándose exclusivamente al ámbito de los videojuegos. En el ámbito de las transmisiones de eventos deportivos suele hacerse uso de anotaciones virtuales aumentadas, colocadas so- bre las tomas del evento permiten que los espectadores puedan llevar la pista de elementos del juego en cuestión cuando estas son dif́ıciles de percibir, ya sea por su tamaño o por su velocidad. Para esto suelen utilizarse técnicas como el chroma-keying [30], que permite que los elementos aumentados puedan ser solapados por objetos del mundo, grabados en video. Por ejemplo, se ha hecho uso de esta tecnoloǵıa para resaltar marcas del terreno o incluso el balón en partidos de fútbol americano [28]. realidad aumentada en videojuegos El auge de los dispositivos móviles en los años recientes, como por ejemplo las tablets y los smartphones, los cuales suelen venir de fábrica con cámaras de video y hardware gráfico de buen desempeño permiten el desarrollo de juegos que toman las caracteŕısticas de los sistemas de realidad aumentada mencionados anteriormente. Diversos juegos han sido desarrollados para las plataformas Android TM e iOS TM , como por ejemplo Droid Shooting1, en el cual el jugador(a) debe destruir robots voladores basados en el logo de Android TM utilizando una pistola láser virtual. Recientemente Nintendo ha incorporado una serie de juegos de realidad aumentada en su ultima consola portátil, el Nintendo 3DS TM , disponible comercialmente desde febrero de 2011. Estos juegos llamados colectivamente AR Games vaŕıan desde sencillamente colocar diferentes personajes virtuales sobre cartas incluidas con la consola, hasta un juego de dis- paros similar en concepto a Droid Shooting, pasando por juegos de pesca, golf o arqueŕıa. Un ejemplo de los juegos del Nintendo 3DS TM puede verse en la Figura 2.3. ARQuake En el año 2000, Bruce Thomas y Wayne Piekarski presentaron un proyecto de investi- gación y desarrollo que consiste en un sistema de juego de realidad aumentada capaz de 1http://bit.ly/ZHrreh 9 http://bit.ly/ZHrreh Figura 2.3: AR Games en el Nintendo 3DS TM . Imagen tomada por el autor. funcionar tanto en interiores como en exteriores [74]. El sistema funciona utilizando un con- junto de dispositivos que incluyen una laptop, un GPS de alta presición, un giroscópio, una cámara y un HMD [73]. Con estos dispositivos se logra que el usuario pueda desplazarse libremente dentro de un área predefinida, sea dentro o fuera de un edificio, con seis gra- dos de libertad [59]. El juego en si consiste en una modificación del juego Quake TM de Id Software R©. Esta modificación fue desarrollada aprovechando la disponibilidad del código fuente2 del juego bajo la licencia GPL versión 23. El sistema utiliza una representación 3D del área en la que se desarrolla la partida [75] la cual se utiliza para poder ocultar elementos del juego como personajes, paquetes de vida, municiones, etc. Esta representación de la escena se registra sobre la escena real utilizando la cámara del sistema para detectar marcadores colocados sobre los edificios. Luego se despliega la escena en negro sin iluminación, de forma que no sea visible en los lentes del HMD. El GPS de alta presición se utiliza para detectar la ubicación del usuario dentro de la escena, para ayudar al sistema de registro. La laptop se utiliza para llevar la partida, utilizando la información del giroscopio para determinar hacia donde esta mirando el jugador(a)[61]. Este sistema de hardware es llamado Tinmith-metro por sus desarrolladores [60]. En la Figura 2.4 se puede observar una partida de ARQuake en desarrollo. Los personajes y los objetos del juego han sido modificados por los desarrolladores de forma que sean fáciles de ver sobre los lentes del Head- Mounted Display [73], utilizando colores brillantes y texturas diferentes a las que incorpora Quake TM por defecto. 2https://github.com/id-Software/Quake 3http://www.gnu.org/licenses/gpl-2.0.html 10 https://github.com/id-Software/Quake http://www.gnu.org/licenses/gpl-2.0.html Figura 2.4: ARQuake en ejecución visto a través de un HMD. Imagen recuperada de [74]. Ambientes colaborativos El uso de sistemas de realidad aumentada en ambientes colaborativos se ha enfocado principalmente a aplicaciones de planificación o control distribuido. Van Krevelen y Poel- man exponen una serie de usos en [77] que van desde análisis de planos arquitectónicos utilizando maquetas virtuales hasta control distribuido de trafico aéreo. Sin embargo, los ejemplos presentados se limitan a sistemas experimentales para ayudar a arquitectos traba- jando en planificación urbana utilizando las mencionadas maquetas virtuales, principalmente utilizando Head-Mounted Displays. Ejemplo de esto es el sistema ARTHUR de Broll y cola- boradores [14] que realiza incluso simulación de tráfico y peatones sobre un modelo virtual de una ciudad; este sistema puede observarse en la Figura 2.5. Tambien se ha estudiado el uso de superficies de proyección de forma que se pueda reducir el aparataje necesario en sistemas basados en Head-Mounted Displays, como es el caso de [65]. 2.2. Robótica En esta sección se presenta una introducción breve al campo de la robótica comenzando por su definición, haciendo énfasis en los robots como objeto de estudio. Posteriormente se realiza un resumen de los distintos tipos de robots reconocidos y de las distintas partes que componen el cuerpo de un robot. Se prosigue con un análisis de distintas áreas de la ciencia, la ingenieŕıa y la industria en las cuales la robótica tiene aplicación. El caṕıtulo concluye con 11 Figura 2.5: Visualización en el sistema ARTHUR. Imagen recuperada de [14]. una reseña histórica del robot modular LEGO Mindstorms TM . 2.2.1. Definición de robótica La Robótica es un campo de investigación multidisciplinario que se enfoca en el estudio y desarrollo de tecnoloǵıas relacionadas con dispositivos mecánicos destinados a relevar al ser humano de tareas que por su naturaleza son peligrosas, sumamente dif́ıciles o requieren de extrema precision [68]. Formalmente, el Diccionario de La Lengua Española define a la Robótica como la “técnica que aplica la informática al diseño y empleo de aparatos que, en sustitución de personas, realizan operaciones o trabajos, por lo general en instalaciones industriales”. El robot es el principal objeto de estudio de la robótica. Existen muchas definiciones de la palabra robot, dadas por las diferentes asociaciones nacionales e internacionales de inves- tigadores, desarrolladores y fabricantes de robots [72]. Para concentrarnos en una definición, podemos considerar la propuesta por la International Organization for Standardization (en adelante ISO). La ISO define a los robots como manipuladores multifuncionales progra- mables, dotados de la capacidad de interactuar con materiales, piezas o herramientas para desempeñar diversas tareas [53]. Independientemente de que definición de robot sea tomada en cuenta, todas ellas comparten los siguientes puntos en común: Los robots deben ser multifuncionales. Es decir, que cambiando su programa- ción debe ser capaz de realizar distintas tareas enmarcadas dentro de una aplicación espećıfica. 12 Los robots deben ser reprogramables. Derivado del punto anterior; debe ser po- sible modificar el comportamiento de los robots cambiando el programa que los dirige. Los robots deben ser autónomos. En otras palabras, el robot debe ser capaz de realizar su trabajo con la menor intervención humana posible. Los robots tienen muchos antecedentes históricos, que datan incluso de varios cientos de años antes de la era cristiana [68]. Se conocen historias y leyendas provenientes de la antigua Grecia que tratan sobre seres artificiales autónomos cuyas acciones pod́ıan ser controladas por sus dueños. Incluso se conoce que Leonardo da Vinci construyó múltiples dispositivos mecánicos parciales o totalmente autónomos [71]. El término robot como se utiliza en la actualidad fue introducido al vocabulario mundial por la obra de teatro Rossum’s Universal Robots [68], del autor checo Karel Čapek, estrenada en 1921. 2.2.2. Tipos de robots Existen tantos robots como aplicaciones hay para ellos. Sin embargo, para poder estu- diarlos hay que clasificarlos de alguna manera. Los robots pueden clasificarse atendiendo a distintos criterios que van desde su forma f́ısica, hasta su aplicación final pasando por su esquema de locomoción [72]. Para propósitos de este trabajo se detalla a continuación la clasificación propuesta por Russell y Norvig en [67], muy similar a la propuesta por Saha [68]. Russell y Norvig distinguen 3 tipos de robots fundamentales, atendiendo simultánea- mente a su método de locomoción y forma de interacción con el medio ambiente: los robots manipuladores, los robots móviles y los robots h́ıbridos. Robots manipuladores Los manipuladores, también llamados robots estacionarios, son aquellos robots que en to- do momento están anclados a una base fija. Toman la forma de brazos mecánicos articulados [67]. Estos son los robots más difundidos comercialmente a nivel mundial [67], y su principal aplicación es la industria [68], siendo utilizados en lineas de ensamblado y construcción. Su nombre proviene del hecho de que son principalmente utilizados para manipular piezas y materiales, usualmente haciendo uso de “manos” o agarraderas electrónicas. La Figura 2.6 muestra un diagrama que representa a un robot manipulador industrial t́ıpico. Los manipuladores se subdividen en cuatro tipos dependiendo del sistema de coordenadas por el que rigen sus movimientos [68]. Esta clasificación se deriva del tipo de articulaciones que suelen poseer estos robots, las cuales pueden ser extensibles (tambien llamadas prismáti- cas), giratorias o rectiĺıneas. Los cuatro tipos de manipuladores son: Robots cartesianos. Cuando las distintas partes del robot se mueven de forma rec- tiĺınea y sus movimientos pueden definirse completamente dentro de un paraleleṕıpedo sólido, se dice que el robot en cuestión es cartesiano. 13 Figura 2.6: Diagrama de un manipulador industrial t́ıpico. Imagen recuperada de [67]. Robots ciĺındricos. Son aquellos robots cuya articulación principal puede girar sobre un eje fijo, definiendo un cilindro sólido dentro del cual pueden moverse las demás articulaciones del robot. Robots polares. También son llamados robots esféricos. Estos robots poseen dos articulaciones giratorias que culminan en una articulación prismática, lo que les permite moverse dentro de una semiesfera sólida. Robots de revolución. Son similares a los robots polares, en el sentido de que su volumen de acción es esférico. Estos robots poseen tres articulaciones giratorias, lo que les permite un mayor rango de acción. El volumen dentro del cual pueden moverse estos robots es más completo que en el caso de los robots polares. Estos robots forzosamente necesitan calcular sus movimientos utilizando técnicas de cinemática inversa recursiva [68], lo que los hace más d́ıciles de programar. Robots móviles Los robots móviles son aquellos que pueden moverse libremente dentro de su entorno ya sea utilizando ruedas, piernas, cintas transportadoras o incluso volando o nadando[67]. Suelen tomar formas muy variadas dependiendo de su aplicación final, como por ejemplo los robots vehiculares utilizados por la NASA (National Aeronautics and Space Administration - Administración Nacional para la Aeronáutica y el Espacio) para la exploración de Marte, o incluso robots con forma de serpiente [29] o de insectos, utilizados para exploración y búsqueda en espacios de dif́ıcil acceso. En la Figura 2.7 se observa un robot móvil tipo rover para la exploración espacial. Se distinguen tres tipos de robots móviles vehiculares [67]: Veh́ıculos terrestres sin tripulación. Del inglés unmanned land vehicule, son aque- llos robots cuyo medio de locomoción es el suelo. Tienen bastantes aplicaciones comer- 14 Figura 2.7: Un robot móvil para exploración espacial. Imagen recuperada de [67]. ciales como por ejemplo los robots de servicio, utilizados para realizar tareas domésticas [72]. Veh́ıculos aéreos sin tripulación. Son aquellos robots que tienen la capacidad de volar por los aires. Suelen tomar forma de aviones o helicopteros pequeños, o incluso de insectos [24]. Su nombre proviene del inglés unmanned aerial vehicle [62]. Veh́ıculos submarinos autónomos. Son robots para navegación submarina utiliza- dos para la exploración en oceanograf́ıa. Son llamados aśı por los autonomous under- water vehicles [4]. Robots h́ıbridos Se definen sencillamente como un robot móvil equipado con manipuladores [67]. A esta categoŕıa pertenecen los robots humanoides como los robots ASIMO, construido por Honda [69]; Nao de Aldebaran Robotics [31] ; y el par Robonaut-1 y Robonaut-2 de NASA [20]. Este tipo de robot poseen la capacidad de manipular materiales con mayor libertad en un espacio de trabajo mayor que los robots meramente manipuladores; sin embargo, son mucho más complejos en su diseño y programación que los otros tipos de robots [67]. En la Figura 2.8 se puede ver al robot ASIMO de Honda, el cual es un ejemplo de un robot h́ıbrido humanoide. Otros Aparte de los tres tipos de robots mencionados, existen otros robots de uso espećıfico que no encajan dentro de las categoŕıas anteriores [67]. Estos robots son los siguientes: Prótesis robóticas. Son prótesis mecánicas que responden de manera automática a las instrucciones dadas por el paciente. 15 Figura 2.8: Los robots ASIMO y P3 de Honda [67]. Robots modulares. Son aquellos robots que pueden ser desensamblados y reensam- blados en diferentes configuraciones dependiendo de la aplicación a la que estén desti- nados. Los robots LEGO MindstormsTM(vease la sección A) son un ejemplo de robots modulares. Entornos inteligentes. Son edificaciones autónomas que controlan parametros am- bientales e interactúan f́ısicamente con sus habitantes. Son el objeto de estudio de la domótica, que es la rama de la ingenieŕıa que estudia y desarrolla sistemas autónomos computarizados para control doméstico [82]. Enjambres. Del inglés swarms, son los sistemas robóticos multiagentes compuestos por múltiples robots autónomos cooperativos. 2.2.3. Mecanismos de movimiento para robots móviles terrestres Para poder desplazarse los robots móviles terrestres hacen uso de una amplia variedad de métodos de locomoción. En particular se distinguen cuatro tipos de medios de locomoción, que se distinguen por los componentes que realizan el desplazamiento del robot. Los métodos en cuestión son los siguientes: Piernas o Patas. Este método de locomoción se basa en la forma en que se desplazan los organismos vivos en la naturaleza. Mediante el uso de piernas o patas se otorga al robot de una amplia capacidad de movimiento, siendo especialmente eficaces en terrenos abruptos. Para poder realizar traslación y rotación del robot estas patas deben poseer al menos dos grados de libertad [72]. Los robots con patas son susceptibles de dos problemas propios conocidos como inestabilidad estática e inestabilidad dinámica, los cuales se refieren a la necesidad de mantener el equilibrio del robot tanto cuando 16 está en reposo, como cuando está en movimiento respectivamente. Para garantizar el equilibrio del robot es necesario lograr que este mantenga su centro de gravedad dentro del área definida por sus patas [72]. Existen tres tipos de robots con patas: los b́ıpedos, los cuadrúpedos y los hexápodos. Los robots b́ıpedos son aquellos que, al igual que los seres humanos, mantienen a lo sumo dos pies sobre el suelo. Estos robots suelen poseer pies considerablemente grandes para poder lidiar con la inestabilidad, tanto estática como dinámica. Por su parte los robots cuadrúpedos utilizan cuatro patas para su locomoción. Este tipo de robots suelen ser mucho más estables que los robots b́ıpedos siempre y cuando mantengan al menos tres patas sobre el suelo en todo momento [72]. Finalmente los robots hexápodos son aquellos que utilizan seis patas, tres en cada lado del robot. Estos robots son muy estables pero necesitan de un software de control más sofisticado que sea capaz de coordinar las seis patas para poder trasladar o rotar al robot. Orugas o cadenas de tracción. Este método de locomoción utiliza cadenas de trac- ción de manera similar a un tanque de guerra. Se suelen aplicar en robots que necesiten desplazarse en terrenos muy abruptos o deslizantes puesto que las cadenas de tracción proporcionan muy buena tracción sobre el terreno [72]. Ruedas. Son el método de transporte más común [72]. Los robots pueden hacer uso de cuatro tipos de ruedas, dependiendo del sistema de locomoción que utilicen: • Ruedas motrices. Son las ruedas conectadas a un efector que impulsa su movi- miento. Este efector suele ser un servomotor. • Ruedas direccionales. Son ruedas conectadas a un eje de giro y se utilizan para cambiar la dirección de desplazamiento del robot. No están conectadas a efectores. • Ruedas libres. Son ruedas capaces de girar libremente sobre un eje, el cual puede ser fijo u oscilante. En el caso de que el eje sea oscilante se dice que la rueda es una rueda loca (del inglés caster wheel.) • Ruedas omnidireccionales. Estas ruedas poseen rodillos en su borde, lo que les permite realizar desplazamiento en dos direcciones sin tener que girar el cuerpo del robot. Esquemas de movimiento con ruedas Para el caso de los robots con ruedas, estas pueden utilizarse en múltiples configuraciones que afectan la movilidad del robot, lo que a su vez determina la complejidad del sistema de control motriz del mismo. Se distinguen cuatro esquemas de movimiento diferentes: Sistema diferencial. Los robots móviles que hacen uso de sistemas diferenciales para su movimiento son construidos utilizando dos ruedas motrices conectadas a motores independientes, una en cada lado del robot. Adicionalmente se utilizan una o más ruedas locas en la parte posterior del robot para ayudarle a mantener el equilibrio 17 [72]. Para controlar la dirección del robot se vaŕıa la velocidad de giro de cada motor. Si el motor derecho gira a mayor velocidad que el motor izquierdo entonces el robot girará a la derecha, o viceversa. Los robots que presentan este sistema de locomoción suelen necesitar de un código de control bastante sofisticado debido a que para poder desplazarse en linea recta es necesario que ambos motores giren a exactamente la misma velocidad. Esto implica que el robot debe analizar el estado de los motores en todo momento y compensar cuando estos se desfasan. El robot diseñado por Carlos Tovar y Paolo Tosiani en su trabajo especial de grado es un ejemplo de un robot con sistema de locomoción diferencial [76]. El robot Tribot TM de The LEGO Group es otro ejemplo [36]. Sistema de Ackerman. Es el sistema de locomoción utilizado por los automóviles comunes. Se compone de dos pares de ruedas: un par de ruedas motrices conectadas a un único motor y un par de ruedas direccionales conectadas al mismo eje de giro. También es posible utilizar combinaciones tipo triciclo, utilizando dos ruedas motrices y una única rueda direccional. El sistema de control de movimiento en este tipo de robots suele ser más sencillo que en el caso de los robots con sistema diferencial; sin embargo, es necesario que el robot realice maniobras más complejas para poder desplazarse, debido a que el robot no puede girar sobre su propio eje vertical como un robot con sistema diferencial [72]. Sistema synchro drive . Este sistema utiliza tres ruedas las cuales son motrices y direccionales simultáneamente. Esto combina las ventajas de los sistemas diferencial y de Ackerman [72], permitiendo que el robot pueda girar sobre su eje vertical, al mismo tiempo que se simplifica el código de control. Sin embargo, este sistema es más complejo en su aspecto mecánico. Sistema diferencial dual. En este sistema se hace uso de dos pares diferenciales de ruedas motrices. Esto permite que el robot pueda desplazarse en linea recta o girar de forma más sencilla que un sistema diferencial puro. Su desventaja radica en el hecho de que los robots que utilizan este sistema no son capaces de avanzar o retroceder mientras giran, lo que obliga al robot a descomponer trayectorias curvas en segmentos de desplazamientos rectos [72]. 2.2.4. Aplicaciones de la robótica Gracias a su capacidad de reprogramación los robots pueden utilizarse para una gran cantidad de tareas. En particular se distinguen dos tipos de tareas o grupos de aplicación para los robots: industriales y de propósito espećıfico [68]. Los robots de uso industrial suelen ser manipuladores o robots móviles utilizados para diversas areas relacionadas a la construcción, ensamblado o transporte de piezas y materiales. Por otro lado, los robots de propósito espećıfico no siguen una norma común. Estos robots son diseñados y construidos a la medida dependiendo del trabajo final que vayan a realizar. 18 Subir Kumar Saha en su libro Introducción a la Robótica [68] enumera cuatro reglas que tienen como intención ayudar a determinar si un campo de aplicación es apto para ser automatizado con robots: 1. Si el trabajo es sucio, aburrido, peligroso o dif́ıcil. Esto es conocido como las cuatro D de la robótica (del inglés Dirty, Dull, Dangerous or Difficult.) 2. El uso de robots en dicho trabajo no debe dejar a algún ser humano desempleado. 3. Si es imposible conseguir a alguien que tenga la disposición para realizar el trabajo. 4. El uso de robots en dicho trabajo provee beneficios económicos a corto y largo plazo. Robots en la industria En el ámbito industrial se ha hecho uso de robots para realizar trabajos que por su na- turaleza son muy dif́ıciles para un ser humano. Esto comienza entre los años 1958 y 1961 cuando Joseph Engelberger y George Devol fundan la empresa UNIMATION Robotics Com- pany. Esta empresa introdujo el primer manipulador industrial, el cual serv́ıa para realizar la fundición y moldeado de metales para fabricar manijas de puertas. Otras áreas de la industria donde se hace un uso extensivo de robots son la fabricación y ensamblado de piezas para veh́ıculos; ensamblado de dispositivos electrónicos; fabricación de piezas para óptica; manipulación de productos alimenticios en fábricas; etc. Robots en la exploración Los robots han sido utilizados para ayudar a las labores de exploración en distintos ámbitos, dando apoyo cuando se trata de ambientes de dif́ıcil acceso para los seres humanos [67]. En particular, se ha hecho uso extensivo de robots para apoyar la construcción y el mantenimiento de la Estación Espacial Internacional, aśı como para la exploración de Marte y otros planetas. Un ejemplo muy llamativo es la sonda robótica Huygens, la cual aterrizó en Titán, el satélite más grande de Saturno, lugar donde capturó la fotograf́ıa que se observa en la Figura 2.9. Dentro del planeta Tierra, los robots para exploración han sido utilizados para examinar y manipular entornos peligrosos. En particular han sido utilizados para la exploración y registro de minas abandonadas, para el estudio de cráteres de volcanes, o incluso para limpiar residuos radioactivos en las plantas nucleares de Chernobyl y Three Mile Island conocidas por los terribles accidentes que ocurrieron en ellas [67]. Robots en la medicina Los robots han sido utilizados en la medicina en tres grandes areas: el apoyo al diagnóstico y la ciruǵıa; prótesis y sillas de rueda robotizadas; y apoyo a enfermeŕıa [67]. En el caso de la ciruǵıa son utilizados para ayudar a los cirujanos a colocar y manipular instrumentos cuando se realizan ciruǵıas del cerebro, ojos o corazón, las cuales necesitan de una extrema 19 Figura 2.9: Fotograf́ıa de Titán tomada por la sonda Huygens. Imagen recuperada de [52]. presición. Por su parte, las prótesis robóticas son utilizadas para otorgarle a los pacientes un mayor grado de control y precisión sobre sus miembros artificiales. Finalmente, los robots son utilizados en enfermeŕıa para ayudar en el transporte de alimentos, medicinas e insumos médicos. Robots en entornos militares Los robots han sido utilizados para transportar armamento y municiones, aśı como para buscar y desactivar minas en zonas de guerra. Un tipo peculiar de robot militar son los llamados drones, los cuales son una forma de veh́ıculos aéreos sin tripulación. Estos robots son utilizados en zonas de guerra para realizar reconocimiento y exploración de lineas enemigas. Robots en el entretenimiento El uso de la robótica para el entretenimiento se ha centrado en la creación de juguetes autónomos programables, a veces con fines comerciales, otras veces con fines educativos. Ejemplos de esto son los robots AIBO TM de Sony, o el robot LEGO Mindstorms TM de The 20 LEGO Group. Estos robots fueron creados y comercializados como juguetes; sin embargo, su gran capacidad de reprogramación los ha llevado a convertirse en sujetos de estudio en áreas de investigación como la Inteligencia Artificial o la Visión por Computador (del inglés Computer Vision.) En particular los AIBO han sido utilizados como participantes en el torneo de futbol RoboCup [43], incluso después de haber sido descontinuados por Sony. Robots en la educación En 1950 el investigador Grey Walter diseño y construyó un robot móvil al que llamó Tor- tuga [79]. Este robot fue construido para poder estudiar las teoŕıas de control cibernético que comenzaban a popularizarse en aquella época. Posteriormente, entre los años 1967 y 1969, Seymour Papert y Wallace Feurzeig diseñaron e implementaron un lenguaje de programación llamado Logo el cual se utilizaba para controlar una versión rediseñada del robot Tortuga de Walter, permitiéndole al robot dibujar figuras sobre papel utilizando un marcador colocado sobre un pequeño brazo incorporado al mismo [57]. Este robot fue utilizado exitosamente por Papert y sus colaboradores para incentivar la práctica de temas relacionados con la programación y la tecnoloǵıa en estudiantes de primaria. El MIT Programable Brick y los robots LEGO Mindstorms TM son ejemplos más recientes del uso de robots en entornos educativos. Este kit provee una amplia variedad de piezas, incluyendo sensores, las cuales pueden combinarse de distintas maneras para formar robots u otros tipos de máquinas programables autónomas. Mucho trabajo se ha realizado con la intención de aplicar estos robots para fomentar el estudio de carreras cient́ıfico-tecnológicas. En su trabajo de investigación Emanuelle Mene- gatti y Michele Moro plantearon un esquema de enseñanza construccionista de la tecnoloǵıa utilizando robótica [48]; ellos diseñaron diversas actividades y evaluaciones para niveles de educación desde secundaria hasta magistratura que se centran en la resolución de problemas construyendo diversos tipos de robots. Por su parte, Alexander Behrens y sus colaboradores diseñaron un sistema que permite ejecutar programas en robots NXT de manera remota utilizando MATLAB TM , siendo los estudiantes de ingenieŕıa y ciencias de la computación su público objetivo [11]. En [54] Jürgen Mottok y Armin Gardeia describen un experimento social que utiliza robots NXT para incentivar el estudio de materias relacionadas con la tecnoloǵıa en estudiantes alemanes de secundaria. 2.3. La realidad aumentada y el control de robots móvi- les Ya desde 1993 Paul Milgram y colaboradores hab́ıan presentado un trabajo de investiga- ción y desarrollo en el cual haćıan uso de un sistema de realidad aumentada para controlar a un robot[51]. Este trabajo consit́ıa en una estación de trabajo conectada a un manipula- dor industrial el cual era filmado por una cámara de video. En la pantalla de la estación de trabajo se mostraba al usuario una imagen como la que puede observarse en la Figura 2.10, en la cual un modelo 3D era desplegado en modo wireframe sobre el robot. El usuario 21 teńıa la posibilidad de manipular el modelo del robot, cambiándolo de posición o rotando sus distintos componentes; luego el robot real imitaŕıa los movimientos producidos por el usuario en el modelo. Una variación reciente sobre este trabajo fue presentada por Chong y colaboradores en el 2009 [18]. Figura 2.10: El sistema de control presentado por Milgram. Imagen recuperada de [7]. Una investigación similar orientada al área de los robots móviles se encuentra en el traba- jo de Alonzo Kelly y colaboradores, quienes presentaron una serie de técnicas y herramientas que tratan de minimizar la dificultad inherente al control remoto de robots móviles en enlaces inalámbricos, particularmente en entornos donde las distancias o la latencia de las comuni- caciones en red hace imposible el obtener una respuesta inmediata del robot [42]. El sistema funciona construyendo un modelo 3D del entorno del robot en tiempo real basándose en imagenes de video capturadas por el robot, utilizando la información obtenida de los otros sensores del robot como complemento. Por su parte Tian Xie y colaboradores presentaron un framework para el control remoto de robots por realidad aumentada llamado ARATG (Augmented Reality Aided Teleoperation Guidance - Orientación en Teleoperación Asistida por realidad aumentada) usando los resultados de Kelly y colaboradores como base. Esta clase de sistemas de control basados en realidad aumentada fueron estudiados por Scott Green y colaboradores en [32]. Ellos realizaron un estudio experimental en el cual se compararon dos sistemas de control con robots simulados: uno basado en una cámara que graba el punto de vista del robot y otro basado en realidad aumentada que permite al operador observar al robot dentro del contexto de su entorno de trabajo. Los resultados del estudio mostraron que el sistema basado en realidad aumentada mejoraba la capacidad de planificación de los operadores permitiéndoles realizar las tareas planteadas por el estudio con un mejor desempeño, eliminando la necesidad de inferir las condiciones del entorno de trabajo del robot como es necesario con el sistema básado en la cámara. 22 Minoru Kojima y colaboradores presentaron un diseño y su respectiva implementación de un juego de realidad aumentada para robots móviles llamado Augmented Colliseum [44]. Este juego consiste en un sistema de realidad aumentada proyectada en la cual un conjunto de robots móviles deben “destruir” a sus contrincantes dentro de un coliseo virtual el cual es proyectado sobre los robots mediante un proyector de techo. El juego viene a ser como una representación real de una partida del juego RoboCode4. Augmented Colliseum fue desarro- llado con la biblioteca ARToolkit5. La Figura 2.11 muestra una fotograf́ıa de una partida de Augmented Colliseum en desarrollo. Figura 2.11: El juego Augmented Colliseum. Imagen recuperada de [44]. Jakob Leitner y colaboradores implementaron un juego basado en realidad aumentada proyectada llamado IncreTable [46]. Este juego que está inspirado en The Incredible Ma- chine TM de Sierra Online R© permite a los usuarios crear maquinarias complejas tipo Rube Goldberg utilizando combinaciones de elementos reales y virtuales, incluso con robots móvi- les. Kazuhiro Hosoi y colaboradores presentaron en el 2007 un trabajo en el cual proponen una arquitectura de control para robots móviles llamada VisiCon [39]. VisiCon consiste en el uso de proyectores de mano para guiar el camino de un robot móvil el cual sigue la luz emitida por el proyector sobre el suelo. Basados en este trabajo, Hosoi y colaboradores diseñaron un juego de realidad aumentada en el cual dos jugadores deben turnarse guiando a un robot móvil delimitando su camino con rutas de realidad aumentada emitidas por los proyectores, en este caso un proyector por cada jugador. Este juego es llamado CoGame dada la naturaleza colaborativa del mismo [38]. 4http://robocode.sourceforge.net 5http://www.hitl.washington.edu/artoolkit 23 http://robocode.sourceforge.net http://www.hitl.washington.edu/artoolkit Caṕıtulo 3 Método de investigación y herramientas utilizadas En este caṕıtulo se describen a detalle las metodoloǵıas de investigación y desarrollo utilizadas y las herramientas sobre las cuales se construyo el proyecto desarrollado. 3.1. Metodoloǵıa Para el desarrollo de este proyecto se utilizó una metodoloǵıa basada en prototipos evolu- tivos los cuales eran revisados y evaluados cada 7 o 15 d́ıas dependiendo de las funcionalidades que se quisieran agregar al prototipo en algún momento determinado. El desarrollo por pro- totipos evolutivos consiste en el desarrollo de un software de calidad de producción al cual se le van agregando funcionalidades una por una a medida que sus requerimientos son estu- diados y comprendidos [19]. Esta metodoloǵıa se contrasta principalmente con el desarrollo por prototipos descartables, en la cual se desarrollan series de prototipos sencillos que imple- mentan una única funcionalidad espećıfica con el objetivo de depurar su diseño, esto con el interés de agregar dicha funcionalidad a un sistema de producción aparte. Algunos segmen- tos del sistema desarrollado fueron implementados primero como prototipos descartables y luego incorporados al prototipo evolutivo, particularmente aquellos algoritmos relacionados con visión por computador, procesamiento digital de imágenes y transmisión de video. Un ejemplo de estos prototipos descartables puede observarse en la Figura 4.13, la cual consiste en un conjunto de capturas de pantalla de una aplicación para computadoras de escritorio que se desarrolló para depurar los algoritmos de visión por computador. A nivel de programación, se utilizó el llamado successfull Git branching model especi- ficado por Vincent Driessen [23], el cual define un esquema de trabajo para el sistema de control de versiones Git. Este esquema consiste en separar el desarrollo del sistema en dos ramas fundamentales. La primera rama es la llamada rama maestra en la cual únicamente se encuentra el código fuente probado y estable que se puede compilar en una aplicación lista para ponerse en producción. La segunda rama es llamada rama de desarrollo que se utiliza para incorporar todos los cambios que se agreguen al código fuente pero que no estén 24 depurados completamente. Adicionalmente se definen tres tipos de ramas tangenciales las cuales se utilizan para identificar hitos en el desarrollo del sistema. La primera de este tipo de ramas es llamada rama de funcionalidad o caracteŕıstica. Estas ramas se utilizan para agregar nuevas funcionalidades al sistema, manteniendo dichas funcionalidades aparte de las bases de código fuente hasta que hayan sido completadas. Una vez que las funcionalidades están completas se mueven a las llamadas ramas candidatas a publicación, las cuales se utilizan para depurar dichas funcionalidades antes de incorporarlas a las ramas de desarrollo y maestra. Finalmente se definen las ramas de reparación de errores en las cuales se agrega el código fuente para acomodar errores graves detectados en el sistema que se está desarrollando. Un ejemplo del esquema completo se puede observar en la Figura 3.1 Figura 3.1: El successfull Git branching model. Imagen adaptada y traducida de http://nvie.com. 3.2. Herramientas En esta sección se describen las diferentes herramientas y dispositivos utilizados durante el desarrollo de este trabajo. Estas descripciones son dadas principalmente fuera de contexto, centrándose únicamente en las especificaciones técnicas de cada herramienta. El uso que se le dió a cada herramienta se expone con más detalle en los Caṕıtulos 4 y 5. 25 http://nvie.com 3.2.1. Herramientas de hardware La arquitectura desarrollada en este trabajo abarca diversos dispositivos de hardware sobre los cuales se implementó la especificación de la misma (véase la Sección 4.1). Entre los dispositivos utilizados se encuentran los robots LEGO Mindstorms NXT TM descritos con detalle en el Anexo A; varios dispositivos móviles basados en el sistema operativo Android y la consola de videojuegos OUYA, aśı como un punto de acceso inalámbrico. Dispositivos móviles Para el desarrollo de la implementación de referencia (véase la Sección 4.2) se utilizaron dos tablets Samsung Galaxy Tab 2, una de 7 pulgadas y otra de 10.1 pulgadas, aśı como un smartphone Samsung Galaxy Nexus. Para las pruebas se utilizaron adicionalmente un smartphone Samsung Galaxy S4 y un smartphone Pantech Burst, aśı como una consola de videojuegos OUYA que se describe más adelante. La Tabla 3.1 resume las caracteŕısticas de hardware de los dispositivos utilizados1. Caracteŕıstica Galaxy Tab 2 Galaxy Nexus Galaxy S4 Pantech Burst Cortex A9 Cortex A15 Cortex A9 Snapdragon Procesador 1 Ghz 1.2 Ghz 1.6 Ghz 1.5 Ghz ARM + Neon ARM + Neon ARM + Neon ARM + Neon GPU PowerVR PowerVR PowerVR Adreno Memoria RAM 1 GB 1 GB 2 GB 1 GB Cámara 3.5 MP 5 MP 13 MP 5 MP Bluetooth v3.0 v3.0 v4.0 + LE v3.0 WLAN 802.11 b/g/n b/g/n b/g/n/ac b/g/n Tabla 3.1: Dispositivos móviles utilizados. Datos recuperados de http://www.gsmarena.com. La consola OUYA La consola de videojuegos OUYA es un dispositivo basado en el sistema operativo Android 4.1 desarrollada por la empresa OUYA Inc. 2. Este dispositivo actua como una consola de videojuegos de sobremesa completamente hackeable y programable, diseñada principalmente para desarrolladores de videojuegos independientes y aficionados. La consola esta basada en el procesador Tegra 3 de Nvidia el cual implementa el API (Application Programming Interface - Interfaz de Programación de Aplicaciones) definido por el estanda OpenGL ES 2.0 (Open Graphics Library for Embedded Systems - Biblioteca Abierta de Gráficos para Sistemas Embebidos). El procesador funciona a 2 Ghz de frecuencia e incluye 8 gigabytes de 1La columna Galaxy Tab 2 incluye tanto la Galaxy Tab 2 7.0 como la 10.1 dado que la única diferencia relevante entre ambas es el tamaño de sus pantallas. 2https://www.ouya.tv 26 http://www.gsmarena.com https://www.ouya.tv almacenamiento interno y 1 gigabyte de memoria RAM. Adicionalmente la consola cuenta con conectividad 802.11 y Bluetooth, aśı como un puerto Fast Ethernet, un puerto USB 2, un puerto mini USB y salida de video HDMI (High Definition Multimedia Interface - Interfaz Multimedia de Alta Definición) a 1080p. La Figura 3.2 muestra el aspecto de la consola y su control. Figura 3.2: La consola de videojuegos OUYA y su control. Imagen recuperada de https://www.ouya.tv. La consola es manipulada por un control inalámbrico que se conecta con la misma utili- zando Bluetooth. Este control sigue el modelo t́ıpico para gamepads, contando con 4 botones identificados por letras (O, U, Y y A), un pad direccional, dos palancas analógicas, dos gati- llos digitales, dos gatillos analógicos y un botón de inicio. Adicionalmente el control incluye un pequeño panel táctil que funciona como un trackpad el cual puede utilizarse para emular controles de pantalla táctil. El punto de acceso inalámbrico Durante el desarrollo de este trabajo de investigación se utilizó un punto de acceso inalámbrico D-LINK modelo di-634m. Este es un punto de acceso doméstico con soporte para IEEE 802.11b e IEEE 802.11g. Este modelo incluye soporte para la tecnoloǵıa MIMO (Multiple Input and Multiple Output - Entrada Múltiple y Salida Multiple) con dos antenas, además de soporte para el protocolo propietario WiFi Super G. El punto de acceso incluye 4 puertos Fast Ethernet y tiene un alcance de aproximadamente 100 metros en la interfaz inálambrica. 3.2.2. Herramientas de software El desarrollo de la parte de software de la arquitectura hizo uso de múltiples bibliotecas y componentes, algunos de los cuales son necesarios para trabajar con el hardware antes descrito y otros que fueron escogidos por diversas razones que se exponen en esta sección. Casi la totalidad de la aplicación (unas 9081 lineas de código fuente de un total de 9511 lineas) 27 https://www.ouya.tv fue desarrollada con el lenguaje de programación Java, el cual es el lenguaje de programación principal utilizado por el sistema operativo Android. El resto de la aplicación fue desarrollada en el lenguaje C++, en particular las funcionalidades relacionadas con procesamiento digital de imágenes. Para el desarrollo se utilizaron las siguientes bibliotecas de software: OpenCV3 para el procesamiento digital de imágenes. LibGDX4 para el desarrollo del sistema de juego y otras funcionalidades relacionanadas con la Computación Gráfica. Artemis Entity-System Framework 5 para la implementación del patrón de diseño Entidad- Componente-Sistema (véase la Sección 4.2.3). Java Universal Tween Engine6 para la aplicación de interpolaciones en los efectos gráficos del sistema de juego. Cada una de estas bibliotecas se detalla a continuación. OpenCV OpenCV es el acrónimo utilizado por el proyecto Open Computer Vision (Visión por Computador Abierta) llevado a cabo por las empresas Willow Garage e Itzees para desa- rrollar una biblioteca eficiente y sofisticada de procesamiento digital de imágenes. OpenCV es utilizada para manipular y obtener información a partir de imágenes ya sean estáticas u obtenidas a partir de video, teniendo varios miles de funciones que permiten trabajar a distintos niveles de abstracción, ya sea con operaciones aritméticas o filtros de bajo nivel has- ta distintos algoritmos de detección de caracteŕısticas utilizando clasificadores y máquinas vectoriales de soporte (del inglés support vector machines). En este proyecto se utilizó esta biblioteca para desarrollar las funcionalidades referentes a la calibración de cámaras de video y detección de marcadores. Fue escogida por tres grandes razones: la primera, la biblioteca es software libre y gratuito; la segunda, esta biblioteca es reconocida por ser altamente eficiente; y la tercera, posee soporte para múltiples plataformas de hardware y sistemas operativos, en particular el sistema operativo Android, ya que como se mencionó en la sección anterior fue la plataforma escogida para el desarrollo del proyecto. La principal ventaja del soporte multiplataforma de OpenCV es que utiliza exactamente la misma interfaz de programación de aplicaciones independientemente del sistema operativo o hardware para el que se programe. Esto permitió desarrollar y probar los distintos algorit- mos de calibración de cámaras, detección y decodificación de marcadores en una computadora de escritorio; facilitando enormemente la depuración de los mismos, con la seguridad de que el mismo código fuente podŕıa luego ser portado directamente al sistema operativo Android sin tener que aplicar luego ninguna modificación. 3http://opencv.org 4http://libgdx.badlogicgames.com 5http://gamadu.com/artemis 6https://code.google.com/p/java- universal-tween-engine 28 http://opencv.org http://libgdx.badlogicgames.com http://gamadu.com/artemis https://code.google.com/p/java-universal-tween-engine LibGDX A pesar de su nombre LibGDX es un framework más que una biblioteca y tiene como objetivo el simplificar el desarrollo de videojuegos múltiplataforma en el lenguaje Java; es desarrollada por el estudio de desarrollo independiente Badlogic Games. En realidad LibGDX es una agrupación de un gran conjunto de diferentes bibliotecas de desarrollo de aplicaciones gráficas. Entre las distintas bibliotecas incorporadas por LibGDX se encuentran: OpenGL para despliegue de gráficos 2D y 3D. Dependiendo de la plataforma en la que se ejecute la aplicación desarrollada LibGDX se encarga de utilizar ya sea OpenGL estándar u OpenGL ES. Box2D para realizar simulaciones f́ısicas en dos dimensiones. Bullet para simulaciones f́ısicas en tres dimensiones. FreeType para manipulación de fuentes y despliegue de textos. OpenAL, Vorbis y Mpg 123 para reproducción de sonido. GWT, LWJGL y RoboVM para las funcionalidades dependientes del sistema operativo. En este proyecto se decidió utilizar LibGDX principalmente por las simplificaciones que provee a la hora de trabajar con OpenGL, aśı como la posibilidad de reutilizar la misma base de código en múltiples plataformas de hardware. Artemis Entity-System Framework Artemis es un framework escrito en Java puro el cual implementa de forma eficiente el patrón de diseño Entidad-Componente-Sistema. En resumen este patrón de diseño permite separar las distintas funcionalidades del juego desarrollado en sistemas bien diferenciados los cuales actúan sobre entidades, que a su vez son agrupaciones de componentes. Esta orga- nización en entidades y componentes aśı mismo permite eliminar las relaciones de herencia entre las clases que implementan los entes que pueblan el mundo de juego. Una descripción más detallada del funcionamiento y la motivación detrás de este patrón de diseño se da en la Sección 4.2.3. Artemis es desarrollada de forma independiente por Arni Arent. Java Universal Tween Engine El Java Universal Tween Engine (motor universal de interpolación) es una biblioteca implementada en Java puro que sirve para interpolar cualquier atributo numérico de cual- quier objeto de Java aplicando distintas funciones de interpolación. Su principal uso es el modificar atributos de objetos gráficos para realizar toda clase de efectos especiales aunque puede utilizarse para cualquier otro caso de uso en el cual se tenga que interpolar entre dos o más valores. Un ejemplo de algunas de las distintas funciones de interpolación implemen- tadas por la biblioteca pueden verse en la Figura 3.3, la cual es una captura de pantalla de 29 la aplicación de demostración de la misma. Java Universal Tween Engine es desarrollado de forma independiente por Aurelien Ribon. Figura 3.3: Funciones de interpolación del Java Universal Tween Engine. Esta biblioteca fue utilizada para implementar efectos de transición entre las distintas vistas de la aplicación y para incorporar efectos de desvanecimiento en el módulo de juego. 30 Caṕıtulo 4 Descripción de la solución implementada En este caṕıtulo se presenta el diseño de la architectura propuesta, aśı como también los detalles de la implementación de referencia. La descripción de la arquitectura esta dada siguiendo un esquema top-down, comenzando con el diseño general a grosso modo y poste- riormente ahondando en los detalles. El caṕıtulo concluye con una explicación del escenario de demostración desarrollado. 4.1. Diseño arquitectónico de la solución En el presente trabajo se propone una arquitectura de control para robots móviles basada en realidad aumentada compuesta por 3 elementos: el robot, un dispositivo con cámara capaz de grabar video y un dispositivo de control. Estos tres dispositivos deben poseer la capacidad de comunicarse entre śı de forma inalámbrica ya sea utilizando enlaces Bluetooth u 802.11. La Figura 4.1 ilustra estos componentes y la relación entre ellos. Como puede apreciarse, esta arquitectura esta diseñada alrededor del robot LEGO Minds- torms NXT TM1. Se escogió este robot dada la facilidad de incorporarle otros dispositivos mediante su conectividad Bluetooth, para ser utilizados como sensores o procesadores ex- ternos. Este robot debe seguir un esquema de movimiento diferencial (como se describió en la Sección 2.2.3), y debe tener algún mecanismo que permita incorporarle el dispositivo con cámara como un sensor adicional. Como mı́nimo el robot debe poseer dos motores separa- dos para su locomoción (un motor derecho y otro izquierdo), aunque se puede incorporar un tercer motor que permita mover el dispositivo con cámara de forma que el robot pueda “ver” en una dirección diferente a su dirección de movimiento. El dispositivo con cámara mostrado en la Figura 4.1 corresponde con cualquier dispositivo programable capaz de grabar video o tomar fotograf́ıas con bastante rapidez. Este dispositivo además actúa como un puente entre el robot y el dispositivo de control, siendo este último 1Esto no es limitativo. Facilmente se puede sustituir al robot LEGO Mindstorms NXT TM por un robot LEGO Mindstorms EV3 TM o por cualquier otro robot programable con conectividad inalámbrica. 31 Figura 4.1: Diseño general de la arquitectura. el dispositivo utilizado directamente por el usuario para controlar al robot. Este dispositivo de control puede ser cualquier dispositivo programable capaz de recibir interacción por parte del usuario y de conectarse con el dispositivo con cámara, como por ejemplo pueden ser las tablets, computadoras de escritorio o incluso consolas de videojuegos. 4.1.1. Diseño de casos de uso Para establecer los casos de uso de la arquitectura propuesta se define como posibles actores al jugador y al diseñador de escenarios. El jugador es el usuario que se encarga de ejecutar una partida para el escenario que haya sido desarrollado, utilizando las diversas in- teracciones propuestas por la arquitectura para completar algún objetivo de juego espećıfico. Por su parte el diseñador de escenarios es el encargado de diseñar y programar los escenarios que podrá utilizar el jugador. Esto puede observarse en la Figura 4.2. Figura 4.2: Casos de uso de nivel 0. Estos casos de uso pueden detallarse como se observa en la Figura 4.3. Para el caso del jugador se puede observar que jugar una partida de un escenario implica tres actividades. La 32 primera es la calibración de la cámara que tiene que realizarse para asegurar un despliegue de objetos virtuales correcto. La segunda actividad es la ejecución del escenario manual del robot que representa el juego en śı. El escenario de control manual debe permitir al usuario el manipular al robot y a un “brazo mecanico” virtual mediante controles que se describen más adelante en este caṕıtulo, de forma que utilizando ambos el usuario tenga la posibilidad de cumplir el objetivo propuesto por el escenario. Finalmente el usuario debe tener la posibilidad de ejecutar un algoritmo de control autónomo para el robot el cual dependerá del escenario que se esté jugando. Por su parte el diseñador de escenarios tiene la posibilidad de desarrollar los componentes que ejecuta el jugador, siendo opcional aunque recomendado el implementar las pantallas de resumen que se le muestran al jugador al completar cada módulo de juego. Figura 4.3: Casos de uso de nivel 1. 4.1.2. Control del robot El mecanismo de interacción con el robot depende de la naturaleza del dispositivo de control. Por ejemplo, en una tablet el control debeŕıa realizarse por medio de la pantalla tactil de la misma, o en el caso de una computadora de escritorio por medio del teclado y el ratón. Independientemente del mecanismo f́ısico mediante el cual se manipule al robot, este debe soportar los siguientes controles: Avanzar y retroceder cada motor por separado. Mover el brazo virtual. Retornar el motor del dispositivo con cámara a su posición inicial. 33 Control del brazo virtual El brazo virtual es una representación de un efector lógico añadido al robot de forma que este pueda interactuar con los objetos virtuales del escenario de juego. Este brazo debe poder desplazarse libremente en un plano paralelo a la pantalla del dispositivo de control, y avanzar y retroceder en un vector perpendicular a la misma. El diagrama de la Figura 4.4 ilustra como se realiza este movimiento Figura 4.4: Movimiento del brazo virtual. 4.1.3. Escenarios jugables y despliegue de objetos virtuales. Un escenario consiste en un conjunto de reglas que definen un objetivo, el cual debe cum- plir el jugador utilizando las herramientas de control descritas anteriormente. Como mı́nimo un escenario debe definir un conjunto de objetos virtuales con los cuales puede interactuar el jugador y las condiciones para completar el escenario, ya sea en victoria o derrota. Los objetos virtuales deben consistir en modelos 3D los cuales deben ser desplegados por pan- talla alineados con marcadores similares a los que se pueden observar en la Figura 4.12. La información que codifican los marcadores es responsabilidad de cada implementación. La interacción de los objetos virtuales con el robot se debe basar en la detección de colisiones de dichos objetos con el brazo virtual. 34 4.2. Descripción de la implementación de referencia En esta sección se describe a detalle la aplicación desarrollada que implementa la ar- quitectura descrita en la sección anterior. Esta implementación, llamada en conjunto como NxtAR, abarca tres apliaciones: dos para el sistema operativo Android y una más para el robot Mindstorms NXT. Las aplicaciones desarrolladas para dispositivos Android son lla- madas NxtAR-cam y NxtAR-core e implementan los módulos del dispositivo de visión y el dispositivo de control respectivamente. La aplicación para el robot es llamada NxtAR-bot y tiene como objetivo el aplicar las instrucciones de control que se generan desde NxtAR-core. Las aplicaciones desarrolladas son software libre2 publicado bajo la licencia Apache 2.03. Las aplicaciones para Android tienen soporte para dispositivos móviles como tablets y smartp- hones y en el caso de la aplicación para el dispositivo de control se posee soporte adicional para la consola de videojuegos OUYA, aplicación cuyo icono puede observarse en la Figura 4.5. Figura 4.5: Icono de la implementación de referencia en la consola OUYA. 4.2.1. Diseño de clases En la Figura 4.6 se puede observar un diagrama de clases simplificado de la aplicación NxtAR-core. En dicho diagrama se puede observar la composición de los paquetes más sig- nificativos que agrupan las clases de la aplicación. Lo primero que se puede detallar es que la aplicación está dividida en dos paquetes principales: NxtAR-core propiamente y NxtAR- android. Este segundo paquete representa el frontend para el sistema operativo Android, el cual se implementa en un paquete separado siguiendo los requisitos del framework LibGDX. Este paquete también incluye un subpaquete llamado cv proc el cual contiene la implementa- ción nativa de los algoritmos de detección y descodificación de marcadores (véase la Sección 4.2.4). El paquete NxtAR-core se divide en 3 subpaquetes principales: interfaces, escenarios y estados. El paquete de interfaces define los puntos de acceso a través de los cuales el núcleo 2Descargables en https://github.com/sagge-miky 3http://www.apache.org/licenses/LICENSE-2.0 35 https://github.com/sagge-miky http://www.apache.org/licenses/LICENSE-2.0 Figura 4.6: Diagrama de clases de NxtAR-core. de la aplicación (definido en la clase core) y el frontend para Android (por medio de la clase MainActivity) pueden comunicarse. En particular la interfaz ActionResolver se utiliza para abstraer las funcionalidades del sistema operativo de forma que el núcleo pueda tener acceso a las mismas de forma independiente del sistema. La interfaz CVProcessor cumple la misma función con respecto al subpaquete cv proc de NxtAR-android. Por último la interfaz ApplicationEventsListener se utiliza internamente con los hilos que se encargan de las funciones de comunicación por red para que puedan notificar al núcleo de eventos que afectan el comportamiento del sistema, implementando el patrón de diseño Observer. El subpaquete de escenarios de NxtAR-core define un conjunto de clases abstractas y una clase estática las cuales definen el escenario de juego disponible al jugador. La clase estática es la clase ScenarioImplementation, la cual consiste en un conjunto de apuntadores a las implementaciones de las clases abstractas definidas en el paquete (todas ellas cuyos nombres terminan en Base). Estas implementaciones se definen dentro de un subpaquete de escenario 36 junto a cualquier otra clase auxiliar necesaria. Por otro lado, el subpaquete de estados de NxtAR-core contiene las implementaciones de los estados que componen la máquina de estados finitos de la aplicación, descritos en detalle más adelante en este caṕıtulo. 4.2.2. Detalles del desarrollo de los módulos del robot y la cámara Los dos primeros módulos descritos en el diseño de la arquitectura son implementados por las aplicaciones NxtAR-bot y NxtAR-cam antes mencionadas. Estas dos aplicaciones funcionan de manera completamente autónoma, comunicándose entre ellas por medio de un enlace Bluetooth por el perfil de puerto serial, y con la aplicación de control utilizando enlaces TCP y UDP dependiendo de la información que debe transmitirse. Estas aplicaciones, junto con el esquema de comunicación que utilizan se describen a continuación. La aplicación NxtAR-bot NxtAR-bot es una pequeña aplicación desarrollada en el lenguaje Java para ser ejecutada en robots LEGO Mindstorms TM NXT que posean el firmware del sistema operativo LeJOS. Esta aplicación se encarga de realizar dos únicas funciones, la primera de las cuales es reportar las medidas de los sensores del robot. La versión desarrollada solamente realiza reportes del sensor de luz del robot, que es el único sensor instalado en el diseño utilizado. Este reporte se realiza automáticamente entre las aplicaciones NxtAR-bot y NxtAR-cam, siendo esta segunda aplicación la que se encarga de redirigir las medidas tomadas a la aplicación de control utilizando un enlace TCP. La aplicación de control puede luego consultar dichas medidas mediante un esquema de polling. La otra función de esta aplicación es el ejecutar las instrucciones de control generadas por el usuario o el sistema de control automático (descrito más adelante). Estas instruccio- nes toman la forma de un mensaje de dos bytes transmitido v́ıa Bluetooth al robot desde la aplicación NxtAR-cam, originándose en la aplicación de control. El primero de estos by- tes codifica la instrucción de control mientras que el segundo representa un parámetro de la misma. Las instrucciones de control son codificadas utilizando los bits del primer byte mencionado de la siguiente manera: 0x01: Control del motor en el puerto A. 0x02: Control del motor en el puerto B. 0x04: Control del motor en el puerto C. 0x08: Dirección del movimiento de los motores. 0x10: Regresar el motor en el puerto B a su posición original. 0x20: Rotar el motor en el puerto B en 90 grados en sentido contrarreloj. 0x40: Disponible para expansiones futuras. 37 0x80: Disponible para expansiones futuras. El byte de parámetro se utiliza actualmente para indicar la potencia que se debe aplicar a los motores cuando se incluyen instrucciones de control en el rango 0x01 a 0x04. Las demás instrucciones utilizan una potencia constante predefinida en 50 por ciento. La instrucción de dirección (bit 0x08) se utiliza como un parámetro adicional de las tres instrucciones anteriores, indicando que el motor debe avanzar cuando el bit está encendido y retroceder cuando está apagado. Utilizar máscaras de bits de esta manera permite componer instrucciones de control más complejas que las definidas por el protocolo LCP (LEGO Communication Protocol - Protocolo de Comunicaciones de LEGO) implementado por el firmware estándar incluido por los robots LEGO Mindstorms NXT TM (véase la Sección A.2.2 en el Anexo A), siendo posible encender más de un motor por mensaje de control. En el caso de las operaciones que afectan al motor B, de ser definidas dos o más de forma simultánea, se aplican en el orden definido por el listado anterior con cada operación reemplazando a las anteriores. La aplicación NxtAR-cam NxtAR-cam es la aplicación encargada de implementar el módulo del dispositivo con cámara mostrado en la arquitectura. Su principal funcionalidad es la de capturar y trans- mitir un flujo de video en tiempo real hacia la aplicación de control, al mismo tiempo que retransmite hacia el robot las instrucciones de control generadas por el usuario. El flujo de video es capturado por la cámara del dispositivo a una tasa de cuadros por segundo definida por el sistema operativo Android dependiendo de las posibilidades del sensor de la misma. La captura se realiza cuadro a cuadro los cuales son colocados por el hilo principal de la aplicación, que es el encargado de realizar la captura, dentro de un objeto monitor de donde el hilo de transmisión los obtiene para enviarlos a la aplicación de control. El monitor implementa un esquema de doble buffer para que los hilos no tengan que funcionar a la rapidez del hilo más lento. Con este esquema el hilo de transmisión siempre toma el cuadro de video ubicado en el buffer frontal, mientras que el hilo de captura coloca los cuadros nuevos en el buffer posterior y luego invierte las referencias a los mismos, de forma que los cuadros nuevos se almacenen en lo que antes era el buffer frontal y los cuadros a transmitir se tomen de lo que era el buffer posterior. Un esquema similar se utilizó en el lado receptor, donde un hilo receptor coloca los cuadros recibidos en un monitor con doble buffer de donde el hilo de despliegue los obtiene para mostrarlos por pantalla. La transmisión del video se realiza utilizando datagramas UDP (User Datagram Protocol - Protocolo de Datagramas de Usuario) sin esquemas particulares para control de flujo o corrección de errores. Las instrucciones de control son enviadas directamente de la aplicación de control a NxtAR-cam utilizando enlaces TCP. Al ser recibidas son almacenadas en una cola con ca- pacidad para 10 elementos para luego ser codificadas utilizando el esquema de máscaras de bits explicado anteriormente y finalmente ser transmitidas al robot con un enlace Bluetooth. Si la cola de instrucciones se llena NxtAR-cam env́ıa un mensaje de espera a la aplicación 38 de control, luego enviando un mensaje indicando que puede continuar la transmisión cuando la cola se vaćıa. Comunicación entre módulos Para interconectar los distintos módulos entre śı se sigue el siguiente esquema: 1. Primero se inicia la aplicación NxtAR-bot en el robot y se realizan los pasos para calibrar el sensor de luz. Una vez listo esto la aplicación se coloca en modo de espera por una conexión Bluetooth. 2. Se inicia la aplicación NxtAR-core (la aplicación de control), la cual comienza a trans- mitir paquetes multicast anunciándose a la red. 3. NxtAR-cam se conecta con el robot. El robot debe estar emparejado de antemano con el dispositivo de captura de video utilizando las funcionalidades de comunicación por Bluetooth del sistema operativo Android. 4. Finalmente se indica a NxtAR-cam que busque al dispositivo de control utilizando un protocolo ad hoc sencillo de descubrimiento de servicios. La conexión entre NxtAR-cam y NxtAR-core se realiza de forma automática. La Figura 4.7 ilustra los enlaces de comunicación utilizados por las 3 aplicaciones. Figura 4.7: Enlaces de red en las aplicaciones que componen NxtAR. 39 4.2.3. Patrones de diseño utilizados en la aplicación de control NxtAR-core hace uso extenso de dos patrones de diseño que facilitaron su desarrollo al mismo tiempo que proporcionan una forma clara y concisa de organización del código fuente. Estos patrones son los llamados state pattern (patrón de estados) y patrón Entidad- Componente-Sistema. State pattern El state pattern es un patrón de diseño que consiste en modelar las distintas funcionali- dades de una aplicación como una máquina de estados finitos [27]. Los distintos estados en los que puede estar la aplicación en un momento dado son implementados como subclases de una superclase estado abstracta de la cual se posee un vector que contiene las implemen- taciones de los distintos estados concretos. Adicionalmente se posee un ı́ndice a dicho vector el cual indica cuál es el estado actual de la aplicación. Este patrón de diseño permite separar las distintas funcionalidades de la aplicación de manera gruesa, agrupando funcionalidades similares o relacionadas dentro de un mismo estado. La aplicación desarrollada define los siguientes 7 estados, los cuales coinciden con las diferentes vistas que posee la misma: Menú principal. Calibración de la cámara. En juego. Información del juego. Resumen del juego. Acción automática. Resumen de la acción automática. Estos 7 estados a su vez se pueden agrupar en 4 macro-estados: menú principal, calibra- ción, juego y acción automática. Como se puede deducir de su nombre, el estado meú prin- cipal se encarga de mostrar al usuario las distintas opciones que puede realizar al iniciar la aplicación. El estado de calibración se encarga de realizar las acciones de calibración de la cámara del dispositivo con cámara mostrado en el diseño de la arquitectura. El macro-estado de juego agrupa los tres estados que controlan el desarrollo del escenario de juego y permi- ten al usuario el controlar al robot de forma manual. Por último el macro-estado de acción automática permite que el robot realice acciones preprogramadas de manera autónoma. El estado de calibración, que puede observarse en la Figura 4.8.a permite al usuario obtener los parámetros de la cámara necesarios para poder realizar un despliegue correcto de los objetos virtuales sobre las imágenes obtenidas del flujo de video. Estos parámetros son la llamada matriz intŕınseca de la cámara y los coeficientes de distorsión, que juntos dan un 40 Figura 4.8: Los estados principales de NxtAR-core en tablets. (a) Menú principal. (b) Calibración de la cámara. (c) Acción automática. (d) En juego, modo de control del robot. modelo matemático aproximado del lente de la cámara que está capturando el video. Estos parámetros son obtenidos por medio de OpenCV y luego son alimentados a OpenGL durante el despliegue de figuras como una matriz de proyección. La matriz de proyección tiene la forma que puede apreciarse en la Equación (4.1) donde fx y fy son las coordenadas del punto focal de la cámara; cx y cy son las coordenadas del centro óptico de la cámara en pixeles; NEAR y FAR son las distancias de los planos de corte cercano y lejano respectivamente; y w y h son la anchura y altura del viewport.  −2fx w 0 −2cx w − 1 0 0 2fy h −2cy h − 1 0 0 0 −(FAR+NEAR) FAR−NEAR −2(FAR×NEAR) FAR−NEAR 0 0 −1 0   (4.1) El proceso de calibración de la cámara consiste en apuntar la misma hacia un patrón impreso similar al que se puede observar en la Figura 4.9. La aplicación toma automática- mente 10 muestras de dicho patrón y las utiliza para calcular los parámetros mencionados anteriormente. El patrón de calibración forsozamente debe poseer 54 puntos de intersección divididos en 9 filas por 6 columnas o viceversa. Antes de poder pasar a los macro-estados de control manual o acción automática es obligatorio que el usuario realice primero la calibración de la cámara. Una vez realizada la calibración el usuario puede utilizar cualquiera de los macro-estados de forma indistinta, e incluso tiene la posibilidad de repetir el proceso de calibración de ser necesario. El estado de acción automática puede verse en la Figura 4.8.c y permite al usuario ejecutar un programa automático predefinido con el robot, el cual muestra un resumen al culminar su ejecución. Este estado es genérico y necesita que cada escenario implementado incluya 41 Figura 4.9: Patrón de calibración de la cámara. su propio código de acción automática y su propia definición de la pantalla de resumen. El juego de demostración desarrollado implementa un código de acción automática sencillo el cual es descrito con detalle en la Sección 4.2.5. Al igual que el estado de acción automática el estado de control manual también es genérico y necesita que cada escenario implemente un conjunto de entidades y sistemas de procesamiento definidos con más detalle en la siguiente sección. Este estado permite al usuario jugar el escenario programado en la aplicación utilizando diferentes modos de control para manipular al robot dependiendo de la plataforma en la que se ejecute la aplicación. La versión implementada incluye soporte para control por pantalla táctil, gamepads y teclado con mouse. Adicionalmente este estado permite manipular un brazo virtual el cual puede utilizarse para interactuar con los diferentes objetos virtuales que implementa el escenario. Las Figuras 4.10.a y 4.10.b muestran el aspecto gráfico del brazo del robot y los controles definidos para los dispositivos con pantalla táctil. Patrón Entidad-Componente-Sistema El patrón Entidad-Componente-Sistema, también conocido como Entidad-Componente [3], es un patrón de diseño arquitectónico que trata de minimizar el uso de relaciones de herencia entre las clases en un sistema de software orientado a objetos, sustituyéndolas con relaciones indirectas entre clases sencillas llamadas componentes las cuales se agrupan dentro de unidades lógicas llamadas entidades. Este patrón se origina en el desarrollo de juegos del tipo CRPG (Computer Role Playing Game - Juego de Roles por Computador) los cuales pueden sufrir fácilmente de problemas durante el desarrollo a causa de complejas relaciones de herencia múltiple directa e indirecta entre las distintas entidades que componen el juego [12]. Este patrón es ampliamente utilizado en el desarrollo de juegos del tipo MMORPG (Massively Multiplayer Online RPG - RPG en Linea Masivamente Multijugador) principalmente porque es muy sencillo representar las relaciones entre entidades y componentes como tablas en una 42 Figura 4.10: El brazo virtual implementado y controles para tablets. (a) En su posición inicial. (b) En movimiento. La linea negra denota la trayectoria seguida por el brazo. base de datos relacional. Otra ventaja de este patrón es que permite separar completamente las distintas funcionalidades de la aplicación lo que a su vez facilita el desarrollo de código fuente mantenible, y además permite repartir tareas entre el equipo de desarrollo sin temor a que estas tareas se solapen [41]. El elemento fundamental de este patrón de diseño es el componente, el cual se representa como una clase sencilla únicamente compuesta por atributos públicos, o a lo sumo por un conjunto de métodos get y set para los distintos atributos. Un componente se encarga de representar una única propiedad que puede poseer una entidad, por ejemplo la posición de la entidad en el espacio, o un mallado que puede utilizarse para desplegar esa entidad en la pantalla. Entonces se puede definir a una entidad como una agrupación de componentes. Las entidades suelen implementarse como ı́ndices a una tabla que a su vez contiene ı́ndices a los componentes los cuales se organizan como filas en distintas tablas, una tabla por cada componente y una fila por cada entidad que utiliza ese componente. Esta implementación es t́ıpica dado que su representación en una base de datos relacional es trivial [12]; sin embargo, el cómo se representan y se relacionan las entidades y los componentes depende del criterio del desarrollador. Las entidades son luego procesadas por los sistemas, los cuales se encargan de imple- mentar la lógica de alguna parte de la aplicación. Por ejemplo se puede tener un sistema 43 de posicionamiento el cual se encarga de actualizar la posición de todas las entidades que posean un componente de posición, o un sistema de despliegue el cual muestra por pantalla todas las entidades que posean un componente de mallado. Los sistemas son creados en un orden espećıfico y son procesados en el mismo orden. En la Figura 4.11 se observa una im- plementación de la relación entre entidades y componentes propuesta por Mick West en [81]. Se puede observar que cada fila es una entidad y cada columna es un componente manejado por un administrador de componentes que se encarga de relacionar cada entidad con las instancias espećıficas de cada componente que utiliza. Figura 4.11: Composición de entidades como agrupaciones de componentes. Imagen recuperada de [81] La implementación de referencia de la arquitectura desarrollada hace uso del patrón Entidad-Componente-Sistema principalmente dentro del estado de juego, separando las fun- cionalidades de la arquitectura en 10 sistemas y 12 componentes básicos. De estos 10 sistemas 8 son independientes del escenario de juego implementado; aśı mismo, los 12 componentes son independientes del escenario, aunque cada escenario tiene la posibilidad de definir componen- tes adicionales según sea necesario. No existen entidades por defecto en la implementación, siendo cada escenario responsable de crear todas las entidades que necesite. Otros estados de la aplicación también pueden hacer uso de los sistemas, activándolos o desactivándolos según sea necesario. Los 8 sistemas generales que define la arquitectura son los siguientes, listados en el orden en que son creados y procesados: Posicionamiento de objetos con marcador asociado. 44 Posicionamiento del brazo virtual del robot. Aplicación de transformaciones geométricas (escalamiento, rotación y traslación). Aplicación de animaciones. Detección de colisiones. Despliegue de objetos con marcadores. Despliegue del brazo del robot. Despliegue de efectos especiales. Adicionalmente se definen dos sistemas que deben ser implementados por cada escenario. Estos son el sistema de lógica de juego y el sistema de procesamiento del jugador. El primer sistema se debe encargar de evaluar las reglas de juego que definen el escenario, modificando las distintas entidades del mismo dependiendo de las interacciones que detecten los sistemas anteriores, aśı como también se encarga de generar los distintos efectos especiales que pueda utilizar el escenario. Por su parte el sistema de procesamiento del jugador se debe encargar de evaluar las condiciones que determinan cuando el jugador completa el escenario ya sea de forma satisfactoria o insatisfactoria. 4.2.4. Detección de marcadores La detección de marcadores es un procedimiento que consiste en tres operaciones: la detección visual del marcador, la descodificación y la determinación de la transformación geométrica del marcador en el espacio. Estas tres operaciones son llevadas a cabo por la biblioteca OpenCV y se basan en los algoritmos planteados en el caṕıtulo 2 del libro Mas- tering OpenCV with practical computer vision projects (Dominando OpenCV con proyectos prácticos de visión por computador) [8]. Estos algoritmos, que se detallan a continuación, están implementados en el lenguaje C++ aprovechando el soporte del sistema operativo Android para la interfaz JNI (Java Native Interface - Interfaz Nativa de Java), la cual per- mitió el reutilizar completamente una implementación de los algoritmos para computadoras de escritorio que fue desarrollada y depurada de forma independiente. Los marcadores utilizados en este proyecto son los mismos definidos en [8], los cuales tienen un aspecto visual como el que se aprecia en la Figura 4.12. Estos marcadores son impresos o transferidos a una superficie de tonalidad clara de forma que posean un alto contraste con dicha superficie para hacerlos más faciles de detectar. El tamaño f́ısico del marcador impreso solo afecta a qué distancias puede ser detectado correctamente, sin cau- sar otros efectos en el desarrollo del juego implementado o en los distintos algoritmos que dependen de la detección del marcador en cuestión. 45 Figura 4.12: Uno de los marcadores utilizados y sus cuatro posibles representaciones. Detección visual de marcadores La detección visual de los marcadores consiste en identificar regiones de la imagen de entrada recibida del dispositivo con cámara en las cuales es posible que se encuentre un marcador. Este proceso se lleva a cabo mediante los siguientes pasos: 1. La imagen de entrada es llevada a escala de grises. 2. Se aplica una operación de umbral adaptativo con una vecindad de tamaño 7 a la imagen. 3. Se aplica una función de detección de contornos a la imagen binarizada. 4. La lista de contornos obtenida es sustituida por una lista de contornos aproximados. Aquellos contornos que no sean aproximados por cuadriláteros son descartados. 5. Se examinan los contornos restantes y se descartan los que estén muy cercanos entre śı. 6. Las regiones definidas por los contornos que superen la prueba anterior son recortadas de la imagen en escala de grises. 7. A estas regiones recortadas se les aplica una transformación de deformación de pers- pectiva, que genera una nueva imagen de 350 por 350 pixeles. 8. A las imagenes resultantes se les aplica una operación de umbral utilizando el algoritmo de Otsu [56]. La Figura 4.13 muestra los resultados de algunos de los pasos anteriores. El resultado de este algoritmo es una lista de imágenes que representan todos los candidatos a marcadores reconocidos en la imagen de entrada. Esta lista luego es utilizada como parámetro para el algoritmo de descodificación de marcadores que se explica a continuación. 46 Figura 4.13: Pasos del algoritmo de detección de marcadores. (a) Imagen de entrada. (b) Umbral adaptativo. (c) Candidatos detectados. (d) Arriba: Candidato despues de la corrección de perspectiva. Abajo: Candidato verificado y descodificado. (e) Imagen de salida. Descodificación de marcadores Una vez que una región de la imagen de entrada es identificada como un posible candidato a marcador se procede con la fase de descodificación. Esta fase se divide en tres etapas, la primera de las cuales consiste en examinar el borde externo de la región identificada, buscando que este sea mayormente de color negro. Si el candidato supera esta prueba se procede a descodificar propiamente la información contenida en la matriz de 5x5 interna. Esta matriz codifica un número entero de 10 bits como cinco palabras de 5 bits cada una, de los cuales 3 son bits de redundancia y dos son bits de datos, utilizando el código de bloques que se describe a continuación: Para cada palabra de 5 bits los bits 2 y 4 son los bits de datos. Todos los demás son los bits de redundancia. Todos los bits de paridad cuentan la paridad par de los bits de datos. 47 El bit 1 codifica el negado de la paridad del bit 2. Se utiliza el negado de la paridad para evitar que el marcador que codifica el número 0 sea completamente negro, de forma que no pueda confundirse con elementos del entorno. El bit 3 codifica la paridad del bit 4. El bit 5 codifica la paridad de los bits 2 y 4. A manera de ejemplo, el marcador mostrado en la Figura 4.12 (la representación más a la izquierda) codifica el número 89 como la secuencia binaria 1000010111101110100110111. Para realizar la descodificación de un marcador se calcula la distancia de Hamming de las cinco palabras del marcador con todas las posibles palabras de 5 bits en espera de que la suma de las 5 distancias sea 0. En caso de que no se encuentre una combinación de palabras que tenga distancia 0 se procede a rotar el marcador 90 grados en el sentido del reloj y se vuelve a ejecutar el proceso anterior; esto se repite hasta que se hayan verificado las 4 rotaciones de 90 grados o se encuentre una matriz que cumpla con los requisitos. Si todas las rotaciones poseen distancias mayores a 0 entonces se descarta el marcador. En caso contrario, el número del marcador es ensamblado a partir de los bits de datos. Utilizando el código de bloques descrito se desarrolló un script en el lenguaje de progra- mación Python para generar todos los posibles marcadores que usan dicho código. El listado del código fuente de este script se encuentra en el Anexo B. Como nota adiciona, en el libro Mastering OpenCV with practical computer vision projects [8] se indica que el código utili- zado por los marcadores es un código de Hamming tipo (5, 2), afirmación que no coincide con lo que se encontró durante el desarrollo. Determinación de las transformaciones geométricas Con los marcadores ya identificados y descodificados se procede a determinar cuales son las transformaciónes geométricas del marcador. Para esto se asume que la cámara esta posicionada en el origen del sistema de coordenadas (punto (0, 0, 0)) viendo en la dirección del eje Z negativo (el vector canónico (0, 0, -1) en un sistema de coordenadas de mano derecha como el utilizado por OpenGL). Utilizando los parámetros intŕınsecos de la cámara se ejecuta la función de OpenCV solvePnP, la cual se encarga de calcular las transformaciones de rotación y traslación que hay que aplicar a la cámara de forma que el marcador detectado se vea como un poĺıgono cuadrado espećıfico, que en este caso es un poĺıgono cuadrado canónico de lado 1. Luego estas transformaciones se invierten para obtener las transformaciones que hay que aplicar al poĺıgono canónico para que se ubique sobre el marcador detectado relativo a la posición de la cámara. La Figura 4.14 ilustra este procedimiento. Estas transformaciones son luego combinadas como la matriz de modelado que se aplica al objeto virtual relacionado con el marcador detectado, de forma que dicho objeto virtual se alinee correctamente con el marcador en la imagen de entrada. 48 Figura 4.14: Obtención de las transformaciones de un marcador. 4.2.5. Bomb Game, el escenario de demostración NxtAR incluye un escenario sencillo como demostración de las capacidades de la arquitec- tura titulado Bomb Game. Este juego, como su nombre lo indica, consiste en desarmar una serie de bombas cada una de las cuales posee un mecanismo diferente para ser desactivada. El escenario incluye 3 tipos de bomba diferentes e implementa el modo de acción automática. Figura 4.15: Las bombas del juego Bomb Game. El modo de acción automática de Bomb Game consiste en un recorrido lineal sencillo en el cual el robot, avanzando en ĺınea recta, trata de detectar si existen marcadores a su izquierda cuando pasa cerca de una marca en el suelo que sirve de indicador. Al detectar tantos indicadores como bombas han sido configuradas en el escenario el robot se detiene y se le muestra un resumen al usuario sobre cuántas bombas virtuales fueron detectadas y cuántas se esperaba encontrar. En el modo de control manual se implementó el juego de desactivación de bombas, en el cual el usuario debe manipular al robot dentro del entorno buscando los distintos marcadores que representan las bombas y desactivarlas según sus diferentes mecanismos utilizando el brazo virtual con los controles provistos para tal fin. Como se mencionó, el juego implementa tres bombas: Bomba de cables. 49 Bomba de combinación. Bomba de nivel. La bomba de cables consiste en un conjunto de cables de colores de entre los cuales el usuario debe cortar uno (el azul) posicionando el brazo virtual para tal fin. Por su parte la bomba de combinación posee cuatro botones de colores los cuales deben ser presionados en un orden predeterminado (azul, rojo, negro y luego verde). Por último la bomba de nivel posee un único botón el cual debe ser presionado únicamente cuando el indicador de inclinación del dispositivo está en verde. En el caso de que la aplicación sea ejecutada en dispositivos como la consola OUYA, la cual no posee sensores que le permitan determinar su orientación, este tipo de bomba puede ser desactivada sencillamente con presionar su botón. Las tres bombas pueden verse en la Figura 4.15. 50 Caṕıtulo 5 Pruebas y resultados En este caṕıtulo se describen cinco distintas pruebas a las que fue sometida la implemen- tación de referencia. Las pruebas se enfocaron principalmente en los aspectos más resaltantes de las aplicaciones desarrolladas, en particular lo referente a la transmisión de video, la detec- ción de marcadores y el despliegue de objetos virtuales. Adicionalmente se realizó una prueba con usuarios para comparar los distintos esquemas y dispositivos de control disponibles. Las cinco pruebas se pueden agrupar en las siguientes 3 categoŕıas: pruebas emṕıricas, pruebas estad́ısticas y pruebas de usuario. La prueba número 1 se realizó de manera emṕırica. Las pruebas número 2, 3 y 4 fueron realizadas por medio de un estudio estad́ıstico, y la prueba 5 consistió en una prueba de usuario llevada a cabo por una encuesta de 8 preguntas. 5.1. Medición de distancias óptimas para reconocimien- to de marcadores Esta prueba se diseñó para determinar cuales son las distancias mı́nima y máxima entre las cuales los marcadores pueden ser detectados correctamente para los tres smartphones detallados en la Sección 2.2. Para esto se utilizaron marcadores de 12.5 por 12.5 cent́ımetros, con las cámaras de los dispositivos configuradas para transmitir video a 355 por 288 ṕıxeles con calidad de compresión JPEG fija en 90 (véase la prueba descrita en la Sección 5.3 de este mismo caṕıtulo para una explicación del porque se escogieron dichos valores). Los resultados de esta prueba se pueden observar en la Tabla 5.1. Cabe resaltar que la distancia máxima no representa la distancia a partir de la cual los marcadores no pueden ser detectados en absoluto, sino que a partir de dicha distancia estos son detectados con errores de alineación notables asumiendo que la cámara fue calibrada correctamente. En este con- texto los errores de alineación indican que los objetos virtuales utilizados en el juego no son desplegados correctamente, ya sea por fallas en el cálculo de las transformaciones geométri- cas o por errores notables al hacer coincidir las esquinas del marcador con las esquinas del objeto virtual, entre otros artefactos visuales posibles. 51 Cámara Distancia mı́nima Distancia máxima Galaxy Nexus 19 cm 109.5 cm Galaxy S4 16 cm 88 cm Pantech Burst 19 cm 114.5 cm Tabla 5.1: Distancias de reconocimiento de marcadores por dispositivo. 5.2. Perfilado del procesamiento en el estado de juego Esta prueba fue realizada con la finalidad de determinar el tiempo consumido por las diferentes acciones llevadas a cabo durante el procesamiento de un cuadro en el estado de juego. Estas acciones se dividieron en dos categoŕıas. La primera es todo lo referente al algoritmo de detección de marcadores y la segunda es el resto del procesamiento del cuadro, incluyendo la lógica del juego y el despliegue de objetos 3D. La prueba consistió en la medición del tiempo de procesamiento promedio por cuadro durante un minuto, desglosado en las categoŕıas antes mencionadas. Se probaron dos escena- rios, uno utilizando la Galaxy Tab 2 como dispositivo de control y otro utilizando la consola OUYA. Ambos escenarios fueron repetidos 30 veces cada uno y luego promediados con la intención de mitigar el error estad́ıstico. Todas las medidas fueron tomadas en milisegundos de procesamiento por cuadro. En ambos escenarios el dispositivo con cámara fue un smartp- hone Galaxy Nexus y el video fue fijado a una resolución de 352 por 288 ṕıxeles, utilizando compresión JPEG con el parámetro de calidad fijo en 90. 5.2.1. Resultados y análisis El primer análisis sobre los datos capturados fue el comparar el desempeño entre los 2 dispositivos utilizados, análisis que puede observarse en el gráfico de la Figura 5.1. Lo más relevante en este gráfico es el hecho de que el tiempo de procesamiento dedicado a la detección de marcadores consume la mayor parte del tiempo total dedicado a cada cuadro en ambos dispositivos. Aśı mismo se puede observar que en ambos dispositivos el tiempo total dedicado al procesamiento de cuadros es considerablemente alto, siendo en promedio aproximadamente 180 milisegundos por cuadro en la Galaxy Tab 2 y 100 milisegundos por cuadro en la consola OUYA. Esto permite procesar entre unos 5,5 y 10 cuadros por segundo en cada dispositivo respectivamente. Luego se calculó el porcentaje de tiempo consumido por ambas categoŕıas de procesa- miento y se obtuvieron los gráficos mostrados en la Figura 5.2. Como se puede apreciar, más del 75 % del tiempo de procesamiento se dedica a la detección de marcadores en ambos dispositivos. De este estudio y del anterior se puede concluir entonces que el algoritmo de detección de marcadores es candidato a ser optimizado, de forma que se pueda obtener una mejor tasa de cuadros por segundo en los dispositivos de control, lo que debeŕıa traducirse en mejor experiencia de usuario. Adicionalmente se realizó un estudio de la distribución de estos tiempos para ambos dispositivos utilizando el box plot mostrado en la Figura D.1. En el gráfico puede observarse 52 Figura 5.1: Tiempo promedio de procesamiento por cuadro durante 1 minuto. que la consola OUYA generó unas medidas de tiempo mucho más estables comparadas con las medidas arrojadas por la Galaxy Tab 2. Esto es de esperarse dado que la consola OUYA es un sistema de tiempo real suave sin multitarea, caracteŕısticas diametralmente opuestas a las provistas por la tablet. 5.3. Tasas de recepción de video en el dispositivo de control En esta prueba se midió la tasa de recepción de video para diferentes tamaños de imagen y calidades de compresión JPEG. El objetivo fue determinar el valor óptimo para ambos parámetros. Se probaron 3 escenarios con imágenes de dimensiones ubicadas entre 176x144 ṕıxeles, 352x288 ṕıxeles y 720x576 ṕıxeles, variando la calidad de la compresión JPEG de los cuadros entre 1 y 100 inclusive, en pasos de 10 en 10 para cada escenario. De estos escenarios solo se conservaron los dos últimos dado que la biblioteca OpenCV fallaba al tratar de realizar la calibración de la cámara con las imágenes de tamaño 176x144 ṕıxeles. Cada escenario fue repetido 10 veces por cada calidad de compresión, teniendo cada repetición una duración de 10 segundos y además utilizando un punto de acceso inalámbrico dedicado. Todas las pruebas fueron realizadas con la Galaxy Tab 2. 5.3.1. Resultados y análisis En la Figura 5.3 se pueden observar las tasas de transmisión promedio para los dos esce- narios probados. Como puede observarse en ambos escenarios la tasa de recepción disminuye 53 Figura 5.2: Porcentaje de tiempo consumido por distintos procesamientos. (a) Caso de estudio Galaxy Tab 2. (b) Caso de estudio OUYA. a medida de que se incrementa la calidad de la compresión, presentándose la mayor pérdida de cuadros de video cuando dicho parámetro supera el valor de 90. De hecho cuando la compresión alcanza el valor de 100 la pérdida de imágenes es casi absoluta. Otro detalle que resalta es el hecho de que la tasa de recepción disminuye a un poco menos de la mitad en el segundo escenario con respecto al primero. Figura 5.3: Tasas de transmisión de video para diferentes calidades de compresión JPEG. (a) Para imágenes de 352x288 ṕıxeles. (b) Para imágenes de 720x576 ṕıxeles En base a estos resultados concluimos que los valores óptimos para los parámetros en estudio son el tamaño de imágen de 355 por 288 ṕıxeles y la calidad de compresión en 90. Escogimos estos valores porque de esta forma la tasa de recepción de imágenes de video 54 se aproxima bastante al tiempo de procesamiento del juego en el mejor caso detectado (10 cuadros por segundo). Esto disminuye la cantidad de imágenes desperdiciadas cuando el procesador del dispositivo de control no es capaz de procesarlas lo suficientemente rápido. 5.4. Determinación del punto de saturación del video Esta prueba se diseñó con el objetivo de examinar el comportamiento de la transmisión y recepción del video enviado entre los dispositivos móviles involucrados en la arquitectura. Dada la ubicuidad de los puntos de acceso inalámbricos en los entornos laborales y caseros se busca determinar la posibilidad de insertar la arquitectura desarrollada en una red existente. Para realizar este estudio se utilizó el punto de acceso inalámbrico descrito en la Sección 2.2 como punto intermedio, y la tablet Galaxy Tab 2 como dispositivo de control. Se utilizó un enfoque bottom-up, midiendo las tasas de recepción y pérdida de cuadros de video por segundo para diferentes escenarios de utilización del canal de transmisión. Para llevar a cabo la saturación del canal se utilizó el sistema D-ITG (Distributed Internet Traffic Generator - Generador de Tráfico para Internet Distribuido) descrito en [13], el cual puede generar distintos tipos de tráfico para diversos protocolos de capa de red y transporte, utilizando procesos constantes o estocásticos. En este estudio se generó un tŕafico constante utilizando paquetes de 1500 bytes, y variando la tasa de paquetes transmitidos por segundo para saturar el canal con tasas de transmisión de bits que vaŕıan entre 0 y 9 Mbps, incrementando la tasa de transmisión de bits en pasos de 1 Mbps en cada escenario. Cada escenario constó de una duración de 10 segundos y fue repetido 10 veces, para luego ser promediado. La tasa de paquetes por segundo para cada escenario fue calculada despejando x en la Ecuación 5.1. yKbps = 12 Kb paquete ∗ x paquete s ; ∀y = 1000, 2000, . . . , 9000 (5.1) 5.4.1. Resultados y análisis Una vez completado el estudio diseñado para esta prueba se obtuvieron los datos que pueden observarse en el gráfico de la Figura 5.4. Como se puede apreciar, cuando el punto de acceso está dedicado exclusivamente a la transmisión del video los resultados coinciden con los obtenidos en la prueba descrita en la Sección 5.3. Sin embargo, a medida que se comienza a inyectar tráfico en el canal el rendimiento de la transmición comienza a degradarse, obte- niendo una tasa de pérdida de cuadros mayor que la tasa de cuadros recibidos correctamente a medida que el tráfico se acerca y eventualmente supera los 8 Mbps. Tomando como un mı́nimo aceptable una transmisión de video de 10 cuadros recibidos por segundo1 (correcta o incorrectamente), podemos establecer el punto de saturación de la transmisión de video alrededor de los 5 Mbps, punto en el cual la tasa de cuadros recibidos correctamente es considerablemente menor a 9 cuadros por segundo, lo cual implica que aproximadamente 2 de cada 10 cuadros por segundo poseen errores que no permiten su 1Una tasa menor a 10 cuadros por segundo se considera que provee una muy mala experiencia [5]. 55 Figura 5.4: Comportamiento de la recepción de video para distintas tasas de utilización del canal de transmisión. decodificación. Más allá de 5 Mbps la tasa de cuadros recibidos independientemente de su estado alcanza niveles muy bajos, a medida que la tasa de cuadros perdidos se hace aún mayor. 5.5. Pruebas de usuario En esta prueba se realizó un estudio de usuario para determinar cuál de los dos esquemas de control es el preferido por los mismos. Se le pidió a 52 usuarios que jugaran una partida de Bomb Game utilizando tanto la pantalla táctil de la Galaxy Tab 2 como el gamepad de la consola OUYA, después de lo cual se les realizó el cuestionario que se encuentra en el Anexo C. Durante la realización de la prueba los usuarios estuvieron ubicados en una habitación diferente a la que alojaba al robot, de forma que todo el control fuera realizado por teleoperación. Esta prueba tuvo como objetivo el determinar cuál de los dos dispositivos de control provee la mejor experiencia de usuario, aśı como el obtener la apreciación de los usuarios con respecto a las interfaces gráficas de usuario desarrolladas, en particular la interfaz utilizada en las tablets como se muestra en la Figura 4.8.d. 2La prueba se realizó con solamente 5 usuarios siguiendo las recomendaciones de Nielsen en [55]. 56 5.5.1. Resultados y análisis En la primera pregunta del cuestionario se le pidió a los usuarios que indicaran cuál de los dos dispositivos de control evaluados ofrece la mejor experiencia de usuario, tomando en cuenta factores como la comodidad de uso, los tiempos de respuesta y de despliegue, entre otros. Como puede observarse en la Figura 5.5 la mayoŕıa de los usuarios prefirió la expe- riencia de uso provista por la consola de videojuegos OUYA, con solo un usuario prefiriendo la experiencia provista por la tablet Galaxy Tab 2. Figura 5.5: Preferencia de los usuarios por los diferentes dispositivos de control. Con la segunda pregunta realizada se le pidió a los usuarios que evaluaran los dos meca- nismos de control, independientemente de factores relacionados al dispositivo. En particular se buscó determinar entre la pantalla táctil y el gamepad cual es el preferido por los usuarios. En la Figura 5.6 se puede observar que 3 de los 5 usuarios prefirieron utilizar el gamepad como mecanismo de control. La tercera pregunta consistió en evaluar el tiempo de respuesta de la pantalla táctil, entendiéndose por tiempo de respuesta el tiempo transcurrido entre el uso del mecanismo de control y la correspondiente respuesta ejecutada por el robot. Como puede observarse en la Figura 5.7 la apreciación del tiempo de respuesta de la pantalla táctil es favorable. De igual forma se le pidió a los usuarios su apreciación del tiempo de respuesta del gamepad, apreciación que puede verse en la Figura 5.8, correspondiente a la cuarta pregunta. En este caso la mayoŕıa de los usuarios consideraron que el gamepad ofrece un tiempo de respuesta excelente, resultados que contrastan con los obtenidos para el caso de la pantalla táctil. 57 Figura 5.6: Preferencia de los usuarios por los diferentes mecanismos de control. En la pregunta 5 los usuarios evaluaron la dificultad de controlar al robot de forma remota. Los resultados de esta pregunta se pueden observar en la Figura 5.9. Como puede verse en los resultados, los usuarios consideraron al control remoto del robot como una actividad de dificultad promedio tendiendo a fácil, con un único usuario indicando que dicha actividad le pareció dif́ıcil de realizar. Con la pregunta 6 se pidió a los usuarios que dieran su apreciación acerca de la distribución de los elementos de la interfaz de juego en la pantalla táctil de la tablet Galaxy Tab 2 (visible en la Figura 4.8.d). Se establecieron como criterios a considerar la organización temática de los elementos, aśı como su facilidad de acceso. Como puede verse en el gráfico de la Figura 5.10, los usuarios consideraron la distribución de los elementos como una distribución decente tendiendo a buena. En la pregunta 7 se le pidió a los usuarios que evaluaran la utilidad de la pantalla de ayuda implementada, evaluación mostrada en la Figura 5.11. Como puede observarse los usuarios consideraron que la pantalla de ayuda es un elemento útil durante el transcurso del juego desarrollado. Por último en la pregunta 8 se pidió a los usuarios que evaluaran la dificultad de controlar el brazo virtual. Los resultados pueden observarse en la Figura 5.12. En este caso 3 de los 5 usuarios consideraron que controlar el brazo virtual es una taréa dif́ıcil, con 2 usuarios considerándolo una tarea fácil. 58 Figura 5.7: Apreciación del tiempo de respuesta de la pantalla táctil. Figura 5.8: Apreciación del tiempo de respuesta del gamepad. 59 Figura 5.9: Apreciación de la dificultad de controlar al robot de forma remota. Figura 5.10: Apreciación de la interfaz en la tablet. 60 Figura 5.11: Utilidad de la pantalla de ayuda durante el transcurso de una partida. Figura 5.12: Dificultad de controlar el brazo virtual del robot. 61 Caṕıtulo 6 Conclusiones En este trabajo de investigación se diseño y desarrolló una arquitectura de control para robots móbiles sobre la cual es posible implementar juegos que hacen uso de tecnoloǵıas de realidad aumentada. Para esto se realizó un estudio conciso acerca de las distintas tecnoloǵıas y herramientas disponibles tanto en el campo de la robótica como en el de la realidad aumen- tada. La arquitectura de control fue diseñada para ser genérica, es decir, independiente del hardware utilizado en su implementación, limitándose a plantear un esquema de comunica- ción entre los dispositivos involucrados en la misma, aśı como un conjunto de posibles casos de uso que se deben cumplir. Esta arquitectura fue implementada luego en varios dispositivos móviles basados en el sistema operativo Android, adicionalmente utilizando un robot LEGO Mindstorms NXT. Para llevar a cabo esta implementación de referencia se realizó primero un estudio del hardware disponible sobre el cual se realizó el desarrollo. La implementación fue desarrollada para soportar tanto tablets basadas en el sistema operativo Android como la consola de videojuegos OUYA como dispositivos de control del robot. Adicionalmente se diseñó un conjunto de cinco escenarios de prueba con el objetivo de evaluar distintos aspectos de la implementación de referencia. En particular se evaluaron aspectos relacionados a la transmisión de video entre los dispositivos involucrados, cuánto tiempo toma el procesamiento de dicho video cuadro por cuadro y diversos puntos relacio- nados con la usabilidad de la implementación. Con estas pruebas se pudo determinar las limitaciones de la transmisión de video, lo que nos permitió establecer diferentes parámetros para garantizar que este proceso no afecte la experiencia de usuario. Aśı mismo se pudo de- terminar que el procesamiento del video es la operación más costosa en cuanto a tiempo de ejecución dentro de la aplicación de control, tomando incluso más de tres cuartas partes del tiempo de procesamiento por cuadro. En el caso de las pruebas de usuario se determinó que la consola OUYA y el uso de gamepads en general provee una experiencia de usuario más satisfactoria para el control del robot móvil en comparación con la experiencia provista por la tablet. En base a lo antes planteado se puede concluir que los objetivos propuestos al comienzo del trabajo fueron cumplidos a cabalidad, obteniendo como resultado un sistema de control basado en realidad aumentada para el sistema operativo Android. 62 6.1. Contribuciones Las contribuciones de este trabajo de investigación son las siguientes: Se diseñó una arquitectura genérica para desarrollar sistemas de control para robots que incorporen juegos y simulaciones de realidad aumentada. Esta arquitectura define un modelo sencillo de casos de uso y un esquema de comunicación sin entrar en muchos detalles ni exigiendo algoritmos o criterios de implementación espećıficos, lo cual per- mite que los desarrolladores que deseen utilizar esta arquitectura puedan incorporar paradigmas adicionales dependiendo del objetivo principal de cada implementación. Se desarrollaron de forma satisfactoria múltiples aplicaciones para el sistema operativo Android y los robots LEGO Mindstorms NXT, las cuales implementan completamente los casos de uso propuestos por la arquitectura. Con esto se demuestra que la arqui- tectura diseñada es lo suficientemente completa como para cumplir con los objetivos propuestos al comienzo de este trabajo de investigación. Se aplicaron de forma satisfactoria los patrones de diseño state pattern y Entidad- Componente-Sistema, lo que demuestra su utilidad en el desarrollo de aplicaciones de realidad aumentada. 6.2. Limitaciones La principal limitación de este trabajo radicó en la falta de más dispositivos móviles para utilizar como dispositivos de control, lo que limitó la realización de las pruebas a solo dos dispositivos completamente diferentes. 6.3. Trabajos futuros Se proponen los siguientes trabajos a futuro: Optimizar el algoritmo de detección de marcadores para obtener menores tiempos de procesamiento. Diseñar e implementar nuevos escenarios de juego de mayor complejidad. Portar la implementación de referencia a otros dispositivos de control, en particular a computadoras de escritorio, aprovechando las capacidades del framework LibGDX. Evaluar la factibilidad de extender la arquitectura para soportar múltiples saltos de red entre los distintos dispositivos involucrados, en particular entre el dispositivo de control y el dispositivo con cámara, de forma que el usuario pueda encontrarse a una mayor distancia del robot que la permitidas por un único punto de acceso inalámbrico. 63 Evaluar la factibilidad de agregar soporte de realidad aumentada sin marcadores a la arquitectura y a la implementación de referencia. 64 Anexos 65 Anexo A El robot LEGO Mindstorms TM El Mindstorms es un proyecto de The LEGO Group que tiene como objetivo el democra- tizar el acceso a la robótica proveyendo un kit de piezas t́ıpicas de la ĺınea LEGO Technic R© junto a un bloque inteligente, llamado comúnmente como el brick. Este kit permite construir y programar robots modulares apoyados por diversos paquetes de software que permiten pro- gramar el comportamiento del robot. El nombre Mindstorms se debe al libro Mindstorms: Children, computers, and powerful ideas publicado por Seymour Papert en 1980 [58]. En este anexo se detalla la historia del LEGO Mindstorms, comenzando por el Pro- grammable Brick del MIT, proyecto en el cual se basa The LEGO Group para desarrollar el Mindstorms. Luego se exponen las diferentes versiones del LEGO Mindstorms, desde el Mindstorms RCX del kit Mindstorms Robotics Invention System (RIS), pasando por el Mindstorms NXT, y por ultimo el Mindstorms EV3. A.1. El MIT Programmable Brick El MIT Programmable Brick fue un proyecto de investigación llevado a cabo por Resnick y colaboradores en asociación con The LEGO Group entre 1987 y 1998 [64]. El objetivo del proyecto era diseñar y desarrollar una micro-computadora programable que estaŕıa co- locada dentro de un bloque de LEGO. Este proyecto estaba enmarcado dentro del área de la Computación Ubicua, con la intención de llevar este tipo de computación a entornos educativos infantiles en un esquema de educación marcado por el constructivismo [64] [63]. Resnick y colaboradores se inspiraron en un proyecto anterior llamado LEGO/Logo, el cual integraba un veh́ıculo programable constrúıdo con piezas de LEGO, junto al lenguaje de programación Logo. LEGO/Logo permit́ıa programar el veh́ıculo de la misma forma que se programaban los llamados robots Tortuga [1]. El trabajo de Resnick y colaboradores se encargó, entre otras cosas, de incorporar sensores al sistema y de eliminar el cableado que poséıan los veh́ıculos de LEGO/Logo. El resultado fue un brick controlador programable que funcionaba con bateŕıas, y pod́ıa incorporar distintos sensores o motores según fuera necesario para el modelo que se estuviera trabajando. Se diseñaron cuatro versiones diferentes del Programmable Brick [9], la última de las cuales puede observarse en la Figura A.1. 66 Figura A.1: El MIT Programmable Brick . Imagen recuperada de [9]. A.2. Evolución del LEGO Mindstorms En 1998, al mismo tiempo que Resnick y colaboradores publicaban su trabajo sobre el Programmable Brick, The LEGO Group por su parte lanzó al mercado un rediseño completo del mismo al que llamaron RCX, comercializándolo como el Mindstorms Robotics Invention System, mejor conocido por sus siglas RIS. Con esto se comenzó la linea de productos LEGO Mindstorms la cual hasta el momento ha pasado por tres grandes iteraciones, que se describen a continuación. A.2.1. LEGO Mindstorms RCX El kit Mindstorms Robotics Invention System RCX fue la primera iteración del paquete Mindstorms. Estuvo disponible comercialmente entre 1998 y 2006, y consist́ıa en un conjunto de aproximadamente 700 piezas, incluyendo el brick RCX, los motores, los sensores y la documentación. El brick RCX era la unidad central de procesamiento del kit, la cual puede verse en la Figura A.2. Este brick estaba compuesto por un microcontrolador programable Hitachi H8/3292 que corŕıa a una frecuencia de reloj de 16 MHz. El Hitachi H8 contaba con 16 KB de memoria RAM en el chip y un bloque ROM de 512 bytes en el cual estaba embebido el firmware básico del sistema y algunos programas de demostración. El RCX contaba adicio- nalmente con 32 KB de memoria RAM externa en la cual se almacenaban el software de 67 sistema susceptible de ser actualizado, y los programas instalados por el usuario(a). El RCX contaba con tres puertos de entrada para sensores y tres puertos de salida para motores. Los sensores y los motores utilizaban conectores propietarios con forma de piezas de LEGO [9]. La transmisión de datos entre el brick y la computadora del usuario se realizaba mediante un puerto para comunicaciones seriales por infrarrojo incorporado en el brick, el cual se conectaba con una pieza llamada IR tower. Se comercializaron dos versiones de esta IR Tower. La primera se inclúıa con el RCX 1.0 y el RCX 1.5 y se conectaba a la computadora por medio de un puerto serial RS-232, necesitando bateŕıas propias. La segunda versión formaba parte del kit RCX 2.0 y utilizaba el puerto USB eliminando la necesidad de bateŕıas. La IR Tower pod́ıa utilizarse también para enviar comandos directos al RCX. El alcance de la IR Tower estaba entre 60 cent́ımetros y 8 metros [9]. Figura A.2: El Brick del Mindstorms RCX 2.0. Imagen recuperada de [9]. A.2.2. LEGO Mindstorms NXT El kit Mindstorms NXT es la segunda iteración del paquete Mindstorms disponible co- mercialmente desde julio de 2006, con una actualización llamada NXT 2.0 disponible desde agosto del 2009. Este kit consiste en un conjunto de 577 piezas entre las cuales se encuentran el brick controlador, los sensores, un CD con software asociado y stickers. El brick está com- puesto por un procesador ARM Atmel de 32 bits que funciona a 45 MHz. Este procesador dispone de 64 KB de memoria RAM, y 256 KB de memoria flash para almacenamiento per- sistente. Adicionalmente el brick contiene un coprocesador AVR Atmel de 8 MHz. Se puede establecer comunicación con el brick utilizando ya sea la interfaz USB 2.0 o Bluetooth. Para lo segundo el brick dispone de un chip Bluecore 4 que implementa el perfil de puerto serial de Bluetooth 2.0. El kit producido por The LEGO Group incluye 4 sensores: un sensor táctil, un sensor de sonido, un sensor ultrasónico y un sensor puntual monocromático de luz. En el NXT 2.0 el sensor de luz es sustitúıdo por un sensor de color RGB. Adicionalmente se pueden adquirir distintos sensores compatibles fabricados por entidades externas a LEGO. 68 Tanto los sensores como los servomotores del kit utilizan conectores propietarios basados en RJ12. Una imagen del brick puede observarse en la Figura A.3. Este brick necesita de seis bateŕıas AA para funcionar. Figura A.3: El brick del Mindstorms NXT. Imagen recuperada de generationrobots.com. The LEGO Group provee varios conjuntos de documentación, llamados Developer Kits, enfocados a usuarios avanzados del sistema. Existen 3 de estos Developer Kits : el BDK [33], que detalla el funcionamiento del protocolo de comunicación LCP (LEGO Communication Protocol) utilizado por el brick y el hardware de la interfaz Bluetooth; el HDK [34] que detalla el funcionamiento y la estructura electrónica del hardware del brick y los sensores estándar; y el SDK [35], que detalla el software controlador del robot y el funcionamiento de la máquina virtual que ejecuta programas de usuario en el brick. Los robots Mindstorms NXT pueden ser programados usando una gran cantidad de tec- noloǵıas y lenguajes de programación. El entorno básico es provisto por The LEGO Group con el kit y consiste en un IDE (Integrated Development Environmet - Entorno de Desarrollo Integrado) visual, llamado NXT-G. Este entorno se caracteriza por ser sumamente fácil de aprender y utilizar. Los robots se programan combinando series de bloques que se ejecu- tan linealmente sobre carriles paralelos. Estos bloques controlan los distintos componentes 69 http://www.generationrobots.com electrónicos del robot y pueden combinarse utilizando diferentes estructuras de control como lo son condicionales y ciclos. De manera alternativa, dada la calidad abierta del hardware y el software de los kits Mindstorms NXT, existe una gran variedad de entornos de desarrollo extraoficiales a The LEGO Group. Algunos de los más conocidos son el proyecto LejOS1, que suplanta al firmware estándar de los bricks NXT y permite ejecutar programas escritos en un subconjunto del lenguaje Java sobre el robot. Otro es el proyecto NXC2 (Not eXactly C ), que permite compilar programas escritos en un lenguaje similar a C para ser ejecutados sobre el firmware básico. También es posible utilizar entornos especializados para robótica como lo son el Microsoft Robotics Developer Studio 43, y el sistema Player/Stage4. A.2.3. LEGO Mindstorms EV3 El kit LEGO Mindstorms EV3 es la tercera iteración del paquete Mindstorms, pensado para estar disponible comercialmente en otoño de 2013. Este nuevo kit agrega una gran cantidad de nuevas funcionalidades a la serie Mindstorms, tomando ventaja de las tecnoloǵıas móviles que han aparecido en los años recientes desde que el Mindstorms NXT 2.0 fue introducido al mercado. El brick del Mindstorms EV3 puede verse en la Figura A.4. Figura A.4: El brick del Mindstorms EV3. Imagen recuperada de www.theverge.com. 1http://lejos.sourceforge.net 2http://bricxcc.sourceforge.net/nbc 3https://www.microsoft.com/robotics 4http://playerstage.sourceforge.net 70 http://www.theverge.com http://lejos.sourceforge.net http://bricxcc.sourceforge.net/nbc https://www.microsoft.com/robotics http://playerstage.sourceforge.net Entre las nuevas caracteŕısticas incorporadas al Mindstorms EV3 se incluyen las siguien- tes: Un nuevo procesador de tipo ARM 9. 16 MB de memoria ram. Compatibilidad con redes Wi-Fi utilizando un dongle usb. Sistema operativo basado en GNU\Linux en el brick. Interoperabilidad de fábrica con sistemas Android TM e iOS TM . Un nuevo sensor infrarrojo y sensores clásicos mejorados. Retro-compatibilidad con los sensores del Mindstorms NXT. Posibilidad de expandir el almacenamiento interno con tarjetas micro SD. Cuatro puertos para servomotores. Adicionalmente el brick tiene soporte para localización en 10 idiomas, y los nuevos senso- res son al menos tres veces más rápidos que los sensores del Mindstorms NXT. Los sensores del EV3 pueden realizar alrededor de 1000 capturas por segundo, en contraste con los del NXT que realizan unas 330 capturas por segundo dependiendo del sensor [37]. 71 Anexo B Listado del código de generación de marcadores #! /usr / b in /env python import pygame OUTPUT SIZE = (700 , 700) def generateMarker ( b i t s ) : ””” Genera un marcador de 5x5 u t i l i z a n d o e l cod igo d e f i n i d o en e l l i b r o ’ Mastering OpenCV with p r a c t i c a l computer v i s i o n p r o j e c t s ’ ””” par i ty0 = False par i ty1 = False par i ty2 = False par i ty3 = False par i ty4 = False par i ty5 = False par i ty6 = False par i ty7 = False par i ty8 = False par i ty9 = False par i ty10 = False par i ty11 = False par i ty12 = False par i ty13 = False par i ty14 = False # Crea una s u p e r f i c i e negra . marker = pygame . Sur face ( ( 7 , 7 ) ) marker . f i l l ( ( 0 , 0 , 0 ) ) # Bloquea l a s u p e r f i c i e para modi f i car sus p i x e l e s . marker . l o ck ( ) 72 # Rel l ena l a s c e l d a s de datos . i f b i t s [ 0 ] == True : marker . s e t a t ( ( 4 , 5 ) , (255 , 255 , 255)) i f b i t s [ 1 ] == True : marker . s e t a t ( ( 2 , 5 ) , (255 , 255 , 255)) i f b i t s [ 2 ] == True : marker . s e t a t ( ( 4 , 4 ) , (255 , 255 , 255)) i f b i t s [ 3 ] == True : marker . s e t a t ( ( 2 , 4 ) , (255 , 255 , 255)) i f b i t s [ 4 ] == True : marker . s e t a t ( ( 4 , 3 ) , (255 , 255 , 255)) i f b i t s [ 5 ] == True : marker . s e t a t ( ( 2 , 3 ) , (255 , 255 , 255)) i f b i t s [ 6 ] == True : marker . s e t a t ( ( 4 , 2 ) , (255 , 255 , 255)) i f b i t s [ 7 ] == True : marker . s e t a t ( ( 2 , 2 ) , (255 , 255 , 255)) i f b i t s [ 8 ] == True : marker . s e t a t ( ( 4 , 1 ) , (255 , 255 , 255)) i f b i t s [ 9 ] == True : marker . s e t a t ( ( 2 , 1 ) , (255 , 255 , 255)) # Calcu la l a s par idades . par i ty0 = b i t s [ 0 ] pa r i ty1 = not b i t s [ 1 ] pa r i ty2 = b i t s [ 0 ] i s not b i t s [ 1 ] pa r i ty3 = b i t s [ 2 ] pa r i ty4 = not b i t s [ 3 ] pa r i ty5 = b i t s [ 2 ] i s not b i t s [ 3 ] pa r i ty6 = b i t s [ 4 ] pa r i ty7 = not b i t s [ 5 ] pa r i ty8 = b i t s [ 4 ] i s not b i t s [ 5 ] pa r i ty9 = b i t s [ 6 ] par i ty10 = not b i t s [ 7 ] par i ty11 = b i t s [ 6 ] i s not b i t s [ 7 ] par i ty12 = b i t s [ 8 ] par i ty13 = not b i t s [ 9 ] 73 par i ty14 = b i t s [ 8 ] i s not b i t s [ 9 ] # Rel l ena l a s c e l d a s de paridad . i f par i ty0 == True : marker . s e t a t ( ( 3 , 5 ) , (255 , 255 , 255)) i f par i ty1 == True : marker . s e t a t ( ( 1 , 5 ) , (255 , 255 , 255)) i f par i ty2 == True : marker . s e t a t ( ( 5 , 5 ) , (255 , 255 , 255)) i f par i ty3 == True : marker . s e t a t ( ( 3 , 4 ) , (255 , 255 , 255)) i f par i ty4 == True : marker . s e t a t ( ( 1 , 4 ) , (255 , 255 , 255)) i f par i ty5 == True : marker . s e t a t ( ( 5 , 4 ) , (255 , 255 , 255)) i f par i ty6 == True : marker . s e t a t ( ( 3 , 3 ) , (255 , 255 , 255)) i f par i ty7 == True : marker . s e t a t ( ( 1 , 3 ) , (255 , 255 , 255)) i f par i ty8 == True : marker . s e t a t ( ( 5 , 3 ) , (255 , 255 , 255)) i f par i ty9 == True : marker . s e t a t ( ( 3 , 2 ) , (255 , 255 , 255)) i f par i ty10 == True : marker . s e t a t ( ( 1 , 2 ) , (255 , 255 , 255)) i f par i ty11 == True : marker . s e t a t ( ( 5 , 2 ) , (255 , 255 , 255)) i f par i ty12 == True : marker . s e t a t ( ( 3 , 1 ) , (255 , 255 , 255)) i f par i ty13 == True : marker . s e t a t ( ( 1 , 1 ) , (255 , 255 , 255)) i f par i ty14 == True : marker . s e t a t ( ( 5 , 1 ) , (255 , 255 , 255)) # Libera l a s u p e r f i c i e . marker . unlock ( ) return marker def addOne( b i t s ) : ””” Suma 1 a un numero entero p o s i t i v o c od i f i c a do como una l i s t a de b i t s . ””” i f b i t s [ 0 ] == False : b i t s [ 0 ] = True else : b i t s [ 0 ] = False 74 for i in range (1 , 1 0 ) : i f b i t s [ i ] == False : b i t s [ i ] = True break else : b i t s [ i ] = Fal se def main ( ) : ””” El metodo p r i n c i p a l . Genera l o s 1024 marcadores numerados d e l 0 a l 1023 ””” code = 0 b i t s = [ False , False , False , False , False , False ,\ False , False , False , Fa l se ] done = False pygame . i n i t ( ) output = pygame . Sur face (OUTPUT SIZE) while not done : try : # Genera e l marcador y l o guarda en d i s co . marker = generateMarker ( b i t s ) pygame . trans form . s c a l e ( marker , OUTPUT SIZE, output ) f i l e n a m e = ” marker ” + s t r ( code ) + ” . png” pygame . image . save ( output , f i l e n a m e ) # Calcu la e l s i g u i e n t e codigo code += 1 addOne( b i t s ) # Termina cuando todos l o s marcadores # han s ido generados . i f code >= 1024 : done = True except KeyboardInterrupt : done = True pygame . qu i t ( ) i f name == ” main ” : # Punto de entrada . main ( ) 75 Anexo C Cuestionario utilizado en la prueba de usuario 1. ¿De los dos dispositivos utilizados cuál le parece que ofrece la mejor experiencia en general? a) La tablet b) La consola de videojuegos 2. De entre los dos esquemas de control ¿cuál es su preferido? a) Pantalla táctil b) Gamepad 3. ¿Cómo considera el tiempo de respuesta de la pantalla táctil? a) Excelente b) Bueno c) Decente d) Deficiente e) Malo 4. ¿Cómo considera el tiempo de respuesta del gamepad? a) Excelente b) Bueno c) Decente d) Deficiente e) Malo 76 5. ¿Cómo calificaŕıa usted la dificultad de controlar al robot de forma remota? a) Muy dif́ıcil b) Dif́ıcil c) Normal d) Fácil e) Muy fácil 6. ¿Cómo calificaŕıa la distribución de los elementos de interfaz en la tablet? Con 5 indi- cando una muy buena distribución y 1 indicando una muy mala distribución. a) 5 b) 4 c) 3 d) 2 e) 1 7. Con respecto al juego, ¿que tan útil le pareció la pantalla de ayuda? a) Muy útil b) Suficientemente útil c) Parcialmente útil d) Parcialmente inútil e) Completamente inútil 8. Con respecto al juego, ¿Cómo calificaŕıa la dificultad de control del “brazo” virtual? a) Muy dif́ıcil b) Dif́ıcil c) Normal d) Fácil e) Muy fácil 77 Anexo D Datos y gráficos adicionales de las pruebas realizadas En este anexo se incluyen tablas con los datos estad́ısticos recabados durante la realización de las pruebas descritas en el Caṕıtulo 5. Adicionalmente se muestran gráficos generados a partir de estos datos que sirven para complementar lo descrito en dicho Caṕıtulo. Figura D.1: Distribución del tiempo promedio de procesamiento por cuadro. 78 Valor estad́ıstico Galaxy Tab 2 OUYA Mı́nimo 145 98 Percentil 25 178.25 102 Mediana 183 104 Percentil 75 189.75 108 Máximo 221 112 Desviación estándar 17.03 4.05 Tabla D.1: Datos estad́ısticos del tiempo promedio de procesamiento por cuadro. Figura D.2: Distribución de las tasas de recepción de video. Valor 352x288 352x288 352x288 720x576 720x576 720x576 estad́ıstico recibidos perdidos correctos recibidos perdidos correctos Mı́nimo 6.04 0.03 1.67 2.09 0.04 0.17 Percentil 25 12.21 0.15 11.98 4.56 0.09 4.35 Mediana 12.75 0.19 12.57 5.01 0.13 4.81 Percentil 75 13.19 0.36 12.98 5.35 0.26 5.29 Máximo 13.82 4.37 13.77 5.9 2.1 5.87 Desviación 2.14 1.26 3.37 1.02 0.76 1.94 estándar Tabla D.2: Datos estad́ısticos de la recepción de video. 79 Figura D.3: Distribución del comportamiento de la recepción de video. Valor cuadros cuadros cuadros estad́ıstico recibidos perdidos correctos Mı́nimo 4.87 0.69 0.83 Percentil 25 9.64 1.10 7.18 Mediana 10.43 1.62 8.69 Percentil 75 11.9 2.6 9.79 Máximo 11.51 4.28 10.82 Desviación 2.004 1.28 3.2 estándar Tabla D.3: Datos estad́ısticos del comportamiento de la recepción de video. 80 Referencias [1] Abelson, Harold y Andrea A Di Sessa: Turtle geometry: The computer as a medium for exploring mathematics. the MIT Press, 1986. [2] Adam, Eugene C: Fighter cockpits of the future. En Digital Avionics Systems Conferen- ce, 1993. 12th DASC., AIAA/IEEE, páginas 318–323. IEEE, 1993. [3] Alatalo, Toni: An entity-component model for extensible virtual worlds. Internet Com- puting, IEEE, 15(5):30–37, 2011. [4] Alt, Christopher von: Autonomous underwater vehicles. En Autonomous Underwater Lagrangian Platforms and Sensors Workshop, volumen 3, 2003. [5] Apteker, Ronnie T, James A Fisher, Valentin S Kisimov y Hanoch Neishlos: Video acceptability and frame rate. IEEE multimedia, 2(3):32–40, 1995. [6] Azuma, Ronald, Yohan Baillot, Reinhold Behringer, Steven Feiner, Simon Julier y Blair MacIntyre: Recent advances in augmented reality. Computer Graphics and Applications, IEEE, 21(6):34–47, 2001. [7] Azuma, Ronald T y cols.: A survey of augmented reality. Presence-Teleoperators and Virtual Environments, 6(4):355–385, 1997. [8] Baggio, Daniel Lélis: Mastering OpenCV with practical computer vision projects. Packt Publishing Ltd, 2012. [9] Bagnall, Brian: Core Lego Mindstorms Programming. Prentice Hall, 2002. [10] Bajura, Michael, Henry Fuchs y Ryutarou Ohbuchi: Merging virtual objects with the real world: Seeing ultrasound imagery within the patient. En ACM SIGGRAPH Computer Graphics, volumen 26, páginas 203–210. ACM, 1992. [11] Behrens, Alexander, Linus Atorf, Robert Schwann, Bernd Neumann, Rainer Schnitz- ler, Johannes Balle, Thomas Herold, Aulis Telle, Tobias G Noll, Kay Hameyer y cols.: MATLAB meets LEGO Mindstorms—A freshman introduction course into practical en- gineering. Education, IEEE Transactions on, 53(2):306–317, 2010. 81 [12] Bilas, Scott: A data-driven game object system. En Game Developers Conference Pro- ceedings, 2002. [13] Botta, Alessio, Alberto Dainotti y Antonio Pescapè: A tool for the generation of realistic network workload for emerging networking scenarios. Computer Networks, 56(15):3531– 3547, 2012. [14] Broll, Wolfgang, Irma Lindt, Jan Ohlenburg, Michael Wittkämper, Chunrong Yuan, Thomas Novotny, C Mottram, A Fatah gen Schieck y A Strothman: Arthur: A collabo- rative augmented environment for architectural design and urban planning. Journal of Virtual Reality and Broadcasting, 1(1), 2004. [15] Brooks Jr, Frederick P: The computer scientist as toolsmith II. Communications of the ACM, 39(3):61–68, 1996. [16] Caudell, Thomas P y David W Mizell: Augmented reality: An application of heads- up display technology to manual manufacturing processes. En System Sciences, 1992. Proceedings of the Twenty-Fifth Hawaii International Conference on, volumen 2, páginas 659–669. IEEE, 1992. [17] Chen, David T, Chris Tector, Andrew Brandt, Hong Chen, Ryutarou Ohbuchi, Mi- ke Bajura, Henry Fuchs y cols.: Observing a volume rendered fetus within a pregnant patient. En Visualization, 1994., Visualization’94, Proceedings., IEEE Conference on, páginas 364–368. IEEE, 1994. [18] Chong, JWS, SK Ong, AYC Nee y K Youcef-Youmi: Robot programming using aug- mented reality: An interactive method for planning collision-free paths. Robotics and Computer-Integrated Manufacturing, 25(3):689–701, 2009. [19] Davis, Alan M: Operational prototyping: A new development approach. Software, IEEE, 9(5):70–78, 1992. [20] Diftler, MA, JS Mehling, ME Abdallah, NA Radford, LB Bridgwater, AM Sanders, RS Askew, DM Linn, JD Yamokoski, FA Permenter y cols.: Robonaut 2-the first huma- noid robot in space. En Robotics and Automation (ICRA), 2011 IEEE International Conference on, páginas 2178–2183. IEEE, 2011. [21] Drascic, David: Stereoscopic vision and Augmented Reality. Scientific Computing & Automation, 9(7):31–34, 1993. [22] Drewes, T, E Mynatt, Maribeth Gandy y cols.: Sleuth: An audio experience. En Pro- ceedings of The International Conference on Auditory Display, 2000. [23] Driessen, Vincent: A successful Git branching model. web, Julio 2014. http://nvie. com/posts/a-successful-git-branching-model/. 82 http://nvie.com/posts/a-successful-git-branching-model/ http://nvie.com/posts/a-successful-git-branching-model/ [24] Fearing, Ronald S., Ken H. Chiang, Michael H. Dickinson, DL Pick, Metin Sitti y Jo- seph Yan: Wing transmission for a micromechanical flying insect. En Robotics and Au- tomation, 2000. Proceedings. ICRA’00. IEEE International Conference on, volumen 2, páginas 1509–1516. IEEE, 2000. [25] Feiner, Steven, Blair Macintyre y Dorée Seligmann: Knowledge-based augmented reality. Communications of the ACM, 36(7):53–62, 1993. [26] Fuchs, Henry, Mark A Livingston, Ramesh Raskar, Kurtis Keller, Jessica R Crawford, Paul Rademacher, Samuel H Drake, Anthony A Meyer y cols.: Augmented reality visua- lization for laparoscopic surgery. En Medical Image Computing and Computer-Assisted Interventation—MICCAI’98, páginas 934–943. Springer, 1998. [27] Gamma, Erich, Richard Helm, Ralph Johnson y John Vlissides: Design patterns: ele- ments of reusable object-oriented software. Pearson Education, 1994. [28] Gandhi, Heer, Michael Collins, Michael Chuang y Priya Narasimhan: Real-time tracking of game assets in american football for automated camera selection and motion capture. Procedia Engineering, 2(2):2667–2673, 2010. [29] Gao, Qin, Zhelong Wang, Hong Shang, Weijian Hu y Ming Jiang: Mechanism Design and Locomotion of a Snake Robot. En Intelligent Autonomous Systems 12, páginas 731–738. Springer, 2013. [30] Gibbs, Simon, Michael Hoch y cols.: System and method for data assisted chroma-keying, 2011. US Patent 8,022,965. [31] Gouaillier, David, Vincent Hugel, Pierre Blazevic, Chris Kilner, Jérôme Monceaux, Pas- cal Lafourcade, Brice Marnier, Julien Serre y Bruno Maisonnier: Mechatronic design of NAO humanoid. En Robotics and Automation, 2009. ICRA’09. IEEE International Conference on, páginas 769–774. IEEE, 2009. [32] Green, Scott A, J Geoffrey Chase, XiaoQi Chen y Mark Billinghurst: Evaluating the augmented reality human-robot collaboration system. International journal of intelligent systems technologies and applications, 8(1):130–143, 2010. [33] Group, The LEGO: LEGO MINDSTORMS NXT Bluetooth Developer Kit. Retrieved December, 1:1–10, 2006. [34] Group, The LEGO: LEGO MINDSTORMS NXT Hardware Developer Kit. Retrieved December, 1:1–25, 2006. [35] Group, The LEGO: LEGO MINDSTORMS NXT Software Developer Kit. Retrieved December, 1:1–84, 2006. [36] Group, The LEGO: Lego Mindstorms NXT: Tribot, 2010. 83 [37] Group, The LEGO: LEGO MINDSTORMS EV3 Frequently Asked Questions. web, Enero 2013. http://mindstorms.lego.com/en-us/News/ReadMore/Default.aspx? id=476781. [38] Hosoi, Kazuhiro, Vinh Ninh Dao, Akihiro Mori y Masanori Sugimoto: CoGAME: mani- pulation using a handheld projector. En ACM SIGGRAPH 2007 emerging technologies, página 2. ACM, 2007. [39] Hosoi, Kazuhiro, Vinh Ninh Dao, Akihiro Mori y Masanori Sugimoto: VisiCon: a robot control interface for visualizing manipulation using a handheld projector. En Proceedings of the international conference on Advances in computer entertainment technology, pági- nas 99–106. ACM, 2007. [40] Janin, Adam L, David W Mizell y Thomas P Caudell: Calibration of head-mounted displays for augmented reality applications. En Virtual Reality Annual International Symposium, 1993., 1993 IEEE, páginas 246–255. IEEE, 1993. [41] Kabus, Patric y Alejandro P Buchmann: A Framework for Network-Agnostic Multipla- yer Games. En GAMEON, páginas 18–26, 2007. [42] Kelly, Alonzo, Nicholas Chan, Herman Herman, Daniel Huber, Robert Meyers, Pete Rander, Randy Warner, Jason Ziglar y Erin Capstick: Real-time photorealistic virtua- lized reality interface for remote mobile robot control. The International Journal of Robotics Research, 30(3):384–404, 2011. [43] Kitano, Hiroaki, Minoru Asada, Yasuo Kuniyoshi, Itsuki Noda y Eiichi Osawa: Robocup: The robot world cup initiative. En Proceedings of the first international conference on Autonomous agents, páginas 340–347. ACM, 1997. [44] Kojima, Minoru, Maki Sugimoto, Akihiro Nakamura, Masahiro Tomita, Hideaki Nii y Masahiko Inami: Augmented coliseum: An augmented game environment with small vehicles. En Horizontal Interactive Human-Computer Systems, 2006. TableTop 2006. First IEEE International Workshop on, páginas 6–pp. IEEE, 2006. [45] Kotranza, Aaron y Benjamin Lok: Virtual human+ tangible interface= mixed reality human an initial exploration with a virtual breast exam patient. En Virtual Reality Conference, 2008. VR’08. IEEE, páginas 99–106. IEEE, 2008. [46] Leitner, Jakob, Michael Haller, Kyungdahm Yun, Woontack Woo, Maki Sugimoto y Masahiko Inami: IncreTable, a mixed reality tabletop game experience. En Proceedings of the 2008 International Conference on Advances in Computer Entertainment Technology, páginas 9–16. ACM, 2008. [47] Lyons, Kent, Maribeth Gandy y Thad Starner: Guided by voices: An audio augmented reality system. En International Conference on Auditory Display. Citeseer, 2000. 84 http://mindstorms.lego.com/en-us/News/ReadMore/Default.aspx?id=476781 http://mindstorms.lego.com/en-us/News/ReadMore/Default.aspx?id=476781 [48] Menegatti, Emanuele y Michele Moro: Educational Robotics from high-school to Mas- ter of Science. En Workshop Proceedings of Intl. Conf. on Simulation, Modeling and Programming for Autonomous Robots (SIMPAR 2010), páginas 639–648, 2010. [49] Milgram, Paul y Fumio Kishino: A taxonomy of mixed reality visual displays. IEICE TRANSACTIONS on Information and Systems, 77(12):1321–1329, 1994. [50] Milgram, Paul, Haruo Takemura, Akira Utsumi y Fumio Kishino: Augmented reality: A class of displays on the reality-virtuality continuum. En Photonics for Industrial Applications, páginas 282–292. International Society for Optics and Photonics, 1995. [51] Milgram, Paul, Shumin Zhai, David Drascic y J Grodski: Applications of augmen- ted reality for human-robot communication. En Intelligent Robots and Systems’ 93, IROS’93. Proceedings of the 1993 IEEE/RSJ International Conference on, volumen 3, páginas 1467–1472. IEEE, 1993. [52] Milone, Eugene F y Willam JF Wilson: Satellite and Ring Systems. En Solar System Astrophysics, páginas 151–211. Springer, 2008. [53] Moon, Seungbin y Gurvinder S Virk: Survey on ISO standards for industrial and service robots. En ICCAS-SICE, 2009, páginas 1878–1881. IEEE, 2009. [54] Mottok, Jürgen y Armin Gardeia: The Regensburg Concept of P-Seminars—How to organize the interface between secondary school and university education to create a didactic cooperation between teaching and learning of Software Engineering with Lego Mindstorms NXT Embedded Robot Systems. En Global Engineering Education Confe- rence (EDUCON), 2011 IEEE, páginas 917–920. IEEE, 2011. [55] Nielsen, Jakob y Thomas K Landauer: A mathematical model of the finding of usability problems. En Proceedings of the INTERACT’93 and CHI’93 conference on Human factors in computing systems, páginas 206–213. ACM, 1993. [56] Otsu, Nobuyuki: A threshold selection method from gray-level histograms. Automatica, 11(285-296):23–27, 1975. [57] Papert, Seymour: Teaching Children Thinking∗. Programmed Learning and Educational Technology, 9(5):245–255, 1972. [58] Papert, Seymour: Mindstorms: Children, computers, and powerful ideas. Basic Books, Inc., 1980. [59] Piekarski, Wayne y Bruce Thomas: ARQuake: the outdoor augmented reality gaming system. Communications of the ACM, 45(1):36–38, 2002. [60] Piekarski, Wayne y Bruce H Thomas: Tinmith-metro: New outdoor techniques for crea- ting city models with an augmented reality wearable computer. En Wearable Computers, 2001. Proceedings. Fifth International Symposium on, páginas 31–38. IEEE, 2001. 85 [61] Piekarski, Wayne, Bruce H Thomas y cols.: ARQuake-Modifications and hardware for outdoor augmented reality gaming. En 4th Australian Linux Conference, Perth, Austra- lia, 2003. [62] Ramos, Luis: UNMANNED AERIAL VEHICLE. Tesis de Doctorado, Florida Interna- tional University, 2013. [63] Resnick, Mitchel, Fred Martin, Robert Berg, Rick Borovoy, Vanessa Colella, Kwin Kra- mer y Brian Silverman: Digital manipulatives: new toys to think with. En Proceedings of the SIGCHI conference on Human factors in computing systems, páginas 281–287. ACM Press/Addison-Wesley Publishing Co., 1998. [64] Resnick, Mitchel, Fred Martin, Randy Sargent y Brian Silverman: Programmable bricks: Toys to think with. IBM Systems journal, 35(3.4):443–452, 1996. [65] Rolland, Jannick P, Frank Biocca, Felix Hamza-Lup, Yanggang Ha y Ricardo Martins: Development of head-mounted projection displays for distributed, collaborative, augmen- ted reality applications. Presence: Teleoperators & Virtual Environments, 14(5):528–549, 2005. [66] Rozier, Joseph Michael: Hear&There: An augmented reality system of linked audio. Tesis de Doctorado, Massachusetts Institute of Technology, 2000. [67] Russell, Stuart J y Peter Norvig: Inteligencia Artificial: un enfoque moderno. Pearson Education, 2004. [68] Saha, Subir Kumar: Introducción a la robótica. Mc Graw-Hill, 2010. [69] Sakagami, Yoshiaki, Ryujin Watanabe, Chiaki Aoyama, Shinichi Matsunaga, Nobuo Hi- gaki y Kikuo Fujimura: The intelligent ASIMO: System overview and integration. En Intelligent Robots and Systems, 2002. IEEE/RSJ International Conference on, volu- men 3, páginas 2478–2483. IEEE, 2002. [70] Sims, Dave: New realities in aircraft design and manufacture. Computer Graphics and Applications, IEEE, 14(2):91, 1994. [71] Taddei, Mario y Massimiliano Lisa: Atlas Ilustrado de los Robots de Leonardo. Susaeta, 2010. [72] Tercero, Jesús Salido: Cibernética Aplicada: Robots educativos. Ra-Ma, 2009. [73] Thomas, Bruce, Ben Close, John Donoghue, John Squires, Phillip De Bondi y Wayne Piekarski: First person indoor/outdoor augmented reality application: ARQuake. Perso- nal and Ubiquitous Computing, 6(1):75–86, 2002. 86 [74] Thomas, Bruce, Ben Close, John Donoghue, John Squires, Phillip De Bondi, Michael Morris y Wayne Piekarski: ARQuake: An outdoor/indoor augmented reality first person application. En Wearable Computers, The Fourth International Symposium on, páginas 139–146. IEEE, 2000. [75] Thomas, Bruce, Nicholas Krul, Benjamin Close y Wayne Piekarski: Usability and pla- yability issues for arquake. 2002. [76] Tovar, Carlos E y Paolo G Tosiani: Diseño, Construcción y Programación de Robots Móviles para la Captura y Transmisión de Eventos F́ısicos v́ıa Bluetooth. Tesis de Li- cenciatura, Universidad Central de Venezuela, Facultad de Ciencias, Escuela de Compu- tación, 2007. [77] Van Krevelen, DWF y R Poelman: A survey of augmented reality technologies, applica- tions and limitations. International Journal of Virtual Reality, 9(2):1, 2010. [78] Vincent, Thomas, Laurence Nigay, Takeshi Kurata y cols.: Classifying handheld Aug- mented Reality: Three categories linked by spatial mappings. En Workshop on Classifying the AR Presentation Space at ISMAR 2012, 2012. [79] Walter, W Grey: An imitation of life. Scientific American, 182(5):42–45, 1950. [80] Wanstall, Brian: HUD on the Head for Combat Pilots. Interavia, 44:334–338, 1989. [81] West, Mick: Evolve Your Hierarchy: Refactoring Game Entities with Components. Teok- sessa Game Developer Magazine, osa, 13, 2006. [82] Wilde, Emilia LC van Egmond-de, Masi Mohammadi y cols.: Innovations in domo- tics: fulfilling potential or hampered by prevailing technological regime? Construction Innovation: Information, Process, Management, 11(4):470–492, 2011. 87 Índice de figuras Índice de tablas Introducción Planteamiento del problema Objetivo general Objetivos específicos Justificación Distribución del documento Marco Teórico Realidad Aumentada Realidad Mixta y el contexto de la realidad aumentada El continuo Realidad-Virtualidad Aplicaciones de la realidad aumentada Robótica Definición de robótica Tipos de robots Mecanismos de movimiento para robots móviles terrestres Aplicaciones de la robótica La realidad aumentada y el control de robots móviles Método de investigación y herramientas utilizadas Metodología Herramientas Herramientas de hardware Herramientas de software Descripción de la solución implementada Diseño arquitectónico de la solución Diseño de casos de uso Control del robot Escenarios jugables y despliegue de objetos virtuales. Descripción de la implementación de referencia Diseño de clases Detalles del desarrollo de los módulos del robot y la cámara Patrones de diseño utilizados en la aplicación de control Detección de marcadores Bomb Game, el escenario de demostración Pruebas y resultados Medición de distancias óptimas para reconocimiento de marcadores Perfilado del procesamiento en el estado de juego Resultados y análisis Tasas de recepción de video en el dispositivo de control Resultados y análisis Determinación del punto de saturación del video Resultados y análisis Pruebas de usuario Resultados y análisis Conclusiones Contribuciones Limitaciones Trabajos futuros Anexos El robot LEGO Mindstorms™ El MIT Programmable Brick Evolución del LEGO Mindstorms LEGO Mindstorms RCX LEGO Mindstorms NXT LEGO Mindstorms EV3 Listado del código de generación de marcadores Cuestionario utilizado en la prueba de usuario Datos y gráficos adicionales de las pruebas realizadas ReferenciasUniversidad Central de Venezuela Facultad de Ciencias Escuela de Computación Laboratorio de Redes Móviles e Inalámbricas (ICARO) Centro de Computación Gráfica (CCG) Diseño e implementación de una arquitectura de software genérica para el control de robots móviles basada en realidad aumentada Trabajo especial de grado presentado ante la Ilustre Universidad Central de Venezuela Por el Bachiller Miguel Angel Astor Romero Tutores: Prof. David Pérez Abreu y Prof. Walter Hernández Caracas, septiembre de 2014 ii Universidad Central de Venezuela Facultad de Ciencias Escuela de Computación Laboratorio de Redes Móviles e Inalámbricas (ICARO) Centro de Computación Gráfica Acta del veredicto Quienes suscriben, Miembros del Jurado designado por el Consejo de la Escuela de Compu- tación para examinar el Trabajo Especial de Grado, presentado por el Bachiller Miguel Ángel Astor Romero C.I.: 18810993, con el t́ıtulo “Diseño e implementación de una arquitectura de software genérica para el control de robots móviles basada en realidad aumentada”, a los fines de cumplir con el requisito legal para optar al t́ıtulo de Licenciado en Computación, dejan constancia de lo siguiente: Léıdo el trabajo por cada uno de los Miembros del Jurado, se fijó el d́ıa 19 de septiembre de 2014, a las 11:00 am, para que su autor lo defendiera en forma pública en el Centro de Computación Gráfica, lo cual este realizó mediante una exposición oral de su contenido, y luego respondió satisfactoriamente a las preguntas que les fueron formuladas por el Jurado, todo ello conforme a lo dispuesto en la Ley de Universidades y demás normativas vigentes de la Universidad Central de Venezuela. Finalizada la defensa pública del Trabajo Especial de Grado, el jurado decidió aprobarlo. En fe de lo cual se levanta la presente acta, en Caracas el 19 de septiembre de 2014, dejándose también constancia de que actuó como Coordinador del Jurado el Profesor Tutor Walter Hernandez. Prof. Walter Hernández (Tutor) Profa. Zenaida Castillo Prof. David Perez Abreu Prof. Hector Navarro (Jurado principal) (Tutor) (Jurado principal) iv Agradecimientos Gracias a la profesora Karima Velazquez por su inestimable ayuda para determinar el código de bloques utilizado durante el desarrollo de este proyecto. Gracias a Teresa Tavernelli, Francisco Sans, Fernando Crema, Audel Dugarte y Deyban Perez por su ayuda en la realización de las pruebas de usuario. Dedicado a Elba Salazar, a mis padres y a mi hermano. v Resumen T́ıtulo: Diseño e implementación de una arquitectura de software genérica para el control de robots móviles basada en realidad aumentada Autor: Miguel Ángel Astor Romero Tutores: Prof. David Pérez Abreu, Prof. Walter Hernández Hoy en d́ıa la proliferación de los dispositivos móviles de alta potencia para consumidores ha tráıdo con sigo la posibilidad de difundir tecnoloǵıas como la realidad aumentada, algo que en tiempos anteriores era sumamente engorroso e incluso inviable. Aśı mismo, el campo de la robótica se ha beneficiado de la aceptación por parte del público general de los robots para entornos caseros y usuarios aficionados, como los distribuidos por la empresa LEGO. Con base en la amplia disponibilidad de estos dispositivos se hace factible el desarrollo de arquitecturas complejas que combinen ambas tecnoloǵıas manteniendo un bajo costo económico. Por lo expuesto anteriormente, el presente trabajo de investigación plantea el diseño y desarrollo de una arquitectura de hardware y software para el control de robots móviles, la cual hace uso de las posibilidades provistas por la realidad aumentada para crear experiencias de control enriquecidas por interacciones virtuales. Adicionalmente se presenta una implementación de esta arquitectura para el sistema operativo Android y las consolas de videojuegos OUYA, utilizando robots LEGO Mindstorms NXT TM . Palabras clave: Robótica, realidad aumentada, robots móviles, LEGO Mindstorms, Android, OUYA vi Índice general Índice de figuras IX Índice de tablas XI 1. Introducción 1 1.1. Planteamiento del problema . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 1.2. Objetivo general . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 1.3. Objetivos espećıficos . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 1.4. Justificación . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 1.5. Distribución del documento . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 2. Marco Teórico 4 2.1. Realidad Aumentada . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 2.1.1. Realidad Mixta y el contexto de la realidad aumentada . . . . . . . . 4 2.1.2. El continuo Realidad-Virtualidad . . . . . . . . . . . . . . . . . . . . 5 2.1.3. Aplicaciones de la realidad aumentada . . . . . . . . . . . . . . . . . 7 2.2. Robótica . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 2.2.1. Definición de robótica . . . . . . . . . . . . . . . . . . . . . . . . . . 12 2.2.2. Tipos de robots . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 2.2.3. Mecanismos de movimiento para robots móviles terrestres . . . . . . 16 2.2.4. Aplicaciones de la robótica . . . . . . . . . . . . . . . . . . . . . . . . 18 2.3. La realidad aumentada y el control de robots móviles . . . . . . . . . . . . . 21 3. Método de investigación y herramientas utilizadas 24 3.1. Metodoloǵıa . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 3.2. Herramientas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 3.2.1. Herramientas de hardware . . . . . . . . . . . . . . . . . . . . . . . . 26 3.2.2. Herramientas de software . . . . . . . . . . . . . . . . . . . . . . . . . 27 4. Descripción de la solución implementada 31 4.1. Diseño arquitectónico de la solución . . . . . . . . . . . . . . . . . . . . . . . 31 4.1.1. Diseño de casos de uso . . . . . . . . . . . . . . . . . . . . . . . . . . 32 4.1.2. Control del robot . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33 vii 4.1.3. Escenarios jugables y despliegue de objetos virtuales. . . . . . . . . . 34 4.2. Descripción de la implementación de referencia . . . . . . . . . . . . . . . . . 35 4.2.1. Diseño de clases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35 4.2.2. Detalles del desarrollo de los módulos del robot y la cámara . . . . . 37 4.2.3. Patrones de diseño utilizados en la aplicación de control . . . . . . . . 40 4.2.4. Detección de marcadores . . . . . . . . . . . . . . . . . . . . . . . . . 45 4.2.5. Bomb Game, el escenario de demostración . . . . . . . . . . . . . . . 49 5. Pruebas y resultados 51 5.1. Medición de distancias óptimas para reconocimiento de marcadores . . . . . 51 5.2. Perfilado del procesamiento en el estado de juego . . . . . . . . . . . . . . . 52 5.2.1. Resultados y análisis . . . . . . . . . . . . . . . . . . . . . . . . . . . 52 5.3. Tasas de recepción de video en el dispositivo de control . . . . . . . . . . . . 53 5.3.1. Resultados y análisis . . . . . . . . . . . . . . . . . . . . . . . . . . . 53 5.4. Determinación del punto de saturación del video . . . . . . . . . . . . . . . . 55 5.4.1. Resultados y análisis . . . . . . . . . . . . . . . . . . . . . . . . . . . 55 5.5. Pruebas de usuario . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56 5.5.1. Resultados y análisis . . . . . . . . . . . . . . . . . . . . . . . . . . . 57 6. Conclusiones 62 6.1. Contribuciones . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63 6.2. Limitaciones . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63 6.3. Trabajos futuros . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63 Anexos 65 A. El robot LEGO Mindstorms TM 66 A.1. El MIT Programmable Brick . . . . . . . . . . . . . . . . . . . . . . . . . . . 66 A.2. Evolución del LEGO Mindstorms . . . . . . . . . . . . . . . . . . . . . . . . 67 A.2.1. LEGO Mindstorms RCX . . . . . . . . . . . . . . . . . . . . . . . . . 67 A.2.2. LEGO Mindstorms NXT . . . . . . . . . . . . . . . . . . . . . . . . . 68 A.2.3. LEGO Mindstorms EV3 . . . . . . . . . . . . . . . . . . . . . . . . . 70 B. Listado del código de generación de marcadores 72 C. Cuestionario utilizado en la prueba de usuario 76 D. Datos y gráficos adicionales de las pruebas realizadas 78 Referencias 81 viii Índice de figuras 2.1. El continuo Realidad-Virtualidad. . . . . . . . . . . . . . . . . . . . . . . . . 6 2.2. Despliegue de ultrasonido sobre una paciente. . . . . . . . . . . . . . . . . . 8 2.3. AR Games en el Nintendo 3DS TM . . . . . . . . . . . . . . . . . . . . . . . . . 10 2.4. ARQuake en ejecución visto a través de un HMD. . . . . . . . . . . . . . . . 11 2.5. Visualización en el sistema ARTHUR. . . . . . . . . . . . . . . . . . . . . . . 12 2.6. Diagrama de un manipulador industrial t́ıpico. . . . . . . . . . . . . . . . . . 14 2.7. Un robot móvil para exploración espacial. . . . . . . . . . . . . . . . . . . . 15 2.8. Los robots ASIMO y P3 de Honda. . . . . . . . . . . . . . . . . . . . . . . . 16 2.9. Fotograf́ıa de Titán tomada por la sonda Huygens. . . . . . . . . . . . . . . 20 2.10. El sistema de control presentado por Milgram. . . . . . . . . . . . . . . . . . 22 2.11. El juego Augmented Colliseum. . . . . . . . . . . . . . . . . . . . . . . . . . 23 3.1. El successfull Git branching model . . . . . . . . . . . . . . . . . . . . . . . . 25 3.2. La consola de videojuegos OUYA. . . . . . . . . . . . . . . . . . . . . . . . . 27 3.3. El Java Universal Tween Engine. . . . . . . . . . . . . . . . . . . . . . . . . 30 4.1. Diseño general de la arquitectura. . . . . . . . . . . . . . . . . . . . . . . . . 32 4.2. Casos de uso de nivel 0. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32 4.3. Casos de uso de nivel 1. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33 4.4. Movimiento del brazo virtual. . . . . . . . . . . . . . . . . . . . . . . . . . . 34 4.5. Icono de la implementación de referencia. . . . . . . . . . . . . . . . . . . . . 35 4.6. Diagrama de clases de NxtAR-core. . . . . . . . . . . . . . . . . . . . . . . . 36 4.7. Enlaces de red en NxtAR. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39 4.8. Los estados principales de NxtAR-core. . . . . . . . . . . . . . . . . . . . . . 41 4.9. Patrón de calibración de la cámara. . . . . . . . . . . . . . . . . . . . . . . . 42 4.10. El brazo virtual implementado. . . . . . . . . . . . . . . . . . . . . . . . . . 43 4.11. Composición de entidades. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44 4.12. Uno de los marcadores utilizados. . . . . . . . . . . . . . . . . . . . . . . . . 46 4.13. Pasos del algoritmo de detección de marcadores. . . . . . . . . . . . . . . . . 47 4.14. Obtención de las transformaciones de un marcador. . . . . . . . . . . . . . . 49 4.15. Las bombas del juego Bomb Game. . . . . . . . . . . . . . . . . . . . . . . . 49 5.1. Tiempo promedio de procesamiento por cuadro. . . . . . . . . . . . . . . . . 53 ix 5.2. Porcentaje de tiempo consumido por distintos procesamientos. . . . . . . . . 54 5.3. Tasas de transmisión de video. . . . . . . . . . . . . . . . . . . . . . . . . . . 54 5.4. Comportamiento de la recepción de video. . . . . . . . . . . . . . . . . . . . 56 5.5. Preferencia por dispositivos de control. . . . . . . . . . . . . . . . . . . . . . 57 5.6. Preferencia por mecanismo de control. . . . . . . . . . . . . . . . . . . . . . 58 5.7. Apreciación del tiempo de respuesta de la pantalla táctil. . . . . . . . . . . . 59 5.8. Apreciación del tiempo de respuesta del gamepad. . . . . . . . . . . . . . . . 59 5.9. Apreciación de la dificultad de controlar al robot. . . . . . . . . . . . . . . . 60 5.10. Apreciación de la interfaz en la tablet. . . . . . . . . . . . . . . . . . . . . . . 60 5.11. Utilidad de la pantalla de ayuda. . . . . . . . . . . . . . . . . . . . . . . . . 61 5.12. Dificultad de controlar el brazo virtual del robot. . . . . . . . . . . . . . . . 61 A.1. El MIT Programmable Brick . . . . . . . . . . . . . . . . . . . . . . . . . . . 67 A.2. El brick del Mindstorms RCX 2.0. . . . . . . . . . . . . . . . . . . . . . . . . 68 A.3. El brick del Mindstorms NXT. . . . . . . . . . . . . . . . . . . . . . . . . . . 69 A.4. El brick del Mindstorms EV3. . . . . . . . . . . . . . . . . . . . . . . . . . . 70 D.1. Distribución del tiempo promedio de procesamiento por cuadro. . . . . . . . 78 D.2. Distribución de las tasas de recepción de video. . . . . . . . . . . . . . . . . 79 D.3. Distribución del comportamiento de la recepción de video. . . . . . . . . . . 80 x Índice de tablas 2.1. Clases de dispositivos de Realidad Mixta. . . . . . . . . . . . . . . . . . . . . 7 3.1. Dispositivos móviles utilizados. . . . . . . . . . . . . . . . . . . . . . . . . . 26 5.1. Distancias de reconocimiento de marcadores. . . . . . . . . . . . . . . . . . . 52 D.1. Datos estad́ısticos del tiempo de procesamiento por cuadro. . . . . . . . . . . 79 D.2. Datos estad́ısticos de la recepción de video. . . . . . . . . . . . . . . . . . . . 79 D.3. Datos estad́ısticos del comportamiento del video. . . . . . . . . . . . . . . . . 80 xi xii Caṕıtulo 1 Introducción Hoy en d́ıa la alta proliferación de dispositivos móviles programables de alto desempeño coloca en las manos de los usuarios y los desarrolladores la posibilidad de investigar y desa- rrollar a plenitud el uso de tecnoloǵıas que dadas su naturaleza se benefician ampliamente de las prestaciones de estos dispositivos. Un ejemplo de esto es la realidad aumentada, la cual a pesar de no ser una tecnoloǵıa reciente ha presentado dificultad en su difusión dado el engorro que representan todos los equipos y dispositivos necesarios para obtener movilidad con buen desempeño, esto antes de la llegada de los smartphones y tablets. Por otro lado, la robótica de bajo costo también ha disfrutado de un gran empuje en tiempos recientes gracias a proyectos enfocados a llevar esta tecnoloǵıa a las masas. Ejemplos de estos son los proyectos Arduino1, Raspberry Pi2 y LEGO Mindstorms. Estos proyectos permiten a los usuarios profesionales y aficionados realizar desarrollos que involucren robots y sistemas de sensores por muy bajo costo y con una gran cantidad de herramientas de apoyo. La disponibilidad de estas tecnoloǵıas mencionadas permite entonces el desarrollo de proyectos de investigación que las combinen de distintas maneras para obtener sistemas complejos. Una categoŕıa de estos sistemas son los sistemas de control, los cuales permiten a un operador humano el manipular a uno o más robots para lograr un objetivo. Y es en esta área donde se ubica el presente trabajo de investigación. 1.1. Planteamiento del problema Actualmente han sido desarrollados múltiples sistemas de control para robots móviles que hacen uso de la realidad aumentada para enriquecer la simulación presentada al usuario. Sin embargo, todos estos trabajos hacen uso ya sea de herramientas ad hoc diseñadas para el problema en cuestión o de dispositivos comerciales para propósitos espećıficos. Esto limita a quienes desean realizar investigaciones en esta área dado que dichas soluciones, si es que pueden ser adquiridas en primer lugar, suelen implicar un desembolso económico muy elevado 1http://www.arduino.cc 2http://www.raspberrypi.org 1 http://www.arduino.cc http://www.raspberrypi.org que fácilmente puede estar fuera del alcance de instituciones de investigación modestas. Esto nos lleva preguntarnos si será posible aprovechar las capacidades provistas por los dispositivos móviles para consumidores, verbigracia los smartphones y las tablets, junto a robots modulares de bajo costo para desarrollar un sistema de control por realidad aumentada con caracteŕısticas sofisticadas. 1.2. Objetivo general El objetivo principal del presente trabajo es diseñar y desarrollar un sistema de realidad aumentada para tablets y smartphones, el cual debe involucrar un robot móvil controlado por teleoperación. 1.3. Objetivos espećıficos Definir y diseñar una arquitectura de hardware y software. Desarrollar un módulo que permita capturar imágenes con la cámara del dispositivo las cuales serán transmitidas v́ıa WiFi a otro dispositivo para su procesamiento. Desarrollar un módulo el cual reciba las imágenes capturadas para incorporarles una escena virtual. Modificar el robot diseñado en trabajos previos [76], de forma que sea posible incorpo- rarle un smartphone que pueda utilizar como sistema de visión. Diseñar e implementar un protocolo de capa de aplicación que permita que los progra- mas desarrollados puedan comunicarse entre śı correctamente. Diseñar e implementar el conjunto de reglas y objetos que componen un videojuego el cual formará parte de la escena aumentada de la segunda aplicación mencionada. Implementar el protocolo necesario para comunicarse directamente con el robot móvil v́ıa Bluetooth. 1.4. Justificación Este trabajo se justifica en las posibilidades de investigación y desarrollo que se abren al poseer una arquitectura que permita controlar robots móviles de forma remota, presentando a los usuarios con un entorno virtual con el cual es posible simular distintos escenarios que en la realidad pueden ser muy costosos o peligrosos. Un ejemplo claro de esto es una simulación de un campo minado, con la cual se pueden entrenar operadores de robots móviles teleoperados especializados en la detección y desactivación de dichos armamentos, esto sin la necesidad de poner en riesgo ni al robot o al operador del mismo. Aśı mismo muchas 2 otras aplicaciones pueden desarrollarse a partir de la solución mostrada en esta propuesta, pasando por campos tan diversos como la exploración, el entretenimiento, el entrenamiento, entre otros. 1.5. Distribución del documento El presente documento describe la investigación y el desarrollo llevados a cabo para cumplir con los objetivos antes descritos. El documento está organizado en los siguientes 6 caṕıtulos: El presente Caṕıtulo 1 contiene el planteamiento del problema y su justificación, presentando además los objetivos propuestos para resolverlo. El Caṕıtulo 2 detalla los ba- samentos teóricos necesarios para comprender el trabajo realizado, incluyendo un resumen de los antecedentes y los trabajos relacionados con esta investigación. En el Caṕıtulo 3 se describen las caracteŕısticas y prestaciones de las distintas herramientas de hardware y soft- ware utilizadas durante el desarrollo del trabajo, presentadas de forma independiente del contexto en el que fueron utilizadas. El Caṕıtulo 4 contiene la descripción detallada del tra- bajo realizado para cumplir los objetivos propuestos, haciendo especial énfasis en los detalles del desarrollo de los módulos propuestos. El Caṕıtulo 5 se muestran los resultados de cinco pruebas realizadas a las aplicaciones desarrolladas. Finalmente, en el Caṕıtulo 6 se presentan las conclusiones del trabajo realizado, describiendo los aportes realizados y las limitaciones encontradas, junto al planteamiento de posibles trabajos futuros. 3 Caṕıtulo 2 Marco Teórico En este caṕıtulo se presenta de manera concisa los fundamentos teóricos necesarios para comprender y utilizar las distintas tecnoloǵıas involucradas en este trabajo de investigación. Primero se describen las bases teóricas de la realidad aumentada aśı como los trabajos más representativos desarrollados en el área. Luego se da una breve introducción al campo de la robótica, haciendo especial énfasis en los distintos tipos de robots y sus sistemas de control y locomoción. Finalmente se presenta un estado del arte acerca de sistemas de control para robots que incorporan componentes de realidad virtual o aumentada para mejorar o extender la experiencia del usuario. 2.1. Realidad Aumentada En esta sección se describen los conceptos básicos que definen a la realidad aumentada como campo de investigación y desarrollo. El caṕıtulo comienza con una introducción al con- cepto de realidad aumentada, ubicándola dentro del Continuo Realidad-Virtualidad definido por Milgram y Kishino en [49], con el objetivo de colocar a la Realidad Aumentada dentro de un marco de referencia, comparándola y contrastándola con la Virtualidad Aumentada y la Realidad Virtual. Posteriormente se describen distintos campos de estudio en los cuales se han realizado investigaciones sobre Realidad Aumentada, realizando un breve resumen de estudios significativos en cada área. 2.1.1. Realidad Mixta y el contexto de la realidad aumentada Azuma define en [7] a la realidad aumentada como una variante de la Realidad Virtual, la cual en lugar de intentar suplantar completamente al mundo real, se dedica a complementar la percepción que el usuario tiene de este, agregando elementos virtuales con la intención de presentar al usuario con información que puede percibir directamente con sus sentidos. Un sistema tiene que poseer las siguientes caracteŕısticas para ser considerado dentro del esquema de la realidad aumentada: Combinar objetos virtuales con el entorno real. 4 Poseer interactividad en tiempo real. Realizar registro (alineación) de objetos en 3D. Para poder cumplir con estas caracteŕısticas, los sistemas de realidad aumentada deben hacer uso de múltiples tecnoloǵıas y dispositivos. Como mı́nimo se necesita disponer de un sistema de despliegue para poder mostrar la escena aumentada sobre la escena real y de un sistema de sensores para captar datos de entrada provenientes del ambiente real y del usuario. La naturaleza de los sensores y del sistema de despliegue depende del tipo de realidad aumentada que se esté utilizando, sea visual, auditiva o háptica (lease táctil). Esta lista de caracteŕısticas permite un amplio abanico de tecnoloǵıas de sensores y despliegue, pasando por los HMD (Head-Mounted Displays - Visualizadores Montados en la Cabeza), los monitores monoscópicos o estereoscópicos, y los VRD (Virtual Retinal Displays - Visualizadores Virtuales en la Retina), entre otros. Quedan fuera de esta definición medios como el cine de efectos especiales (por ejemplo Jurassic Park) debido a la falta de interacción; aśı como los videojuegos en su definición clásica por la falta de registro de objetos reales y virtuales, y los sistemas que realizan superposición de elementos 2D sobre el entorno real visible. En [6] Azuma y colaboradores modifican la lista de caracteŕısticas anterior eliminando el requisito de que la alineación de objetos se realice en 3 dimensiones, reescribiendo este punto como sigue: Alinea objetos reales y virtuales entre si. Esto permite la inclusión de los sistemas que realizan superposición interactiva de elemen- tos 2-D sobre el entorno real dentro del esquema de la realidad aumentada. Adicionalmente esta nueva definición también extiende el ámbito de percepción de la realidad aumentada más allá del sentido de la vista, incluyendo a los sistemas que proveen al usuario con informa- ción táctil utilizando dispositivos hápticos e información auditiva utilizando sonido surround, entre otros [47] [66] [22]. 2.1.2. El continuo Realidad-Virtualidad En [49] y [50] Paul Milgram, Fumio Kishino y colaboradores definen al continuo Realidad- Virtualidad como una agrupación de tecnoloǵıas de despliegue que involucran la represen- tación de una realidad, organizadas según la virtualidad de los elementos que la conforman. Las tecnoloǵıas que caen dentro de este continuo son llamadas tecnoloǵıas de Realidad Mix- ta. En un extremo se tiene a la realidad tal cual la percibimos con nuestros sentidos, sea en persona o de manera indirecta (por ejemplo, por medio de un v́ıdeo). En el extremo opuesto del continuo se tienen los llamados ambientes virtuales que corresponden con la Realidad Virtual, donde todos los objetos que percibe el usuario son completamente sintéticos. Este continuo puede observarse en la Figura 2.1. El objetivo de este continuo es poder clasificar los distintos sistemas de Realidad Mix- ta, atendiendo principalmente a la proporción de objetos reales contra virtuales presentes 5 Figura 2.1: El continuo Realidad-Virtualidad. Figura recuperada de [49]. en el sistema de despliegue. Estos sistemas pueden clasificarse como sistemas de realidad aumentada, donde una escena predominantemente real se complementa con algunos objetos virtuales; sistemas de Virtualidad Aumentada, en los cuales una escena sintética se comple- menta con algunos objetos extráıdos del entorno real del usuario; o sistemas de Realidad Virtual, donde todos los objetos que puede percibir el usuario son completamente sintéticos. Cabe resaltar que Milgram y Kishino no plantean una escala exacta para determinar a partir de que proporción de objetos reales contra virtuales se considera que un sistema es de reali- dad aumentada o de Virtualidad Aumentada, quedando esto a criterio de los diseñadores del sistema en cuestión. De acuerdo con Milgram, Kishino y colaboradores los dispositivos de Realidad Mixta, entendiéndose por estos a los sistemas de realidad aumentada, Virtualidad Aumentada y Realidad Virtual, se pueden categorizar dentro de 7 clases [50] visibles en el Cuadro 2.1. Estas clases se definen en base a cuatro parámetros que determinan la posición de dichas clases en el continuo Realidad-Virtualidad. Los cuatro parámetros son la virtualidad del mundo circundante, principalmente real o principalmente virtual; el esquema de visión, que es directo cuando el usuario puede percibir de manera inmediata al mundo que le rodea con sus sentidos o indirecto en caso contrario; el marco de referencia que se refiere a si el usuario se encuentra dentro del mundo a medida que lo percibe (egocéntrico), o si se encuentra fuera de este (exocéntrico); y la representación conforme, que se refiere a la necesidad de realizar un registro exacto de los objetos aumentados con los objetos que pertenecen intŕınsecamente al mundo [78], sean estos los objetos reales o los objetos virtuales. Cabe resaltar que el cuadro 2.1 se refiere únicamente a los sistemas de Realidad Mixta que se enfocan en el aspecto visual. Como se mencionó, el objetivo principal de la realidad aumentada es el presentar al usua- rio con información complementaria al mundo real que no puede ser percibida directamente con los sentidos. La finalidad es que esta información adicional ayude al usuario a desem- peñar tareas con mayor facilidad dentro de un dominio espećıfico. En este sentido, la realidad aumentada cabe dentro de la llamada Amplificación de la Inteligencia definida por Frederick Brooks en [15], que se refiere al uso de computadoras como herramientas para hacer que determinadas tareas sean más fáciles de realizar. 6 Clase Mundo Esquema Marco de Representación de Sistema de Visión Referencia Conforme Vı́deo basado en monitores con Real Indirecta Exocéntrico No superposición virtual Vı́deo basado en HMD con Real Indirecta Egocéntrico No superposición virtual Óptica basada en HMD con Real Directa Egocéntrico Si superposición virtual Vı́deo basado en HMD con Real Indirecta Egocéntrico Si superposición virtual Mundo virtual por monitor Virtual Indirecta Exocéntrico No con superposición de v́ıdeo. Mundo virtual por HMD con Virtual Indirecta Egocéntrico No superposición de v́ıdeo. Mundo virtual con intervención Virtual Directa o Egocéntrico Si de objetos indirecta reales. Tabla 2.1: Clases de dispositivos de Realidad Mixta. Tabla recuperada de [50]. 2.1.3. Aplicaciones de la realidad aumentada Desde que se despertó el interés por la realidad aumentada durante la década de los 90s como se menciona en [6], estas tecnoloǵıas se han utilizado en una amplia variedad de campos de trabajo e investigación. En [77] Van Krevelen y Poelman, al igual que Azuma y colaboradores en [7] y [6], describen la aplicabilidad de la realidad aumentada en los siguientes campos: medicina, industria, aviación, entretenimiento, videojuegos y entornos de trabajo colaborativos enfocados en ambientes de oficina. Es por esto que a continuación se abordarán las investigaciones y trabajos más relevantes en cada una de estas áreas. 7 Medicina La realidad aumentada ha sido utilizada en aplicaciones médicas en ámbitos que van desde el entrenamiento de personal médico hasta la presentación de datos volumétricos ob- tenidos directamente del paciente sobre el mismo en tiempo real [7] [77]. Estas aplicaciones tienen requisitos técnicos muy altos en lo referente a velocidad y precisión de la información presentada, la cual dependiendo de la aplicación final debe ser obtenida de bases de datos (por ejemplo un sistema que muestra historias médicas sobre el paciente) o directamente del paciente utilizando dispositivos de ultrasonido, rayos X, o tomograf́ıas, como se observa en la Figura 2.2. Figura 2.2: Despliegue de ultrasonido sobre una paciente. Imagen recuperada de [17]. Los dispositivos de realidad aumentada, principalmente basados en See-Through HMD han sido utilizados exitosamente en diferentes áreas de la medicina como lo son la ciruǵıa laparoscópica [26], la obstetricia [17] [10] o el entrenamiento [45]. Industria En la industria la realidad aumentada ha sido utilizada exitosamente en áreas como la manufactura y ensamblado de componentes [16] [40] [70], el entrenamiento de personal técnico, reparación de dispositivos [25] e incluso la planificación de rutas para robots ma- nipuladores industriales [51] [21] (véase la sección 2.2.2). En estas aplicaciones el uso de la realidad aumentada suele estar ligado a dispositivos HMD, sean See-Through o de video, y usualmente toman la forma de gráficos 3-D superpuestos sobre la maquinaria o las piezas que se estén manipulando. Estos gráficos son tomados de planos o esquemas técnicos de la maquinaria en cuestión. 8 Aviación y usos militares La aviación militar ha hecho uso de dispositivos See-Through para proveer a los pilotos con información referente a los parámetros actuales de su aeroplano, aśı como identificación de otros aviones en vuelo o “mirillas” automáticas para los distintos armamentos del avión [80] [2]. Suelen hacer uso de tecnoloǵıas llamadas HUD (Heads Up Displays - Pantalla de Información) similares en concepto a los HMD. Los HUD permiten mostrar la información aumentada sobre una lámina de vidrio, utilizando en este caso gráficos vectoriales [7]. Estos dispositivos toman la forma de un panel de control colocado en la cabina del avión, o de un HMD incorporado al casco del piloto. Entretenimiento La realidad aumentada ha sido aplicada a diversos sistemas de entretenimiento, no li- mitándose exclusivamente al ámbito de los videojuegos. En el ámbito de las transmisiones de eventos deportivos suele hacerse uso de anotaciones virtuales aumentadas, colocadas so- bre las tomas del evento permiten que los espectadores puedan llevar la pista de elementos del juego en cuestión cuando estas son dif́ıciles de percibir, ya sea por su tamaño o por su velocidad. Para esto suelen utilizarse técnicas como el chroma-keying [30], que permite que los elementos aumentados puedan ser solapados por objetos del mundo, grabados en video. Por ejemplo, se ha hecho uso de esta tecnoloǵıa para resaltar marcas del terreno o incluso el balón en partidos de fútbol americano [28]. realidad aumentada en videojuegos El auge de los dispositivos móviles en los años recientes, como por ejemplo las tablets y los smartphones, los cuales suelen venir de fábrica con cámaras de video y hardware gráfico de buen desempeño permiten el desarrollo de juegos que toman las caracteŕısticas de los sistemas de realidad aumentada mencionados anteriormente. Diversos juegos han sido desarrollados para las plataformas Android TM e iOS TM , como por ejemplo Droid Shooting1, en el cual el jugador(a) debe destruir robots voladores basados en el logo de Android TM utilizando una pistola láser virtual. Recientemente Nintendo ha incorporado una serie de juegos de realidad aumentada en su ultima consola portátil, el Nintendo 3DS TM , disponible comercialmente desde febrero de 2011. Estos juegos llamados colectivamente AR Games vaŕıan desde sencillamente colocar diferentes personajes virtuales sobre cartas incluidas con la consola, hasta un juego de dis- paros similar en concepto a Droid Shooting, pasando por juegos de pesca, golf o arqueŕıa. Un ejemplo de los juegos del Nintendo 3DS TM puede verse en la Figura 2.3. ARQuake En el año 2000, Bruce Thomas y Wayne Piekarski presentaron un proyecto de investi- gación y desarrollo que consiste en un sistema de juego de realidad aumentada capaz de 1http://bit.ly/ZHrreh 9 http://bit.ly/ZHrreh Figura 2.3: AR Games en el Nintendo 3DS TM . Imagen tomada por el autor. funcionar tanto en interiores como en exteriores [74]. El sistema funciona utilizando un con- junto de dispositivos que incluyen una laptop, un GPS de alta presición, un giroscópio, una cámara y un HMD [73]. Con estos dispositivos se logra que el usuario pueda desplazarse libremente dentro de un área predefinida, sea dentro o fuera de un edificio, con seis gra- dos de libertad [59]. El juego en si consiste en una modificación del juego Quake TM de Id Software R©. Esta modificación fue desarrollada aprovechando la disponibilidad del código fuente2 del juego bajo la licencia GPL versión 23. El sistema utiliza una representación 3D del área en la que se desarrolla la partida [75] la cual se utiliza para poder ocultar elementos del juego como personajes, paquetes de vida, municiones, etc. Esta representación de la escena se registra sobre la escena real utilizando la cámara del sistema para detectar marcadores colocados sobre los edificios. Luego se despliega la escena en negro sin iluminación, de forma que no sea visible en los lentes del HMD. El GPS de alta presición se utiliza para detectar la ubicación del usuario dentro de la escena, para ayudar al sistema de registro. La laptop se utiliza para llevar la partida, utilizando la información del giroscopio para determinar hacia donde esta mirando el jugador(a)[61]. Este sistema de hardware es llamado Tinmith-metro por sus desarrolladores [60]. En la Figura 2.4 se puede observar una partida de ARQuake en desarrollo. Los personajes y los objetos del juego han sido modificados por los desarrolladores de forma que sean fáciles de ver sobre los lentes del Head- Mounted Display [73], utilizando colores brillantes y texturas diferentes a las que incorpora Quake TM por defecto. 2https://github.com/id-Software/Quake 3http://www.gnu.org/licenses/gpl-2.0.html 10 https://github.com/id-Software/Quake http://www.gnu.org/licenses/gpl-2.0.html Figura 2.4: ARQuake en ejecución visto a través de un HMD. Imagen recuperada de [74]. Ambientes colaborativos El uso de sistemas de realidad aumentada en ambientes colaborativos se ha enfocado principalmente a aplicaciones de planificación o control distribuido. Van Krevelen y Poel- man exponen una serie de usos en [77] que van desde análisis de planos arquitectónicos utilizando maquetas virtuales hasta control distribuido de trafico aéreo. Sin embargo, los ejemplos presentados se limitan a sistemas experimentales para ayudar a arquitectos traba- jando en planificación urbana utilizando las mencionadas maquetas virtuales, principalmente utilizando Head-Mounted Displays. Ejemplo de esto es el sistema ARTHUR de Broll y cola- boradores [14] que realiza incluso simulación de tráfico y peatones sobre un modelo virtual de una ciudad; este sistema puede observarse en la Figura 2.5. Tambien se ha estudiado el uso de superficies de proyección de forma que se pueda reducir el aparataje necesario en sistemas basados en Head-Mounted Displays, como es el caso de [65]. 2.2. Robótica En esta sección se presenta una introducción breve al campo de la robótica comenzando por su definición, haciendo énfasis en los robots como objeto de estudio. Posteriormente se realiza un resumen de los distintos tipos de robots reconocidos y de las distintas partes que componen el cuerpo de un robot. Se prosigue con un análisis de distintas áreas de la ciencia, la ingenieŕıa y la industria en las cuales la robótica tiene aplicación. El caṕıtulo concluye con 11 Figura 2.5: Visualización en el sistema ARTHUR. Imagen recuperada de [14]. una reseña histórica del robot modular LEGO Mindstorms TM . 2.2.1. Definición de robótica La Robótica es un campo de investigación multidisciplinario que se enfoca en el estudio y desarrollo de tecnoloǵıas relacionadas con dispositivos mecánicos destinados a relevar al ser humano de tareas que por su naturaleza son peligrosas, sumamente dif́ıciles o requieren de extrema precision [68]. Formalmente, el Diccionario de La Lengua Española define a la Robótica como la “técnica que aplica la informática al diseño y empleo de aparatos que, en sustitución de personas, realizan operaciones o trabajos, por lo general en instalaciones industriales”. El robot es el principal objeto de estudio de la robótica. Existen muchas definiciones de la palabra robot, dadas por las diferentes asociaciones nacionales e internacionales de inves- tigadores, desarrolladores y fabricantes de robots [72]. Para concentrarnos en una definición, podemos considerar la propuesta por la International Organization for Standardization (en adelante ISO). La ISO define a los robots como manipuladores multifuncionales progra- mables, dotados de la capacidad de interactuar con materiales, piezas o herramientas para desempeñar diversas tareas [53]. Independientemente de que definición de robot sea tomada en cuenta, todas ellas comparten los siguientes puntos en común: Los robots deben ser multifuncionales. Es decir, que cambiando su programa- ción debe ser capaz de realizar distintas tareas enmarcadas dentro de una aplicación espećıfica. 12 Los robots deben ser reprogramables. Derivado del punto anterior; debe ser po- sible modificar el comportamiento de los robots cambiando el programa que los dirige. Los robots deben ser autónomos. En otras palabras, el robot debe ser capaz de realizar su trabajo con la menor intervención humana posible. Los robots tienen muchos antecedentes históricos, que datan incluso de varios cientos de años antes de la era cristiana [68]. Se conocen historias y leyendas provenientes de la antigua Grecia que tratan sobre seres artificiales autónomos cuyas acciones pod́ıan ser controladas por sus dueños. Incluso se conoce que Leonardo da Vinci construyó múltiples dispositivos mecánicos parciales o totalmente autónomos [71]. El término robot como se utiliza en la actualidad fue introducido al vocabulario mundial por la obra de teatro Rossum’s Universal Robots [68], del autor checo Karel Čapek, estrenada en 1921. 2.2.2. Tipos de robots Existen tantos robots como aplicaciones hay para ellos. Sin embargo, para poder estu- diarlos hay que clasificarlos de alguna manera. Los robots pueden clasificarse atendiendo a distintos criterios que van desde su forma f́ısica, hasta su aplicación final pasando por su esquema de locomoción [72]. Para propósitos de este trabajo se detalla a continuación la clasificación propuesta por Russell y Norvig en [67], muy similar a la propuesta por Saha [68]. Russell y Norvig distinguen 3 tipos de robots fundamentales, atendiendo simultánea- mente a su método de locomoción y forma de interacción con el medio ambiente: los robots manipuladores, los robots móviles y los robots h́ıbridos. Robots manipuladores Los manipuladores, también llamados robots estacionarios, son aquellos robots que en to- do momento están anclados a una base fija. Toman la forma de brazos mecánicos articulados [67]. Estos son los robots más difundidos comercialmente a nivel mundial [67], y su principal aplicación es la industria [68], siendo utilizados en lineas de ensamblado y construcción. Su nombre proviene del hecho de que son principalmente utilizados para manipular piezas y materiales, usualmente haciendo uso de “manos” o agarraderas electrónicas. La Figura 2.6 muestra un diagrama que representa a un robot manipulador industrial t́ıpico. Los manipuladores se subdividen en cuatro tipos dependiendo del sistema de coordenadas por el que rigen sus movimientos [68]. Esta clasificación se deriva del tipo de articulaciones que suelen poseer estos robots, las cuales pueden ser extensibles (tambien llamadas prismáti- cas), giratorias o rectiĺıneas. Los cuatro tipos de manipuladores son: Robots cartesianos. Cuando las distintas partes del robot se mueven de forma rec- tiĺınea y sus movimientos pueden definirse completamente dentro de un paraleleṕıpedo sólido, se dice que el robot en cuestión es cartesiano. 13 Figura 2.6: Diagrama de un manipulador industrial t́ıpico. Imagen recuperada de [67]. Robots ciĺındricos. Son aquellos robots cuya articulación principal puede girar sobre un eje fijo, definiendo un cilindro sólido dentro del cual pueden moverse las demás articulaciones del robot. Robots polares. También son llamados robots esféricos. Estos robots poseen dos articulaciones giratorias que culminan en una articulación prismática, lo que les permite moverse dentro de una semiesfera sólida. Robots de revolución. Son similares a los robots polares, en el sentido de que su volumen de acción es esférico. Estos robots poseen tres articulaciones giratorias, lo que les permite un mayor rango de acción. El volumen dentro del cual pueden moverse estos robots es más completo que en el caso de los robots polares. Estos robots forzosamente necesitan calcular sus movimientos utilizando técnicas de cinemática inversa recursiva [68], lo que los hace más d́ıciles de programar. Robots móviles Los robots móviles son aquellos que pueden moverse libremente dentro de su entorno ya sea utilizando ruedas, piernas, cintas transportadoras o incluso volando o nadando[67]. Suelen tomar formas muy variadas dependiendo de su aplicación final, como por ejemplo los robots vehiculares utilizados por la NASA (National Aeronautics and Space Administration - Administración Nacional para la Aeronáutica y el Espacio) para la exploración de Marte, o incluso robots con forma de serpiente [29] o de insectos, utilizados para exploración y búsqueda en espacios de dif́ıcil acceso. En la Figura 2.7 se observa un robot móvil tipo rover para la exploración espacial. Se distinguen tres tipos de robots móviles vehiculares [67]: Veh́ıculos terrestres sin tripulación. Del inglés unmanned land vehicule, son aque- llos robots cuyo medio de locomoción es el suelo. Tienen bastantes aplicaciones comer- 14 Figura 2.7: Un robot móvil para exploración espacial. Imagen recuperada de [67]. ciales como por ejemplo los robots de servicio, utilizados para realizar tareas domésticas [72]. Veh́ıculos aéreos sin tripulación. Son aquellos robots que tienen la capacidad de volar por los aires. Suelen tomar forma de aviones o helicopteros pequeños, o incluso de insectos [24]. Su nombre proviene del inglés unmanned aerial vehicle [62]. Veh́ıculos submarinos autónomos. Son robots para navegación submarina utiliza- dos para la exploración en oceanograf́ıa. Son llamados aśı por los autonomous under- water vehicles [4]. Robots h́ıbridos Se definen sencillamente como un robot móvil equipado con manipuladores [67]. A esta categoŕıa pertenecen los robots humanoides como los robots ASIMO, construido por Honda [69]; Nao de Aldebaran Robotics [31] ; y el par Robonaut-1 y Robonaut-2 de NASA [20]. Este tipo de robot poseen la capacidad de manipular materiales con mayor libertad en un espacio de trabajo mayor que los robots meramente manipuladores; sin embargo, son mucho más complejos en su diseño y programación que los otros tipos de robots [67]. En la Figura 2.8 se puede ver al robot ASIMO de Honda, el cual es un ejemplo de un robot h́ıbrido humanoide. Otros Aparte de los tres tipos de robots mencionados, existen otros robots de uso espećıfico que no encajan dentro de las categoŕıas anteriores [67]. Estos robots son los siguientes: Prótesis robóticas. Son prótesis mecánicas que responden de manera automática a las instrucciones dadas por el paciente. 15 Figura 2.8: Los robots ASIMO y P3 de Honda [67]. Robots modulares. Son aquellos robots que pueden ser desensamblados y reensam- blados en diferentes configuraciones dependiendo de la aplicación a la que estén desti- nados. Los robots LEGO MindstormsTM(vease la sección A) son un ejemplo de robots modulares. Entornos inteligentes. Son edificaciones autónomas que controlan parametros am- bientales e interactúan f́ısicamente con sus habitantes. Son el objeto de estudio de la domótica, que es la rama de la ingenieŕıa que estudia y desarrolla sistemas autónomos computarizados para control doméstico [82]. Enjambres. Del inglés swarms, son los sistemas robóticos multiagentes compuestos por múltiples robots autónomos cooperativos. 2.2.3. Mecanismos de movimiento para robots móviles terrestres Para poder desplazarse los robots móviles terrestres hacen uso de una amplia variedad de métodos de locomoción. En particular se distinguen cuatro tipos de medios de locomoción, que se distinguen por los componentes que realizan el desplazamiento del robot. Los métodos en cuestión son los siguientes: Piernas o Patas. Este método de locomoción se basa en la forma en que se desplazan los organismos vivos en la naturaleza. Mediante el uso de piernas o patas se otorga al robot de una amplia capacidad de movimiento, siendo especialmente eficaces en terrenos abruptos. Para poder realizar traslación y rotación del robot estas patas deben poseer al menos dos grados de libertad [72]. Los robots con patas son susceptibles de dos problemas propios conocidos como inestabilidad estática e inestabilidad dinámica, los cuales se refieren a la necesidad de mantener el equilibrio del robot tanto cuando 16 está en reposo, como cuando está en movimiento respectivamente. Para garantizar el equilibrio del robot es necesario lograr que este mantenga su centro de gravedad dentro del área definida por sus patas [72]. Existen tres tipos de robots con patas: los b́ıpedos, los cuadrúpedos y los hexápodos. Los robots b́ıpedos son aquellos que, al igual que los seres humanos, mantienen a lo sumo dos pies sobre el suelo. Estos robots suelen poseer pies considerablemente grandes para poder lidiar con la inestabilidad, tanto estática como dinámica. Por su parte los robots cuadrúpedos utilizan cuatro patas para su locomoción. Este tipo de robots suelen ser mucho más estables que los robots b́ıpedos siempre y cuando mantengan al menos tres patas sobre el suelo en todo momento [72]. Finalmente los robots hexápodos son aquellos que utilizan seis patas, tres en cada lado del robot. Estos robots son muy estables pero necesitan de un software de control más sofisticado que sea capaz de coordinar las seis patas para poder trasladar o rotar al robot. Orugas o cadenas de tracción. Este método de locomoción utiliza cadenas de trac- ción de manera similar a un tanque de guerra. Se suelen aplicar en robots que necesiten desplazarse en terrenos muy abruptos o deslizantes puesto que las cadenas de tracción proporcionan muy buena tracción sobre el terreno [72]. Ruedas. Son el método de transporte más común [72]. Los robots pueden hacer uso de cuatro tipos de ruedas, dependiendo del sistema de locomoción que utilicen: • Ruedas motrices. Son las ruedas conectadas a un efector que impulsa su movi- miento. Este efector suele ser un servomotor. • Ruedas direccionales. Son ruedas conectadas a un eje de giro y se utilizan para cambiar la dirección de desplazamiento del robot. No están conectadas a efectores. • Ruedas libres. Son ruedas capaces de girar libremente sobre un eje, el cual puede ser fijo u oscilante. En el caso de que el eje sea oscilante se dice que la rueda es una rueda loca (del inglés caster wheel.) • Ruedas omnidireccionales. Estas ruedas poseen rodillos en su borde, lo que les permite realizar desplazamiento en dos direcciones sin tener que girar el cuerpo del robot. Esquemas de movimiento con ruedas Para el caso de los robots con ruedas, estas pueden utilizarse en múltiples configuraciones que afectan la movilidad del robot, lo que a su vez determina la complejidad del sistema de control motriz del mismo. Se distinguen cuatro esquemas de movimiento diferentes: Sistema diferencial. Los robots móviles que hacen uso de sistemas diferenciales para su movimiento son construidos utilizando dos ruedas motrices conectadas a motores independientes, una en cada lado del robot. Adicionalmente se utilizan una o más ruedas locas en la parte posterior del robot para ayudarle a mantener el equilibrio 17 [72]. Para controlar la dirección del robot se vaŕıa la velocidad de giro de cada motor. Si el motor derecho gira a mayor velocidad que el motor izquierdo entonces el robot girará a la derecha, o viceversa. Los robots que presentan este sistema de locomoción suelen necesitar de un código de control bastante sofisticado debido a que para poder desplazarse en linea recta es necesario que ambos motores giren a exactamente la misma velocidad. Esto implica que el robot debe analizar el estado de los motores en todo momento y compensar cuando estos se desfasan. El robot diseñado por Carlos Tovar y Paolo Tosiani en su trabajo especial de grado es un ejemplo de un robot con sistema de locomoción diferencial [76]. El robot Tribot TM de The LEGO Group es otro ejemplo [36]. Sistema de Ackerman. Es el sistema de locomoción utilizado por los automóviles comunes. Se compone de dos pares de ruedas: un par de ruedas motrices conectadas a un único motor y un par de ruedas direccionales conectadas al mismo eje de giro. También es posible utilizar combinaciones tipo triciclo, utilizando dos ruedas motrices y una única rueda direccional. El sistema de control de movimiento en este tipo de robots suele ser más sencillo que en el caso de los robots con sistema diferencial; sin embargo, es necesario que el robot realice maniobras más complejas para poder desplazarse, debido a que el robot no puede girar sobre su propio eje vertical como un robot con sistema diferencial [72]. Sistema synchro drive . Este sistema utiliza tres ruedas las cuales son motrices y direccionales simultáneamente. Esto combina las ventajas de los sistemas diferencial y de Ackerman [72], permitiendo que el robot pueda girar sobre su eje vertical, al mismo tiempo que se simplifica el código de control. Sin embargo, este sistema es más complejo en su aspecto mecánico. Sistema diferencial dual. En este sistema se hace uso de dos pares diferenciales de ruedas motrices. Esto permite que el robot pueda desplazarse en linea recta o girar de forma más sencilla que un sistema diferencial puro. Su desventaja radica en el hecho de que los robots que utilizan este sistema no son capaces de avanzar o retroceder mientras giran, lo que obliga al robot a descomponer trayectorias curvas en segmentos de desplazamientos rectos [72]. 2.2.4. Aplicaciones de la robótica Gracias a su capacidad de reprogramación los robots pueden utilizarse para una gran cantidad de tareas. En particular se distinguen dos tipos de tareas o grupos de aplicación para los robots: industriales y de propósito espećıfico [68]. Los robots de uso industrial suelen ser manipuladores o robots móviles utilizados para diversas areas relacionadas a la construcción, ensamblado o transporte de piezas y materiales. Por otro lado, los robots de propósito espećıfico no siguen una norma común. Estos robots son diseñados y construidos a la medida dependiendo del trabajo final que vayan a realizar. 18 Subir Kumar Saha en su libro Introducción a la Robótica [68] enumera cuatro reglas que tienen como intención ayudar a determinar si un campo de aplicación es apto para ser automatizado con robots: 1. Si el trabajo es sucio, aburrido, peligroso o dif́ıcil. Esto es conocido como las cuatro D de la robótica (del inglés Dirty, Dull, Dangerous or Difficult.) 2. El uso de robots en dicho trabajo no debe dejar a algún ser humano desempleado. 3. Si es imposible conseguir a alguien que tenga la disposición para realizar el trabajo. 4. El uso de robots en dicho trabajo provee beneficios económicos a corto y largo plazo. Robots en la industria En el ámbito industrial se ha hecho uso de robots para realizar trabajos que por su na- turaleza son muy dif́ıciles para un ser humano. Esto comienza entre los años 1958 y 1961 cuando Joseph Engelberger y George Devol fundan la empresa UNIMATION Robotics Com- pany. Esta empresa introdujo el primer manipulador industrial, el cual serv́ıa para realizar la fundición y moldeado de metales para fabricar manijas de puertas. Otras áreas de la industria donde se hace un uso extensivo de robots son la fabricación y ensamblado de piezas para veh́ıculos; ensamblado de dispositivos electrónicos; fabricación de piezas para óptica; manipulación de productos alimenticios en fábricas; etc. Robots en la exploración Los robots han sido utilizados para ayudar a las labores de exploración en distintos ámbitos, dando apoyo cuando se trata de ambientes de dif́ıcil acceso para los seres humanos [67]. En particular, se ha hecho uso extensivo de robots para apoyar la construcción y el mantenimiento de la Estación Espacial Internacional, aśı como para la exploración de Marte y otros planetas. Un ejemplo muy llamativo es la sonda robótica Huygens, la cual aterrizó en Titán, el satélite más grande de Saturno, lugar donde capturó la fotograf́ıa que se observa en la Figura 2.9. Dentro del planeta Tierra, los robots para exploración han sido utilizados para examinar y manipular entornos peligrosos. En particular han sido utilizados para la exploración y registro de minas abandonadas, para el estudio de cráteres de volcanes, o incluso para limpiar residuos radioactivos en las plantas nucleares de Chernobyl y Three Mile Island conocidas por los terribles accidentes que ocurrieron en ellas [67]. Robots en la medicina Los robots han sido utilizados en la medicina en tres grandes areas: el apoyo al diagnóstico y la ciruǵıa; prótesis y sillas de rueda robotizadas; y apoyo a enfermeŕıa [67]. En el caso de la ciruǵıa son utilizados para ayudar a los cirujanos a colocar y manipular instrumentos cuando se realizan ciruǵıas del cerebro, ojos o corazón, las cuales necesitan de una extrema 19 Figura 2.9: Fotograf́ıa de Titán tomada por la sonda Huygens. Imagen recuperada de [52]. presición. Por su parte, las prótesis robóticas son utilizadas para otorgarle a los pacientes un mayor grado de control y precisión sobre sus miembros artificiales. Finalmente, los robots son utilizados en enfermeŕıa para ayudar en el transporte de alimentos, medicinas e insumos médicos. Robots en entornos militares Los robots han sido utilizados para transportar armamento y municiones, aśı como para buscar y desactivar minas en zonas de guerra. Un tipo peculiar de robot militar son los llamados drones, los cuales son una forma de veh́ıculos aéreos sin tripulación. Estos robots son utilizados en zonas de guerra para realizar reconocimiento y exploración de lineas enemigas. Robots en el entretenimiento El uso de la robótica para el entretenimiento se ha centrado en la creación de juguetes autónomos programables, a veces con fines comerciales, otras veces con fines educativos. Ejemplos de esto son los robots AIBO TM de Sony, o el robot LEGO Mindstorms TM de The 20 LEGO Group. Estos robots fueron creados y comercializados como juguetes; sin embargo, su gran capacidad de reprogramación los ha llevado a convertirse en sujetos de estudio en áreas de investigación como la Inteligencia Artificial o la Visión por Computador (del inglés Computer Vision.) En particular los AIBO han sido utilizados como participantes en el torneo de futbol RoboCup [43], incluso después de haber sido descontinuados por Sony. Robots en la educación En 1950 el investigador Grey Walter diseño y construyó un robot móvil al que llamó Tor- tuga [79]. Este robot fue construido para poder estudiar las teoŕıas de control cibernético que comenzaban a popularizarse en aquella época. Posteriormente, entre los años 1967 y 1969, Seymour Papert y Wallace Feurzeig diseñaron e implementaron un lenguaje de programación llamado Logo el cual se utilizaba para controlar una versión rediseñada del robot Tortuga de Walter, permitiéndole al robot dibujar figuras sobre papel utilizando un marcador colocado sobre un pequeño brazo incorporado al mismo [57]. Este robot fue utilizado exitosamente por Papert y sus colaboradores para incentivar la práctica de temas relacionados con la programación y la tecnoloǵıa en estudiantes de primaria. El MIT Programable Brick y los robots LEGO Mindstorms TM son ejemplos más recientes del uso de robots en entornos educativos. Este kit provee una amplia variedad de piezas, incluyendo sensores, las cuales pueden combinarse de distintas maneras para formar robots u otros tipos de máquinas programables autónomas. Mucho trabajo se ha realizado con la intención de aplicar estos robots para fomentar el estudio de carreras cient́ıfico-tecnológicas. En su trabajo de investigación Emanuelle Mene- gatti y Michele Moro plantearon un esquema de enseñanza construccionista de la tecnoloǵıa utilizando robótica [48]; ellos diseñaron diversas actividades y evaluaciones para niveles de educación desde secundaria hasta magistratura que se centran en la resolución de problemas construyendo diversos tipos de robots. Por su parte, Alexander Behrens y sus colaboradores diseñaron un sistema que permite ejecutar programas en robots NXT de manera remota utilizando MATLAB TM , siendo los estudiantes de ingenieŕıa y ciencias de la computación su público objetivo [11]. En [54] Jürgen Mottok y Armin Gardeia describen un experimento social que utiliza robots NXT para incentivar el estudio de materias relacionadas con la tecnoloǵıa en estudiantes alemanes de secundaria. 2.3. La realidad aumentada y el control de robots móvi- les Ya desde 1993 Paul Milgram y colaboradores hab́ıan presentado un trabajo de investiga- ción y desarrollo en el cual haćıan uso de un sistema de realidad aumentada para controlar a un robot[51]. Este trabajo consit́ıa en una estación de trabajo conectada a un manipula- dor industrial el cual era filmado por una cámara de video. En la pantalla de la estación de trabajo se mostraba al usuario una imagen como la que puede observarse en la Figura 2.10, en la cual un modelo 3D era desplegado en modo wireframe sobre el robot. El usuario 21 teńıa la posibilidad de manipular el modelo del robot, cambiándolo de posición o rotando sus distintos componentes; luego el robot real imitaŕıa los movimientos producidos por el usuario en el modelo. Una variación reciente sobre este trabajo fue presentada por Chong y colaboradores en el 2009 [18]. Figura 2.10: El sistema de control presentado por Milgram. Imagen recuperada de [7]. Una investigación similar orientada al área de los robots móviles se encuentra en el traba- jo de Alonzo Kelly y colaboradores, quienes presentaron una serie de técnicas y herramientas que tratan de minimizar la dificultad inherente al control remoto de robots móviles en enlaces inalámbricos, particularmente en entornos donde las distancias o la latencia de las comuni- caciones en red hace imposible el obtener una respuesta inmediata del robot [42]. El sistema funciona construyendo un modelo 3D del entorno del robot en tiempo real basándose en imagenes de video capturadas por el robot, utilizando la información obtenida de los otros sensores del robot como complemento. Por su parte Tian Xie y colaboradores presentaron un framework para el control remoto de robots por realidad aumentada llamado ARATG (Augmented Reality Aided Teleoperation Guidance - Orientación en Teleoperación Asistida por realidad aumentada) usando los resultados de Kelly y colaboradores como base. Esta clase de sistemas de control basados en realidad aumentada fueron estudiados por Scott Green y colaboradores en [32]. Ellos realizaron un estudio experimental en el cual se compararon dos sistemas de control con robots simulados: uno basado en una cámara que graba el punto de vista del robot y otro basado en realidad aumentada que permite al operador observar al robot dentro del contexto de su entorno de trabajo. Los resultados del estudio mostraron que el sistema basado en realidad aumentada mejoraba la capacidad de planificación de los operadores permitiéndoles realizar las tareas planteadas por el estudio con un mejor desempeño, eliminando la necesidad de inferir las condiciones del entorno de trabajo del robot como es necesario con el sistema básado en la cámara. 22 Minoru Kojima y colaboradores presentaron un diseño y su respectiva implementación de un juego de realidad aumentada para robots móviles llamado Augmented Colliseum [44]. Este juego consiste en un sistema de realidad aumentada proyectada en la cual un conjunto de robots móviles deben “destruir” a sus contrincantes dentro de un coliseo virtual el cual es proyectado sobre los robots mediante un proyector de techo. El juego viene a ser como una representación real de una partida del juego RoboCode4. Augmented Colliseum fue desarro- llado con la biblioteca ARToolkit5. La Figura 2.11 muestra una fotograf́ıa de una partida de Augmented Colliseum en desarrollo. Figura 2.11: El juego Augmented Colliseum. Imagen recuperada de [44]. Jakob Leitner y colaboradores implementaron un juego basado en realidad aumentada proyectada llamado IncreTable [46]. Este juego que está inspirado en The Incredible Ma- chine TM de Sierra Online R© permite a los usuarios crear maquinarias complejas tipo Rube Goldberg utilizando combinaciones de elementos reales y virtuales, incluso con robots móvi- les. Kazuhiro Hosoi y colaboradores presentaron en el 2007 un trabajo en el cual proponen una arquitectura de control para robots móviles llamada VisiCon [39]. VisiCon consiste en el uso de proyectores de mano para guiar el camino de un robot móvil el cual sigue la luz emitida por el proyector sobre el suelo. Basados en este trabajo, Hosoi y colaboradores diseñaron un juego de realidad aumentada en el cual dos jugadores deben turnarse guiando a un robot móvil delimitando su camino con rutas de realidad aumentada emitidas por los proyectores, en este caso un proyector por cada jugador. Este juego es llamado CoGame dada la naturaleza colaborativa del mismo [38]. 4http://robocode.sourceforge.net 5http://www.hitl.washington.edu/artoolkit 23 http://robocode.sourceforge.net http://www.hitl.washington.edu/artoolkit Caṕıtulo 3 Método de investigación y herramientas utilizadas En este caṕıtulo se describen a detalle las metodoloǵıas de investigación y desarrollo utilizadas y las herramientas sobre las cuales se construyo el proyecto desarrollado. 3.1. Metodoloǵıa Para el desarrollo de este proyecto se utilizó una metodoloǵıa basada en prototipos evolu- tivos los cuales eran revisados y evaluados cada 7 o 15 d́ıas dependiendo de las funcionalidades que se quisieran agregar al prototipo en algún momento determinado. El desarrollo por pro- totipos evolutivos consiste en el desarrollo de un software de calidad de producción al cual se le van agregando funcionalidades una por una a medida que sus requerimientos son estu- diados y comprendidos [19]. Esta metodoloǵıa se contrasta principalmente con el desarrollo por prototipos descartables, en la cual se desarrollan series de prototipos sencillos que imple- mentan una única funcionalidad espećıfica con el objetivo de depurar su diseño, esto con el interés de agregar dicha funcionalidad a un sistema de producción aparte. Algunos segmen- tos del sistema desarrollado fueron implementados primero como prototipos descartables y luego incorporados al prototipo evolutivo, particularmente aquellos algoritmos relacionados con visión por computador, procesamiento digital de imágenes y transmisión de video. Un ejemplo de estos prototipos descartables puede observarse en la Figura 4.13, la cual consiste en un conjunto de capturas de pantalla de una aplicación para computadoras de escritorio que se desarrolló para depurar los algoritmos de visión por computador. A nivel de programación, se utilizó el llamado successfull Git branching model especi- ficado por Vincent Driessen [23], el cual define un esquema de trabajo para el sistema de control de versiones Git. Este esquema consiste en separar el desarrollo del sistema en dos ramas fundamentales. La primera rama es la llamada rama maestra en la cual únicamente se encuentra el código fuente probado y estable que se puede compilar en una aplicación lista para ponerse en producción. La segunda rama es llamada rama de desarrollo que se utiliza para incorporar todos los cambios que se agreguen al código fuente pero que no estén 24 depurados completamente. Adicionalmente se definen tres tipos de ramas tangenciales las cuales se utilizan para identificar hitos en el desarrollo del sistema. La primera de este tipo de ramas es llamada rama de funcionalidad o caracteŕıstica. Estas ramas se utilizan para agregar nuevas funcionalidades al sistema, manteniendo dichas funcionalidades aparte de las bases de código fuente hasta que hayan sido completadas. Una vez que las funcionalidades están completas se mueven a las llamadas ramas candidatas a publicación, las cuales se utilizan para depurar dichas funcionalidades antes de incorporarlas a las ramas de desarrollo y maestra. Finalmente se definen las ramas de reparación de errores en las cuales se agrega el código fuente para acomodar errores graves detectados en el sistema que se está desarrollando. Un ejemplo del esquema completo se puede observar en la Figura 3.1 Figura 3.1: El successfull Git branching model. Imagen adaptada y traducida de http://nvie.com. 3.2. Herramientas En esta sección se describen las diferentes herramientas y dispositivos utilizados durante el desarrollo de este trabajo. Estas descripciones son dadas principalmente fuera de contexto, centrándose únicamente en las especificaciones técnicas de cada herramienta. El uso que se le dió a cada herramienta se expone con más detalle en los Caṕıtulos 4 y 5. 25 http://nvie.com 3.2.1. Herramientas de hardware La arquitectura desarrollada en este trabajo abarca diversos dispositivos de hardware sobre los cuales se implementó la especificación de la misma (véase la Sección 4.1). Entre los dispositivos utilizados se encuentran los robots LEGO Mindstorms NXT TM descritos con detalle en el Anexo A; varios dispositivos móviles basados en el sistema operativo Android y la consola de videojuegos OUYA, aśı como un punto de acceso inalámbrico. Dispositivos móviles Para el desarrollo de la implementación de referencia (véase la Sección 4.2) se utilizaron dos tablets Samsung Galaxy Tab 2, una de 7 pulgadas y otra de 10.1 pulgadas, aśı como un smartphone Samsung Galaxy Nexus. Para las pruebas se utilizaron adicionalmente un smartphone Samsung Galaxy S4 y un smartphone Pantech Burst, aśı como una consola de videojuegos OUYA que se describe más adelante. La Tabla 3.1 resume las caracteŕısticas de hardware de los dispositivos utilizados1. Caracteŕıstica Galaxy Tab 2 Galaxy Nexus Galaxy S4 Pantech Burst Cortex A9 Cortex A15 Cortex A9 Snapdragon Procesador 1 Ghz 1.2 Ghz 1.6 Ghz 1.5 Ghz ARM + Neon ARM + Neon ARM + Neon ARM + Neon GPU PowerVR PowerVR PowerVR Adreno Memoria RAM 1 GB 1 GB 2 GB 1 GB Cámara 3.5 MP 5 MP 13 MP 5 MP Bluetooth v3.0 v3.0 v4.0 + LE v3.0 WLAN 802.11 b/g/n b/g/n b/g/n/ac b/g/n Tabla 3.1: Dispositivos móviles utilizados. Datos recuperados de http://www.gsmarena.com. La consola OUYA La consola de videojuegos OUYA es un dispositivo basado en el sistema operativo Android 4.1 desarrollada por la empresa OUYA Inc. 2. Este dispositivo actua como una consola de videojuegos de sobremesa completamente hackeable y programable, diseñada principalmente para desarrolladores de videojuegos independientes y aficionados. La consola esta basada en el procesador Tegra 3 de Nvidia el cual implementa el API (Application Programming Interface - Interfaz de Programación de Aplicaciones) definido por el estanda OpenGL ES 2.0 (Open Graphics Library for Embedded Systems - Biblioteca Abierta de Gráficos para Sistemas Embebidos). El procesador funciona a 2 Ghz de frecuencia e incluye 8 gigabytes de 1La columna Galaxy Tab 2 incluye tanto la Galaxy Tab 2 7.0 como la 10.1 dado que la única diferencia relevante entre ambas es el tamaño de sus pantallas. 2https://www.ouya.tv 26 http://www.gsmarena.com https://www.ouya.tv almacenamiento interno y 1 gigabyte de memoria RAM. Adicionalmente la consola cuenta con conectividad 802.11 y Bluetooth, aśı como un puerto Fast Ethernet, un puerto USB 2, un puerto mini USB y salida de video HDMI (High Definition Multimedia Interface - Interfaz Multimedia de Alta Definición) a 1080p. La Figura 3.2 muestra el aspecto de la consola y su control. Figura 3.2: La consola de videojuegos OUYA y su control. Imagen recuperada de https://www.ouya.tv. La consola es manipulada por un control inalámbrico que se conecta con la misma utili- zando Bluetooth. Este control sigue el modelo t́ıpico para gamepads, contando con 4 botones identificados por letras (O, U, Y y A), un pad direccional, dos palancas analógicas, dos gati- llos digitales, dos gatillos analógicos y un botón de inicio. Adicionalmente el control incluye un pequeño panel táctil que funciona como un trackpad el cual puede utilizarse para emular controles de pantalla táctil. El punto de acceso inalámbrico Durante el desarrollo de este trabajo de investigación se utilizó un punto de acceso inalámbrico D-LINK modelo di-634m. Este es un punto de acceso doméstico con soporte para IEEE 802.11b e IEEE 802.11g. Este modelo incluye soporte para la tecnoloǵıa MIMO (Multiple Input and Multiple Output - Entrada Múltiple y Salida Multiple) con dos antenas, además de soporte para el protocolo propietario WiFi Super G. El punto de acceso incluye 4 puertos Fast Ethernet y tiene un alcance de aproximadamente 100 metros en la interfaz inálambrica. 3.2.2. Herramientas de software El desarrollo de la parte de software de la arquitectura hizo uso de múltiples bibliotecas y componentes, algunos de los cuales son necesarios para trabajar con el hardware antes descrito y otros que fueron escogidos por diversas razones que se exponen en esta sección. Casi la totalidad de la aplicación (unas 9081 lineas de código fuente de un total de 9511 lineas) 27 https://www.ouya.tv fue desarrollada con el lenguaje de programación Java, el cual es el lenguaje de programación principal utilizado por el sistema operativo Android. El resto de la aplicación fue desarrollada en el lenguaje C++, en particular las funcionalidades relacionadas con procesamiento digital de imágenes. Para el desarrollo se utilizaron las siguientes bibliotecas de software: OpenCV3 para el procesamiento digital de imágenes. LibGDX4 para el desarrollo del sistema de juego y otras funcionalidades relacionanadas con la Computación Gráfica. Artemis Entity-System Framework 5 para la implementación del patrón de diseño Entidad- Componente-Sistema (véase la Sección 4.2.3). Java Universal Tween Engine6 para la aplicación de interpolaciones en los efectos gráficos del sistema de juego. Cada una de estas bibliotecas se detalla a continuación. OpenCV OpenCV es el acrónimo utilizado por el proyecto Open Computer Vision (Visión por Computador Abierta) llevado a cabo por las empresas Willow Garage e Itzees para desa- rrollar una biblioteca eficiente y sofisticada de procesamiento digital de imágenes. OpenCV es utilizada para manipular y obtener información a partir de imágenes ya sean estáticas u obtenidas a partir de video, teniendo varios miles de funciones que permiten trabajar a distintos niveles de abstracción, ya sea con operaciones aritméticas o filtros de bajo nivel has- ta distintos algoritmos de detección de caracteŕısticas utilizando clasificadores y máquinas vectoriales de soporte (del inglés support vector machines). En este proyecto se utilizó esta biblioteca para desarrollar las funcionalidades referentes a la calibración de cámaras de video y detección de marcadores. Fue escogida por tres grandes razones: la primera, la biblioteca es software libre y gratuito; la segunda, esta biblioteca es reconocida por ser altamente eficiente; y la tercera, posee soporte para múltiples plataformas de hardware y sistemas operativos, en particular el sistema operativo Android, ya que como se mencionó en la sección anterior fue la plataforma escogida para el desarrollo del proyecto. La principal ventaja del soporte multiplataforma de OpenCV es que utiliza exactamente la misma interfaz de programación de aplicaciones independientemente del sistema operativo o hardware para el que se programe. Esto permitió desarrollar y probar los distintos algorit- mos de calibración de cámaras, detección y decodificación de marcadores en una computadora de escritorio; facilitando enormemente la depuración de los mismos, con la seguridad de que el mismo código fuente podŕıa luego ser portado directamente al sistema operativo Android sin tener que aplicar luego ninguna modificación. 3http://opencv.org 4http://libgdx.badlogicgames.com 5http://gamadu.com/artemis 6https://code.google.com/p/java- universal-tween-engine 28 http://opencv.org http://libgdx.badlogicgames.com http://gamadu.com/artemis https://code.google.com/p/java-universal-tween-engine LibGDX A pesar de su nombre LibGDX es un framework más que una biblioteca y tiene como objetivo el simplificar el desarrollo de videojuegos múltiplataforma en el lenguaje Java; es desarrollada por el estudio de desarrollo independiente Badlogic Games. En realidad LibGDX es una agrupación de un gran conjunto de diferentes bibliotecas de desarrollo de aplicaciones gráficas. Entre las distintas bibliotecas incorporadas por LibGDX se encuentran: OpenGL para despliegue de gráficos 2D y 3D. Dependiendo de la plataforma en la que se ejecute la aplicación desarrollada LibGDX se encarga de utilizar ya sea OpenGL estándar u OpenGL ES. Box2D para realizar simulaciones f́ısicas en dos dimensiones. Bullet para simulaciones f́ısicas en tres dimensiones. FreeType para manipulación de fuentes y despliegue de textos. OpenAL, Vorbis y Mpg 123 para reproducción de sonido. GWT, LWJGL y RoboVM para las funcionalidades dependientes del sistema operativo. En este proyecto se decidió utilizar LibGDX principalmente por las simplificaciones que provee a la hora de trabajar con OpenGL, aśı como la posibilidad de reutilizar la misma base de código en múltiples plataformas de hardware. Artemis Entity-System Framework Artemis es un framework escrito en Java puro el cual implementa de forma eficiente el patrón de diseño Entidad-Componente-Sistema. En resumen este patrón de diseño permite separar las distintas funcionalidades del juego desarrollado en sistemas bien diferenciados los cuales actúan sobre entidades, que a su vez son agrupaciones de componentes. Esta orga- nización en entidades y componentes aśı mismo permite eliminar las relaciones de herencia entre las clases que implementan los entes que pueblan el mundo de juego. Una descripción más detallada del funcionamiento y la motivación detrás de este patrón de diseño se da en la Sección 4.2.3. Artemis es desarrollada de forma independiente por Arni Arent. Java Universal Tween Engine El Java Universal Tween Engine (motor universal de interpolación) es una biblioteca implementada en Java puro que sirve para interpolar cualquier atributo numérico de cual- quier objeto de Java aplicando distintas funciones de interpolación. Su principal uso es el modificar atributos de objetos gráficos para realizar toda clase de efectos especiales aunque puede utilizarse para cualquier otro caso de uso en el cual se tenga que interpolar entre dos o más valores. Un ejemplo de algunas de las distintas funciones de interpolación implemen- tadas por la biblioteca pueden verse en la Figura 3.3, la cual es una captura de pantalla de 29 la aplicación de demostración de la misma. Java Universal Tween Engine es desarrollado de forma independiente por Aurelien Ribon. Figura 3.3: Funciones de interpolación del Java Universal Tween Engine. Esta biblioteca fue utilizada para implementar efectos de transición entre las distintas vistas de la aplicación y para incorporar efectos de desvanecimiento en el módulo de juego. 30 Caṕıtulo 4 Descripción de la solución implementada En este caṕıtulo se presenta el diseño de la architectura propuesta, aśı como también los detalles de la implementación de referencia. La descripción de la arquitectura esta dada siguiendo un esquema top-down, comenzando con el diseño general a grosso modo y poste- riormente ahondando en los detalles. El caṕıtulo concluye con una explicación del escenario de demostración desarrollado. 4.1. Diseño arquitectónico de la solución En el presente trabajo se propone una arquitectura de control para robots móviles basada en realidad aumentada compuesta por 3 elementos: el robot, un dispositivo con cámara capaz de grabar video y un dispositivo de control. Estos tres dispositivos deben poseer la capacidad de comunicarse entre śı de forma inalámbrica ya sea utilizando enlaces Bluetooth u 802.11. La Figura 4.1 ilustra estos componentes y la relación entre ellos. Como puede apreciarse, esta arquitectura esta diseñada alrededor del robot LEGO Minds- torms NXT TM1. Se escogió este robot dada la facilidad de incorporarle otros dispositivos mediante su conectividad Bluetooth, para ser utilizados como sensores o procesadores ex- ternos. Este robot debe seguir un esquema de movimiento diferencial (como se describió en la Sección 2.2.3), y debe tener algún mecanismo que permita incorporarle el dispositivo con cámara como un sensor adicional. Como mı́nimo el robot debe poseer dos motores separa- dos para su locomoción (un motor derecho y otro izquierdo), aunque se puede incorporar un tercer motor que permita mover el dispositivo con cámara de forma que el robot pueda “ver” en una dirección diferente a su dirección de movimiento. El dispositivo con cámara mostrado en la Figura 4.1 corresponde con cualquier dispositivo programable capaz de grabar video o tomar fotograf́ıas con bastante rapidez. Este dispositivo además actúa como un puente entre el robot y el dispositivo de control, siendo este último 1Esto no es limitativo. Facilmente se puede sustituir al robot LEGO Mindstorms NXT TM por un robot LEGO Mindstorms EV3 TM o por cualquier otro robot programable con conectividad inalámbrica. 31 Figura 4.1: Diseño general de la arquitectura. el dispositivo utilizado directamente por el usuario para controlar al robot. Este dispositivo de control puede ser cualquier dispositivo programable capaz de recibir interacción por parte del usuario y de conectarse con el dispositivo con cámara, como por ejemplo pueden ser las tablets, computadoras de escritorio o incluso consolas de videojuegos. 4.1.1. Diseño de casos de uso Para establecer los casos de uso de la arquitectura propuesta se define como posibles actores al jugador y al diseñador de escenarios. El jugador es el usuario que se encarga de ejecutar una partida para el escenario que haya sido desarrollado, utilizando las diversas in- teracciones propuestas por la arquitectura para completar algún objetivo de juego espećıfico. Por su parte el diseñador de escenarios es el encargado de diseñar y programar los escenarios que podrá utilizar el jugador. Esto puede observarse en la Figura 4.2. Figura 4.2: Casos de uso de nivel 0. Estos casos de uso pueden detallarse como se observa en la Figura 4.3. Para el caso del jugador se puede observar que jugar una partida de un escenario implica tres actividades. La 32 primera es la calibración de la cámara que tiene que realizarse para asegurar un despliegue de objetos virtuales correcto. La segunda actividad es la ejecución del escenario manual del robot que representa el juego en śı. El escenario de control manual debe permitir al usuario el manipular al robot y a un “brazo mecanico” virtual mediante controles que se describen más adelante en este caṕıtulo, de forma que utilizando ambos el usuario tenga la posibilidad de cumplir el objetivo propuesto por el escenario. Finalmente el usuario debe tener la posibilidad de ejecutar un algoritmo de control autónomo para el robot el cual dependerá del escenario que se esté jugando. Por su parte el diseñador de escenarios tiene la posibilidad de desarrollar los componentes que ejecuta el jugador, siendo opcional aunque recomendado el implementar las pantallas de resumen que se le muestran al jugador al completar cada módulo de juego. Figura 4.3: Casos de uso de nivel 1. 4.1.2. Control del robot El mecanismo de interacción con el robot depende de la naturaleza del dispositivo de control. Por ejemplo, en una tablet el control debeŕıa realizarse por medio de la pantalla tactil de la misma, o en el caso de una computadora de escritorio por medio del teclado y el ratón. Independientemente del mecanismo f́ısico mediante el cual se manipule al robot, este debe soportar los siguientes controles: Avanzar y retroceder cada motor por separado. Mover el brazo virtual. Retornar el motor del dispositivo con cámara a su posición inicial. 33 Control del brazo virtual El brazo virtual es una representación de un efector lógico añadido al robot de forma que este pueda interactuar con los objetos virtuales del escenario de juego. Este brazo debe poder desplazarse libremente en un plano paralelo a la pantalla del dispositivo de control, y avanzar y retroceder en un vector perpendicular a la misma. El diagrama de la Figura 4.4 ilustra como se realiza este movimiento Figura 4.4: Movimiento del brazo virtual. 4.1.3. Escenarios jugables y despliegue de objetos virtuales. Un escenario consiste en un conjunto de reglas que definen un objetivo, el cual debe cum- plir el jugador utilizando las herramientas de control descritas anteriormente. Como mı́nimo un escenario debe definir un conjunto de objetos virtuales con los cuales puede interactuar el jugador y las condiciones para completar el escenario, ya sea en victoria o derrota. Los objetos virtuales deben consistir en modelos 3D los cuales deben ser desplegados por pan- talla alineados con marcadores similares a los que se pueden observar en la Figura 4.12. La información que codifican los marcadores es responsabilidad de cada implementación. La interacción de los objetos virtuales con el robot se debe basar en la detección de colisiones de dichos objetos con el brazo virtual. 34 4.2. Descripción de la implementación de referencia En esta sección se describe a detalle la aplicación desarrollada que implementa la ar- quitectura descrita en la sección anterior. Esta implementación, llamada en conjunto como NxtAR, abarca tres apliaciones: dos para el sistema operativo Android y una más para el robot Mindstorms NXT. Las aplicaciones desarrolladas para dispositivos Android son lla- madas NxtAR-cam y NxtAR-core e implementan los módulos del dispositivo de visión y el dispositivo de control respectivamente. La aplicación para el robot es llamada NxtAR-bot y tiene como objetivo el aplicar las instrucciones de control que se generan desde NxtAR-core. Las aplicaciones desarrolladas son software libre2 publicado bajo la licencia Apache 2.03. Las aplicaciones para Android tienen soporte para dispositivos móviles como tablets y smartp- hones y en el caso de la aplicación para el dispositivo de control se posee soporte adicional para la consola de videojuegos OUYA, aplicación cuyo icono puede observarse en la Figura 4.5. Figura 4.5: Icono de la implementación de referencia en la consola OUYA. 4.2.1. Diseño de clases En la Figura 4.6 se puede observar un diagrama de clases simplificado de la aplicación NxtAR-core. En dicho diagrama se puede observar la composición de los paquetes más sig- nificativos que agrupan las clases de la aplicación. Lo primero que se puede detallar es que la aplicación está dividida en dos paquetes principales: NxtAR-core propiamente y NxtAR- android. Este segundo paquete representa el frontend para el sistema operativo Android, el cual se implementa en un paquete separado siguiendo los requisitos del framework LibGDX. Este paquete también incluye un subpaquete llamado cv proc el cual contiene la implementa- ción nativa de los algoritmos de detección y descodificación de marcadores (véase la Sección 4.2.4). El paquete NxtAR-core se divide en 3 subpaquetes principales: interfaces, escenarios y estados. El paquete de interfaces define los puntos de acceso a través de los cuales el núcleo 2Descargables en https://github.com/sagge-miky 3http://www.apache.org/licenses/LICENSE-2.0 35 https://github.com/sagge-miky http://www.apache.org/licenses/LICENSE-2.0 Figura 4.6: Diagrama de clases de NxtAR-core. de la aplicación (definido en la clase core) y el frontend para Android (por medio de la clase MainActivity) pueden comunicarse. En particular la interfaz ActionResolver se utiliza para abstraer las funcionalidades del sistema operativo de forma que el núcleo pueda tener acceso a las mismas de forma independiente del sistema. La interfaz CVProcessor cumple la misma función con respecto al subpaquete cv proc de NxtAR-android. Por último la interfaz ApplicationEventsListener se utiliza internamente con los hilos que se encargan de las funciones de comunicación por red para que puedan notificar al núcleo de eventos que afectan el comportamiento del sistema, implementando el patrón de diseño Observer. El subpaquete de escenarios de NxtAR-core define un conjunto de clases abstractas y una clase estática las cuales definen el escenario de juego disponible al jugador. La clase estática es la clase ScenarioImplementation, la cual consiste en un conjunto de apuntadores a las implementaciones de las clases abstractas definidas en el paquete (todas ellas cuyos nombres terminan en Base). Estas implementaciones se definen dentro de un subpaquete de escenario 36 junto a cualquier otra clase auxiliar necesaria. Por otro lado, el subpaquete de estados de NxtAR-core contiene las implementaciones de los estados que componen la máquina de estados finitos de la aplicación, descritos en detalle más adelante en este caṕıtulo. 4.2.2. Detalles del desarrollo de los módulos del robot y la cámara Los dos primeros módulos descritos en el diseño de la arquitectura son implementados por las aplicaciones NxtAR-bot y NxtAR-cam antes mencionadas. Estas dos aplicaciones funcionan de manera completamente autónoma, comunicándose entre ellas por medio de un enlace Bluetooth por el perfil de puerto serial, y con la aplicación de control utilizando enlaces TCP y UDP dependiendo de la información que debe transmitirse. Estas aplicaciones, junto con el esquema de comunicación que utilizan se describen a continuación. La aplicación NxtAR-bot NxtAR-bot es una pequeña aplicación desarrollada en el lenguaje Java para ser ejecutada en robots LEGO Mindstorms TM NXT que posean el firmware del sistema operativo LeJOS. Esta aplicación se encarga de realizar dos únicas funciones, la primera de las cuales es reportar las medidas de los sensores del robot. La versión desarrollada solamente realiza reportes del sensor de luz del robot, que es el único sensor instalado en el diseño utilizado. Este reporte se realiza automáticamente entre las aplicaciones NxtAR-bot y NxtAR-cam, siendo esta segunda aplicación la que se encarga de redirigir las medidas tomadas a la aplicación de control utilizando un enlace TCP. La aplicación de control puede luego consultar dichas medidas mediante un esquema de polling. La otra función de esta aplicación es el ejecutar las instrucciones de control generadas por el usuario o el sistema de control automático (descrito más adelante). Estas instruccio- nes toman la forma de un mensaje de dos bytes transmitido v́ıa Bluetooth al robot desde la aplicación NxtAR-cam, originándose en la aplicación de control. El primero de estos by- tes codifica la instrucción de control mientras que el segundo representa un parámetro de la misma. Las instrucciones de control son codificadas utilizando los bits del primer byte mencionado de la siguiente manera: 0x01: Control del motor en el puerto A. 0x02: Control del motor en el puerto B. 0x04: Control del motor en el puerto C. 0x08: Dirección del movimiento de los motores. 0x10: Regresar el motor en el puerto B a su posición original. 0x20: Rotar el motor en el puerto B en 90 grados en sentido contrarreloj. 0x40: Disponible para expansiones futuras. 37 0x80: Disponible para expansiones futuras. El byte de parámetro se utiliza actualmente para indicar la potencia que se debe aplicar a los motores cuando se incluyen instrucciones de control en el rango 0x01 a 0x04. Las demás instrucciones utilizan una potencia constante predefinida en 50 por ciento. La instrucción de dirección (bit 0x08) se utiliza como un parámetro adicional de las tres instrucciones anteriores, indicando que el motor debe avanzar cuando el bit está encendido y retroceder cuando está apagado. Utilizar máscaras de bits de esta manera permite componer instrucciones de control más complejas que las definidas por el protocolo LCP (LEGO Communication Protocol - Protocolo de Comunicaciones de LEGO) implementado por el firmware estándar incluido por los robots LEGO Mindstorms NXT TM (véase la Sección A.2.2 en el Anexo A), siendo posible encender más de un motor por mensaje de control. En el caso de las operaciones que afectan al motor B, de ser definidas dos o más de forma simultánea, se aplican en el orden definido por el listado anterior con cada operación reemplazando a las anteriores. La aplicación NxtAR-cam NxtAR-cam es la aplicación encargada de implementar el módulo del dispositivo con cámara mostrado en la arquitectura. Su principal funcionalidad es la de capturar y trans- mitir un flujo de video en tiempo real hacia la aplicación de control, al mismo tiempo que retransmite hacia el robot las instrucciones de control generadas por el usuario. El flujo de video es capturado por la cámara del dispositivo a una tasa de cuadros por segundo definida por el sistema operativo Android dependiendo de las posibilidades del sensor de la misma. La captura se realiza cuadro a cuadro los cuales son colocados por el hilo principal de la aplicación, que es el encargado de realizar la captura, dentro de un objeto monitor de donde el hilo de transmisión los obtiene para enviarlos a la aplicación de control. El monitor implementa un esquema de doble buffer para que los hilos no tengan que funcionar a la rapidez del hilo más lento. Con este esquema el hilo de transmisión siempre toma el cuadro de video ubicado en el buffer frontal, mientras que el hilo de captura coloca los cuadros nuevos en el buffer posterior y luego invierte las referencias a los mismos, de forma que los cuadros nuevos se almacenen en lo que antes era el buffer frontal y los cuadros a transmitir se tomen de lo que era el buffer posterior. Un esquema similar se utilizó en el lado receptor, donde un hilo receptor coloca los cuadros recibidos en un monitor con doble buffer de donde el hilo de despliegue los obtiene para mostrarlos por pantalla. La transmisión del video se realiza utilizando datagramas UDP (User Datagram Protocol - Protocolo de Datagramas de Usuario) sin esquemas particulares para control de flujo o corrección de errores. Las instrucciones de control son enviadas directamente de la aplicación de control a NxtAR-cam utilizando enlaces TCP. Al ser recibidas son almacenadas en una cola con ca- pacidad para 10 elementos para luego ser codificadas utilizando el esquema de máscaras de bits explicado anteriormente y finalmente ser transmitidas al robot con un enlace Bluetooth. Si la cola de instrucciones se llena NxtAR-cam env́ıa un mensaje de espera a la aplicación 38 de control, luego enviando un mensaje indicando que puede continuar la transmisión cuando la cola se vaćıa. Comunicación entre módulos Para interconectar los distintos módulos entre śı se sigue el siguiente esquema: 1. Primero se inicia la aplicación NxtAR-bot en el robot y se realizan los pasos para calibrar el sensor de luz. Una vez listo esto la aplicación se coloca en modo de espera por una conexión Bluetooth. 2. Se inicia la aplicación NxtAR-core (la aplicación de control), la cual comienza a trans- mitir paquetes multicast anunciándose a la red. 3. NxtAR-cam se conecta con el robot. El robot debe estar emparejado de antemano con el dispositivo de captura de video utilizando las funcionalidades de comunicación por Bluetooth del sistema operativo Android. 4. Finalmente se indica a NxtAR-cam que busque al dispositivo de control utilizando un protocolo ad hoc sencillo de descubrimiento de servicios. La conexión entre NxtAR-cam y NxtAR-core se realiza de forma automática. La Figura 4.7 ilustra los enlaces de comunicación utilizados por las 3 aplicaciones. Figura 4.7: Enlaces de red en las aplicaciones que componen NxtAR. 39 4.2.3. Patrones de diseño utilizados en la aplicación de control NxtAR-core hace uso extenso de dos patrones de diseño que facilitaron su desarrollo al mismo tiempo que proporcionan una forma clara y concisa de organización del código fuente. Estos patrones son los llamados state pattern (patrón de estados) y patrón Entidad- Componente-Sistema. State pattern El state pattern es un patrón de diseño que consiste en modelar las distintas funcionali- dades de una aplicación como una máquina de estados finitos [27]. Los distintos estados en los que puede estar la aplicación en un momento dado son implementados como subclases de una superclase estado abstracta de la cual se posee un vector que contiene las implemen- taciones de los distintos estados concretos. Adicionalmente se posee un ı́ndice a dicho vector el cual indica cuál es el estado actual de la aplicación. Este patrón de diseño permite separar las distintas funcionalidades de la aplicación de manera gruesa, agrupando funcionalidades similares o relacionadas dentro de un mismo estado. La aplicación desarrollada define los siguientes 7 estados, los cuales coinciden con las diferentes vistas que posee la misma: Menú principal. Calibración de la cámara. En juego. Información del juego. Resumen del juego. Acción automática. Resumen de la acción automática. Estos 7 estados a su vez se pueden agrupar en 4 macro-estados: menú principal, calibra- ción, juego y acción automática. Como se puede deducir de su nombre, el estado meú prin- cipal se encarga de mostrar al usuario las distintas opciones que puede realizar al iniciar la aplicación. El estado de calibración se encarga de realizar las acciones de calibración de la cámara del dispositivo con cámara mostrado en el diseño de la arquitectura. El macro-estado de juego agrupa los tres estados que controlan el desarrollo del escenario de juego y permi- ten al usuario el controlar al robot de forma manual. Por último el macro-estado de acción automática permite que el robot realice acciones preprogramadas de manera autónoma. El estado de calibración, que puede observarse en la Figura 4.8.a permite al usuario obtener los parámetros de la cámara necesarios para poder realizar un despliegue correcto de los objetos virtuales sobre las imágenes obtenidas del flujo de video. Estos parámetros son la llamada matriz intŕınseca de la cámara y los coeficientes de distorsión, que juntos dan un 40 Figura 4.8: Los estados principales de NxtAR-core en tablets. (a) Menú principal. (b) Calibración de la cámara. (c) Acción automática. (d) En juego, modo de control del robot. modelo matemático aproximado del lente de la cámara que está capturando el video. Estos parámetros son obtenidos por medio de OpenCV y luego son alimentados a OpenGL durante el despliegue de figuras como una matriz de proyección. La matriz de proyección tiene la forma que puede apreciarse en la Equación (4.1) donde fx y fy son las coordenadas del punto focal de la cámara; cx y cy son las coordenadas del centro óptico de la cámara en pixeles; NEAR y FAR son las distancias de los planos de corte cercano y lejano respectivamente; y w y h son la anchura y altura del viewport.  −2fx w 0 −2cx w − 1 0 0 2fy h −2cy h − 1 0 0 0 −(FAR+NEAR) FAR−NEAR −2(FAR×NEAR) FAR−NEAR 0 0 −1 0   (4.1) El proceso de calibración de la cámara consiste en apuntar la misma hacia un patrón impreso similar al que se puede observar en la Figura 4.9. La aplicación toma automática- mente 10 muestras de dicho patrón y las utiliza para calcular los parámetros mencionados anteriormente. El patrón de calibración forsozamente debe poseer 54 puntos de intersección divididos en 9 filas por 6 columnas o viceversa. Antes de poder pasar a los macro-estados de control manual o acción automática es obligatorio que el usuario realice primero la calibración de la cámara. Una vez realizada la calibración el usuario puede utilizar cualquiera de los macro-estados de forma indistinta, e incluso tiene la posibilidad de repetir el proceso de calibración de ser necesario. El estado de acción automática puede verse en la Figura 4.8.c y permite al usuario ejecutar un programa automático predefinido con el robot, el cual muestra un resumen al culminar su ejecución. Este estado es genérico y necesita que cada escenario implementado incluya 41 Figura 4.9: Patrón de calibración de la cámara. su propio código de acción automática y su propia definición de la pantalla de resumen. El juego de demostración desarrollado implementa un código de acción automática sencillo el cual es descrito con detalle en la Sección 4.2.5. Al igual que el estado de acción automática el estado de control manual también es genérico y necesita que cada escenario implemente un conjunto de entidades y sistemas de procesamiento definidos con más detalle en la siguiente sección. Este estado permite al usuario jugar el escenario programado en la aplicación utilizando diferentes modos de control para manipular al robot dependiendo de la plataforma en la que se ejecute la aplicación. La versión implementada incluye soporte para control por pantalla táctil, gamepads y teclado con mouse. Adicionalmente este estado permite manipular un brazo virtual el cual puede utilizarse para interactuar con los diferentes objetos virtuales que implementa el escenario. Las Figuras 4.10.a y 4.10.b muestran el aspecto gráfico del brazo del robot y los controles definidos para los dispositivos con pantalla táctil. Patrón Entidad-Componente-Sistema El patrón Entidad-Componente-Sistema, también conocido como Entidad-Componente [3], es un patrón de diseño arquitectónico que trata de minimizar el uso de relaciones de herencia entre las clases en un sistema de software orientado a objetos, sustituyéndolas con relaciones indirectas entre clases sencillas llamadas componentes las cuales se agrupan dentro de unidades lógicas llamadas entidades. Este patrón se origina en el desarrollo de juegos del tipo CRPG (Computer Role Playing Game - Juego de Roles por Computador) los cuales pueden sufrir fácilmente de problemas durante el desarrollo a causa de complejas relaciones de herencia múltiple directa e indirecta entre las distintas entidades que componen el juego [12]. Este patrón es ampliamente utilizado en el desarrollo de juegos del tipo MMORPG (Massively Multiplayer Online RPG - RPG en Linea Masivamente Multijugador) principalmente porque es muy sencillo representar las relaciones entre entidades y componentes como tablas en una 42 Figura 4.10: El brazo virtual implementado y controles para tablets. (a) En su posición inicial. (b) En movimiento. La linea negra denota la trayectoria seguida por el brazo. base de datos relacional. Otra ventaja de este patrón es que permite separar completamente las distintas funcionalidades de la aplicación lo que a su vez facilita el desarrollo de código fuente mantenible, y además permite repartir tareas entre el equipo de desarrollo sin temor a que estas tareas se solapen [41]. El elemento fundamental de este patrón de diseño es el componente, el cual se representa como una clase sencilla únicamente compuesta por atributos públicos, o a lo sumo por un conjunto de métodos get y set para los distintos atributos. Un componente se encarga de representar una única propiedad que puede poseer una entidad, por ejemplo la posición de la entidad en el espacio, o un mallado que puede utilizarse para desplegar esa entidad en la pantalla. Entonces se puede definir a una entidad como una agrupación de componentes. Las entidades suelen implementarse como ı́ndices a una tabla que a su vez contiene ı́ndices a los componentes los cuales se organizan como filas en distintas tablas, una tabla por cada componente y una fila por cada entidad que utiliza ese componente. Esta implementación es t́ıpica dado que su representación en una base de datos relacional es trivial [12]; sin embargo, el cómo se representan y se relacionan las entidades y los componentes depende del criterio del desarrollador. Las entidades son luego procesadas por los sistemas, los cuales se encargan de imple- mentar la lógica de alguna parte de la aplicación. Por ejemplo se puede tener un sistema 43 de posicionamiento el cual se encarga de actualizar la posición de todas las entidades que posean un componente de posición, o un sistema de despliegue el cual muestra por pantalla todas las entidades que posean un componente de mallado. Los sistemas son creados en un orden espećıfico y son procesados en el mismo orden. En la Figura 4.11 se observa una im- plementación de la relación entre entidades y componentes propuesta por Mick West en [81]. Se puede observar que cada fila es una entidad y cada columna es un componente manejado por un administrador de componentes que se encarga de relacionar cada entidad con las instancias espećıficas de cada componente que utiliza. Figura 4.11: Composición de entidades como agrupaciones de componentes. Imagen recuperada de [81] La implementación de referencia de la arquitectura desarrollada hace uso del patrón Entidad-Componente-Sistema principalmente dentro del estado de juego, separando las fun- cionalidades de la arquitectura en 10 sistemas y 12 componentes básicos. De estos 10 sistemas 8 son independientes del escenario de juego implementado; aśı mismo, los 12 componentes son independientes del escenario, aunque cada escenario tiene la posibilidad de definir componen- tes adicionales según sea necesario. No existen entidades por defecto en la implementación, siendo cada escenario responsable de crear todas las entidades que necesite. Otros estados de la aplicación también pueden hacer uso de los sistemas, activándolos o desactivándolos según sea necesario. Los 8 sistemas generales que define la arquitectura son los siguientes, listados en el orden en que son creados y procesados: Posicionamiento de objetos con marcador asociado. 44 Posicionamiento del brazo virtual del robot. Aplicación de transformaciones geométricas (escalamiento, rotación y traslación). Aplicación de animaciones. Detección de colisiones. Despliegue de objetos con marcadores. Despliegue del brazo del robot. Despliegue de efectos especiales. Adicionalmente se definen dos sistemas que deben ser implementados por cada escenario. Estos son el sistema de lógica de juego y el sistema de procesamiento del jugador. El primer sistema se debe encargar de evaluar las reglas de juego que definen el escenario, modificando las distintas entidades del mismo dependiendo de las interacciones que detecten los sistemas anteriores, aśı como también se encarga de generar los distintos efectos especiales que pueda utilizar el escenario. Por su parte el sistema de procesamiento del jugador se debe encargar de evaluar las condiciones que determinan cuando el jugador completa el escenario ya sea de forma satisfactoria o insatisfactoria. 4.2.4. Detección de marcadores La detección de marcadores es un procedimiento que consiste en tres operaciones: la detección visual del marcador, la descodificación y la determinación de la transformación geométrica del marcador en el espacio. Estas tres operaciones son llevadas a cabo por la biblioteca OpenCV y se basan en los algoritmos planteados en el caṕıtulo 2 del libro Mas- tering OpenCV with practical computer vision projects (Dominando OpenCV con proyectos prácticos de visión por computador) [8]. Estos algoritmos, que se detallan a continuación, están implementados en el lenguaje C++ aprovechando el soporte del sistema operativo Android para la interfaz JNI (Java Native Interface - Interfaz Nativa de Java), la cual per- mitió el reutilizar completamente una implementación de los algoritmos para computadoras de escritorio que fue desarrollada y depurada de forma independiente. Los marcadores utilizados en este proyecto son los mismos definidos en [8], los cuales tienen un aspecto visual como el que se aprecia en la Figura 4.12. Estos marcadores son impresos o transferidos a una superficie de tonalidad clara de forma que posean un alto contraste con dicha superficie para hacerlos más faciles de detectar. El tamaño f́ısico del marcador impreso solo afecta a qué distancias puede ser detectado correctamente, sin cau- sar otros efectos en el desarrollo del juego implementado o en los distintos algoritmos que dependen de la detección del marcador en cuestión. 45 Figura 4.12: Uno de los marcadores utilizados y sus cuatro posibles representaciones. Detección visual de marcadores La detección visual de los marcadores consiste en identificar regiones de la imagen de entrada recibida del dispositivo con cámara en las cuales es posible que se encuentre un marcador. Este proceso se lleva a cabo mediante los siguientes pasos: 1. La imagen de entrada es llevada a escala de grises. 2. Se aplica una operación de umbral adaptativo con una vecindad de tamaño 7 a la imagen. 3. Se aplica una función de detección de contornos a la imagen binarizada. 4. La lista de contornos obtenida es sustituida por una lista de contornos aproximados. Aquellos contornos que no sean aproximados por cuadriláteros son descartados. 5. Se examinan los contornos restantes y se descartan los que estén muy cercanos entre śı. 6. Las regiones definidas por los contornos que superen la prueba anterior son recortadas de la imagen en escala de grises. 7. A estas regiones recortadas se les aplica una transformación de deformación de pers- pectiva, que genera una nueva imagen de 350 por 350 pixeles. 8. A las imagenes resultantes se les aplica una operación de umbral utilizando el algoritmo de Otsu [56]. La Figura 4.13 muestra los resultados de algunos de los pasos anteriores. El resultado de este algoritmo es una lista de imágenes que representan todos los candidatos a marcadores reconocidos en la imagen de entrada. Esta lista luego es utilizada como parámetro para el algoritmo de descodificación de marcadores que se explica a continuación. 46 Figura 4.13: Pasos del algoritmo de detección de marcadores. (a) Imagen de entrada. (b) Umbral adaptativo. (c) Candidatos detectados. (d) Arriba: Candidato despues de la corrección de perspectiva. Abajo: Candidato verificado y descodificado. (e) Imagen de salida. Descodificación de marcadores Una vez que una región de la imagen de entrada es identificada como un posible candidato a marcador se procede con la fase de descodificación. Esta fase se divide en tres etapas, la primera de las cuales consiste en examinar el borde externo de la región identificada, buscando que este sea mayormente de color negro. Si el candidato supera esta prueba se procede a descodificar propiamente la información contenida en la matriz de 5x5 interna. Esta matriz codifica un número entero de 10 bits como cinco palabras de 5 bits cada una, de los cuales 3 son bits de redundancia y dos son bits de datos, utilizando el código de bloques que se describe a continuación: Para cada palabra de 5 bits los bits 2 y 4 son los bits de datos. Todos los demás son los bits de redundancia. Todos los bits de paridad cuentan la paridad par de los bits de datos. 47 El bit 1 codifica el negado de la paridad del bit 2. Se utiliza el negado de la paridad para evitar que el marcador que codifica el número 0 sea completamente negro, de forma que no pueda confundirse con elementos del entorno. El bit 3 codifica la paridad del bit 4. El bit 5 codifica la paridad de los bits 2 y 4. A manera de ejemplo, el marcador mostrado en la Figura 4.12 (la representación más a la izquierda) codifica el número 89 como la secuencia binaria 1000010111101110100110111. Para realizar la descodificación de un marcador se calcula la distancia de Hamming de las cinco palabras del marcador con todas las posibles palabras de 5 bits en espera de que la suma de las 5 distancias sea 0. En caso de que no se encuentre una combinación de palabras que tenga distancia 0 se procede a rotar el marcador 90 grados en el sentido del reloj y se vuelve a ejecutar el proceso anterior; esto se repite hasta que se hayan verificado las 4 rotaciones de 90 grados o se encuentre una matriz que cumpla con los requisitos. Si todas las rotaciones poseen distancias mayores a 0 entonces se descarta el marcador. En caso contrario, el número del marcador es ensamblado a partir de los bits de datos. Utilizando el código de bloques descrito se desarrolló un script en el lenguaje de progra- mación Python para generar todos los posibles marcadores que usan dicho código. El listado del código fuente de este script se encuentra en el Anexo B. Como nota adiciona, en el libro Mastering OpenCV with practical computer vision projects [8] se indica que el código utili- zado por los marcadores es un código de Hamming tipo (5, 2), afirmación que no coincide con lo que se encontró durante el desarrollo. Determinación de las transformaciones geométricas Con los marcadores ya identificados y descodificados se procede a determinar cuales son las transformaciónes geométricas del marcador. Para esto se asume que la cámara esta posicionada en el origen del sistema de coordenadas (punto (0, 0, 0)) viendo en la dirección del eje Z negativo (el vector canónico (0, 0, -1) en un sistema de coordenadas de mano derecha como el utilizado por OpenGL). Utilizando los parámetros intŕınsecos de la cámara se ejecuta la función de OpenCV solvePnP, la cual se encarga de calcular las transformaciones de rotación y traslación que hay que aplicar a la cámara de forma que el marcador detectado se vea como un poĺıgono cuadrado espećıfico, que en este caso es un poĺıgono cuadrado canónico de lado 1. Luego estas transformaciones se invierten para obtener las transformaciones que hay que aplicar al poĺıgono canónico para que se ubique sobre el marcador detectado relativo a la posición de la cámara. La Figura 4.14 ilustra este procedimiento. Estas transformaciones son luego combinadas como la matriz de modelado que se aplica al objeto virtual relacionado con el marcador detectado, de forma que dicho objeto virtual se alinee correctamente con el marcador en la imagen de entrada. 48 Figura 4.14: Obtención de las transformaciones de un marcador. 4.2.5. Bomb Game, el escenario de demostración NxtAR incluye un escenario sencillo como demostración de las capacidades de la arquitec- tura titulado Bomb Game. Este juego, como su nombre lo indica, consiste en desarmar una serie de bombas cada una de las cuales posee un mecanismo diferente para ser desactivada. El escenario incluye 3 tipos de bomba diferentes e implementa el modo de acción automática. Figura 4.15: Las bombas del juego Bomb Game. El modo de acción automática de Bomb Game consiste en un recorrido lineal sencillo en el cual el robot, avanzando en ĺınea recta, trata de detectar si existen marcadores a su izquierda cuando pasa cerca de una marca en el suelo que sirve de indicador. Al detectar tantos indicadores como bombas han sido configuradas en el escenario el robot se detiene y se le muestra un resumen al usuario sobre cuántas bombas virtuales fueron detectadas y cuántas se esperaba encontrar. En el modo de control manual se implementó el juego de desactivación de bombas, en el cual el usuario debe manipular al robot dentro del entorno buscando los distintos marcadores que representan las bombas y desactivarlas según sus diferentes mecanismos utilizando el brazo virtual con los controles provistos para tal fin. Como se mencionó, el juego implementa tres bombas: Bomba de cables. 49 Bomba de combinación. Bomba de nivel. La bomba de cables consiste en un conjunto de cables de colores de entre los cuales el usuario debe cortar uno (el azul) posicionando el brazo virtual para tal fin. Por su parte la bomba de combinación posee cuatro botones de colores los cuales deben ser presionados en un orden predeterminado (azul, rojo, negro y luego verde). Por último la bomba de nivel posee un único botón el cual debe ser presionado únicamente cuando el indicador de inclinación del dispositivo está en verde. En el caso de que la aplicación sea ejecutada en dispositivos como la consola OUYA, la cual no posee sensores que le permitan determinar su orientación, este tipo de bomba puede ser desactivada sencillamente con presionar su botón. Las tres bombas pueden verse en la Figura 4.15. 50 Caṕıtulo 5 Pruebas y resultados En este caṕıtulo se describen cinco distintas pruebas a las que fue sometida la implemen- tación de referencia. Las pruebas se enfocaron principalmente en los aspectos más resaltantes de las aplicaciones desarrolladas, en particular lo referente a la transmisión de video, la detec- ción de marcadores y el despliegue de objetos virtuales. Adicionalmente se realizó una prueba con usuarios para comparar los distintos esquemas y dispositivos de control disponibles. Las cinco pruebas se pueden agrupar en las siguientes 3 categoŕıas: pruebas emṕıricas, pruebas estad́ısticas y pruebas de usuario. La prueba número 1 se realizó de manera emṕırica. Las pruebas número 2, 3 y 4 fueron realizadas por medio de un estudio estad́ıstico, y la prueba 5 consistió en una prueba de usuario llevada a cabo por una encuesta de 8 preguntas. 5.1. Medición de distancias óptimas para reconocimien- to de marcadores Esta prueba se diseñó para determinar cuales son las distancias mı́nima y máxima entre las cuales los marcadores pueden ser detectados correctamente para los tres smartphones detallados en la Sección 2.2. Para esto se utilizaron marcadores de 12.5 por 12.5 cent́ımetros, con las cámaras de los dispositivos configuradas para transmitir video a 355 por 288 ṕıxeles con calidad de compresión JPEG fija en 90 (véase la prueba descrita en la Sección 5.3 de este mismo caṕıtulo para una explicación del porque se escogieron dichos valores). Los resultados de esta prueba se pueden observar en la Tabla 5.1. Cabe resaltar que la distancia máxima no representa la distancia a partir de la cual los marcadores no pueden ser detectados en absoluto, sino que a partir de dicha distancia estos son detectados con errores de alineación notables asumiendo que la cámara fue calibrada correctamente. En este con- texto los errores de alineación indican que los objetos virtuales utilizados en el juego no son desplegados correctamente, ya sea por fallas en el cálculo de las transformaciones geométri- cas o por errores notables al hacer coincidir las esquinas del marcador con las esquinas del objeto virtual, entre otros artefactos visuales posibles. 51 Cámara Distancia mı́nima Distancia máxima Galaxy Nexus 19 cm 109.5 cm Galaxy S4 16 cm 88 cm Pantech Burst 19 cm 114.5 cm Tabla 5.1: Distancias de reconocimiento de marcadores por dispositivo. 5.2. Perfilado del procesamiento en el estado de juego Esta prueba fue realizada con la finalidad de determinar el tiempo consumido por las diferentes acciones llevadas a cabo durante el procesamiento de un cuadro en el estado de juego. Estas acciones se dividieron en dos categoŕıas. La primera es todo lo referente al algoritmo de detección de marcadores y la segunda es el resto del procesamiento del cuadro, incluyendo la lógica del juego y el despliegue de objetos 3D. La prueba consistió en la medición del tiempo de procesamiento promedio por cuadro durante un minuto, desglosado en las categoŕıas antes mencionadas. Se probaron dos escena- rios, uno utilizando la Galaxy Tab 2 como dispositivo de control y otro utilizando la consola OUYA. Ambos escenarios fueron repetidos 30 veces cada uno y luego promediados con la intención de mitigar el error estad́ıstico. Todas las medidas fueron tomadas en milisegundos de procesamiento por cuadro. En ambos escenarios el dispositivo con cámara fue un smartp- hone Galaxy Nexus y el video fue fijado a una resolución de 352 por 288 ṕıxeles, utilizando compresión JPEG con el parámetro de calidad fijo en 90. 5.2.1. Resultados y análisis El primer análisis sobre los datos capturados fue el comparar el desempeño entre los 2 dispositivos utilizados, análisis que puede observarse en el gráfico de la Figura 5.1. Lo más relevante en este gráfico es el hecho de que el tiempo de procesamiento dedicado a la detección de marcadores consume la mayor parte del tiempo total dedicado a cada cuadro en ambos dispositivos. Aśı mismo se puede observar que en ambos dispositivos el tiempo total dedicado al procesamiento de cuadros es considerablemente alto, siendo en promedio aproximadamente 180 milisegundos por cuadro en la Galaxy Tab 2 y 100 milisegundos por cuadro en la consola OUYA. Esto permite procesar entre unos 5,5 y 10 cuadros por segundo en cada dispositivo respectivamente. Luego se calculó el porcentaje de tiempo consumido por ambas categoŕıas de procesa- miento y se obtuvieron los gráficos mostrados en la Figura 5.2. Como se puede apreciar, más del 75 % del tiempo de procesamiento se dedica a la detección de marcadores en ambos dispositivos. De este estudio y del anterior se puede concluir entonces que el algoritmo de detección de marcadores es candidato a ser optimizado, de forma que se pueda obtener una mejor tasa de cuadros por segundo en los dispositivos de control, lo que debeŕıa traducirse en mejor experiencia de usuario. Adicionalmente se realizó un estudio de la distribución de estos tiempos para ambos dispositivos utilizando el box plot mostrado en la Figura D.1. En el gráfico puede observarse 52 Figura 5.1: Tiempo promedio de procesamiento por cuadro durante 1 minuto. que la consola OUYA generó unas medidas de tiempo mucho más estables comparadas con las medidas arrojadas por la Galaxy Tab 2. Esto es de esperarse dado que la consola OUYA es un sistema de tiempo real suave sin multitarea, caracteŕısticas diametralmente opuestas a las provistas por la tablet. 5.3. Tasas de recepción de video en el dispositivo de control En esta prueba se midió la tasa de recepción de video para diferentes tamaños de imagen y calidades de compresión JPEG. El objetivo fue determinar el valor óptimo para ambos parámetros. Se probaron 3 escenarios con imágenes de dimensiones ubicadas entre 176x144 ṕıxeles, 352x288 ṕıxeles y 720x576 ṕıxeles, variando la calidad de la compresión JPEG de los cuadros entre 1 y 100 inclusive, en pasos de 10 en 10 para cada escenario. De estos escenarios solo se conservaron los dos últimos dado que la biblioteca OpenCV fallaba al tratar de realizar la calibración de la cámara con las imágenes de tamaño 176x144 ṕıxeles. Cada escenario fue repetido 10 veces por cada calidad de compresión, teniendo cada repetición una duración de 10 segundos y además utilizando un punto de acceso inalámbrico dedicado. Todas las pruebas fueron realizadas con la Galaxy Tab 2. 5.3.1. Resultados y análisis En la Figura 5.3 se pueden observar las tasas de transmisión promedio para los dos esce- narios probados. Como puede observarse en ambos escenarios la tasa de recepción disminuye 53 Figura 5.2: Porcentaje de tiempo consumido por distintos procesamientos. (a) Caso de estudio Galaxy Tab 2. (b) Caso de estudio OUYA. a medida de que se incrementa la calidad de la compresión, presentándose la mayor pérdida de cuadros de video cuando dicho parámetro supera el valor de 90. De hecho cuando la compresión alcanza el valor de 100 la pérdida de imágenes es casi absoluta. Otro detalle que resalta es el hecho de que la tasa de recepción disminuye a un poco menos de la mitad en el segundo escenario con respecto al primero. Figura 5.3: Tasas de transmisión de video para diferentes calidades de compresión JPEG. (a) Para imágenes de 352x288 ṕıxeles. (b) Para imágenes de 720x576 ṕıxeles En base a estos resultados concluimos que los valores óptimos para los parámetros en estudio son el tamaño de imágen de 355 por 288 ṕıxeles y la calidad de compresión en 90. Escogimos estos valores porque de esta forma la tasa de recepción de imágenes de video 54 se aproxima bastante al tiempo de procesamiento del juego en el mejor caso detectado (10 cuadros por segundo). Esto disminuye la cantidad de imágenes desperdiciadas cuando el procesador del dispositivo de control no es capaz de procesarlas lo suficientemente rápido. 5.4. Determinación del punto de saturación del video Esta prueba se diseñó con el objetivo de examinar el comportamiento de la transmisión y recepción del video enviado entre los dispositivos móviles involucrados en la arquitectura. Dada la ubicuidad de los puntos de acceso inalámbricos en los entornos laborales y caseros se busca determinar la posibilidad de insertar la arquitectura desarrollada en una red existente. Para realizar este estudio se utilizó el punto de acceso inalámbrico descrito en la Sección 2.2 como punto intermedio, y la tablet Galaxy Tab 2 como dispositivo de control. Se utilizó un enfoque bottom-up, midiendo las tasas de recepción y pérdida de cuadros de video por segundo para diferentes escenarios de utilización del canal de transmisión. Para llevar a cabo la saturación del canal se utilizó el sistema D-ITG (Distributed Internet Traffic Generator - Generador de Tráfico para Internet Distribuido) descrito en [13], el cual puede generar distintos tipos de tráfico para diversos protocolos de capa de red y transporte, utilizando procesos constantes o estocásticos. En este estudio se generó un tŕafico constante utilizando paquetes de 1500 bytes, y variando la tasa de paquetes transmitidos por segundo para saturar el canal con tasas de transmisión de bits que vaŕıan entre 0 y 9 Mbps, incrementando la tasa de transmisión de bits en pasos de 1 Mbps en cada escenario. Cada escenario constó de una duración de 10 segundos y fue repetido 10 veces, para luego ser promediado. La tasa de paquetes por segundo para cada escenario fue calculada despejando x en la Ecuación 5.1. yKbps = 12 Kb paquete ∗ x paquete s ; ∀y = 1000, 2000, . . . , 9000 (5.1) 5.4.1. Resultados y análisis Una vez completado el estudio diseñado para esta prueba se obtuvieron los datos que pueden observarse en el gráfico de la Figura 5.4. Como se puede apreciar, cuando el punto de acceso está dedicado exclusivamente a la transmisión del video los resultados coinciden con los obtenidos en la prueba descrita en la Sección 5.3. Sin embargo, a medida que se comienza a inyectar tráfico en el canal el rendimiento de la transmición comienza a degradarse, obte- niendo una tasa de pérdida de cuadros mayor que la tasa de cuadros recibidos correctamente a medida que el tráfico se acerca y eventualmente supera los 8 Mbps. Tomando como un mı́nimo aceptable una transmisión de video de 10 cuadros recibidos por segundo1 (correcta o incorrectamente), podemos establecer el punto de saturación de la transmisión de video alrededor de los 5 Mbps, punto en el cual la tasa de cuadros recibidos correctamente es considerablemente menor a 9 cuadros por segundo, lo cual implica que aproximadamente 2 de cada 10 cuadros por segundo poseen errores que no permiten su 1Una tasa menor a 10 cuadros por segundo se considera que provee una muy mala experiencia [5]. 55 Figura 5.4: Comportamiento de la recepción de video para distintas tasas de utilización del canal de transmisión. decodificación. Más allá de 5 Mbps la tasa de cuadros recibidos independientemente de su estado alcanza niveles muy bajos, a medida que la tasa de cuadros perdidos se hace aún mayor. 5.5. Pruebas de usuario En esta prueba se realizó un estudio de usuario para determinar cuál de los dos esquemas de control es el preferido por los mismos. Se le pidió a 52 usuarios que jugaran una partida de Bomb Game utilizando tanto la pantalla táctil de la Galaxy Tab 2 como el gamepad de la consola OUYA, después de lo cual se les realizó el cuestionario que se encuentra en el Anexo C. Durante la realización de la prueba los usuarios estuvieron ubicados en una habitación diferente a la que alojaba al robot, de forma que todo el control fuera realizado por teleoperación. Esta prueba tuvo como objetivo el determinar cuál de los dos dispositivos de control provee la mejor experiencia de usuario, aśı como el obtener la apreciación de los usuarios con respecto a las interfaces gráficas de usuario desarrolladas, en particular la interfaz utilizada en las tablets como se muestra en la Figura 4.8.d. 2La prueba se realizó con solamente 5 usuarios siguiendo las recomendaciones de Nielsen en [55]. 56 5.5.1. Resultados y análisis En la primera pregunta del cuestionario se le pidió a los usuarios que indicaran cuál de los dos dispositivos de control evaluados ofrece la mejor experiencia de usuario, tomando en cuenta factores como la comodidad de uso, los tiempos de respuesta y de despliegue, entre otros. Como puede observarse en la Figura 5.5 la mayoŕıa de los usuarios prefirió la expe- riencia de uso provista por la consola de videojuegos OUYA, con solo un usuario prefiriendo la experiencia provista por la tablet Galaxy Tab 2. Figura 5.5: Preferencia de los usuarios por los diferentes dispositivos de control. Con la segunda pregunta realizada se le pidió a los usuarios que evaluaran los dos meca- nismos de control, independientemente de factores relacionados al dispositivo. En particular se buscó determinar entre la pantalla táctil y el gamepad cual es el preferido por los usuarios. En la Figura 5.6 se puede observar que 3 de los 5 usuarios prefirieron utilizar el gamepad como mecanismo de control. La tercera pregunta consistió en evaluar el tiempo de respuesta de la pantalla táctil, entendiéndose por tiempo de respuesta el tiempo transcurrido entre el uso del mecanismo de control y la correspondiente respuesta ejecutada por el robot. Como puede observarse en la Figura 5.7 la apreciación del tiempo de respuesta de la pantalla táctil es favorable. De igual forma se le pidió a los usuarios su apreciación del tiempo de respuesta del gamepad, apreciación que puede verse en la Figura 5.8, correspondiente a la cuarta pregunta. En este caso la mayoŕıa de los usuarios consideraron que el gamepad ofrece un tiempo de respuesta excelente, resultados que contrastan con los obtenidos para el caso de la pantalla táctil. 57 Figura 5.6: Preferencia de los usuarios por los diferentes mecanismos de control. En la pregunta 5 los usuarios evaluaron la dificultad de controlar al robot de forma remota. Los resultados de esta pregunta se pueden observar en la Figura 5.9. Como puede verse en los resultados, los usuarios consideraron al control remoto del robot como una actividad de dificultad promedio tendiendo a fácil, con un único usuario indicando que dicha actividad le pareció dif́ıcil de realizar. Con la pregunta 6 se pidió a los usuarios que dieran su apreciación acerca de la distribución de los elementos de la interfaz de juego en la pantalla táctil de la tablet Galaxy Tab 2 (visible en la Figura 4.8.d). Se establecieron como criterios a considerar la organización temática de los elementos, aśı como su facilidad de acceso. Como puede verse en el gráfico de la Figura 5.10, los usuarios consideraron la distribución de los elementos como una distribución decente tendiendo a buena. En la pregunta 7 se le pidió a los usuarios que evaluaran la utilidad de la pantalla de ayuda implementada, evaluación mostrada en la Figura 5.11. Como puede observarse los usuarios consideraron que la pantalla de ayuda es un elemento útil durante el transcurso del juego desarrollado. Por último en la pregunta 8 se pidió a los usuarios que evaluaran la dificultad de controlar el brazo virtual. Los resultados pueden observarse en la Figura 5.12. En este caso 3 de los 5 usuarios consideraron que controlar el brazo virtual es una taréa dif́ıcil, con 2 usuarios considerándolo una tarea fácil. 58 Figura 5.7: Apreciación del tiempo de respuesta de la pantalla táctil. Figura 5.8: Apreciación del tiempo de respuesta del gamepad. 59 Figura 5.9: Apreciación de la dificultad de controlar al robot de forma remota. Figura 5.10: Apreciación de la interfaz en la tablet. 60 Figura 5.11: Utilidad de la pantalla de ayuda durante el transcurso de una partida. Figura 5.12: Dificultad de controlar el brazo virtual del robot. 61 Caṕıtulo 6 Conclusiones En este trabajo de investigación se diseño y desarrolló una arquitectura de control para robots móbiles sobre la cual es posible implementar juegos que hacen uso de tecnoloǵıas de realidad aumentada. Para esto se realizó un estudio conciso acerca de las distintas tecnoloǵıas y herramientas disponibles tanto en el campo de la robótica como en el de la realidad aumen- tada. La arquitectura de control fue diseñada para ser genérica, es decir, independiente del hardware utilizado en su implementación, limitándose a plantear un esquema de comunica- ción entre los dispositivos involucrados en la misma, aśı como un conjunto de posibles casos de uso que se deben cumplir. Esta arquitectura fue implementada luego en varios dispositivos móviles basados en el sistema operativo Android, adicionalmente utilizando un robot LEGO Mindstorms NXT. Para llevar a cabo esta implementación de referencia se realizó primero un estudio del hardware disponible sobre el cual se realizó el desarrollo. La implementación fue desarrollada para soportar tanto tablets basadas en el sistema operativo Android como la consola de videojuegos OUYA como dispositivos de control del robot. Adicionalmente se diseñó un conjunto de cinco escenarios de prueba con el objetivo de evaluar distintos aspectos de la implementación de referencia. En particular se evaluaron aspectos relacionados a la transmisión de video entre los dispositivos involucrados, cuánto tiempo toma el procesamiento de dicho video cuadro por cuadro y diversos puntos relacio- nados con la usabilidad de la implementación. Con estas pruebas se pudo determinar las limitaciones de la transmisión de video, lo que nos permitió establecer diferentes parámetros para garantizar que este proceso no afecte la experiencia de usuario. Aśı mismo se pudo de- terminar que el procesamiento del video es la operación más costosa en cuanto a tiempo de ejecución dentro de la aplicación de control, tomando incluso más de tres cuartas partes del tiempo de procesamiento por cuadro. En el caso de las pruebas de usuario se determinó que la consola OUYA y el uso de gamepads en general provee una experiencia de usuario más satisfactoria para el control del robot móvil en comparación con la experiencia provista por la tablet. En base a lo antes planteado se puede concluir que los objetivos propuestos al comienzo del trabajo fueron cumplidos a cabalidad, obteniendo como resultado un sistema de control basado en realidad aumentada para el sistema operativo Android. 62 6.1. Contribuciones Las contribuciones de este trabajo de investigación son las siguientes: Se diseñó una arquitectura genérica para desarrollar sistemas de control para robots que incorporen juegos y simulaciones de realidad aumentada. Esta arquitectura define un modelo sencillo de casos de uso y un esquema de comunicación sin entrar en muchos detalles ni exigiendo algoritmos o criterios de implementación espećıficos, lo cual per- mite que los desarrolladores que deseen utilizar esta arquitectura puedan incorporar paradigmas adicionales dependiendo del objetivo principal de cada implementación. Se desarrollaron de forma satisfactoria múltiples aplicaciones para el sistema operativo Android y los robots LEGO Mindstorms NXT, las cuales implementan completamente los casos de uso propuestos por la arquitectura. Con esto se demuestra que la arqui- tectura diseñada es lo suficientemente completa como para cumplir con los objetivos propuestos al comienzo de este trabajo de investigación. Se aplicaron de forma satisfactoria los patrones de diseño state pattern y Entidad- Componente-Sistema, lo que demuestra su utilidad en el desarrollo de aplicaciones de realidad aumentada. 6.2. Limitaciones La principal limitación de este trabajo radicó en la falta de más dispositivos móviles para utilizar como dispositivos de control, lo que limitó la realización de las pruebas a solo dos dispositivos completamente diferentes. 6.3. Trabajos futuros Se proponen los siguientes trabajos a futuro: Optimizar el algoritmo de detección de marcadores para obtener menores tiempos de procesamiento. Diseñar e implementar nuevos escenarios de juego de mayor complejidad. Portar la implementación de referencia a otros dispositivos de control, en particular a computadoras de escritorio, aprovechando las capacidades del framework LibGDX. Evaluar la factibilidad de extender la arquitectura para soportar múltiples saltos de red entre los distintos dispositivos involucrados, en particular entre el dispositivo de control y el dispositivo con cámara, de forma que el usuario pueda encontrarse a una mayor distancia del robot que la permitidas por un único punto de acceso inalámbrico. 63 Evaluar la factibilidad de agregar soporte de realidad aumentada sin marcadores a la arquitectura y a la implementación de referencia. 64 Anexos 65 Anexo A El robot LEGO Mindstorms TM El Mindstorms es un proyecto de The LEGO Group que tiene como objetivo el democra- tizar el acceso a la robótica proveyendo un kit de piezas t́ıpicas de la ĺınea LEGO Technic R© junto a un bloque inteligente, llamado comúnmente como el brick. Este kit permite construir y programar robots modulares apoyados por diversos paquetes de software que permiten pro- gramar el comportamiento del robot. El nombre Mindstorms se debe al libro Mindstorms: Children, computers, and powerful ideas publicado por Seymour Papert en 1980 [58]. En este anexo se detalla la historia del LEGO Mindstorms, comenzando por el Pro- grammable Brick del MIT, proyecto en el cual se basa The LEGO Group para desarrollar el Mindstorms. Luego se exponen las diferentes versiones del LEGO Mindstorms, desde el Mindstorms RCX del kit Mindstorms Robotics Invention System (RIS), pasando por el Mindstorms NXT, y por ultimo el Mindstorms EV3. A.1. El MIT Programmable Brick El MIT Programmable Brick fue un proyecto de investigación llevado a cabo por Resnick y colaboradores en asociación con The LEGO Group entre 1987 y 1998 [64]. El objetivo del proyecto era diseñar y desarrollar una micro-computadora programable que estaŕıa co- locada dentro de un bloque de LEGO. Este proyecto estaba enmarcado dentro del área de la Computación Ubicua, con la intención de llevar este tipo de computación a entornos educativos infantiles en un esquema de educación marcado por el constructivismo [64] [63]. Resnick y colaboradores se inspiraron en un proyecto anterior llamado LEGO/Logo, el cual integraba un veh́ıculo programable constrúıdo con piezas de LEGO, junto al lenguaje de programación Logo. LEGO/Logo permit́ıa programar el veh́ıculo de la misma forma que se programaban los llamados robots Tortuga [1]. El trabajo de Resnick y colaboradores se encargó, entre otras cosas, de incorporar sensores al sistema y de eliminar el cableado que poséıan los veh́ıculos de LEGO/Logo. El resultado fue un brick controlador programable que funcionaba con bateŕıas, y pod́ıa incorporar distintos sensores o motores según fuera necesario para el modelo que se estuviera trabajando. Se diseñaron cuatro versiones diferentes del Programmable Brick [9], la última de las cuales puede observarse en la Figura A.1. 66 Figura A.1: El MIT Programmable Brick . Imagen recuperada de [9]. A.2. Evolución del LEGO Mindstorms En 1998, al mismo tiempo que Resnick y colaboradores publicaban su trabajo sobre el Programmable Brick, The LEGO Group por su parte lanzó al mercado un rediseño completo del mismo al que llamaron RCX, comercializándolo como el Mindstorms Robotics Invention System, mejor conocido por sus siglas RIS. Con esto se comenzó la linea de productos LEGO Mindstorms la cual hasta el momento ha pasado por tres grandes iteraciones, que se describen a continuación. A.2.1. LEGO Mindstorms RCX El kit Mindstorms Robotics Invention System RCX fue la primera iteración del paquete Mindstorms. Estuvo disponible comercialmente entre 1998 y 2006, y consist́ıa en un conjunto de aproximadamente 700 piezas, incluyendo el brick RCX, los motores, los sensores y la documentación. El brick RCX era la unidad central de procesamiento del kit, la cual puede verse en la Figura A.2. Este brick estaba compuesto por un microcontrolador programable Hitachi H8/3292 que corŕıa a una frecuencia de reloj de 16 MHz. El Hitachi H8 contaba con 16 KB de memoria RAM en el chip y un bloque ROM de 512 bytes en el cual estaba embebido el firmware básico del sistema y algunos programas de demostración. El RCX contaba adicio- nalmente con 32 KB de memoria RAM externa en la cual se almacenaban el software de 67 sistema susceptible de ser actualizado, y los programas instalados por el usuario(a). El RCX contaba con tres puertos de entrada para sensores y tres puertos de salida para motores. Los sensores y los motores utilizaban conectores propietarios con forma de piezas de LEGO [9]. La transmisión de datos entre el brick y la computadora del usuario se realizaba mediante un puerto para comunicaciones seriales por infrarrojo incorporado en el brick, el cual se conectaba con una pieza llamada IR tower. Se comercializaron dos versiones de esta IR Tower. La primera se inclúıa con el RCX 1.0 y el RCX 1.5 y se conectaba a la computadora por medio de un puerto serial RS-232, necesitando bateŕıas propias. La segunda versión formaba parte del kit RCX 2.0 y utilizaba el puerto USB eliminando la necesidad de bateŕıas. La IR Tower pod́ıa utilizarse también para enviar comandos directos al RCX. El alcance de la IR Tower estaba entre 60 cent́ımetros y 8 metros [9]. Figura A.2: El Brick del Mindstorms RCX 2.0. Imagen recuperada de [9]. A.2.2. LEGO Mindstorms NXT El kit Mindstorms NXT es la segunda iteración del paquete Mindstorms disponible co- mercialmente desde julio de 2006, con una actualización llamada NXT 2.0 disponible desde agosto del 2009. Este kit consiste en un conjunto de 577 piezas entre las cuales se encuentran el brick controlador, los sensores, un CD con software asociado y stickers. El brick está com- puesto por un procesador ARM Atmel de 32 bits que funciona a 45 MHz. Este procesador dispone de 64 KB de memoria RAM, y 256 KB de memoria flash para almacenamiento per- sistente. Adicionalmente el brick contiene un coprocesador AVR Atmel de 8 MHz. Se puede establecer comunicación con el brick utilizando ya sea la interfaz USB 2.0 o Bluetooth. Para lo segundo el brick dispone de un chip Bluecore 4 que implementa el perfil de puerto serial de Bluetooth 2.0. El kit producido por The LEGO Group incluye 4 sensores: un sensor táctil, un sensor de sonido, un sensor ultrasónico y un sensor puntual monocromático de luz. En el NXT 2.0 el sensor de luz es sustitúıdo por un sensor de color RGB. Adicionalmente se pueden adquirir distintos sensores compatibles fabricados por entidades externas a LEGO. 68 Tanto los sensores como los servomotores del kit utilizan conectores propietarios basados en RJ12. Una imagen del brick puede observarse en la Figura A.3. Este brick necesita de seis bateŕıas AA para funcionar. Figura A.3: El brick del Mindstorms NXT. Imagen recuperada de generationrobots.com. The LEGO Group provee varios conjuntos de documentación, llamados Developer Kits, enfocados a usuarios avanzados del sistema. Existen 3 de estos Developer Kits : el BDK [33], que detalla el funcionamiento del protocolo de comunicación LCP (LEGO Communication Protocol) utilizado por el brick y el hardware de la interfaz Bluetooth; el HDK [34] que detalla el funcionamiento y la estructura electrónica del hardware del brick y los sensores estándar; y el SDK [35], que detalla el software controlador del robot y el funcionamiento de la máquina virtual que ejecuta programas de usuario en el brick. Los robots Mindstorms NXT pueden ser programados usando una gran cantidad de tec- noloǵıas y lenguajes de programación. El entorno básico es provisto por The LEGO Group con el kit y consiste en un IDE (Integrated Development Environmet - Entorno de Desarrollo Integrado) visual, llamado NXT-G. Este entorno se caracteriza por ser sumamente fácil de aprender y utilizar. Los robots se programan combinando series de bloques que se ejecu- tan linealmente sobre carriles paralelos. Estos bloques controlan los distintos componentes 69 http://www.generationrobots.com electrónicos del robot y pueden combinarse utilizando diferentes estructuras de control como lo son condicionales y ciclos. De manera alternativa, dada la calidad abierta del hardware y el software de los kits Mindstorms NXT, existe una gran variedad de entornos de desarrollo extraoficiales a The LEGO Group. Algunos de los más conocidos son el proyecto LejOS1, que suplanta al firmware estándar de los bricks NXT y permite ejecutar programas escritos en un subconjunto del lenguaje Java sobre el robot. Otro es el proyecto NXC2 (Not eXactly C ), que permite compilar programas escritos en un lenguaje similar a C para ser ejecutados sobre el firmware básico. También es posible utilizar entornos especializados para robótica como lo son el Microsoft Robotics Developer Studio 43, y el sistema Player/Stage4. A.2.3. LEGO Mindstorms EV3 El kit LEGO Mindstorms EV3 es la tercera iteración del paquete Mindstorms, pensado para estar disponible comercialmente en otoño de 2013. Este nuevo kit agrega una gran cantidad de nuevas funcionalidades a la serie Mindstorms, tomando ventaja de las tecnoloǵıas móviles que han aparecido en los años recientes desde que el Mindstorms NXT 2.0 fue introducido al mercado. El brick del Mindstorms EV3 puede verse en la Figura A.4. Figura A.4: El brick del Mindstorms EV3. Imagen recuperada de www.theverge.com. 1http://lejos.sourceforge.net 2http://bricxcc.sourceforge.net/nbc 3https://www.microsoft.com/robotics 4http://playerstage.sourceforge.net 70 http://www.theverge.com http://lejos.sourceforge.net http://bricxcc.sourceforge.net/nbc https://www.microsoft.com/robotics http://playerstage.sourceforge.net Entre las nuevas caracteŕısticas incorporadas al Mindstorms EV3 se incluyen las siguien- tes: Un nuevo procesador de tipo ARM 9. 16 MB de memoria ram. Compatibilidad con redes Wi-Fi utilizando un dongle usb. Sistema operativo basado en GNU\Linux en el brick. Interoperabilidad de fábrica con sistemas Android TM e iOS TM . Un nuevo sensor infrarrojo y sensores clásicos mejorados. Retro-compatibilidad con los sensores del Mindstorms NXT. Posibilidad de expandir el almacenamiento interno con tarjetas micro SD. Cuatro puertos para servomotores. Adicionalmente el brick tiene soporte para localización en 10 idiomas, y los nuevos senso- res son al menos tres veces más rápidos que los sensores del Mindstorms NXT. Los sensores del EV3 pueden realizar alrededor de 1000 capturas por segundo, en contraste con los del NXT que realizan unas 330 capturas por segundo dependiendo del sensor [37]. 71 Anexo B Listado del código de generación de marcadores #! /usr / b in /env python import pygame OUTPUT SIZE = (700 , 700) def generateMarker ( b i t s ) : ””” Genera un marcador de 5x5 u t i l i z a n d o e l cod igo d e f i n i d o en e l l i b r o ’ Mastering OpenCV with p r a c t i c a l computer v i s i o n p r o j e c t s ’ ””” par i ty0 = False par i ty1 = False par i ty2 = False par i ty3 = False par i ty4 = False par i ty5 = False par i ty6 = False par i ty7 = False par i ty8 = False par i ty9 = False par i ty10 = False par i ty11 = False par i ty12 = False par i ty13 = False par i ty14 = False # Crea una s u p e r f i c i e negra . marker = pygame . Sur face ( ( 7 , 7 ) ) marker . f i l l ( ( 0 , 0 , 0 ) ) # Bloquea l a s u p e r f i c i e para modi f i car sus p i x e l e s . marker . l o ck ( ) 72 # Rel l ena l a s c e l d a s de datos . i f b i t s [ 0 ] == True : marker . s e t a t ( ( 4 , 5 ) , (255 , 255 , 255)) i f b i t s [ 1 ] == True : marker . s e t a t ( ( 2 , 5 ) , (255 , 255 , 255)) i f b i t s [ 2 ] == True : marker . s e t a t ( ( 4 , 4 ) , (255 , 255 , 255)) i f b i t s [ 3 ] == True : marker . s e t a t ( ( 2 , 4 ) , (255 , 255 , 255)) i f b i t s [ 4 ] == True : marker . s e t a t ( ( 4 , 3 ) , (255 , 255 , 255)) i f b i t s [ 5 ] == True : marker . s e t a t ( ( 2 , 3 ) , (255 , 255 , 255)) i f b i t s [ 6 ] == True : marker . s e t a t ( ( 4 , 2 ) , (255 , 255 , 255)) i f b i t s [ 7 ] == True : marker . s e t a t ( ( 2 , 2 ) , (255 , 255 , 255)) i f b i t s [ 8 ] == True : marker . s e t a t ( ( 4 , 1 ) , (255 , 255 , 255)) i f b i t s [ 9 ] == True : marker . s e t a t ( ( 2 , 1 ) , (255 , 255 , 255)) # Calcu la l a s par idades . par i ty0 = b i t s [ 0 ] pa r i ty1 = not b i t s [ 1 ] pa r i ty2 = b i t s [ 0 ] i s not b i t s [ 1 ] pa r i ty3 = b i t s [ 2 ] pa r i ty4 = not b i t s [ 3 ] pa r i ty5 = b i t s [ 2 ] i s not b i t s [ 3 ] pa r i ty6 = b i t s [ 4 ] pa r i ty7 = not b i t s [ 5 ] pa r i ty8 = b i t s [ 4 ] i s not b i t s [ 5 ] pa r i ty9 = b i t s [ 6 ] par i ty10 = not b i t s [ 7 ] par i ty11 = b i t s [ 6 ] i s not b i t s [ 7 ] par i ty12 = b i t s [ 8 ] par i ty13 = not b i t s [ 9 ] 73 par i ty14 = b i t s [ 8 ] i s not b i t s [ 9 ] # Rel l ena l a s c e l d a s de paridad . i f par i ty0 == True : marker . s e t a t ( ( 3 , 5 ) , (255 , 255 , 255)) i f par i ty1 == True : marker . s e t a t ( ( 1 , 5 ) , (255 , 255 , 255)) i f par i ty2 == True : marker . s e t a t ( ( 5 , 5 ) , (255 , 255 , 255)) i f par i ty3 == True : marker . s e t a t ( ( 3 , 4 ) , (255 , 255 , 255)) i f par i ty4 == True : marker . s e t a t ( ( 1 , 4 ) , (255 , 255 , 255)) i f par i ty5 == True : marker . s e t a t ( ( 5 , 4 ) , (255 , 255 , 255)) i f par i ty6 == True : marker . s e t a t ( ( 3 , 3 ) , (255 , 255 , 255)) i f par i ty7 == True : marker . s e t a t ( ( 1 , 3 ) , (255 , 255 , 255)) i f par i ty8 == True : marker . s e t a t ( ( 5 , 3 ) , (255 , 255 , 255)) i f par i ty9 == True : marker . s e t a t ( ( 3 , 2 ) , (255 , 255 , 255)) i f par i ty10 == True : marker . s e t a t ( ( 1 , 2 ) , (255 , 255 , 255)) i f par i ty11 == True : marker . s e t a t ( ( 5 , 2 ) , (255 , 255 , 255)) i f par i ty12 == True : marker . s e t a t ( ( 3 , 1 ) , (255 , 255 , 255)) i f par i ty13 == True : marker . s e t a t ( ( 1 , 1 ) , (255 , 255 , 255)) i f par i ty14 == True : marker . s e t a t ( ( 5 , 1 ) , (255 , 255 , 255)) # Libera l a s u p e r f i c i e . marker . unlock ( ) return marker def addOne( b i t s ) : ””” Suma 1 a un numero entero p o s i t i v o c od i f i c a do como una l i s t a de b i t s . ””” i f b i t s [ 0 ] == False : b i t s [ 0 ] = True else : b i t s [ 0 ] = False 74 for i in range (1 , 1 0 ) : i f b i t s [ i ] == False : b i t s [ i ] = True break else : b i t s [ i ] = Fal se def main ( ) : ””” El metodo p r i n c i p a l . Genera l o s 1024 marcadores numerados d e l 0 a l 1023 ””” code = 0 b i t s = [ False , False , False , False , False , False ,\ False , False , False , Fa l se ] done = False pygame . i n i t ( ) output = pygame . Sur face (OUTPUT SIZE) while not done : try : # Genera e l marcador y l o guarda en d i s co . marker = generateMarker ( b i t s ) pygame . trans form . s c a l e ( marker , OUTPUT SIZE, output ) f i l e n a m e = ” marker ” + s t r ( code ) + ” . png” pygame . image . save ( output , f i l e n a m e ) # Calcu la e l s i g u i e n t e codigo code += 1 addOne( b i t s ) # Termina cuando todos l o s marcadores # han s ido generados . i f code >= 1024 : done = True except KeyboardInterrupt : done = True pygame . qu i t ( ) i f name == ” main ” : # Punto de entrada . main ( ) 75 Anexo C Cuestionario utilizado en la prueba de usuario 1. ¿De los dos dispositivos utilizados cuál le parece que ofrece la mejor experiencia en general? a) La tablet b) La consola de videojuegos 2. De entre los dos esquemas de control ¿cuál es su preferido? a) Pantalla táctil b) Gamepad 3. ¿Cómo considera el tiempo de respuesta de la pantalla táctil? a) Excelente b) Bueno c) Decente d) Deficiente e) Malo 4. ¿Cómo considera el tiempo de respuesta del gamepad? a) Excelente b) Bueno c) Decente d) Deficiente e) Malo 76 5. ¿Cómo calificaŕıa usted la dificultad de controlar al robot de forma remota? a) Muy dif́ıcil b) Dif́ıcil c) Normal d) Fácil e) Muy fácil 6. ¿Cómo calificaŕıa la distribución de los elementos de interfaz en la tablet? Con 5 indi- cando una muy buena distribución y 1 indicando una muy mala distribución. a) 5 b) 4 c) 3 d) 2 e) 1 7. Con respecto al juego, ¿que tan útil le pareció la pantalla de ayuda? a) Muy útil b) Suficientemente útil c) Parcialmente útil d) Parcialmente inútil e) Completamente inútil 8. Con respecto al juego, ¿Cómo calificaŕıa la dificultad de control del “brazo” virtual? a) Muy dif́ıcil b) Dif́ıcil c) Normal d) Fácil e) Muy fácil 77 Anexo D Datos y gráficos adicionales de las pruebas realizadas En este anexo se incluyen tablas con los datos estad́ısticos recabados durante la realización de las pruebas descritas en el Caṕıtulo 5. Adicionalmente se muestran gráficos generados a partir de estos datos que sirven para complementar lo descrito en dicho Caṕıtulo. Figura D.1: Distribución del tiempo promedio de procesamiento por cuadro. 78 Valor estad́ıstico Galaxy Tab 2 OUYA Mı́nimo 145 98 Percentil 25 178.25 102 Mediana 183 104 Percentil 75 189.75 108 Máximo 221 112 Desviación estándar 17.03 4.05 Tabla D.1: Datos estad́ısticos del tiempo promedio de procesamiento por cuadro. Figura D.2: Distribución de las tasas de recepción de video. Valor 352x288 352x288 352x288 720x576 720x576 720x576 estad́ıstico recibidos perdidos correctos recibidos perdidos correctos Mı́nimo 6.04 0.03 1.67 2.09 0.04 0.17 Percentil 25 12.21 0.15 11.98 4.56 0.09 4.35 Mediana 12.75 0.19 12.57 5.01 0.13 4.81 Percentil 75 13.19 0.36 12.98 5.35 0.26 5.29 Máximo 13.82 4.37 13.77 5.9 2.1 5.87 Desviación 2.14 1.26 3.37 1.02 0.76 1.94 estándar Tabla D.2: Datos estad́ısticos de la recepción de video. 79 Figura D.3: Distribución del comportamiento de la recepción de video. Valor cuadros cuadros cuadros estad́ıstico recibidos perdidos correctos Mı́nimo 4.87 0.69 0.83 Percentil 25 9.64 1.10 7.18 Mediana 10.43 1.62 8.69 Percentil 75 11.9 2.6 9.79 Máximo 11.51 4.28 10.82 Desviación 2.004 1.28 3.2 estándar Tabla D.3: Datos estad́ısticos del comportamiento de la recepción de video. 80 Referencias [1] Abelson, Harold y Andrea A Di Sessa: Turtle geometry: The computer as a medium for exploring mathematics. the MIT Press, 1986. [2] Adam, Eugene C: Fighter cockpits of the future. En Digital Avionics Systems Conferen- ce, 1993. 12th DASC., AIAA/IEEE, páginas 318–323. IEEE, 1993. [3] Alatalo, Toni: An entity-component model for extensible virtual worlds. Internet Com- puting, IEEE, 15(5):30–37, 2011. [4] Alt, Christopher von: Autonomous underwater vehicles. En Autonomous Underwater Lagrangian Platforms and Sensors Workshop, volumen 3, 2003. [5] Apteker, Ronnie T, James A Fisher, Valentin S Kisimov y Hanoch Neishlos: Video acceptability and frame rate. IEEE multimedia, 2(3):32–40, 1995. [6] Azuma, Ronald, Yohan Baillot, Reinhold Behringer, Steven Feiner, Simon Julier y Blair MacIntyre: Recent advances in augmented reality. Computer Graphics and Applications, IEEE, 21(6):34–47, 2001. [7] Azuma, Ronald T y cols.: A survey of augmented reality. Presence-Teleoperators and Virtual Environments, 6(4):355–385, 1997. [8] Baggio, Daniel Lélis: Mastering OpenCV with practical computer vision projects. Packt Publishing Ltd, 2012. [9] Bagnall, Brian: Core Lego Mindstorms Programming. Prentice Hall, 2002. [10] Bajura, Michael, Henry Fuchs y Ryutarou Ohbuchi: Merging virtual objects with the real world: Seeing ultrasound imagery within the patient. En ACM SIGGRAPH Computer Graphics, volumen 26, páginas 203–210. ACM, 1992. [11] Behrens, Alexander, Linus Atorf, Robert Schwann, Bernd Neumann, Rainer Schnitz- ler, Johannes Balle, Thomas Herold, Aulis Telle, Tobias G Noll, Kay Hameyer y cols.: MATLAB meets LEGO Mindstorms—A freshman introduction course into practical en- gineering. Education, IEEE Transactions on, 53(2):306–317, 2010. 81 [12] Bilas, Scott: A data-driven game object system. En Game Developers Conference Pro- ceedings, 2002. [13] Botta, Alessio, Alberto Dainotti y Antonio Pescapè: A tool for the generation of realistic network workload for emerging networking scenarios. Computer Networks, 56(15):3531– 3547, 2012. [14] Broll, Wolfgang, Irma Lindt, Jan Ohlenburg, Michael Wittkämper, Chunrong Yuan, Thomas Novotny, C Mottram, A Fatah gen Schieck y A Strothman: Arthur: A collabo- rative augmented environment for architectural design and urban planning. Journal of Virtual Reality and Broadcasting, 1(1), 2004. [15] Brooks Jr, Frederick P: The computer scientist as toolsmith II. Communications of the ACM, 39(3):61–68, 1996. [16] Caudell, Thomas P y David W Mizell: Augmented reality: An application of heads- up display technology to manual manufacturing processes. En System Sciences, 1992. Proceedings of the Twenty-Fifth Hawaii International Conference on, volumen 2, páginas 659–669. IEEE, 1992. [17] Chen, David T, Chris Tector, Andrew Brandt, Hong Chen, Ryutarou Ohbuchi, Mi- ke Bajura, Henry Fuchs y cols.: Observing a volume rendered fetus within a pregnant patient. En Visualization, 1994., Visualization’94, Proceedings., IEEE Conference on, páginas 364–368. IEEE, 1994. [18] Chong, JWS, SK Ong, AYC Nee y K Youcef-Youmi: Robot programming using aug- mented reality: An interactive method for planning collision-free paths. Robotics and Computer-Integrated Manufacturing, 25(3):689–701, 2009. [19] Davis, Alan M: Operational prototyping: A new development approach. Software, IEEE, 9(5):70–78, 1992. [20] Diftler, MA, JS Mehling, ME Abdallah, NA Radford, LB Bridgwater, AM Sanders, RS Askew, DM Linn, JD Yamokoski, FA Permenter y cols.: Robonaut 2-the first huma- noid robot in space. En Robotics and Automation (ICRA), 2011 IEEE International Conference on, páginas 2178–2183. IEEE, 2011. [21] Drascic, David: Stereoscopic vision and Augmented Reality. Scientific Computing & Automation, 9(7):31–34, 1993. [22] Drewes, T, E Mynatt, Maribeth Gandy y cols.: Sleuth: An audio experience. En Pro- ceedings of The International Conference on Auditory Display, 2000. [23] Driessen, Vincent: A successful Git branching model. web, Julio 2014. http://nvie. com/posts/a-successful-git-branching-model/. 82 http://nvie.com/posts/a-successful-git-branching-model/ http://nvie.com/posts/a-successful-git-branching-model/ [24] Fearing, Ronald S., Ken H. Chiang, Michael H. Dickinson, DL Pick, Metin Sitti y Jo- seph Yan: Wing transmission for a micromechanical flying insect. En Robotics and Au- tomation, 2000. Proceedings. ICRA’00. IEEE International Conference on, volumen 2, páginas 1509–1516. IEEE, 2000. [25] Feiner, Steven, Blair Macintyre y Dorée Seligmann: Knowledge-based augmented reality. Communications of the ACM, 36(7):53–62, 1993. [26] Fuchs, Henry, Mark A Livingston, Ramesh Raskar, Kurtis Keller, Jessica R Crawford, Paul Rademacher, Samuel H Drake, Anthony A Meyer y cols.: Augmented reality visua- lization for laparoscopic surgery. En Medical Image Computing and Computer-Assisted Interventation—MICCAI’98, páginas 934–943. Springer, 1998. [27] Gamma, Erich, Richard Helm, Ralph Johnson y John Vlissides: Design patterns: ele- ments of reusable object-oriented software. Pearson Education, 1994. [28] Gandhi, Heer, Michael Collins, Michael Chuang y Priya Narasimhan: Real-time tracking of game assets in american football for automated camera selection and motion capture. Procedia Engineering, 2(2):2667–2673, 2010. [29] Gao, Qin, Zhelong Wang, Hong Shang, Weijian Hu y Ming Jiang: Mechanism Design and Locomotion of a Snake Robot. En Intelligent Autonomous Systems 12, páginas 731–738. Springer, 2013. [30] Gibbs, Simon, Michael Hoch y cols.: System and method for data assisted chroma-keying, 2011. US Patent 8,022,965. [31] Gouaillier, David, Vincent Hugel, Pierre Blazevic, Chris Kilner, Jérôme Monceaux, Pas- cal Lafourcade, Brice Marnier, Julien Serre y Bruno Maisonnier: Mechatronic design of NAO humanoid. En Robotics and Automation, 2009. ICRA’09. IEEE International Conference on, páginas 769–774. IEEE, 2009. [32] Green, Scott A, J Geoffrey Chase, XiaoQi Chen y Mark Billinghurst: Evaluating the augmented reality human-robot collaboration system. International journal of intelligent systems technologies and applications, 8(1):130–143, 2010. [33] Group, The LEGO: LEGO MINDSTORMS NXT Bluetooth Developer Kit. Retrieved December, 1:1–10, 2006. [34] Group, The LEGO: LEGO MINDSTORMS NXT Hardware Developer Kit. Retrieved December, 1:1–25, 2006. [35] Group, The LEGO: LEGO MINDSTORMS NXT Software Developer Kit. Retrieved December, 1:1–84, 2006. [36] Group, The LEGO: Lego Mindstorms NXT: Tribot, 2010. 83 [37] Group, The LEGO: LEGO MINDSTORMS EV3 Frequently Asked Questions. web, Enero 2013. http://mindstorms.lego.com/en-us/News/ReadMore/Default.aspx? id=476781. [38] Hosoi, Kazuhiro, Vinh Ninh Dao, Akihiro Mori y Masanori Sugimoto: CoGAME: mani- pulation using a handheld projector. En ACM SIGGRAPH 2007 emerging technologies, página 2. ACM, 2007. [39] Hosoi, Kazuhiro, Vinh Ninh Dao, Akihiro Mori y Masanori Sugimoto: VisiCon: a robot control interface for visualizing manipulation using a handheld projector. En Proceedings of the international conference on Advances in computer entertainment technology, pági- nas 99–106. ACM, 2007. [40] Janin, Adam L, David W Mizell y Thomas P Caudell: Calibration of head-mounted displays for augmented reality applications. En Virtual Reality Annual International Symposium, 1993., 1993 IEEE, páginas 246–255. IEEE, 1993. [41] Kabus, Patric y Alejandro P Buchmann: A Framework for Network-Agnostic Multipla- yer Games. En GAMEON, páginas 18–26, 2007. [42] Kelly, Alonzo, Nicholas Chan, Herman Herman, Daniel Huber, Robert Meyers, Pete Rander, Randy Warner, Jason Ziglar y Erin Capstick: Real-time photorealistic virtua- lized reality interface for remote mobile robot control. The International Journal of Robotics Research, 30(3):384–404, 2011. [43] Kitano, Hiroaki, Minoru Asada, Yasuo Kuniyoshi, Itsuki Noda y Eiichi Osawa: Robocup: The robot world cup initiative. En Proceedings of the first international conference on Autonomous agents, páginas 340–347. ACM, 1997. [44] Kojima, Minoru, Maki Sugimoto, Akihiro Nakamura, Masahiro Tomita, Hideaki Nii y Masahiko Inami: Augmented coliseum: An augmented game environment with small vehicles. En Horizontal Interactive Human-Computer Systems, 2006. TableTop 2006. First IEEE International Workshop on, páginas 6–pp. IEEE, 2006. [45] Kotranza, Aaron y Benjamin Lok: Virtual human+ tangible interface= mixed reality human an initial exploration with a virtual breast exam patient. En Virtual Reality Conference, 2008. VR’08. IEEE, páginas 99–106. IEEE, 2008. [46] Leitner, Jakob, Michael Haller, Kyungdahm Yun, Woontack Woo, Maki Sugimoto y Masahiko Inami: IncreTable, a mixed reality tabletop game experience. En Proceedings of the 2008 International Conference on Advances in Computer Entertainment Technology, páginas 9–16. ACM, 2008. [47] Lyons, Kent, Maribeth Gandy y Thad Starner: Guided by voices: An audio augmented reality system. En International Conference on Auditory Display. Citeseer, 2000. 84 http://mindstorms.lego.com/en-us/News/ReadMore/Default.aspx?id=476781 http://mindstorms.lego.com/en-us/News/ReadMore/Default.aspx?id=476781 [48] Menegatti, Emanuele y Michele Moro: Educational Robotics from high-school to Mas- ter of Science. En Workshop Proceedings of Intl. Conf. on Simulation, Modeling and Programming for Autonomous Robots (SIMPAR 2010), páginas 639–648, 2010. [49] Milgram, Paul y Fumio Kishino: A taxonomy of mixed reality visual displays. IEICE TRANSACTIONS on Information and Systems, 77(12):1321–1329, 1994. [50] Milgram, Paul, Haruo Takemura, Akira Utsumi y Fumio Kishino: Augmented reality: A class of displays on the reality-virtuality continuum. En Photonics for Industrial Applications, páginas 282–292. International Society for Optics and Photonics, 1995. [51] Milgram, Paul, Shumin Zhai, David Drascic y J Grodski: Applications of augmen- ted reality for human-robot communication. En Intelligent Robots and Systems’ 93, IROS’93. Proceedings of the 1993 IEEE/RSJ International Conference on, volumen 3, páginas 1467–1472. IEEE, 1993. [52] Milone, Eugene F y Willam JF Wilson: Satellite and Ring Systems. En Solar System Astrophysics, páginas 151–211. Springer, 2008. [53] Moon, Seungbin y Gurvinder S Virk: Survey on ISO standards for industrial and service robots. En ICCAS-SICE, 2009, páginas 1878–1881. IEEE, 2009. [54] Mottok, Jürgen y Armin Gardeia: The Regensburg Concept of P-Seminars—How to organize the interface between secondary school and university education to create a didactic cooperation between teaching and learning of Software Engineering with Lego Mindstorms NXT Embedded Robot Systems. En Global Engineering Education Confe- rence (EDUCON), 2011 IEEE, páginas 917–920. IEEE, 2011. [55] Nielsen, Jakob y Thomas K Landauer: A mathematical model of the finding of usability problems. En Proceedings of the INTERACT’93 and CHI’93 conference on Human factors in computing systems, páginas 206–213. ACM, 1993. [56] Otsu, Nobuyuki: A threshold selection method from gray-level histograms. Automatica, 11(285-296):23–27, 1975. [57] Papert, Seymour: Teaching Children Thinking∗. Programmed Learning and Educational Technology, 9(5):245–255, 1972. [58] Papert, Seymour: Mindstorms: Children, computers, and powerful ideas. Basic Books, Inc., 1980. [59] Piekarski, Wayne y Bruce Thomas: ARQuake: the outdoor augmented reality gaming system. Communications of the ACM, 45(1):36–38, 2002. [60] Piekarski, Wayne y Bruce H Thomas: Tinmith-metro: New outdoor techniques for crea- ting city models with an augmented reality wearable computer. En Wearable Computers, 2001. Proceedings. Fifth International Symposium on, páginas 31–38. IEEE, 2001. 85 [61] Piekarski, Wayne, Bruce H Thomas y cols.: ARQuake-Modifications and hardware for outdoor augmented reality gaming. En 4th Australian Linux Conference, Perth, Austra- lia, 2003. [62] Ramos, Luis: UNMANNED AERIAL VEHICLE. Tesis de Doctorado, Florida Interna- tional University, 2013. [63] Resnick, Mitchel, Fred Martin, Robert Berg, Rick Borovoy, Vanessa Colella, Kwin Kra- mer y Brian Silverman: Digital manipulatives: new toys to think with. En Proceedings of the SIGCHI conference on Human factors in computing systems, páginas 281–287. ACM Press/Addison-Wesley Publishing Co., 1998. [64] Resnick, Mitchel, Fred Martin, Randy Sargent y Brian Silverman: Programmable bricks: Toys to think with. IBM Systems journal, 35(3.4):443–452, 1996. [65] Rolland, Jannick P, Frank Biocca, Felix Hamza-Lup, Yanggang Ha y Ricardo Martins: Development of head-mounted projection displays for distributed, collaborative, augmen- ted reality applications. Presence: Teleoperators & Virtual Environments, 14(5):528–549, 2005. [66] Rozier, Joseph Michael: Hear&There: An augmented reality system of linked audio. Tesis de Doctorado, Massachusetts Institute of Technology, 2000. [67] Russell, Stuart J y Peter Norvig: Inteligencia Artificial: un enfoque moderno. Pearson Education, 2004. [68] Saha, Subir Kumar: Introducción a la robótica. Mc Graw-Hill, 2010. [69] Sakagami, Yoshiaki, Ryujin Watanabe, Chiaki Aoyama, Shinichi Matsunaga, Nobuo Hi- gaki y Kikuo Fujimura: The intelligent ASIMO: System overview and integration. En Intelligent Robots and Systems, 2002. IEEE/RSJ International Conference on, volu- men 3, páginas 2478–2483. IEEE, 2002. [70] Sims, Dave: New realities in aircraft design and manufacture. Computer Graphics and Applications, IEEE, 14(2):91, 1994. [71] Taddei, Mario y Massimiliano Lisa: Atlas Ilustrado de los Robots de Leonardo. Susaeta, 2010. [72] Tercero, Jesús Salido: Cibernética Aplicada: Robots educativos. Ra-Ma, 2009. [73] Thomas, Bruce, Ben Close, John Donoghue, John Squires, Phillip De Bondi y Wayne Piekarski: First person indoor/outdoor augmented reality application: ARQuake. Perso- nal and Ubiquitous Computing, 6(1):75–86, 2002. 86 [74] Thomas, Bruce, Ben Close, John Donoghue, John Squires, Phillip De Bondi, Michael Morris y Wayne Piekarski: ARQuake: An outdoor/indoor augmented reality first person application. En Wearable Computers, The Fourth International Symposium on, páginas 139–146. IEEE, 2000. [75] Thomas, Bruce, Nicholas Krul, Benjamin Close y Wayne Piekarski: Usability and pla- yability issues for arquake. 2002. [76] Tovar, Carlos E y Paolo G Tosiani: Diseño, Construcción y Programación de Robots Móviles para la Captura y Transmisión de Eventos F́ısicos v́ıa Bluetooth. Tesis de Li- cenciatura, Universidad Central de Venezuela, Facultad de Ciencias, Escuela de Compu- tación, 2007. [77] Van Krevelen, DWF y R Poelman: A survey of augmented reality technologies, applica- tions and limitations. International Journal of Virtual Reality, 9(2):1, 2010. [78] Vincent, Thomas, Laurence Nigay, Takeshi Kurata y cols.: Classifying handheld Aug- mented Reality: Three categories linked by spatial mappings. En Workshop on Classifying the AR Presentation Space at ISMAR 2012, 2012. [79] Walter, W Grey: An imitation of life. Scientific American, 182(5):42–45, 1950. [80] Wanstall, Brian: HUD on the Head for Combat Pilots. Interavia, 44:334–338, 1989. [81] West, Mick: Evolve Your Hierarchy: Refactoring Game Entities with Components. Teok- sessa Game Developer Magazine, osa, 13, 2006. [82] Wilde, Emilia LC van Egmond-de, Masi Mohammadi y cols.: Innovations in domo- tics: fulfilling potential or hampered by prevailing technological regime? Construction Innovation: Information, Process, Management, 11(4):470–492, 2011. 87 Índice de figuras Índice de tablas Introducción Planteamiento del problema Objetivo general Objetivos específicos Justificación Distribución del documento Marco Teórico Realidad Aumentada Realidad Mixta y el contexto de la realidad aumentada El continuo Realidad-Virtualidad Aplicaciones de la realidad aumentada Robótica Definición de robótica Tipos de robots Mecanismos de movimiento para robots móviles terrestres Aplicaciones de la robótica La realidad aumentada y el control de robots móviles Método de investigación y herramientas utilizadas Metodología Herramientas Herramientas de hardware Herramientas de software Descripción de la solución implementada Diseño arquitectónico de la solución Diseño de casos de uso Control del robot Escenarios jugables y despliegue de objetos virtuales. Descripción de la implementación de referencia Diseño de clases Detalles del desarrollo de los módulos del robot y la cámara Patrones de diseño utilizados en la aplicación de control Detección de marcadores Bomb Game, el escenario de demostración Pruebas y resultados Medición de distancias óptimas para reconocimiento de marcadores Perfilado del procesamiento en el estado de juego Resultados y análisis Tasas de recepción de video en el dispositivo de control Resultados y análisis Determinación del punto de saturación del video Resultados y análisis Pruebas de usuario Resultados y análisis Conclusiones Contribuciones Limitaciones Trabajos futuros Anexos El robot LEGO Mindstorms™ El MIT Programmable Brick Evolución del LEGO Mindstorms LEGO Mindstorms RCX LEGO Mindstorms NXT LEGO Mindstorms EV3 Listado del código de generación de marcadores Cuestionario utilizado en la prueba de usuario Datos y gráficos adicionales de las pruebas realizadas Referencias