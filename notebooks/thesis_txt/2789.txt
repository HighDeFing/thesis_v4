Universidad Central de Venezuela Facultad de Ciencias Escuela de Computación Cálculo Científico y Tecnológico Precondicionamiento por aproximaciones a la matriz pseudoinversa para el problema de mínimos cuadrados lineales Trabajo Especial de Grado presentado ante la Ilustre Universidad Central de Venezuela Por el Bachiller Oskar Raúl Cahueñas Camargo para optar al título de Licenciado en Computación Tutores Prof. Marcos Raydán y Prof. Luis Manuel Hernández Caracas, Julio de 2009 ACTA Quienes suscriben, miembros del jurado designado por el Consejo de Escuela de Computación de la Facultad de Ciencias, para examinar el Trabajo Especial de Grado presentado por el bachiller Oskar Raúl Cahueñas Camargo, portador de la cédula de identidad V-12.422.793, con el título “Precondicionamiento por aproximaciones a la matriz pseudoinversa para el problema de mínimos cuadrados lineales”, para optar al título de Licenciado en Computación, dejan constancia de lo siguiente: Leído como fue este trabajo, por cada uno de los miembros del jurado, se fijó el día 06 de julio de 2009 a las 11:00 am, para que su autor lo defendiera en forma pública en el Salón Leandro Aristiguieta de la Facultad de Ciencias de la Universidad Central de Venezuela, mediante una presentación oral de su contenido, luego de lo cual respondió a las preguntas formuladas. Finalizada la defensa pública del Trabajo Especial de Grado, el jurado decidió aprobarlo. En fe de lo cual se levanta la presente Acta, en Caracas el día seis de julio del año dos mil nueve, dejándose también constancia de que actuó como Coordinador del Jurado el profesor Luis Manuel Hernández. ________________________________________________ Profa. Joalí Moreno (Jurado Principal) ________________________________________________ Prof. Otilio Rojas (Jurado Principal) ________________________________________________ Prof. Luis Manuel Hernández (Tutor) Resumen Dada una matriz A ∈ Rm×n con m > n de rango completo y un vector b ∈ Rn, se propone utilizar una aproximación a la matriz pseudoinversa de A, A† ∈ Rn×m como precondicionador para el problema de mínimos cuadrados lineales asociado a A y b. Dicha aproximación a A† se obtendrá a partir del método iterativo de Schulz, un esquema iterativo basado en el método de Newton en espacios de matrices para el cálculo de matrices inversas y pseudoinversas. El problema de mínimos cuadrados precondicionado se resolverá a través del esquema de aceleración de convergencia Richardson-PR2, un método iterativo-residual obtenido como una generalización del método de Richardson de primer orden para resolver sistemas de ecuaciones lineales. Se estudiarán las propiedades del precondicionador y se realizarán pruebas numéricas con diversas matrices. iii iv Índice general Introducción xiii 1. Mínimos Cuadrados Lineales (PMCL) 1 1.1. Las ecuaciones normales . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 1.2. La condición de ortogonalidad . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 1.3. Propiedades geométricas de la solución del PMCL . . . . . . . . . . . . . . . . . 3 1.4. La matriz pseudoinversa . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 1.4.1. Definición de la pseudoinversa . . . . . . . . . . . . . . . . . . . . . . . . 5 1.4.2. La Descomposición en Valores Singulares . . . . . . . . . . . . . . . . . 6 2. Métodos Clásicos para el PMCL 11 2.1. Métodos directos para el PMCL . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 2.1.1. Factorización de Cholesky . . . . . . . . . . . . . . . . . . . . . . . . . . 12 2.1.2. Descomposición QR . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 2.1.3. Descomposición QR vía transformaciones ortogonales . . . . . . . . . . . 14 2.1.4. Descomposición QR vía ortogonalización . . . . . . . . . . . . . . . . . . 16 2.2. Métodos iterativos para el PMCL . . . . . . . . . . . . . . . . . . . . . . . . . . 17 2.2.1. Métodos Estacionarios . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 2.2.2. El método del Gradiente Conjugado para las Ecuaciones Normales . . . 25 2.3. Precondicionamiento para las ecuaciones normales . . . . . . . . . . . . . . . . 28 2.3.1. Gradiente Conjugado Precondicionado para las ecuaciones normales . . 29 2.3.2. Algunos precondicionadores conocidos . . . . . . . . . . . . . . . . . . . 29 3. El Método de Schulz 35 3.1. El método de Newton . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35 3.2. El método de Schulz . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38 3.3. Propiedades del Método de Schulz para matrices rectangulares . . . . . . . . . . 42 4. El Método Richardson-PR2 45 4.1. El esquema de aceleración de convergencia Richardson-PR2 . . . . . . . . . . . 46 4.2. Elección del tamaño de paso para el esquema Richardson-PR2 . . . . . . . . . . 46 4.3. Elección de la dirección de búsqueda para Richardson-PR2 . . . . . . . . . . . . 48 4.4. El método iterativo de Richardson-PR2 . . . . . . . . . . . . . . . . . . . . . . 48 4.5. Algunas estrategias de precondicionamiento para Richardson-PR2 . . . . . . . . 50 4.5.1. Precondicionador constante . . . . . . . . . . . . . . . . . . . . . . . . . 50 4.5.2. Precondicionador lineal iterativo . . . . . . . . . . . . . . . . . . . . . . 50 v 4.5.3. Precondicionador cuadrático iterativo . . . . . . . . . . . . . . . . . . . 50 5. Precondicionamiento del PMCL 53 5.1. Mk como precondicionador al PMCL . . . . . . . . . . . . . . . . . . . . . . . . 53 5.2. Cálculo de MkA y Mkb . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54 5.3. Algoritmo PMCL-Richardson-PR2 1 . . . . . . . . . . . . . . . . . . . . . . . . 56 5.4. Algoritmo PMCL-Richardson-PR2 2 . . . . . . . . . . . . . . . . . . . . . . . . 59 6. Experimentación Numérica 61 6.1. Plataforma computacional . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61 6.2. Matrices de prueba . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62 6.2.1. Matrices densas de prueba . . . . . . . . . . . . . . . . . . . . . . . . . . 62 6.2.2. Matrices sparse de prueba . . . . . . . . . . . . . . . . . . . . . . . . . . 63 6.3. Estudio numérico de Mk como precondicionador . . . . . . . . . . . . . . . . . . 63 6.3.1. Grupo de Experimentos A . . . . . . . . . . . . . . . . . . . . . . . . . . 64 6.3.2. Grupo de Experimentos B . . . . . . . . . . . . . . . . . . . . . . . . . . 69 6.4. Experimentos numéricos de la resolución del PMCL . . . . . . . . . . . . . . . . 77 6.4.1. Formato de los experimentos numéricos de la resolución del PMCL . . . 77 6.4.2. Grupo de Experimentos A . . . . . . . . . . . . . . . . . . . . . . . . . . 78 6.4.3. Grupo de Experimentos B . . . . . . . . . . . . . . . . . . . . . . . . . . 84 7. Conclusiones 89 vi Índice de Algoritmos 2.1. Factorización de Cholesky de la matriz A . . . . . . . . . . . . . . . . . . . . . 13 2.2. Solución del PMCL vía factorización de Cholesky . . . . . . . . . . . . . . . . . 14 2.3. Método de Golub para la solución del PMCL vía descomposición QR . . . . . . 16 2.4. Método de Richardson de primer orden para las ecuaciones normales . . . . . . 19 2.5. Método de Jacobi para las ecuaciones normales . . . . . . . . . . . . . . . . . . 21 2.6. Método de Gauss-Seidel para las ecuaciones normales . . . . . . . . . . . . . . . 22 2.7. Método SOR para las ecuaciones normales . . . . . . . . . . . . . . . . . . . . . 24 2.8. Gradiente Conjugado Genérico para una matriz SPD . . . . . . . . . . . . . . . 26 2.9. GC aplicado a las ecuaciones normales . . . . . . . . . . . . . . . . . . . . . . . 27 2.10. GC Precondicionado . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 2.11. Factorización de Cholesky incompleta . . . . . . . . . . . . . . . . . . . . . . . 31 2.12. GC Precondicionado con Factorización LU . . . . . . . . . . . . . . . . . . . . . 34 3.1. Método de Newton en varias variables . . . . . . . . . . . . . . . . . . . . . . . 37 3.2. Iteración de Schulz . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41 4.1. Iteración de Richardson-PR2 cruda . . . . . . . . . . . . . . . . . . . . . . . . . 49 5.1. Cálculo de la potencia A2 k para una matriz A . . . . . . . . . . . . . . . . . . . 55 5.2. Cálculo de la matriz MkA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56 5.3. Cálculo de la matriz MkA y el vector Mkb . . . . . . . . . . . . . . . . . . . . . 57 5.4. Algoritmo PMCL-Richardson-PR2 1 . . . . . . . . . . . . . . . . . . . . . . . . 58 5.5. Algoritmo PMCL-Richardson-PR2 2 . . . . . . . . . . . . . . . . . . . . . . . . 60 vii viii Dedicatoria A mi madre y mi hermana. ix x Agradecimientos A mis tutores, los Profesores Marcos Raydan y Luis Manuel Hernández por su infinita paciencia y apoyo. xi La precisión numérica es el alma de toda ciencia. Sir D’ Arcy Wentworth Thompson En mi teoría del movimiento de los cuerpos celestes, mostré cómo calcular los valores más probables de las incógnitas, dado que las leyes probabilísticas que rigen los errores observacionales son bien conocidas. Sin embargo, en la mayoría de los casos dichas leyes son meras hipótesis. En virtud de ello apliqué a esa teoría a la ley más plausible: que la probabilidad de un error x es proporcional a exxhh. A partir de esta suposición, nació un método que usé durante algún tiempo, especialmente para cálculos astronómicos. Actualmente es utilizado por muchas personas y es conocido como el método de mínimos cuadrados Carl Gauss xii Introducción Si una idea no es absurda al principio, entonces no merece la pena Albert Einstein Uno de los problemas históricamente más relevantes en las matemáticas aplicadas, es el de la búsqueda de métodos numéricos para el ajuste de datos a alguna función. Este problema es una necesidad fundamental en las ciencias básicas y aplicadas, pues el objetivo último de las disciplinas científicas es la explicación de fenómenos naturales a través de modelos matemáticos que deben obtenerse o ser validados a partir del ajuste de datos provenientes de la observación o experimentación de dichos fenómenos. En particular, en una gran cantidad de aplicaciones, se busca establecer la existencia de relaciones lineales entre varias variables que caracterizan determinado fenómeno. Esto es, supóngase que se tienen variables x1, x2, . . . xn y se desea verificar que alguna variable bk satisface la relación bk = i=n∑ i=1 cixi para un conjunto de parámetros reales c1, c2, . . . cn. La ecuación anterior se conoce como modelo de ajuste lineal para n valores. Si se tienen m conjuntos de datos experimentales (x1k, x2k, . . . xnk, bk) para k = 1, . . . ,m con m > n, el problema anterior conlleva a un sistema de ecuaciones lineales sobredeterminado de la forma Ax = b con A ∈ Rm×n, x ∈ Rn y b ∈ Rm. Debido a que este sistema es sobredeterminado, no se garantiza la existencia de una solución en el caso general. No obstante, se desea poder hallar al menos una aproximación a la solución de dicho problema que sea la mejor posible en algún sentido. El problema anterior se conoce como problema de mínimos cuadrados lineales o simple- mente problema de mínimos cuadrados. Históricamente, se tienen referencias de J. Kepler (1571 - 1630) quien usó modelos lineales para ajustar cálculos de órbitas planetarias. R. Cotes (1682 - 1716) y T. Simpson (1710 - 1761) utilizaron ajustes lineales en donde se buscaba minimizar la suma de los cuadrados de los errores de los datos experimentales, aplicados a problemas de astronomía. También S. Laplace (1749-1827) propuso diversas técnicas de ajuste lineal de datos basados en la minimización xiii del error promedio y del error promedio al cuadrado en aplicaciones de geodesia y mediciones terrestres. No obstante, fue C. Gauss (1777 - 1855) quién postuló el modelo de ajuste por mínimos cuadrados tal como se conoce en la actualidad, y finalmente, A. Legendre (1752- 1833) quien acuñó el término Mínimos cuadrados al publicar en 1805 el clásico método de las ecuaciones normales para la resolución de dicho problema. Para el lector interesado en los aspectos históricos del problema de mínimos cuadrados se sugiere consultar [28] y [38]. Por otro lado, el desarrollo de métodos numéricos para el problema de mínimos cuadrados se inició plenamente después de la segunda mitad del siglo XX. En 1965 G. Golub (1932-2007) publica un artículo donde utiliza la descomposición QR vía transformaciones de Householder a la resolución del problema de mínimos cuadrados [16], el cual es uno de los primeros trabajos de métodos numéricos para mínimos cuadrados. Diversos métodos para la resolución del problema a través de métodos como la eliminación Gaussiana, de la descomposición en valores singulares (SVD) entre otros, fueron propuestos por Golub, Kahan y Wilkinson, entre otros, en las décadas de 1960 y 1970. Así mismo en los últimos años se han hecho grandes progresos en los métodos directos para resolver problemas de mínimos cuadrados, así como métodos iterativos para el caso de matrices sparse de grandes dimensiones. En este trabajo se desarrollará un método numérico iterativo para resolver el problema de mínimos cuadrados para matrices densas de grandes dimensiones. En primer lugar, la matriz A será precondicionada usando una aproximación a A†, la matriz pseudoinversa de A. La aproximación a A† proviene de la iteración de Schulz, que es un caso particular del método de Newton en espacios de matrices, aplicado al cálculo de la inversa de una matriz. Se plantea, por consiguiente, el precondicionamiento de una matriz rectangular, un tópico del cual no se encontró ninguna referencia en la literatura, por lo cual representa el aporte original de este trabajo. En segundo lugar, el sistema precondicionado, el cual se convierte en un sistema de n ecuaciones con n incógnitas, se resolverá con un método iterativo que proviene de un esquema de aceleración de convergencia del método de Richardson de primer orden desarrollado en [7]. En este esquema, las iteraciones son de la forma r(0) = b−Ax(0) x(k+1) = x(k) + λkCkr(k) k ≥ 0 r(k+1) = r(k) − λkACkr(k) k ≥ 0 Donde en cada iteración, la solución x avanza por una dirección de búsqueda dada por el residual de la iteración anterior premultiplicado por una matriz precondicionadora Ck, la cual puede variar en cada iteración y un tamaño de paso λk Además, se definirá una extensión de dicho esquema de aceleración de convergencia para iterar sobre la matriz rectangular, usando como familia de matrices precondicionadoras las aproximaciones aA† generadas por la iteración de Schulz. Debe observarse que históricamente, los métodos iterativos suelen ser utilizados princi- palmente en matrices sparse, ya que en el caso de matrices densas, estos tienen un costo computacional más elevado respecto a los métodos directos. Sin embargo, en los últimos años, debido a la creciente popularización del procesamiento paralelo, está surgiendo un interés por el uso de métodos iterativos para la resolución de sistemas lineales con matrices densas de grandes dimensiones, pues estos métodos permiten explotar el paralelismo de una manera mu- cho más eficiente que los métodos directos. Algunas referencias en esta dirección son [10] y [42]. xiv Entre aplicaciones de problemas de mínimos cuadrados que involucran matrices densas se pueden encontrar en áreas como astronomía, geodesia, fotogrametría, estadística entre otras (Ver [11], [21] y [14]) El resto del trabajo se organiza de la siguiente manera. En el capítulo 1 se presentarán los fundamentos teóricos del problema de mínimos cuadrados lineales sobre matrices de rango completo. En el capítulo 2 se dará una breve introducción de los métodos numéricos clásicos encontrados en la literatura para la resolución del problema de mínimos cuadrados. En el capítulo 3 se presentará el método de Schulz como un caso especial del método de Newton en varias variables y se probarán algunas propiedades de la iteración de Schulz para el caso de matrices rectangulares, que implican que dichas matrices son una buena elección para precondicionar el problema de mínimos cuadrados. En el capítulo 4 se introducirá el esquema de aceleración de convergencia de Richardson-PR2. En el capítulo 5 se presentarán los algoritmos para la resolución del problema de mínimos cuadrados, obtenidos a partir de la iteración de Richardson-PR2 combinada con el precondicionamiento por aproximaciones a la matriz pseudoinversa. Los dos últimos capítulos estarán dedicados a presentar los resultados de las pruebas numéricas realizadas y las conclusiones del trabajo. xv xvi Capítulo 1 El Problema de Mínimos Cuadrados Lineales (PMCL) La verdad es demasiado complicada como para permitir nada más allá de meras aproximaciones John von Neumann Considere el sistema de ecuaciones lineales: Ax = b, A ∈ Rm×n, b ∈ Rm m > n (1.1) Como este sistema es sobredeterminado, esto es, con más ecuaciones que incógnitas, no se puede garantizar la existencia de una solución x para una matriz A y un vector b arbitrarios. Por lo tanto, es deseable hallar un vector x, tal que Ax sea la mejor aproximación a b. Una posible escogencia de x, para que Ax sea la mejor aproximación a b, consiste en hallar un vector x que minimice la distancia entre los vectores Ax y b. En otras palabras, hallar el vector x que minimice el vector residual Ax − b bajo la norma euclídea. Esto conlleva al siguiente problema de optimización: min x ‖Ax− b‖2 (1.2) donde ‖ ‖2 es la norma euclídea. Este problema es conocido como el Problema de Mínimos Cuadrados Lineales (PMCL) asociado a la matriz A y el vector b. Esta elección de una mejor aproximación a una solución de la ecuación (1.1) está motivada por las aplicaciones estadísticas que dieron origen al estudio de los mínimos cuadrados. Para una introducción a los problemas de mínimos cuadrados desde el enfoque de la estadística, ver [40], [21] y [41]. En el resto de este capítulo, se estudiarán los fundamentos teóricos del PMCL y su solucio- nes. El trabajo se restringirá al PMCL en el caso de matrices A ∈ Rm×n de rango completo, esto es, rango(A) = n. El lector interesado en métodos numéricos para la resolución del PMCL para matrices con rango(A) < n (conocidas en la literatura como matrices de rango deficiente) puede consultar [13] y [5]. 1 1. Mínimos Cuadrados Lineales (PMCL) 1.1. Las ecuaciones normales A partir de la resolución problema de optimización (1.2) obtiene un sistema de ecuacio- nes lineales asociado al PMCL, el cual se conoce como sistema de ecuaciones normales o simplemente ecuaciones normales, tal como se define a continuación. Definición 1.1.1. Sea A ∈ Rm×n y b ∈ Rm. El sistema de n ecuaciones y n incógnitas ATAx = AT b (1.3) Se conoce como sistema de ecuaciones normales asociado al PMCL. Obsérvese que para el caso de una matriz A de rango completo, la matriz ATA es una matriz simétrico positivo definida (SPD), lo cual implica que todos sus autovalores son positivos y por lo tanto el sistema (1.3) tiene solución única. La importancia de las ecuaciones normales radica en que la solución de (1.3) es la solución del PMCL asociado a A y b, tal como se establece en el siguiente teorema. Teorema 1.1.1. Si x ∈ Rn es solución de las ecuaciones normales, para A ∈ Rm×n de rango completo y b ∈ Rm, entonces, x es solución del PMCL asociado a A y b. Demostración. Ver [5]. Este último teorema, permite obtener una solución cerrada para el PMCL en el caso de que A sea de rango completo: el vector x que satisface la ecuación (1.2) viene dado por x = ( ATA )−1 AT b (1.4) 1.2. La condición de ortogonalidad A través de las ecuaciones normales, presentadas en la sección anterior, puede deducirse una importante propiedad que caracteriza a las soluciones del PMCL asociado a A ∈ Rm×n y b ∈ Rm. Dicha propiedad se conoce como la condición de ortogonalidad de las soluciones del PMCL y se establece en el siguiente teorema. Teorema 1.2.1. Sea S = {x ∈ Rn : ‖Ax− b‖2 es mínima} El conjunto de soluciones al PMCL asociado a A y b. Entonces, x ∈ S si y solo si x satisface la siguiente condición de ortogonalidad: AT (b−Ax) = 0 Demostración. (⇒) Suponga que x̂ ∈ S y que x̂ no satisface la condición de ortogonalidad. Esto es, AT r̂ = z 6= 0, donde r̂ = b−Ax̂. Considere x = x̂+ �z para algún � > 0. Así 2 1. Mínimos Cuadrados Lineales (PMCL) r = b−Ax = b−A (x̂+ �z) = (b−Ax̂) + �Az = r̂ − �Az y, por otro lado ‖Ax− b‖2 = rT r = r̂T r̂ − 2�zT z + �2(Az)T (Az) Para �� 1 el término que tiene �2 es despreciable y por lo tanto ‖Ax− b‖2 ≈ r̂T r̂ − 2�zT z < r̂r̂T = ‖Ax̂− b‖2 Esto es, x̂ no minimiza ‖Ax̂−b‖2, lo cual es una contradicción. Por lo tanto, queda probado, que x ∈ S satisface la condición de ortogonalidad. (⇐) Suponga que x̂ satisface la condición de ortogonalidad AT r̂ = 0 con r̂ = b− Ax̂. Sea x ∈ Rn arbitrario, se tiene: r = b−Ax = b−Ax+Ax̂−Ax̂ = (b−Ax̂) +A (x̂− x) = r̂ +Ae Calculando la norma de r, a partir de la expresión anterior se tiene ‖b−Ax‖2 = rT r = (r̂ +Ae)T (r̂ +Ae) = r̂T r + ‖Ae‖2 de donde se concluye que ‖b−Ax‖2 se minimiza cuando e = 0, esto es, x̂− x = 0. Luego x̂ es solución del PMCL. Con este resultado, se tiene una importante herramienta teórica para el estudio de los métodos numéricos para resolver el PMCL, pues provee una condición necesaria y suficiente para verificar que un vector x ∈ Rm sea solución de la ecuación (1.2). 1.3. Propiedades geométricas de la solución del PMCL Para ilustrar la interpretación geométrica de la solución al PMCL, considere los subespacios fundamentales asociados a la matriz A ∈ Rm×n: 3 1. Mínimos Cuadrados Lineales (PMCL) Ax b b−Ax Figura 1.1: Significado geométrico de la solución del PMCL R(A) = {z ∈ Rn : para algún x ∈ Rm Ax = z}, el recorrido o espacio columna de A N (A) = {x ∈ Rm : Ax = 0}, la nulidad o kernel de A Obsérvese que si x es una solución del PMCL, entonces la condición de ortogonalidad implica que r = b−Ax ∈ N (AT ) Por otro lado, es evidente que Ax ∈ R(A) Así, se tiene que x, una solución al PMCL descompone al vector b en dos componentes ortogonales de la forma: b = Ax+ r, Ax ∈ R(A), r ∈ N (AT ) (1.5) En la figura (1.1) se ilustra esta descomposición ortogonal. Además, a partir de la solución al PMCL de la ecuación (1.4), se obtiene una transformación lineal que proyecta cualquier vector de Rm en R(A). La definición formal de proyector se presenta a continuación. Definición 1.3.1. Sea S un subespacio de Rn, una matriz PS ∈ Rn×n se denomina proyector ortogonal sobre S si: PS es simétrica. PS es idempotente, esto es P 2S = PS Para todo x ∈ Rn, PSx ∈ S 4 1. Mínimos Cuadrados Lineales (PMCL) Puede verificarse fácilmente que si PS es un proyector ortogonal sobre el subespacio S, entonces, Im − PS es un proyector ortogonal sobre S⊥, el complemento ortogonal de S. Con la definición anterior, se tiene que si x es la solución de la ecuación (1.2), entonces Ax es la proyección ortogonal de b sobre R(A), así, r = (Im − PR(A)), y usando la ecuación (1.4) se obtiene que PR(A) = A ( ATA )−1 AT (1.6) 1.4. La matriz pseudoinversa La matriz pseudoinversa es una extensión del concepto de matriz inversa para el caso de matrices rectangulares. Esta matriz es un caso particular de las Matrices inversas generaliza- das, las cuales se definen a continuación. Definición 1.4.1. Sea A ∈ Rm×n. Una matriz A− ∈ Rn×m es una inversa generalizada de A si satisface: AA−A = A En esta sección se definirá la matriz pseudoinversa y se presentarán varias de sus propieda- des, que serán de utilidad posteriormente cuando se analice el precondicionamiento del PMCL por aproximaciones a la pseudoinversa. Finalmente, se estudiará la Descomposición en Valores Singulares (SVD). Esta es una factorización que existe para cualquier matriz A ∈ Rm×n. 1.4.1. Definición de la pseudoinversa A continuación se definirá la matriz pseudoinversa de A en función de los proyectores ortogonales asociados a los subespacios R (A) y N (A) introducidos en la sección anterior. Definición 1.4.2. Sea A ∈ Rm×n la matriz pseudoinversa de A o matriz de Moore-Penrose A† ∈ Rn×m es una matriz que satisface: 1. P ≡ A†A es el proyector ortogonal sobre N (A)⊥ 2. P̄ ≡ AA† es el proyector ortogonal sobre R (A)⊥ La matriz pseudoinversa satisface una serie de propiedades análogas a las de la matriz inversa, también conocidas como Condiciones de Penrose. El siguiente teorema establece dichas propiedades. Teorema 1.4.1. Sea A ∈ Rm×n. La pseudoinversa A† satisface las siguientes propiedades (Condiciones de Penrose): 1. A†A es simétrica. 2. AA† es simétrica. 3. AA†A = A. 4. A†AA† = A. 5 1. Mínimos Cuadrados Lineales (PMCL) Demostración. Ver [39] Existen otras propiedades relevantes de la pseudoinversa, que pueden deducirse de las con- diciones dadas en el teorema (1.4.1) y que también mantienen una analogía con las propiedades de la inversa para el caso de matrices cuadradas. Teorema 1.4.2. Sea A ∈ Rm×n. La pseudoinversa A† satisface las siguientes propiedades: 1. ( A† )† = A. 2. ( A† )T = ( AT )† . 3. (αA)† = α−1A† Para todo α ∈ R tal que α 6= 0 4. ( ATA )† = A† ( A† )T . 5. Si U ∈ Rn×m y V ∈ Rm×n son matrices unitarias, esto es U−1 = UT y V −1 = V T ,( UAV T )† = V A†UT . 6. Si AAT = ATA entonces A†A = AA† y (An)† = ( A† )n para n ∈ Z. 7. rango (A) = rango ( AT ) = rango ( A† ) = rango ( A†A ) = tr ( A†A ) . Demostración. Ver [5] 1.4.2. La Descomposición en Valores Singulares La Descomposición en Valores Singulares (SVD) de una matriz A ∈ Rm×n es una herra- mienta de gran importancia teórica para el estudio del PMCL. La SVD provee una extensión al concepto de Diagonalización de matrices cuadradas, esto es, una transformación ortogonal de A a una matriz diagonal de n×n que preserva la norma ‖ ‖2. El siguiente resultado establece la existencia de la SVD para cualquier matriz rectangular. Teorema 1.4.3. Sea A ∈ Rm×n con rango (A) = r ≤ n, entonces existen matrices unitarias U ∈ Rm×m y V ∈ Rn×n y una matriz Σ ∈ Rm×n tales que: A = UΣV H , Σ = ( Σ1 0 0 0 ) , Σ1 =   σ1 0 . . . 0 0 σ2 . . . 0 . . . 0 0 . . . σr   ∈ Rr×r Los elementos no nulos en la diagonal principal del bloque Σ1 satisfacen σ1 ≥ σ2 ≥ · · · ≥ σn > 0 y se denominan valores singulares de A. Demostración. Ver [15]. Obsérvese que si U = (u1 · · ·ur) y V = (v1, · · · vr) donde ui y vi son las columnas de U y V respectivamente, SVD de A se puede reescribir de la forma: A = UΣH = r∑ i=1 σiuiv H i (1.7) 6 1. Mínimos Cuadrados Lineales (PMCL) Así, se tiene que la SVD de A induce a una descomposición de A como la suma de r = rango(A) matrices de rango 1 (estas son las matrices σiuivi) La SVD de A, es de interés, ya que permite definir la solución al PMCL en función de las matrices U , V y Σ mencionadas en el teorema (1.4.3) Teorema 1.4.4. Considere el problema general de mínimos cuadrados lineales: min x∈S ‖x‖2, S = {x ∈ Rn : ‖Ax− b‖2 es mínima} donde A ∈ Cm×n y rango(A) = r ≤ min(m,n). Este problema siempre tiene una solución única x, la cual se puede escribir en términos de las matrices resultantes de la SVD de A como: x = V ( Σ−1r 0 0 0 ) UHb Donde UH denota la matriz traspuesta conjugada de U . Demostración. En vista de que V y U son matrices unitarias, se tiene que V V H = In. Así b−Ax = b−AV V Hx Por otro lado ‖b−Ax‖2 = ‖UH (b−Ax) ‖2 = ‖UH ( b−AV V H ) ‖2 Ahora considere: z = V Hx = ( z1 z2 ) , c = UHb = ( c1 c2 ) = ( [ UHb ] r[ UHb ] m−r ) Donde z1, c1 ∈ Cr. Con lo anterior, podemos reescribir la norma del residual b−Ax como: ‖b−Ax‖2 = ‖UH ( b−AV V Hx ) ‖2 = ‖UHb− ( UHAV ) V Hx‖2 = ∥∥∥∥ ( c1 c2 ) − ( Σ1 0 0 0 )( z1 z2 )∥∥∥∥ 2 = ∥∥∥∥ ( c1 − Σ1z1 c2 )∥∥∥∥ 2 Puede observarse que para c1 y c2 fijos, el residual se minimiza cuando c1 − Σ1z1 = 0, esto es, z1 = Σ −1 1 c1, mientras que z2 puede ser cualquier valor. En particular, si se escoge z2 = 0, también se minimiza ‖z‖2, lo cual implica que se minimiza ‖x‖2, siendo el valor de x que 7 1. Mínimos Cuadrados Lineales (PMCL) minimiza la norma igual a: x = V z = V ( z1 z2 ) = V ( Σ−11 c1 0 ) = V ( Σ−11 [ UHb ] r + 0 [ UHb ] m−r 0 [ UHb ] r + 0 [ UHb ] m−r ) = V ( Σ−11 0 0 0 )( [ UHb ] r[ UHb ] m−r ) = V ( Σ−11 0 0 0 ) UHb La construcción de la solución del problema de mínimos cuadrados generalizados en la demostración del teorema (1.4.4), define la matriz C = V ( Σ−11 0 0 0 ) UH (1.8) La cual resulta ser A†, tal como se deduce del siguiente resultado: Teorema 1.4.5. La matriz C = V ( Σ−11 0 0 0 ) UH Satisface las condiciones de Penrose establecidas en el teorema (1.4.1) y por consiguiente, C = A†. Demostración. Ver [5] Con este último resultado, y a partir del teorema (1.4.4) se deriva que la solución del PMCL se puede expresar, en términos de A† como: x = A†b (1.9) Para el caso de A de rango completo, usando la ecuación (1.4) se obtiene una expresión para A†. A† = ( ATA )−1 AT (1.10) Obsérvese que la cálculo de la SVD provee desde el punto de vista numérico de un método directo para la resolución del PMCL. Sin embargo,el cálculo de la SVD tiene un altísimo costo computacional, lo cual hace que el enfoque de la SVD sea poco atractivo para la resolución del PMCL en el caso general. No obstante, para el caso de matrices densas de rango deficiente, la SVD es la primera elección para la resolución del PMCL. Ver [15] para un estudio de los métodos clásicos para el cálculo de la SVD y sus aplicaciones al PMCL en matrices de rango deficiente. 8 1. Mínimos Cuadrados Lineales (PMCL) Cabe destacar, además, que la SVD por sí sola, es un tópico de gran interés en el álgebra lineal numérica debido a la cantidad de aplicaciones existentes en áreas como el tratamiento digital de imágenes, filtros digitales, reconocimiento de patrones y series temporales, entre otras. Para un estudio más detallado de la SVD y referencias a sus distintas aplicaciones ver [24] y [1]. Para una introducción al estudio de los orígenes y desarrollo histórico de la SVD ver [36]. Para finalizar este capítulo, se demostrará una de las propiedades más importantes de la matriz pseudoinversa: AA† ≈ In en el sentido de que la distancia entre AA† e In es la mínima posible respecto a AX con X ∈ Rn×m. Teorema 1.4.6. X = A† minimiza la norma ‖AX − Im‖F , donde ‖ ‖F es la norma de Frobenious. Demostración. Sea X la matriz que minimiza la norma. Observe que: ‖AX − I‖F = m∑ i=1 ‖Axi − ei‖22 Donde xi es la i-ésima columna de X y ei es el i-ésimo vector de la base canónica de Rm. Para todo i en 1 . . . n, el término ‖Axi − ei‖2 se minimiza cuando xi es solución al PMCL Ax = ei. Por la ecuación (1.10), la solución del i-ésimo PMCL es xi = A†ei, es decir, xi es la i-ésima columna de A†. Luego, X = A†. 9 1. Mínimos Cuadrados Lineales (PMCL) 10 Capítulo 2 Métodos Numéricos Clásicos para el PMCL Donde quiera que haya un número está la belleza Proclo En este capítulo se dará una breve introducción a los métodos numéricos más conocidos para la resolución del PMCL que se encuentranen la literatura. Los métodos numéricos para el PMCL, se dividen en dos grandes familias: métodos directos, los cuales se aplican principal- mente a matrices densas y los métodos iterativos usados principalmente en matrices sparse. También, se revisarán algunas ideas del precondicionamiento de las ecuaciones normales apli- cado al PMCL. El principal compendio de métodos numéricos para el PMCL es [5]; los métodos directos clásicos como la factorización QR para matrices rectagulares son también cubiertos por [15]. La teoría general para los métodos iterativos estacionarios así como del método del Gradiente Conjugado puede consultarse en [33]. Esta obra igualmente aborda los métodos iterativos para la resolución del sistema (1.3) en particular. Un artículo introductorio al tema del precondicio- namiento de matrices es [3]; para el caso particular del precondicionamiento de las ecuaciones normales, puede consultarse [34] y [5]. Un bosquejo histórico del desarrollo de los métodos iterativos para la resolución de sistemas lineales, desde un punto de vista computacional, se encuentra en [35]. Previo a la presentación de los métodos para el PMCL, se introducirá un concepto de gran importancia para el estudio de los métodos numéricos: el condicionamiento numérico. El condicionamiento numérico de un problema, se refiere a una medida de la sensibilidad que puede tener la solución de dicho problema a pequeñas perturbaciones en los datos de entrada. Para el caso de los sistemas de ecuaciones lineales, sea M ∈ Rn×n y b ∈ Rn y supóngase que se desea resolver el sistema Mx = b (2.1) si los valores numéricos de M y b provienen, por ejemplo, de algún experimento, estos serán susceptibles de tener alguna perturbación debido al redondedo u otros errores experimentales como instrumentos de medición mal calibrados, una lectura incorrecta por parte del experi- mentador, o la propagación de errores en mediciones indirectas, entre otros. Por lo tanto, en 11 2. Métodos Clásicos para el PMCL la práctica, en lugar de resolver el sistema (2.1) se tendrá que resolver el siguiente sistema asociado (M + ∆M)x = b+ ∆b (2.2) donde ∆M y ∆b son una matriz y un vector que contienen los errores experimentales o perturbaciones asociados a M y b respectivamente. Entonces, se dice que el sistema (2.1) está bien condicionado si el error relativo de la solución del sistema perturbado (2.2) respecto a la solución del sistema original es pequeño. En caso contrario, se dice que el problema está mal condicionado. Una medida para el condicionamiento de una matriz en el caso general, es el número de condición, el cual se define a continuación Definición 2.0.3. Sea A ∈ Rn×n, no sigular, y sea ‖ ‖ una norma matricial inducida. El número de condición de A, el cual se denota como κ(A) se define como κ(A) ≡ ‖A‖‖A−1‖ Para el caso de una matriz rectangular A ∈ Rm×n el número de condición se define como κ(A) ≡ ‖A‖‖A†‖ Obsérvese que para toda matriz A, se cumple que κ(A) ≥ 1. El número de condición da una medida del condicionamiento de un sistema de ecuaciones lineales que involucre a la matriz A. Si κ(A) ≈ 1 (esto es, un número de condición pequeño) entonces la matriz está bien condicionada, mientras que un número de condición que sea mucho mayor a 1 (esto es, un número de condición grande) indica que la matriz está mal condicionada, y por lo tanto el resultado de un método numérico que involucre a dicha matriz podría ser poco confiable. Además, debe destacarse que en el caso del PMCL asociado a una matriz A y un vector b, el condicionamiento del problema no depende sólo de A, sino que depende además del valor de b, por lo cual se puede obtener una extensión del número de condición para el PMCL (Ver [5]) tal como se define a continuación Definición 2.0.4. Sea A ∈ Rm×n y b ∈ Rn. Si xLS es una solución al PMCL asociado a A y b, entonces el número de condición del PMCL, usando la norma euclídea, viene dado por κLS(A, b) ≡ κ(A) ( 1 + κ(A) ‖r‖2 ‖A‖2‖xLS‖2 ) donde r = b−AxLS. 2.1. Métodos directos para el PMCL 2.1.1. Factorización de Cholesky El método directo más simple para la resolución de las ecuaciones normales se basa en la factorización de Cholesky, la cual existe para cualquier matriz SPD, tal como se establece en el siguiente teorema. 12 2. Métodos Clásicos para el PMCL Teorema 2.1.1. Sea C ∈ Rn×n una matriz SPD, entonces existe una única matriz R ∈ Rn×n triangular superior tal que: C = RTR La matriz R se denomina Factor de Cholesky asociado a C. Demostración. Ver [22] Para el caso de una matriz C almacenada por filas, la factorización de Cholesky se puede calcular con el algoritmo (2.1). Algoritmo 2.1 Factorización de Cholesky de la matriz A Algoritmo FactChol Descripción Calcula la factorización de Cholesky de una matriz A SPD. Entradas • A Matriz de n filas por n columnas SPD. Salidas • R Matriz triangular de n filas por n columnas tal que A = RTR. 1: función FactChol(A) 2: R← 0 3: para j ← 1 . . . n hacer 4: para i← 1 . . . j − 1 hacer 5: rij ← cij − ∑i−1 k=1 rkirkj 6: rij ← rij/rii 7: fin para 8: rjj ← ( cjj − ∑j−1 k=1 r 2 kj )1/2 9: fin para 10: ← R 11: fin función Finalmente, conociendo la factorización de Cholesky de A, el algoritmo (2.2) define un método para la resolución del PMCL. 2.1.2. Descomposición QR Uno de los métodos directos más utilizados para la resolución del PMCL en sistemas sobredeterminados, es el de la descomposición ortogonal de la matriz A. Esta estrategia es factible ya que, de que si Q ∈ Rm×m es una matriz ortogonal, esta preserva la norma euclídea y por consiguiente, el problema de optimización min x ∥∥QT (Ax− b)∥∥ 2 (2.3) 13 2. Métodos Clásicos para el PMCL Algoritmo 2.2 Solución del PMCL vía factorización de Cholesky Algoritmo CholPMCL Descripción Resuelve el PMCL asociado a A y b usando la factorización de Cholesky. Entradas • A Matriz de rango completo de m filas por n columnas con m > n. • b Vector columna de n filas. Salidas • xLS Solución al PMCL asociado a A y b. 1: función CholPMCL(A, b) 2: R← FactChol(C) 3: d← AT b 4: Resolver el sistema Ry = d 5: Resolver el sistema RTxLS = y 6: ← xLS 7: fin función es equivalente al planteado en la ecuación (1.2). El objetivo es, entonces, hallar alguna matriz Q ∈ Rm×m tal que la ecuación (2.3) sea más fácil de resolver que la ecuación (1.2). En general, para cualquier matriz rectangular, existe una descomposición ortogonal cono- cida como descomposición QR cuya existencia se garantiza por el siguiente teorema Teorema 2.1.2. Sea A ∈ Rm×n con m ≥ n, entonces existe una matriz ortogonal Q ∈ Rm×m y una matriz triangular R ∈ Rn×n tales que A = Q ( R 0 ) esta factorización se conoce como descomposición QR de A y la matriz R se conoce como factor R asociado a A. Demostración. Ver [5] En los siguientes apartados se estudiarán algunos de los principales métodos numéricos para la descomposición QR de A y su respectiva aplicación al PMCL 2.1.3. Descomposición QR vía transformaciones ortogonales Una estrategia para hallar la descomposición QR deA, consiste en hallar matrices P1, P2, . . . , Pn de dimensión m×m, tales que Q = PnPn−1 . . . P1, donde Pi es un proyector ortogonal sobre algún subespacio de R(A), tal como se describe a continuación. 14 2. Métodos Clásicos para el PMCL Para comenzar, suponga que A se puede descomponer a partir de la siguiente relación de recurrencia A(1) = A A(k+1) = PkA (k−1), k = 1 . . . n (2.4) donde Pk, k = 1, 2, . . . es una matriz ortogonal que debe construirse de tal forma que A(k+1) tenga una estructura triangular superior sobre las primeras k columnas, es decir, A(k+1) debe tener la estructura A(k+1) = PkPk−1 . . . P1A = ( R11 R12 0 Ã(k+1) ) (2.5) donde R11 ∈ Rk×k es una matriz triangular superior y la matriz Ã(k+1) ∈ R(m−k)×(n−k−1) es ortogonal. Si la estructura de Ã(k) por columnas es: Ã(k) = (ã(k)k , ã (k) k+1, . . . , ã (k) n ) entonces, la matriz Pk debe tener la siguiente forma Pk = ( Ik−1 0 0 P̃k ) (2.6) donde P̃k es una matriz ortogonal de tamaño (m− k + 1)× (m− k + 1) tal que Pkã (k) k = σke1, σk = rkk = ‖ã k k‖ 2 2, e1 = (1, 0, . . . 0) T Obsérvese que en el k-ésimo paso de la recurrencia de la ecuación (2.5) la transformación ortogonal sólo afecta a la submatriz Ã(k+1) de A y (R11, R12) son las primeras k − 1 filas de R. Después de n pasos se obtiene A(n+1) = PnPn−1 . . . P1A = ( R 0 ) (2.7) de donde se deduce que Q = (PnPn−1 . . . P1) −1 = (PnPn−1 . . . P1) T = P T1 . . . P T n (2.8) Una vez calculada la descomposición QR a través de transformaciones ortogonales, el PMCL se resuelve a través del método de Golub [16] cuya implementación (restringida al caso de matrices de rango completo) se presenta en el algoritmo (2.3). Para efectuar la descomposición QR de A falta conocer algíun método que permita calcular la colección de matrices P̃k. Los métodos numéricos más populares para la obteción de matrices de transformación ortogonal son los siguientes: Transformaciones ortogonales de Householder. Matrices de rotación de Givens El lector interesado en los diversos algoritmos existentes para la obtención de la descompo- sición ortogonal de una matriz rectangular a través de los métodos anteriores puede consultar [15] y [5]. 15 2. Métodos Clásicos para el PMCL Algoritmo 2.3 Método de Golub para la solución del PMCL vía descomposición QR Algoritmo GolubPMCL Descripción Resuelve el PMCL asociado a A y b usando la factorización QR. Entradas • A Matriz de rango completo de m filas por n columnas con m > n. • b Vector columna de n filas. Salidas • xLS Solución al PMCL asociado a A y b. 1: función GolubPMCL(A, b) 2: [Q,R]← FactQR(A) 3: d← QT b 4: c← d(1 : n) . n < m 5: Resolver el sistema RxLS = c 6: ← xLS 7: fin función 2.1.4. Descomposición QR vía ortogonalización Un proceso de ortogonalización es un problema clásico del álgebra lineal, el cual tiene como objetivo, dado un conjunto de vectores V = {v1, v2, . . . vr} en Rn, con r ≤ n, hallar un nuevo conjunto U = {u1, u2, . . . ur} tal que span(V ) = span(U) y los vectores de U son ortogonales entre sí. Si además, los vectores resultantes de U son unitarios, el proceso se denomina proceso de ortonormalización El algoritmo clásico para la ortonormalización de un conjunto de vectores es el método de Gram Schmidt (Ver [18]). Dada una matriz A ∈ Rm×n con m ≥ n, se puede utilizar un proceso de ortonormalización sobre las columnas de A para obtener su descomposición QR, de la siguiente manera: si A = (a1, a2, . . . , an), donde ai es la i-ésima columna de A, entonces A = (q1, q2, . . . qn)R (2.9) donde Q = (q1, q2, . . . qn) se obtiene a partir de la ortonormalización de las columnas de A. Evidentemente Q será una matriz ortogonal. En esta sección se presentará el método de Gram Schmidt modificado para obtener la descomposición QR de A a través de a ortogonalización de sus columnas. La estrategia del método de Gram Schmidt modificado, consiste en construir una sucesión de matrices A = A(1), A(2), . . . A(n) = Q donde A(k) tiene la forma A(k) = ( q1, q2, . . . qk−1, a (k) k , . . . , a (k) n ) Así, las k primeras columnas de A(k) correponden a los k primeros pasos del proceso de 16 2. Métodos Clásicos para el PMCL ortonormalización de las columnas de A. En el k-ésimo paso, en primer lugar se obtiene el vector qk normalizando el vector a (k) k , esto es q̃k = a (k) k , rkk = ‖q̃k‖2, qk = q̃k/rkk (2.10) y luego se ortogonalizan a(k)k+1, . . . , a (k) n respecto a qk como sigue a (k+1) j = a (k) j − rkjqk, rkj = q T k a (k) j , j = k + 1, . . . n (2.11) Existen varias implementaciones algoritmicas del método de Gram Schmidt modificado para la descomposición QR de una matriz rectangular. Los detalles de estos algoritmos pueden consultarse en [5] y [22]. 2.2. Métodos iterativos para el PMCL En esta sección se considerarán varios métodos iterativos clásicos, con algunas modifica- ciones para la solución de la ecuación (1.3). El objetivo de los métodos iterativos es generar una sucesión de soluciones aproximadas { x(0), x(1), · · · } donde x(0) es una aproximación inicial dada a la solución o iterado inicial y x(k) para k ≥ 1 puede calcularse a través de una relación de recurrencia bien definida, en función de x(0), x(1), · · · , x(k−1). Otra característica deseable es que el cálculo de x(n) sea de bajo costo computacional. Una condición necesaria para que un método iterativo pueda dar la solución de un sistema de ecuaciones lineales, es que el método sea convergente. La definición formal de convergencia es la siguiente Definición 2.2.1. Un método iterativo, con un iterado inicial x(0) y una relación de recurren- cia bien definida para el cálculo de x(k) para k ≥ 1 es convergente si la sucesión { x(k) }∞ k=0 es convergente para cualquier iterado inicial x(0). Una de las propiedades de los métodos iterativos, es que el proceso de generación de los x(k) puede detenerse en cualquier momento, así se tiene la ventaja respecto a los métodos directos, en el hecho de que el método iterativo tenga una menor complejidad en tiempo en el caso de que se deseen soluciones de poca precisión numérica, lo cual implica un número reducido de iteraciones, o bien, para el caso de matrices de dimensión grande el número de iteraciones para observar convergencia puede ser mucho menor que el tamaño de la matriz. Para el caso de los métodos iterativos aplicados a las ecuaciones normales, es deseable que estos eviten la formación explícita de la matriz ATA, y para ello se suele trabajar con las ecuaciones normales en su forma factorizada que es AT (Ax− b) (2.12) Trabajar con AT y A de manera separada, tiene varias ventajas: En primer lugar, la matriz ATA suele tener un número de condición mucho mayor a A (κ ( ATA ) = κ (A)2 en el caso de que se tome el número de condición usando la norma euclídea), por lo cual una pequeña perturbación en ATA podría producir un error considerable en la resolución del problema. En segundo lugar, para el caso de matrices sparse se evita el relleno que aparece al formar la matriz ATA. 17 2. Métodos Clásicos para el PMCL 2.2.1. Métodos Estacionarios Los métodos iterativos más sencillos para la resolución de sistemas de ecuaciones lineales son los métodos estacionarios. Un método estacionario para la resolución de (1.3) se construye definiendo un par de matrices M y N asociadas a ATA y la iteración: Mx(k+1) = Nx(k) + b, k = 0, 1, . . . (2.13) Las matricesM y N de la ecuación (2.13) son tales que ATA = M−N y además se impone la condición de que M sea no singular. Este esquema de definición de M y N propio de los métodos estacionarios también es conocido como particionamiento o splitting regular de las ecuaciones normales. Para el análisis de la convergencia de los métodos estacionarios, se define: G = M−1N = I −M−1ATA (2.14) c = M−1b (2.15) Así, (2.13) puede reescribirse como: x(k+1) = Gx(k) + c, k = 0, 1, . . . (2.16) La matriz G se conoce como matriz de iteración asociada al método estacionario. Usando dicha matriz de iteración, se puede establecer una condición necesaria y suficiente para la convergencia de un método estacionario, la cual se presenta en el siguiente teorema Teorema 2.2.1. El método estacionario de la ecuación (2.16) es convergente si y sólo si ρ (G) < 1. Demostración. Ver [22] A continuación se presentarán algunos de los métodos estacionarios que se consiguen con mayor frecuencia en la literatura, cada uno de estos métodos se define a partir de un splitting distinto para la matriz ATA. El método de Richardson de primer orden El método de Richardson de primer orden se define a partir del siguiente splitting ATA = 1 α In − ( 1 α In −ATA ) (2.17) M = 1 α In N = 1 α In −ATA Donde α > 0 es un parámetro fijo. Para este método se tiene que G = I − αATA y c = αAT b, así, luego de una manipulación algebraica de la ecuación (2.16) se obtiene la siguiente iteración: 18 2. Métodos Clásicos para el PMCL x(k+1) = x(k) + αAT ( b−Ax(k) ) (2.18) En donde no hace falta realizar el producto ATA de manera explícita, el algoritmo (2.4) sumariza el método. Algoritmo 2.4 Método de Richardson de primer orden para las ecuaciones normales Algoritmo Richardson1ordPMCL Descripción Resuelve el PMCL asociado a A y b usando la iteración de Richardsond de primer orden para las ecuaciones normales. Entradas • x(0) Vector columna de n filas, iterado inicial. • A Matriz de rango completo de m filas por n columnas con m > n. • b Vector columna de n filas. • α Real positivo que proviene del splitting de A. • Nmax Entero positivo. Número máximo de iteraciones. • � Real positivo. Tolerancia mínima para detener la iteración. Salidas • x Solución al PMCL asociado a A y b. 1: función Richardson1ordPMCL(x(0), A, b, α,Nmax, �) 2: para i← 1, . . . , Nmax hacer 3: x(i) ← x(i−1) + αAT ( b−Ax(i−1) ) 4: r(i) ← b−Ax(i) 5: si ‖r(i)‖2 < � entonces 6: ← x(i) 7: fin si 8: fin para 9: Error (Máximo de iteraciones alcanzado) 10: fin función Para garantizar que el método de Richardson de primer orden converge a la solución del PMCL, esto es, converge a x = A†b se establece la siguiente condición Teorema 2.2.2. El método de Richardson de primer orden converge a la solución del PMCL si x(0) ∈ R ( AT ) y 0 < α < 2/σ21 donde σ1 es el máximo de los valores singulares de A. Demostración. Ver [22] 19 2. Métodos Clásicos para el PMCL El método de Jacobi El método de Jacobi se define a partir del siguiente splitting ATA = LA +DA + L T A (2.19) M = DA N = − ( LA + L T A ) Donde DA es la matriz con la diagonal principal de ATA, LA es la matriz que contiene la porción triangular inferior de ATA y LTA es la matriz que contiene la porción triangular superior de ATA. Para este método se tiene que G = In−D−1A A TA y c = D−1A A T b. Reescribiendo la ecuación (2.16) para evitar el producto explícito de ATA, la iteración de Jacobi es la siguiente: x(k+1) = x(k) +D−1A A T ( b−Ax(k) ) (2.20) Finalmente el método se presenta en el algoritmo (2.5) El método de Gauss-Seidel El método de Gauss-Seidel se define a partir del siguiente splitting ATA = LA +DA + L T A (2.21) M = LA +DA (2.22) N = −LA (2.23) A partir de las definiciones anteriores, se tiene que G = −(LA + DA)−1LTA y c = (LA + DA)−1b, y así la iteración de Gauss-Seidel queda como: x(k+1) = x(k) +D−1A ( AT b− LAx(k+1) + ( DA + L T A ) x(k) ) (2.24) Obsérvese que en este caso, la iteración para x(k+1) incluye el propio término en la relación de recurrencia, por lo cual no es posible manipular algebraicamente la expresión (2.24) para evitar la formación explícita de ATA. No obstante, se presentará una alternativa para la construcción de la iteración de Gauss-Seidel sin necesidad de realizar produtos matriciales, conocida como método de reducción de residual ya que genera una sucesión de aproximaciones x(j) tales que ‖r(j+1)‖22 ≤ ‖r (j)‖22 donde r (j) = b−Ax(j). Sea {pj} , j = 1, 2, · · · una sucesión de vectores no nulos tales que pj /∈ N (A) , para j = 1, 2, · · · y considérese una sucesión de aproximaciones de la forma x(k+1) = x(j) + αjpj , αj = pTj A T ( b−Ax(j) ) ‖Apj‖22 (2.25) Dada una aproximación inicial x(0), puede verificarse que (Ver [5]) ‖r(j+1)‖22 = |αj | 2‖Apj‖22 ≤ ‖r (j)‖22 (2.26) En particular, si A ∈ Rm×n es de rango completo, el método de Gauss-Seidel para las ecuaciones normales puede derivarse de la ecuación (2.26) escogiendo la sucesión p1, p2, . . . 20 2. Métodos Clásicos para el PMCL Algoritmo 2.5 Método de Jacobi para las ecuaciones normales Algoritmo JacobiPMCL Descripción Resuelve el PMCL asociado a A y b usando la iteración de Jacobi para las ecuaciones normales. Entradas • x(0) Vector columna de n filas, iterado inicial. • A Matriz de rango completo de m filas por n columnas con m > n. • b Vector columna de n filas. • Nmax Entero positivo. Número máximo de iteraciones. • � Real positivo. Tolerancia mínima para detener la iteración. Salidas • x Solución al PMCL asociado a A y b. 1: función JacobiPMCL(x(0), A, b,Nmax, �) 2: r(0) ← b−Ax(0) 3: para i← 1, . . . , Nmax hacer 4: x(i) = x(i−1) +D−1A A T r(i−1) 5: r(i) ← b−Ax(i) 6: si ‖r(i)‖2 < � entonces 7: ← x(i) 8: fin si 9: fin para 10: Error (Máximo de iteraciones alcanzado) 11: fin función 21 2. Métodos Clásicos para el PMCL Algoritmo 2.6 Método de Gauss-Seidel para las ecuaciones normales Algoritmo GaussSeidelPMCL Descripción Resuelve el PMCL asociado a A y b usando la iteración de Gauss-Seidel para las ecua- ciones normales. Entradas • x(0) Vector columna de n filas, iterado inicial. • A Matriz de rango completo de m filas por n columnas con m > n. • b Vector columna de n filas. • Nmax Entero positivo. Número máximo de iteraciones. • � Real positivo. Tolerancia mínima para detener la iteración. Salidas • x Solución al PMCL asociado a A y b. 1: función GaussSeidelPMCL(x(0), A, b,Nmax, �) 2: r(0) = b−Ax(0) 3: para i← 1, . . . , Nmax hacer 4: z(1) ← x(i−1) . Iterado inicial para el barrido de n subiteraciones 5: r̃(1) ← r(0) 6: para j ← 1, 2, · · · , n hacer 7: a← Aej . j-ésima columna de A 8: d← ‖a‖22 9: δj ← ( aT r̃(j) ) /d 10: z(j+1) ← z(j) + δjej 11: r̃(j+1) ← r̃(j) − δja 12: fin para 13: x(i) ← z(n+1) 14: r(i) ← r̃(n+1) 15: si ‖r(i)‖2 < � entonces 16: ← x(i) 17: fin si 18: fin para 19: Error (Máximo de iteraciones alcanzado) 20: fin función 22 2. Métodos Clásicos para el PMCL como e1, e2, . . . , donde ei es el i-ésimo vector de la base canónica en orden cíclico. Con esta elección, se tiene que Apj = Aej = aj donde aj es la j-ésima columna de A. A partir de la elección anterior, una iteración de Gauss-Seidel se realiza a través de un barrido de n sub-iteraciones. Los pasos para obtener la aproximación x(k) son los siguientes: Sea z(1) = x(k), y r̃(1) = b−Ax(k) Calcular las n− 1 subiteraciones restantes z(2), · · · , z(n+1) usando la recurrencia: δj = aTj r̃ (j) ‖aj‖22 z(j+1) = z(j) + δjej r̃(j+1) = r̃(j) − δjaj Sea x(k+1) = z(n+1) Finalmente, se presenta el método de Gauss-Seidel en el algoritmo (2.6). El Método de Sobrerrelajación sucesiva (SOR) El método SOR es una mejora del método de Gauss-Seidel, la cual consiste en introducir un parámetro ω ∈ R a las sub-iteraciones de un barrido en la iteración de Gauss-Seidel con la finalidad de acelerar la convergencia. Dicho factor se conoce como factor de relajación. Con la introducción del factor de relajación, las recurrencias (2.27) se convierten en: δj = ω aTj r̃ (j) ‖aj‖22 (2.27) z(j+1) = z(j) + δjej (2.28) r̃(j+1) = r̃(j) − δjaj (2.29) La principal desventaja que presenta este método es que para el caso general no es po- sible determinar el factor de relajación óptimo para garantizar la máxima aceleración de la convergencia, no obstante, en vista de que la matriz ATA es SPD, es condición necesaria que 0 < ω < 2. El método sor se presenta en el algoritmo (2.7). Para cierta clase de matrices puede hallarse un factor de relajación óptimo ωopt que garan- tiza la máxima velocidad de convergencia del de convergencia del método SOR, estas son las matrices de orden consistente cuya definición es la siguiente Definición 2.2.2. Sea una matriz de la forma M = ATA donde A ∈ Rm×n y considérese una descomposición M = DA(Im − L− U) donde: DA es una matriz no singular. L es una matriz estrictamente triangular inferior. U es una matriz estrictamente triangular superior. 23 2. Métodos Clásicos para el PMCL Algoritmo 2.7 Método SOR para las ecuaciones normales Algoritmo SOR_PMCL Descripción Resuelve el PMCL asociado a A y b usando la iteración SOR para las ecuaciones nor- males. Entradas • x(0) Vector columna de n filas, iterado inicial. • A Matriz de rango completo de m filas por n columnas con m > n. • b Vector columna de n filas. • ω Real positivo, factor de relajación de SOR. • Nmax Entero positivo. Número máximo de iteraciones. • � Real positivo. Tolerancia mínima para detener la iteración. Salidas • x Solución al PMCL asociado a A y b. 1: función SOR_PMCL(x(0), A, b, ω,Nmax, �) 2: r(0) = b−Ax(0) 3: para i← 1, . . . , Nmax hacer 4: z(1) ← x(i−1) . Iterado inicial para el barrido de n subiteraciones 5: r̃(1) ← r(0) 6: para j ← 1, 2, · · · , n hacer 7: a← Aej . j-ésima columna de A 8: d← ‖a‖22 9: δj ← ω((aT r̃(j))/d) 10: z(j+1) ← z(j) + δjej 11: r̃(j+1) ← r̃(j) − δja 12: fin para 13: x(i) ← z(n+1) 14: r(i) ← r̃(j+1) 15: si ‖r(i)‖2 < � entonces 16: ← x(i) 17: fin si 18: fin para 19: Error (Máximo de iteraciones alcanzado) 20: fin función 24 2. Métodos Clásicos para el PMCL Se dice que M es una matriz de orden consistente si los autovalores del funcional J(α) = αL+ α−1U, α ∈ R, α 6= 0 Teorema 2.2.3. Sea A ∈ Rm×n, si AAT una matriz de orden consistente con AAT = DA(Im − L − U) tal como en la definicion (2.2.2), entonces si B = L + U tiene todos sus autovalores reales y ρ(B) < 1, entonces el factor de relajación óptimo para el método SOR aplicado a la resolución de las ecuaciones normales es ωopt = 2 1 + √ 1− ρ(B)2 Demostración. Ver [44] 2.2.2. El método del Gradiente Conjugado para las Ecuaciones Normales El método del Gradiente Conjugado (GC) fue propuesto originalmente en [20] como un método directo para la resolución de sistemas de ecuaciones lineales de la forma Cx = d, C ∈ Rn×n y C es SPD. No obstante, GC adquirió relevancia al ser reformulado como un método iterativo, en este ámbito el método es muy eficiente sobre todo para el caso de matrices sparse. Para el caso de matrices SPD, el GC proviene a partir de un problema de optimización de funcionales cuadráticas. En efecto paraM ∈ Rn×n, una matriz SPD, el problema de minimizar el funcional cuadrático q(x) = 1 2 xTMx− xT b+ c (2.30) Es equivalente a resolver el sistema Mx = b. El objetivo del método es generar un conjunto de direcciones de búsqueda que cumplen una propiedad conocida como M-Conjugancia respecto a una matriz M , la cual se define a continuación Definición 2.2.3. Sea U = {u1, u2, . . . un} un conjunto de vectores en Rn. Dada una matriz M ∈ Rn×n SPD, se dice que U posee la propiedad de M-conjugancia si para todo 1 ≤ i, j ≤ n se cumple uTj Mui = 0 si i 6= j Así, dado un conjunto U de vectores que poseen la propiedad de M-Conjugancia, existe un método iterativo tal que, usando los vectores de dicho conjunto como direcciones de búsqueda, obtiene la solución al sistema de ecuaciones Mx = b en un número finito de iteraciones, tal como se establece en el siguiente resultado Teorema 2.2.4. Sea M ∈ Rn×n SPD, b ∈ Rn y U = {u1, u2, . . . un} un conjunto de vectores no nulos con la propiedad de M-Conjugancia, entonces, para todo iterado inicial x0 ∈ Rn la sucesión xk = xk−1 + rTk−1uk uTkMuk uk, (rk = b−Mxk) satisface Mxn = b 25 2. Métodos Clásicos para el PMCL Demostración. Ver [27] La conexión entre el método iterativo definido por el teorema anterior y la optimización de funcionales cuadráticas está en el hecho de que el factor (rTk−1uk/u T kMuk)uk es el argumento que minimiza a q(x + αu) donde α es un parámetro real, x y u son valores fijos y q es el funcional definido en la ecuación (2.30) (Ver [27]) En el algoritmo (2.8) presenta el método GC en su forma genérica para una matriz M SPD. Finalmente el algoritmo (2.9) presenta el método GC aplicado a las ecuaciones normales para una matriz A ∈ Rm×n, en donde no es necesario formar explícitamente la matriz ATA. Algoritmo 2.8 Gradiente Conjugado Genérico para una matriz SPD Algoritmo GC_Generico Descripción Resuelve el sistema Mx = b usando GC. Entradas • x(0) Vector columna de n filas, iterado inicial. • M Matriz SPD de n filas por n columnas. • b Vector columna de n filas. iteración. Salidas • x solución del sistema de ecuaciones. 1: función GC_Generico(x(0),M, b) 2: k ← 0 3: r(0) ← b−Mx(0) 4: mientras ‖rk‖2 6= 0 hacer 5: k ← k + 1 6: si k = 1 entonces 7: u1 ← r(0) 8: si no 9: βk ← r(k−1)T r(k−1)/r(k−2)T r(k−2) 10: uk ← r(k−1) − βkuk−1 11: fin si 12: αk ← r(k−1)T r(k−1)/(uTkM Tuk) 13: x(k) ← x(k−1) + αkuk 14: r(k) ← r(k−1) − αkAuk 15: fin mientras 16: ← x(k) 17: fin función 26 2. Métodos Clásicos para el PMCL Algoritmo 2.9 GC aplicado a las ecuaciones normales Algoritmo GC_PMCL Descripción Resuelve el PMCL asociado a A y b usando GC para las ecuaciones normales. Entradas • x(0) Vector columna de n filas, iterado inicial. • A Matriz de rango completo de m filas por n columnas con m > n. • b Vector columna de n filas. • Nmax Entero positivo. Número máximo de iteraciones. • � Real positivo. Tolerancia mínima para detener la iteración. Salidas • x Solución al PMCL asociado a A y b. 1: función GC_PMCL(x(0), A, b,Nmax, �) 2: p(0) ← AT (b−Ax(0)) 3: s(0) ← p(0) 4: γ(0) ← ‖s(0)‖22 5: para k = 0, 1, . . . Nmax hacer 6: q(k) ← Ap(k) 7: αk ← γ(k)/‖q(k)‖22 8: x(k+1) ← x(k) + αkp(k) 9: s(k+1) ← s(k) − αk(AT q(k)) 10: γ(k+1) ← ‖s(k+1)‖22 11: si γ(k+1) < � entonces 12: ← x(k+1) 13: fin si 14: βk ← γ(k+1)/γ(k) 15: p(k+1) ← s(k+1) + βkp(k) 16: fin para 17: Error (Máximo de iteraciones alcanzado) 18: fin función 27 2. Métodos Clásicos para el PMCL 2.3. Precondicionamiento para las ecuaciones normales En muchos casos, la convergencia de los métodos iterativos puede resultar lenta, principal- mente en los casos de matrices mal condicionadas. Es por ello, que en los últimos años, se ha puesto mayor énfasis en precondicionamiento de la matriz de entrada más que en los propios métodos iterativos. Dada A ∈ Rn×n y b ∈ Rn, el precondicionamiento es una técnica que tiene como objetivo transformar el sistema de ecuaciones original Ax = b en otro sistema equivalente Ãx = b̃ tal que Ã sea una matriz mejor condicionada, y por consiguiente un método iterativo aplicado a este último sistema tenga una mayor velocidad de convergencia que para el primer sistema. Existen dos maneras fundamentales para precondicionar el sistema Ax = b: la primera es premultiplicar el sistema por una matriz M ∈ Rn×n no singular, así se obtiene el sistema equivalente MAx = Mb (2.31) Esta forma de precondicionamiento se denomina precondicionamiento por la izquierda. La otra forma de precondicionamiento, el precondicionamiento por la derecha proviene de postmultiplicar A por M−1, obteniendo el problema equivalente: AM−1y = b, Mx = y (2.32) En ambos casos la matriz M se denomina precondicionador del sistema de ecuaciones. Para el caso particular de las ecuaciones normales, en la literatura se encuentran técnicas de precondicionamiento por la derecha. En este caso, sea M ∈ Rn×n no singular, entonces el problema de optimización de la ecuación (1.2) es equivalente a min y ‖(AS−1)y − b‖2, Sx = y (2.33) Donde se elige S tal que la matriz AS−1 tenga un espectro más favorable que la matriz A. En general, es deseable que la matriz S tenga las siguientes características: AS−1 debe tener un número de condición más pequeño que A y debe tener pocos valores singulares distintos. En el caso de matrices sparse, S debe tener el mismo patrón de sparsidad de A, esto es, las posiciones de los elementos no nulos de S debería ser igual a las de A. Debe ser de bajo costo computacional resolver sistemas de ecuaciones que involucren a S y a ST . Una vez hallado un precondicionador S, se procede a resolver el sistema de ecuaciones normales precondicionadas (AS−1)T (AS−1)x = (AS−1)T b (2.34) 28 2. Métodos Clásicos para el PMCL 2.3.1. Gradiente Conjugado Precondicionado para las ecuaciones normales Dado un precondicionador S ∈ Rn×n, puede reformularse el algoritmo (2.8) para resolver el sistema de ecuaciones (2.34). El GC precondicionado se presenta en el algoritmo (2.10) 2.3.2. Algunos precondicionadores conocidos Uno de los problemas actuales en el cálculo numérico es conseguir buenos precondiciona- dores. Lamentablemente no existe una teoría general para hallar precondicionadores que sean aplicables para cualquier sistema de ecuaciones, sino que lo usual es hallar precondicionadores aplicables a algún problema o familia de problemas particulares, basándose en información adicional del contexto de dichos problemas. No obstante, existen unos pocos precondicionadores que pueden aplicarse a una amplia gama de sistemas de ecuaciones. En este trabajo se presentarán los precondicionadores más populares para el PMCL. Precondicionador basado en la Factorización de Cholesky incompleta Sea S = R, donde R es el factor de Cholesky asociado a AAT , puede verificarse que (Ver [5]) κ(AS−1) = 1, lo cual implica que, en teoría, el sistema de ecuaciones normales precondicionado con S necesitaría de sólo una iteración para converger a la solución, para cualquier método iterativo. Este hecho teórico hace intuir que una aproximación al factor de Cholesky de AAT puede ser un buen precondicionador para el PMCL. Una forma de aproximación al factor de Cholesky, es la que se conoce como factorización de Cholesky incompleta, la cual consiste en una matriz R̃ ≡ R tal que C = ATA = RTR− E Donde E es una matriz de error. Se desea que ‖E‖ sea pequeña y R̃ sea sparse con un patrón de sparsidad similar a A. Para calcular dicha R̃ puede usarse el mismo algoritmo (2.1) de la factorización de Cholesky, pero calculando sólo aquellos elementos r̃ij tales que Aij 6= 0. El algoritmo (2.11) presenta la factorización de Cholesky incompleta. Para el algoritmo C = AAT y se define el patrón de sparsidad de A como el conjunto P = {(i, j), Aij 6= 0} y es una de las entradas del algoritmo. Obsérvese que no hace falta calcular de manera explícita la matriz AAT , sino que sólo hace falta acceder a las filas i y j cada vez que se desee calcular cij . Precondicionador basado en la factorización LU Una opción para el precondiciona- miento del PMCL en sistemas sobredeterminados, es usar la factorización de un subbloque cuadrado de A. Para una matriz de rango completo A ∈ Rm×n, si luego de permutaciones de filas sobre A se puede llegar a A = ( A1 A2 ) , A1 ∈ Rn×n (2.35) Tal que A1 sea no singular, entonces, A −1 1 se puede usar como precondicionador derecho, obteniendo el siguiente problema de optimización, equivalente al PMCL. 29 2. Métodos Clásicos para el PMCL Algoritmo 2.10 GC Precondicionado Algoritmo GCPrecondicionadoPMCL Descripción Resuelve el PMCL asociado a A y b usando GC precondicionado para las ecuaciones normales. Entradas • x(0) Vector columna de n filas, iterado inicial. • A Matriz de rango completo de m filas por n columnas con m > n. • b Vector columna de n filas. • S Matriz precondicionadora de n filas por n columnas. • Nmax Entero positivo. Número máximo de iteraciones. • � Real positivo. Tolerancia mínima para detener la iteración. Salidas • x Solución al PMCL asociado a A y b. 1: función GCPrecondicionadoPMCL(x(0), A, b, S,Nmax, �) 2: r(0) ← (b−Ax(0)) 3: Resolver el sistema ST y = AT r(0) 4: p(0) ← y 5: s(0) ← p(0) 6: γ(0) ← ‖s(0)‖22 7: para k ← 0, 1, . . . Nmax hacer 8: Resolver el sistema Sw = p(k) 9: t(k) ← w 10: q(k) ← At(k) 11: αk ← γ(k)/‖q(k)‖22 12: x(k+1) ← x(k) + αkt(k) 13: r(k+1) ← r(k) + αkq(k) 14: u← A(t)r(k+1) 15: Resolver el sistema ST z = u 16: s(k+1) ← z 17: γ(k+1) ← ‖s(k+1)‖22 18: si γ(k+1) < � entonces 19: ← x(k+1) 20: fin si 21: βk ← γ(k+1)/γ(k) 22: p(k+1) ← s(k+1) + βkp(k) 23: fin para 24: Error (Máximo de iteraciones alcanzado) 25: fin función 30 2. Métodos Clásicos para el PMCL Algoritmo 2.11 Factorización de Cholesky incompleta Algoritmo CholeskyIncompleto Descripción Calcula la factorización de Cholesky incompleta de una matriz C SPD con un patrón de sparsidad P . Entradas • C Matriz sparse n filas por n columnas SPD. • P Patrón de sparsidad para la factorización incompleta. Salidas • R Aproximación al factor de Cholesky de C con patrón de sparsidad P . 1: función CholeskyIncompleto(C,P ) 2: para i← 1 . . . n hacer 3: rii ← ( cii − ∑i−1 k=1 r 2 ki )1/2 4: para j ← 1 . . . n hacer 5: si (i, j) ∈ P entonces 6: rij ← cij − ∑i−1 k=1 rkirkj 7: rij ← rij/rii 8: fin si 9: fin para 10: fin para 11: ← C 12: fin función 31 2. Métodos Clásicos para el PMCL min y ‖AA−11 y − b‖2, y = A1x (2.36) En donde el PMCL asociado a AA−11 y b está mejor condicionado que el problema original y se puede resolver con cualquiera de los métodos iterativos cubiertos en las secciones anteriores. Una vez resuelto el PMCL precondicionado, la solución al PMCL original es x = A−11 y. En el caso general para evitar el cálculo explícito de la inversa, se prefiere trabajar con la factorización LU de A1. Precondicionador para GC basado en la factorización LU Un esquema de precondi- cionamiento basado en la factorización LU combinado con GC, y que puede usarse tanto para matrices sparse, como para matrices densas, se presenta en [5]. Si la ecuación (2.35) se sustituye en el problema de optimización (1.2), se obtiene el problema de optimización equivalente min x ∥∥∥∥ ( In C ) y − ( b1 b2 )∥∥∥∥ 2 , b = ( b1 b2 ) , (2.37) donde b1 ∈ Rn, b2 ∈ Rm−n, C = A2A−11 En este caso, con r = b−Ax, las ecuaciones normales pueden ser reescritas como  In 0 In0 Im−n 0 In C T 0     r1r2 y   =   b1b2 0   , r = ( r1 r2 ) (2.38) donde r1 ∈ Rn, r2 ∈ Rm−n Haciendo r2 = 0 se obtiene el siguiente sistema SPD( In + C TC ) r1 = C T (b2 − Cb1) (2.39) Con la solución del sistema de ecuaciones (2.39), se obtiene, finalmente, la solución al PMCL resolviendo el sistema A1x = b1 − r1 (2.40) El algoritmo (2.12) muestra cómo se adapta el GC al precondicionamiento LU explicado anteriormente. Sin embargo, se requiere del cálculo explícito de la matriz C = A2A −1 1 . Una mejora de dicho algoritmo para evitar el cálculo de C, se logra observando que, para cada iteración s = r + CT r = r + (LT )−1((UT )−1(AT2 r)) y q = Cp = A2(U −1(L−1p)) Por consiguiente, sólo basta conocer la factorización LU de A1, y en cada paso del algoritmo (2.12) en donde esté involucrada la matriz C, será preciso resolver varios sistemas de ecuaciones 32 2. Métodos Clásicos para el PMCL lineales donde existan productos matriz-vector de las formas L−1v, U−1v, (LT )−1v y (UT )−1v. Estos sistemas de ecuaciones son de bajo costo computacional, debido a que las matrices U y L son triangulares y sparse. 33 2. Métodos Clásicos para el PMCL Algoritmo 2.12 GC Precondicionado con Factorización LU Algoritmo GC_LU_PMCL Descripción Resuelve el PMCL asociado a A y b usando la GC precondicionado por factorización LU incompleta de un subbloque cuadrado de A. Entradas • x(0) Vector columna de n filas, iterado inicial. • A Matriz de rango completo de m filas por n columnas con m > n. • C Matriz precondicionadora de la forma C = A2A−11 donde A1 y A2 son subbloques de A luego de permutaciones de filas. • b Vector columna de n filas. • Nmax Entero positivo. Número máximo de iteraciones. • � Real positivo. Tolerancia mínima para detener la iteración. Salidas • x Solución al PMCL asociado a A y b. 1: función GC_LU_PMCL(x(0), A,C, b,Nmax, �) 2: r(0) ← b−Ax(0) 3: ρ (0) 1 ← r (0)(1 : n) 4: ρ (0) 2 ← r (0)((n+ 1) : m) 5: p(0) ← ρ(0)1 + C Tρ (0) 2 6: s(0) ← p(0) 7: γ(0) ← ‖s(0)‖22 8: para k = 0, 1, . . . Nmax hacer 9: q(k) ← Cp(k) 10: αk ← γ(k)/‖q(k)‖22 11: ρ (k+1) 1 ← ρ (k) 1 − αkp (k) 12: ρ (k+1) 2 ← ρ (k) 2 − αkq (k) 13: s(k+1) ← ρ(k+1)1 + C Tρ (k+1) 2 14: γ(k+1) ← ‖s(k+1)‖22 15: si γk+1 < � entonces 16: Resolver el sistema A1x = b1 − ρ1 17: ← x 18: fin si 19: βk ← γ(k+1)/γ(k) 20: p(k+1) ← s(k+1) + βkp(k) 21: fin para 22: Error (Máximo de iteraciones alcanzado) 23: fin función 34 Capítulo 3 El método de Schulz El centro no es un punto Si lo fuera, resultaría fácil acertarlo Roberto Juarroz - Segunda poesía vertical Como se vio en el capítulo 1, x = A†b, es la solución al PMCL para el caso de una matriz A de rango completo. Esto permite pensar que una matriz B ≈ A† podría ser un buen precondicionador para la ecuación (1). Por esto, es de interés hallar algún método iterativo que permita obtener una aproximación a A† con la precisión que se desee, y además, sería deseable que dicho método posea una alta velocidad de convergencia. Un método que puede utilizarse para calcular una aproximación a la matriz pseudoinversa de A es el Método iterativo de Schulz [37], un método que se deriva a partir de la iteración de Newton en espacios de matrices. Originalmente, este método fue ideado para el cálculo de la inversa de una matriz cuadrada; no obstante, en este trabajo se reutilizará para calcular una aproximación a la pseudoinversa de una matriz rectangular de rango completo. El resto del capítulo se organiza así: en la primera sección se dará una breve introducción al método de Newton en espacios de matrices. Posteriormente se presentará el método iterativo de Schultz y se estudiarán sus propiedades en cuanto a la convergencia y otros aspectos relevantes. Finalmente, el capítulo concluirá con el estudio de algunas propiedades interesantes de la iteración de Schultz para el caso de matrices rectangulares, las cuales dan una pista teórica de que la aproximación de Schultz es un buen precondicionador. 3.1. El método de Newton Uno de los problemas clásicos del cálculo numérico es hallar el cero de una función; esto es, dada F : Rn → Rn hallar ξ tal que F (ξ) = 0. Para el caso general de funciones no lineales diferenciables, elMétodo iterativo de Newton representa una de las alternativas más porderosas para el cálculo de los ceros de una función no lineal. Sea F : Rn → Rn, si para h ∈ Rn F es diferenciable en algún entorno de h, entonces, el teorema de Taylor [26], implica que para toda matriz X en un subconjunto convexo de dicho entorno de h se tiene la siguiente aproximación a F (X) F (X) ≈ F (h) + (X − h)DF (h) (3.1) 35 3. El Método de Schulz donde DF (h) es la derivada de F en h. En particular, si ξ es un cero de F , para alguna matriz X0 en un entorno de ξ, usando la ecuación (3.1) se tiene 0 = F (ξ) ≈ F (X0) + (ξ −X0)DF (X0) (3.2) Con la información anterior, puede deducirse la iteración de Newton en espacios de matrices a través de un modelo general para el estudio de los métodos iterativos para funciones no lineales: la iteración funcional de punto fijo. En el esquema de iteración funcional de punto fijo, un método iterativo para la función F : Rn → Rn se define a partir de una función Φ asociada a F de la siguiente manera xk+1 = Φ(xk) (3.3) esto es, que cualquier método iterativo para F puede redefinirse como una iteración que con- verja al punto fijo de una función Φ. Para un estudio general de las propiedades y condiciones de convergencia de los métodos de punto fijo, ver [22] y [39]. Así, a partir de la ecuación (3.2), si DF es no singular en un entorno de ξ puede obtenerse una iteración de punto fijo definiendo Φ de la siguiente manera Φ(x) ≡ x− (DF (x)) −1 F (x) (3.4) de donde se deduce la iteración de Newton en espacios de matrices para un iterado inicial X0 cercano al cero de F xk+1 = xk − (DF (xk)) −1 F (xk) (3.5) El algoritmo (3.1) muestra una implementación del esquema iterativo (3.5). En este caso, para evitar el cálculo explícito de la matriz inversa a DF , es necesario resolver un sistema de ecuaciones lineales en cada iteración. La convergencia del método de Newton en espacios de matrices queda establecida a partir del siguiente resultado. Teorema 3.1.1. Sea C ⊆ Rn un conjunto abierto y tal que F : C → Rn y sea C0 ⊆ C un conjunto convexo tal que F es diferenciable en C0 y continua en C. Dado x0 ∈ C0, sea r > 0 tal que {x, ‖x− x0‖2 < r} ⊆ C0 y sean constantes h, α, β y γ tales que r = α/(1 − h) y h = αβγ/2 < 1. Si F satisface las siguientes condiciones: 1. ‖DF (x)−DF (y)‖2 ≤ γ ‖x− y‖2 para todo x, y ∈ C0. 2. (DF (x))−1 existe para todo x ∈ C0 y ∥∥(DF (x))−1∥∥2 ≤ β. 3. ∥∥(DF (x0))−1F (x0)∥∥2 ≤ α. Entonces: 1. La sucesión xk+1 = xk − (DF (xk))−1, k = 0, 1, . . . está bien definida, y además satisface xk ∈ {x, ‖x− x0‖2 < r} para k = 1, 2, . . .. 2. limx→∞ xk = ξ <∞ y F (ξ) = 0. Demostración. Ver [31] 36 3. El Método de Schulz Algoritmo 3.1 Método de Newton en varias variables Algoritmo Newton_Varias_Variables Descripción Calcula el cero de la función F usando el método de Newton en espacios de matrices. Entradas • X0 Vector columna de n filas, iterado inicial. • Nmax Entero positivo. Número máximo de iteraciones. • � Real positivo. Tolerancia mínima para detener la iteración. Salidas • X cero de la función F . 1: función Newton_Varias_Variables(X0, Nmax, �) 2: para i← 1, . . . , Nmax hacer 3: v ← F (X0) 4: M ← DF (X0) 5: Resolver el sistema My = v 6: X1 ← X0 − y 7: si ‖X0 −X1‖2/‖X1‖2 < � entonces 8: ← X1 9: fin si 10: X0 ← X1 11: fin para 12: Error (Máximo de iteraciones alcanzado) 13: fin función 37 3. El Método de Schulz 3.2. El método de Schulz Sea A ∈ Rn×n no singular. En esta sección se construirá la iteración de Schulz como un caso particular del del método de Newton en espacios de matrices. Lo primero que se necesita es la definir F : Rn → Rn tal que F (A) = 0. Una función que satistace esta condición es F (X) = X−1 −A. El próximo paso será calcular la derivada de la función F definida anteriormente. Para esto se usará la expansión en series de Newman para una matriz, la cual queda establecida en el siguiente teorema Teorema 3.2.1. Sea A ∈ Rn×n tal que ‖A‖2 < 1, entonces, la matriz In − A es no singular y además: (In −A)−1 = ∞∑ k=0 Ak Demostración. Ver [22] Con F = X−1−A, escójase alguna matrizB ∈ Rn×n tal que ∥∥A−1B∥∥ 2 ≤ ∥∥A−1∥∥ 2 ‖B‖2 < 1, esto es, B es una matriz cercana a 0. Se desea acotar el diferencial F (A + B) − F (B) −DB para alguna matriz D. En primera instancia, la expresión para F (A+B)− F (B) es F (A+B)− F (B) = ( (A+B)−1 −A ) − ( A−1 −A ) = (A+B)−1 −A−1 = ( A ( In +A −1B ))−1 −A−1 = A−1 ( In +A −1B )−1 −A−1 = (( In +A −1B )−1 − In)A−1 Sustituyendo la expansión del teorema (3.2.1) en la expresión anterior F (B +A)− F (A) = ( ∞∑ k=0 (−1)k(A−1B)kA−1 − In ) A−1 = −A−1BA−1 + ∞∑ k=2 (−1)k(A−1B)kA−1 Finalmente, la expresión para el diferencial viene dada por F (A+B)− F (B)− (−A−1BA−1) = ∞∑ k=2 (−1)k(A−1B)kA−1 (3.6) Por otro lado, se obtendrá una cota para la norma del miembro derecho de la ecuación (3.6) 38 3. El Método de Schulz ∥∥∥∥∥ ∞∑ k=2 (−1)k(A−1B)kA−1 ∥∥∥∥∥ 2 ≤ ∥∥∥∥∥ ∞∑ k=2 (−1)k(A−1B)k ∥∥∥∥∥ 2 ∥∥A−1∥∥ 2 ≤ ∥∥A−1B∥∥2 2 ∥∥A−1∥∥ 2 ∥∥∥∥∥ ∞∑ k=0 (−1)k(A−1B)k ∥∥∥∥∥ 2 ≤ ∥∥A−1∥∥3 2 ‖B‖22 ∞∑ k=0 ‖(A−1B)k‖2︸ ︷︷ ︸ Serie Geométrica (3.7) = ∥∥A−1∥∥3 2 ‖B‖22 1− ‖A−1B‖2 (3.8) Así, con la cota anterior∥∥F (A+B)− F (B)− (−A−1BA−1)∥∥ 2 ‖B‖2 ≤ ∥∥A−1∥∥3 2 1− ‖A−1B‖2 ‖B‖2 (3.9) Luego, el límite cuando la matriz B tiende a 0 es lim ‖B‖2→0 ∥∥F (A+B)− F (B)− (−A−1BA−1)∥∥ 2 ‖B‖2 = 0 (3.10) Lo cual implica que F es diferenciable en X = A y además, por la definición de derivada, DF (A)B = −A−1BA−1. Esta expresión se puede sustituir en la función de iteración del método de Newton, obteniendo: Mk+1 = Φ(Mk) ⇒Mk+1 = Mk − (DF (Mk))−1F (Mk) ⇒DF (Mk)(Mk+1 −Mk)M−1k = F (Mk) = M −1 k −A ⇒M−1k (Mk+1 −Mk)M −1 k = M −1 k −A ⇒Mk+1 −Mk = Mk −MkAMk (3.11) De donde se puede obtener la iteración del método de Schulz Mk+1 = 2Mk −MkAMk (3.12) La condición de convergencia para el método de Schulz se presenta en el siguiente teorema Teorema 3.2.2. La sucesión {Mk} definida por la ecuación (3.12) converge a A−1 si y sólo si ρ(I −AM0) < 1. Demostración. Para k = 1,2,. . . se tiene que In −AMk+1 =In −A(2Mk −MkAMk) =(AMk)2 − 2AMk + In =(In −AMk)2 39 3. El Método de Schulz Con esta relación, puede deducirse indutivamente que In −AMk =(In −AMk−1)2 =((In −AMk−2)2)2 =(((In −AMk−3)2)2)2 = · · · =(In −AM0)2 k Sea la sucesión Bk = AMk, combinándola con el resultado anterior, se puede reescribir como Bk = In − (In −B0)2 k (3.13) Nótese que limk→∞(I − B0)2 k = 0 o equivalentemente, limk→∞Bk = In si y solo si ρ(I −B0) = ρ(I −AM0) < 1. Por otro lado, limk→∞Bk = limk→∞AMk = In de donde se puede deducir que limk→∞Mk = A−1 lo cual concluye la prueba. Para cerrar esta sección, se presentará un resultado que da una elección de iterado inicial del método de Schulz que garantiza la convergencia para cualquier matriz A no singular. Teorema 3.2.3. Sea A ∈ Rn×n y sea p(x) = a0 + a1x+ · · ·+ anxn un polinomio. Si λ es un autovalor de A, entonces p(λ) es un autovalor de P (A). Demostración. Ver [22] Teorema 3.2.4. La elección del iterado inicial M0 = AT ‖A‖2 ‖AT ‖2 garantiza la convergencia del método de Schulz. Demostración. Con tal elección de M0, se tiene que In − AM0 = I − (1/‖A‖2‖AT ‖2)AAT . Considerando el polinomio p(x) = (1 − x)(‖A‖2‖AT ‖2), puede verificarse que AAT = p(In − AM0). Así, por el teorema (3.2.3) se cumple que para todo λ, autovalor de In −AM0, p(λ) = (1− λ)(‖A‖2‖AT ‖2) es autovalor de AAT . Como A es no singular, AAT es SPD, por lo cual todos sus autovalores son positivos. En particular, para todo λ, autovalor de In −AM0 se cumple (1− λ)(‖A‖2‖AT ‖2) > 0⇒ 1− λ > 0⇒ λ < 1 Luego ρ(In −AM0) < 1 y se asegura la convergencia del método. Usando la el iterado inicial del teorema anterior el algoritmo (3.2) presenta el método de Schulz para una matriz A ∈ Rn×n 40 3. El Método de Schulz Algoritmo 3.2 Iteración de Schulz Algoritmo IterSchulz Descripción Dada una matriz A, calcula una aproximación a A† a través del método de Schulz. Entradas • A Matriz de rango completo de m filas por n columnas con m > n. • Nmax Entero positivo. Número máximo de iteraciones. • � Real positivo. Tolerancia mínima para detener la iteración. Salidas • M aproximación a A†. 1: función IterSchulz(A, �,Nmax) 2: M0 ← AT /‖A‖22 3: para i← 1, . . . , Nmax hacer 4: U ←M0A 5: V ← 2In − U 6: M1 ← VM0 7: si ‖M0 −M1‖2/‖M1‖2 < � entonces 8: ← M1 9: fin si 10: M0 ←M1 11: fin para 12: Error (Máximo de iteraciones alcanzado) 13: fin función 41 3. El Método de Schulz 3.3. Propiedades del Método de Schulz para matrices rectan- gulares En la sección anterior, se presentó el método de Schulz para matrices cuadradas A ∈ Rn×n. Este método puede ser aplicado también para matrices rectangulares A ∈ Rm×n de rango completo, en cuyo caso, converge a A†, tal como lo establece el siguiente resultado. Teorema 3.3.1. Sea A ∈ Rm×n de rango completo. La iteración de Schulz (3.12) converge a A † si y sólo si ρ(I −AM0) < 1. Demostración. En primer lugar, es necesario verificar que la expresión de la iteración de Schulz esté bien definida respecto a las dimensiones de la matriz. Para ello es necesario que M0 ∈ Rn×m. Se sabe que A ∈ Rm×n y M0 = AT /‖A‖22 ∈ R n×m. Así, para todo k > 0 se cumple que Mk ∈ Rn×m y por lo tanto MkA ∈ Rn×n. Puede observarse que de esta forma también se cumple la relación de recurrencia para (In−AMk) establecida en la prueba del teorema (3.2.2), luego, sustituyendo A −1 por A† en la prueba de dicho teorema se verifica este resultado. Para finalizar el capítulo, se probarán dos propiedades relevantes de la iteración de Schulz para el caso de matrices rectangulares: en primer lugar, para cualquier iterado Mk si x es la solución al PMCL asociado a A, entonces se cumple una condición de ortogonalidad sobre Mk(b−Ax) análoga a la condición de ortogonalidad para el PMCL. Este es uno de los resultados teóricos centrales de este trabajo, pues da una indicación, en la teoría, de que la elección de Mk para precondicionar el PMCL es correcta en el sentido que el sistema precondicionado (que será de n ecuaciones con n incógnitas, a diferencia del sistema original que es de m ecuaciones con n incógnitas) tendrá la misma solución del sistema original, esto es la solución al PMCL. En segundo lugar, se probará que si λki es el i-ésimo autovalor de MkA, entonces λki es la k-ésima iteración de Newton para hallar el cero de la función f(x) = 1− 1/x (evidentemente el cero de f es x = 1) con iterado inicial λ0i , el i-ésimo autovalor de la matriz M0A. Este es el segundo resultado central de este trabajo, pues muestra en teoría, que los iterados Mk tienden a hacer un agrupamiento (clustering) de los autovalores del sistema inicial, en torno a λ = 1, lo cual es una de las propiedades ideales de un buen precondicionador: agrupar el espectro de autovalores de la matriz original en torno a un único valor. Durante la realización de este trabajo, no se encontró ninguna referencia en la literatura de la demostración de estas propiedades. Antes de demostrar estos teoremas, se probará un resultado preliminar: que los autovalores de la matriz MkA para k = 0, 1, . . . están acotados, tal como se establece a continuación. Teorema 3.3.2. Sea A ∈ Rm×n de rango completo con m ≥ n y sea Mk la k-ésima iteración de Schulz para k = 0, 1, . . . , entonces para todo autovalor λ de MkA, 0 < λ ≤ 1. Demostración. Para k = 0 se tiene que M0 = AT /‖A‖22, así M0A = A TA/‖A‖22. Como A es de rango completo, entonces M0 es SPD y por consiguiente todos sus autovalores son reales y positivos. Por otro lado, se tiene que para cualquier matriz M , el radio espectral ρ(M) es el ínfimo de todas las normas matriciales (Ver [22]), así ρ(M0) = ρ ( ATA ‖A‖22 ) ≤ ∥∥∥∥ATA‖A‖22 ∥∥∥∥ 2 < 1 42 3. El Método de Schulz Lo cual concluye la prueba para k = 0. Ahora, sea k ≥ 1, con la recurrencia obtenida en la prueba del teorema (3.2.2), se tiene que MkA = In − (In −M0A) 2k Si se considera el polinomio p(x) = 1 − (1 − x)2 k , entonces MkA se puede reescribir como MkA = p(M0A) y por el teorema (3.2.3) los autovalores de MkA son de la forma Λ(λ) = 1 − (1 − λ)2 k para λ autovalor de M0A. En vista de que 0 < λ < 1, se sigue que 0 < Λ(λ) < 1, lo cual prueba el teorema para k ≥ 1. Con el teorema anterior, también se puede establecer una cota para los autovalores de la matriz 2In −MkA, a través del siguiente cololario Corolario 3.3.1. Para Mk, k = 0, 1, . . . se verifica que para todo autovalor λ de 2In −MkA, 1 < λ < 2. Demostración. Basta considerar el polinomio p(x) = 1 + x, así 2In −MkA = p(In −MkA), como los autovalores de In −MkA están en el intervalo (0, 1) se verifica el resultado. Teorema 3.3.3. Sea A ∈ Rm×n de rango completo con m ≥ n, sea b ∈ Rn y sea Mk la k-ésima iteración de Schulz para k = 0, 1, . . . , entonces, xM es la solución al PMCL asociado a A y b, si y solo si cumple la siguiente condición de ortogonalidad: Mk(b−AxM ) = 0, k = 0, 1, . . . Demostración. La demostración de hará por inducción sobre k. Para k = 0 se tiene que M0 = AT /‖A‖22, por la condición de ortogonalidad del teorema (1.2.1) para el PMCL se satisface M0(b−AxM ) = 1 ‖A‖22 ( AT (b−Ax) ) = 0 si y solo si xM es la solución al PMCL, lo cual prueba el resultado en este caso. Para k = i, por hipótesis inductiva se satisface la condición de ortogonalidad Mi(b − AxM ) = 0. Falta verificar que el resultado se cumple para k = i + 1. En este caso, la (i + 1)-ésima iteración de Schulz es de la forma Mi+1 = 2Mi −MiAMi = (2In −MiA)Mi así Mi+1(b−AxM ) = (2In −MiA)Mi(b−AxM ) Obsérvese que usando la hipótesis inductivaMi(b−AxM ) = 0 si y solo si xM es la solución al PMCL, y por consiguiente (2In −MiA)Mi(b − AxM ) = 0 si y solo si xM es la solución al PMCL. No obstante, para garantizar que no puede haber alguna cancelación trivial del producto (2In −MiA) por algún vector, es necesario verificar que si b′ ∈ Rn entonces (2In −MiA) b′ = 0 si y solo si b′ = 0. Esto se cumple cuando la matriz (2In −MiA) es no singular, lo cual 43 3. El Método de Schulz está garantizado pues todos los valores de esta matriz son distintos a 0. En efecto, por el corolario (3.3.1) todos los autovalores de (2In −MiA) se encuentran en el intervalo (1, 2). Luego, haciendo b′ = Mi(b − AxM ) se verifica el teorema para k = i + 1, lo cual finaliza la prueba. Teorema 3.3.4. Sea A ∈ Rm×n de rango completo con m ≥ n, sea Mk la k-ésima iteración de Schulz y sea λki el i-ésimo autovalor de la matriz MkA, entonces λki es igual a la k-ésima iteración del método de Newton en una variable para f(x) = 1 − 1/x con iterado inicial λ0i , el i-ésimo autovalor de la matriz M0A, con M0 = AT /‖A‖22. Demostración. Para f(x) = 1 − 1/x se tiene que f ′(x) = 1/x2, así, la iteración de Newton para f(x) es de la forma. xk+1 =xk − f(xk) f ′(xk) =xk − xk (1− xk) =2xk − x2k =1− (1− xk) 2 Esta iteración puede reescribirse como 1− xk+1 = (1− xk) 2 Y así, puede deducirse inductivamente la siguiente relación de recurrencia 1− xk = (1− xk−1) 2 = ( (1− xk−2) 2 )2 = (( (1− xk−3) 2 )2)2 = · · · =1− (1− x0)2 k por lo tanto, la iteración de Newton de f(x) es igual a xk = 1− (1− x0) 2k Por otro lado, para k ≥ 1, se sabe que MkA = In − (In −M0A) 2k . Si se considera el polinomio p(x) = 1−(1− x)2 k , por el teorema (3.2.3), como p(M0A) = MkA entonces p(λ0i) = 1− (1− λ0i) = λki , lo cual completa la prueba. 44 Capítulo 4 El Método de Aceleración de Convergencia Richardson-PR2 Lo supieron los arduos alumnos de Pitágoras los astros y los hombres vuelven cíclicamente Jorge Luis Borges - El otro, el mismo El método de Richardson de primer orden, presentado en la sección (2.2.1) para resolver las ecuaciones normales, fue propuesto originalmente en 1910 (Ver [32]) como un método iterativo para sistemas de ecuaciones lineales de la forma Ax = b donde A ∈ Rn×n y b ∈ Rn. Este método iterativo, obtenido en principio a partir de un splitting regular de A, puede generalizarse de la siguiente manera x(k+1) = x(k) + λkr(k) r(k+1) = b−Ax(k) (4.1) Así, el método de Richardson puede generalizarse como un esquema iterativo donde la dirección de búsqueda viene dada por el residual r(k) = b − Ax(k), mientras que el tamaño de paso λk, que ahora puede variar en cada iteración, se calcula de distintas formas. Algunas estrategias para la construcción de la sucesión {λk} pueden encontrarse en [44], [43] y [29], entre otros. Hay que hacer notar que la investigación en torno a métodos iterativos basados en la iteración general de Richardson no fue muy prolija a lo largo del siglo XX, debido a que estos métodos se consideraban poco competitivos frente a otras familias de métodos iterativos por algunas características poco favorables, tales como su lenta velocidad de convergencia e inestabilidada numérica (Ver [35]) Sin embargo, en los últimos años, ha resurgido algún interés académico en métodos tipo Richardson, motivado principalmente al hecho de que las técnicas de precondicionamiento re- legan a un segundo plano la elección del método iterativo. En la práctica computacional, se ha determinado que si se logra elegir un precondicionador eficiente para un sistema de ecuaciones lineales, la rápida convergencia a su solución se logra independientemente del método iterativo utilizado. Por lo tanto, en ciertos escenarios no merece la pena invertir recursos computacio- nales en estrategias complejas para generar direcciones de búsqueda, cuando la simple elección del vector residual como dirección de búsqueda puede garantizar una muy buena velocidad de convergencia. En [27] se pueden encontrar varias referencias de investigaciones recientes de 45 4. El Método Richardson-PR2 métodos iterativos que usan el vector residual como dirección de búsqueda (también conocidos como métodos iterativo-residuales). Otra línea de pensamiento bastante novedosa busca apli- car esquemas iterativo-residuales ideados incialmente para ecuaciones no lineales en espacios de matrices, a la resolución de sistemas de ecuaciones lineales [23]. En este orden de ideas, en lo que resta del capítulo se presentará un esquema iterativo de aceleración de convergencia basado en la iteración de Richardson, el cual fue propuesto en [7]; conocido como Método de aceleración de Richardson PR2 (Richardson-PR2). El objetivo es utilizar este método iterativo sobre el PMCL precondicionado con la apro- ximación de Schultz. 4.1. El esquema de aceleración de convergencia Richardson- PR2 Supóngase que se tiene un método iterativo que sigue el esquema de la ecuación (4.1), el cual genera una sucesión de iterados { x(k) } que converge a la solución del sistema Ax = b. Se puede definir una nueva sucesión { y(k) } de iterados asociados al problema inicial, de la siguiente manera y(k) = x(k) − λkz(k) ρ(k) = r(k) + λkAz(k) (4.2) Donde ρ(k) = b − Ay(k) es el nuevo residual asociado a y(k) y z(k) es una nueva dirección de búsqueda, que en principio, puede elegirse de manera arbitraria. Si se logra que a partir de este esquema, la sucesión { y(k) } converja más rápidamente a la solución del sistema de ecuaciones que la sucesión { x(k) } , entonces se dice que el esquema iterativo de la ecuación (4.2) es un esquema de aceleración de convergencia del método iterativo (4.1). Para conseguir un esquema de aceleración de convergencia, una elección usual para el tamaño de paso λk es el valor que minimice algún funcional asociado a ρ(k) para k = 1, 2, . . . , mientras que z(k) puede escogerse, entre otras formas, como la proyección de r(k) sobre algún subespacio de Rn. En las siguientes secciones se mostrarán las estrategias de elección del tamaño de paso y la dirección de búsqueda en Richardson-PR2. 4.2. Elección del tamaño de paso para el esquema Richardson- PR2 Para el esquema Richardson-PR2, el tamaño de paso λk se elige de tal forma que minimice el funcional F (λk) ≡ ‖ρ(k)‖2. El argumento que minimiza al funcional anterior, se obtiene a partir del siguiente teorema Teorema 4.2.1. El argumento que minimiza a F (λk) ≡ ‖ρ(k)‖2 para k = 1, 2, . . . con ρ(k) definido como en la ecuación (4.2) es λkmin = − ( Az(k) )T r(k) ‖Az(k)‖22 46 4. El Método Richardson-PR2 Demostración. Ver [20] Evidentemente, tal argumento también minimiza a ‖ρ(k)‖22 = ρ (k)Tρ(k), así, sustituyendo λkmin en la expresión de ρ (k)Tρ(k) min λk ( ‖ρ(k)‖22 ) = ρ(k)Tρ(k) = ( r(k)T + λk(Az (k))T )( r(k) + λk(Az (k)) ) = r(k)T r(k) + 2λk(Az (k))T r(k) + λ2k(Az (k))T (Az(k)) = ‖r(k)‖22 − ( (Az(k))T r(k) )2( (Az(k))T (Az(k)) )2 = ‖r(k)‖22 − ( (Az(k))T r(k) )2 ‖Az(k)‖22‖r(k)‖ 2 2 ‖r(k)‖22 = ( 1− cos2(θk) ) ‖r(k)‖22 Luego min λk ( ‖ρ(k)‖22 ) = ‖r(k)‖22 sen(θk) (4.3) Donde θk es el ángulo entre los vectores r(k) y Az(k). Lo anterior implica que con dicha elección de λk se cumple que ‖ρ(k)‖2 ≤ ‖r(k)‖2 Y así, en vista de que r(k) → 0 cuando k →∞, entonces se deduce que lim k→∞ ‖ρ(k)‖2 ‖r(k)‖2 = 0 ⇐⇒ lim k→∞ θk = 0 ó lim k→∞ θk = π (4.4) Esto es, para la elección de λk del teorema (4.2.1), es condición necesaria y suficiente para la convergencia de ρ(k) que θk → 0 o bien θk → π. Por otro lado, si se define Pk = Az(k)z(k)T ‖Az(k)‖22 (4.5) Se tiene que ρ(k) = (In − Pk)r(k) (4.6) Obsérvese que P 2k = Pk y P T k = Pk, lo que implica que la matriz In − Pk es un proyector ortogonal. Esto permite obtener una interpretación geométrica de la elección del tamaño de paso para el esquema de aceleración Richardson-PR2: en este esquema, el residual ρ(k) es la proyección ortogonal de r(k) sobre Az(k). 47 4. El Método Richardson-PR2 4.3. Elección de la dirección de búsqueda para Richardson-PR2 A partir de la condición (4.4) para la convergencia de ρ(k) se deduce que el residual del esquema de aceleración de convergencia tiende a 0 si y sólo si los vectores r(k) y Az(k) son colineales, esto es, z(k) = ±αA−1r(k) con α ∈ R y α 6= 0. En particular, se puede elegir α = 1. Sin embargo, tal valor para z(k) no es de utilidad en la práctica, pues se tendría que encarar el problema de obtener A−1. Una alternativa más plausible es sustituir A−1 por una sucesión de matrices {Ck} tal que {Ck} converja a A−1 y así, la dirección de búsqueda utilizada, será z(k) = Ckr (k) (4.7) 4.4. El método iterativo de Richardson-PR2 Con la dirección de búsqueda y el tamaño de paso hallados tal como se explicó en las dos secciones precedentes, el esquema de aceleración Richardson-PR2 queda de la siguiente manera y(k) = x(k) − λkCkr(k) ρ(k) = r(k) + λkACkr(k) λk = − (ACkr(k)) T r(k) ‖ACkr(k)‖22 (4.8) Como puede observarse, este esquema de aceleración de convergencia, se obtuvo como una combinación de precondicionamiento por la izquierda y proyección. Finalmente, aplicando la estrategia de reinicio o cycling (Ver [8]), sea x(k+1) = y(k) y r(k+1) = ρ(k), entonces se obtiene el método iterativo Richardson-PR2, el cual, dado un iterado inicial x(0) y haciendo r(0) = b−Ax(0) queda así x(k+1) = x(k) + λkCkr(k) r(k+1) = r(k) − λkACkr(k) λk = (ACkr(k)) T r(k) ‖ACkr(k)‖22 (4.9) Nótese que para k = 1, 2, . . . el residual puede calcularse en su forma explícita r(k) = b−Ax(k), no obstante, en la práctica se prefiere utilizar la fórmula del residual de la ecuación (4.9) la cual se conoce como residual iterativo o residual implícito. En efecto, el residual iterativo suele ser más estable desde el punto de vista numérico que el residual explícito, incluso, en la mayoría de los problemas numéricos, sucede que el residual explícito puede quedarse estancado luego de cierto número de iteraciones, mientras que el residual iterativo sigue decreciendo en norma. Una observación final: Si se hace Ck = In para todo k, se obtiene el método iterativo de Richardson clásico. En el algoritmo (4.1) se presenta el método de Richardson-PR2 en su forma cruda. Se asume que existe algún procedimiento para obtener el precondicionador Ck en cada iteración, dado que se conoce un precondicionador inicial C0. 48 4. El Método Richardson-PR2 Algoritmo 4.1 Iteración de Richardson-PR2 cruda Algoritmo RichardsonPR2Crudo Descripción Resuelve el sistema Ax = b usando el método iterativo Richardson-PR2. Entradas • A Matriz de n filas por n columnas • b Vector columna de n filas. • x(0) Vector columna de n filas, iterado inicial. • C(0) Matriz de n filas por n columnas, precondicionador inicial. • Nmax Entero positivo. Número máximo de iteraciones. • � Real positivo. Tolerancia mínima para detener la iteración. Salidas • x Solución al sistema Ax = b. 1: función RichardsonPR2Crudo(A, b, x(0), C0, Nmax, �) 2: r(0) ← b−Ax(0) 3: para i← 0, 1 . . . , Nmax hacer 4: v ← Ckr(k) 5: u← Av 6: p1 ← uT r(k) 7: p2 ← uTu 8: λk ← p1/p2 9: x(k+1) ← x(k) + λkv 10: r(k+1) ← r(k) − λku 11: si ‖r(k+1)‖2 < � entonces 12: ← x(k+1) 13: fin si 14: fin para 15: Error (Máximo de iteraciones alcanzado) 16: fin función 49 4. El Método Richardson-PR2 4.5. Algunas estrategias de precondicionamiento para Richardson- PR2 En esta sección se presentarán tres posibles estrategias para la elección de la sucesión de precondicionadores {Ck} para la iteración Richardson-PR2; estas son: Precondicionador constante, Precondicionador lineal iteratvo y Precondicionador cuadrático iterativo. Un estu- dio detallado y referencias bibliográficas de estas estrategias de precondicionamiento pueden encontrarse en [8]. El caso particular de los precondicionadores lineal y cuadrático iterativos son de escaso interés en este trabajo y se presentan con propósitos informativos. Estas dos estrategias de precondicionamiento requieren el cálculo explícito de la sucesión de matrices {Ck}, algo que para el caso del precondicionamiento del PMCL se hará a través de la sucesión de iteraciones de Schulz. 4.5.1. Precondicionador constante La estrategia más simple de precondicionamiento para Richardson-PR2 se basa en selec- cionar Ck = C0 para k = 0, 1, 2, . . . una matriz precondicionadora constante. En este caso, lo usual es buscar alguna matriz que se aproxime a A−1 y que sea simple de calcular. Dos de las elecciones más usuales para un precondicionador constante son Ck = diag(A)−1 En este caso ni siquiera es necesario formar explícitamente las matrices Ck, basta con almacenar los valores a −1 ii de la inversa de la diagonal de A Ck = L−1 donde L es la matriz que contiene la porción triangular inferior de A, in- cluyendo la diagonal principal. Este precondicionador, además de dar una velocidad de convergencia bastante aceptable, no requiere el cálculo explícito de L−1. En efecto, el producto L−1r(k) se puede obtener a través el sistema de ecuaciones Lx = r(k), el cual se puede resolver por una simple sustitución hacia adelante, por ser L una matriz triangular. 4.5.2. Precondicionador lineal iterativo Si se conoce un splitting regular de A de la forma A = M − N , donde M es una matriz no singular, se puede definir un esquema iterativo para Ck. Sustituyendo A = M − N en la relación In = AA−1 se obtiene A−1 = (M−1N)A−1 +M−1 (4.10) Lo cual se puede convertir en el procedimiento iterativo Ck+1 = (M −1N)Ck +M −1 (4.11) Con C0 una matriz arbitraria. Esta sucesión converge linealmente a A−1. 4.5.3. Precondicionador cuadrático iterativo Si las matrices de la sucesión {Ck} se obtienen a través de una relación recursiva de la forma 50 4. El Método Richardson-PR2 Ck+1 = CkUk +Dk (4.12) Donde {Uk} y {Dk} son un par de sucesiones de matrices arbitrarias. Considérese la matriz residual Rk definida como Rk = In −ACk (4.13) Entonces, puede hacerse un refinamiento iterativo de la sucesión (4.12) haciendo Dk = Ck y Uk = Rk y así se obtiene una nueva relación de recurrencia para las matrices Ck Ck+1 = CkRk + Ck = Ck (In +Rk) (4.14) La cual converge cuadráticamente a A−1. 51 4. El Método Richardson-PR2 52 Capítulo 5 Precondicionamiento del PMCL por la Aproximación del Método de Schultz La programación es una de las disciplinas más complejas en las matemáticas aplicadas. Es preferible que los malos matemáticos se queden haciendo matemáticas puras Edsger W. Dijkstra En este capítulo se presentarán los algoritmos para la resolución del PMCL a través del mé- todo ierativo Richardson-PR2 , usando como precondicionador la aproximación a A† obtenida por el método de Schultz. Se proponen las siguientes variantes sobre el método Richardson- PR2: Algoritmo 1: Precondicionamiento por la izquierda explícito para la matriz A, usandoMj para algún j > 1. De esta forma se resolverá el sistema MjA = Mjb usando Richardson- PR2 con Ck = In para k = 0, 1, 2, . . . la k-ésima iteración de Richardson-PR2, lo cual es equivalente al método de Richardson de primer orden. Algoritmo 2: Se extenderá el método Richardson-PR2 para iterar directamente sobre la matriz rectangular A, siendo Ck = Mk para k = 0, 1, 2, . . . En la próxima sección se analizará cómo la matriz Mk, mejora el condicionamiento del PMCL. Posteriormente se presentarán los algoritmos para el cálculo del producto matriz- matriz MkA y el producto matriz-vector Mkb. El resto del capítulo muestra los algoritmos mencionados en el párrafo anterior. 5.1. Mk como precondicionador al PMCL Al utilizar la matriz Mk para el precondicionamiento por la izquierda del PMCL, se tiene que el número de condición asociado al PMCL dado por la definición (2.0.4) se convierte en κLS(MkA,Mkb) = κ(MkA) + κ 2(MkA) ‖Mk(b−AxLS)‖2 ‖MkA‖2‖xLS‖2 (5.1) donde xLS es la solución al PMCL. 53 5. Precondicionamiento del PMCL Obsérvese que por el teorema (3.3.3) AT (b −MkxLS) = 0. Por lo tanto, se deduce que el precondicionamiento por la izquierda del PMCL, usando la matriz Mk elimina el término de orden κ2(A) en el número de condición del PMCL. Así, finalmente, el número de condición del PMCL precondicionado por Mk es igual a κLS(MkA,Mkb) = κ(MkA) (5.2) Luego, en vista de queMk → A†, a partir de cierto valor de k, se garantiza que κLS(Mk,Mkb) ≈ 1. Lo anterior muestra que los iterados de Schultz introducen una gran mejora en el condi- cionamiento del PMCL, garantizando que cualquier método iterativo pueda converger rápida- mente a la solución xLS . 5.2. Cálculo de MkA y Mkb Como se vio en la demostración del teorema (3.2.2), el iterado de Schultz Mk satisface la siguiente relación de recurrencia In −MkA = (I −M0A) 2k (5.3) De donde se obtiene MkA = In − (I −M0A) 2k (5.4) Esta relación de recurrencia puede utilizarse en el cálculo de MkA, observando que dada una matriz C ∈ Rn×n la potencia Cp con p ∈ N y p = 2k para algún k ≥ 0 puede expresarse de forma recursiva como sigue Cp =   C p 2C p 2 = ( C p 2 )2 si p > 1 C si p = 1 In si p = 0 (5.5) De esta forma, la potencia C2 k se puede calcular iterativamente realizando k productos matriz-matriz. En el algoritmo (5.1) se presenta el procedimiento para el cálculo de dicha potencia. Usando la estrategia del algoritmo (5.1), el algoritmo (5.2) calcula la matriz MkA para A ∈ rmn y k ≥ 1 Para el cálculo del producto matriz-vector Mkb con b ∈ Rm, debe observarse que a partir de iteración de Schulz se tiene, para k ≥ 1 Mkb = (2Mk−1 −Mk−1AMk−1) b = 2Mk−1b− (Mk−1A)Mk−1 (5.6) Sustituyendo el valor de Mk−1A obtenido a partir de la ecuación (3.13) se obtiene 54 5. Precondicionamiento del PMCL Algoritmo 5.1 Cálculo de la potencia A2 k para una matriz A Algoritmo Potencia Descripción Dada una matriz cuadrada A, calcula la potencia A2 k para k > 0 entero. Entradas • A Matriz de n filas por n columnas. Salidas • P Matriz, potencia 2k de A. 1: función Potencia(A, k) 2: si k = 0 entonces 3: ← In 4: fin si 5: P ← A 6: para i← 0, 1 . . . , k hacer 7: P ← P 2 8: fin para 9: ← P 10: fin función 55 5. Precondicionamiento del PMCL Algoritmo 5.2 Cálculo de la matriz MkA Algoritmo PrecondMkA Descripción Dada una matriz A rectangular, calcula la matrizMkA dondeMk es la k-ésima iteración de Schulz, para k > 0. Entradas • A Matriz de rango completo de m filas por n columnas con m > n. • k Entero positivo. Salidas • Matriz MkA. 1: función PrecondMkA(A, k) 2: P ← In − ( ATA/‖A‖22 ) 3: para i← 0, 1 . . . , k hacer 4: P ← P 2 5: fin para 6: ← In − P 7: fin función Mkb = 2Mk−1b− ( In − (In −M0A)2 k−1 ) Mk−1b = Mk−1b+ (In −M0A) 2k−1 b (5.7) Finalmente, combinando la ecuación (5.6) con el algoritmo (5.2), se puede hallar un nuevo algoritmo que calcula de una sola vez la matriz MkA y el vector Mkb. Basta con calcular el vector M0b antes de entrar en el ciclo principal del algoritmo, y así, en la k-ésima iteración se tendrá el producto matriz vector Mk−1b necesario para el cálculo de Mkb. Esta estrategia se ilustra en el algoritmo (5.3). 5.3. Algoritmo PMCL-Richardson-PR2 1 Para este algoritmo se precondiciona de manera explícita el PMCL y se resuelve el sistema precondicionado de n ecuaciones con n incógnitas MkA = Mkb usando el esquema de aceleracion Richardson-PR2 con Ck = In para k = 0, 1, . . . lo cual es equivalente al método de Richardson de primer orden. En el algoritmo (5.4) se presenta esta estrategia. 56 5. Precondicionamiento del PMCL Algoritmo 5.3 Cálculo de la matriz MkA y el vector Mkb Algoritmo PrecondMkAb Descripción Dada una matriz A rectangular y un vector b, calcula la matriz MkA y el vector Mkb donde Mk es la k-ésima iteración de Schulz, para k > 0. Entradas • A Matriz de rango completo de m filas por n columnas con m > n. • b Vector columna de m filas. • k Entero positivo. Salidas • Matriz MkA. • Vector Mkb. 1: función PrecondMkAb(A, b, k) 2: M ← AT /‖A‖22 3: v ←Mb 4: P ← In −MA 5: para i = 1, . . . k hacer 6: v ← v + Pv 7: P ← P 2 8: fin para 9: ← [In − P, v] 10: fin función 57 5. Precondicionamiento del PMCL Algoritmo 5.4 Algoritmo PMCL-Richardson-PR2 1 Algoritmo PCMLRichardsonPR2_1 Descripción Resuelve el PMCL asociado a A y b a través del método Richardson-PR2, usando como precondicionador una matriz Mk. Entradas • A Matriz de rango completo de m filas por n columnas con m > n. • b Vector columna de m filas. • x(0) Vector columna de n filas, iterado inicial. • k Entero positivo que indica el número de iteraciones de Schulz para generar el precondicionador. • Nmax Entero positivo. Número máximo de iteraciones. • � Real positivo. Tolerancia mínima para detener la iteración. Salidas • x Solución al sistema Ax = b. 1: función PCMLRichardsonPR2_1(A, b, x(0), k,Nmax, �) 2: [b′, C]← PrecondMkAb(A) 3: r(0) ← b′ − Cx(0) 4: para i← 0, 1, . . . , Nmax hacer 5: u← Cr(k) 6: p1 ← uT r(k) 7: p2 ← uTu 8: λk ← p1/p2 9: x(k+1) ← x(k) + λkr(k) 10: r(k+1) ← r(k) − λku 11: si ‖r(k+1)‖2 < � entonces 12: ← x(k+1) 13: fin si 14: fin para 15: fin función 58 5. Precondicionamiento del PMCL 5.4. Algoritmo PMCL-Richardson-PR2 2 El algoritmo PMCL-Richardson-PR2 2 es una adaptación del esquema de aceleración Richardson-PR2 para una matriz rectangular A ∈ Rm×n con m > n. De la ecuación (4.9) se observa que en una iteración de Richardson-PR2 es necesario realizar los siguientes produc- tos matriz-vector: Ckr (k). ACkr (k). Si la matriz A ∈ Rm×n es rectangular deben verificarse que los productos anteriores están bien definidos. Como r(k) = b − Ax(k) para k = 0, 1, . . . , entonces r(k) ∈ Rm; por otro lado, para que el producto A ( Ckr k ) esté bien definido, es necesario que Ckr(k) sea un vector en Rn, lo cual se cumple si Ck ∈ Rn×m. Así, si se tiene una sucesión {Ck} de matrices en Rn×m, entonces la iteración de Richardson-PR2 queda bien definida para una matriz rectangular. Por otro lado, para el caso de matrices cuadradas, en el esquema de aceleración Richardson-PR2 se escoge una sucesión de matrices Ck que converja a A−1, luego, una elección heurística para el caso rectangular, puede ser una sucesión de matrices Ck que converja a A†. A partir de lo anterior, se propone la elección de Ck = Mk donde Mk es el k-ésimo iterado del método de Schulz para A, con lo cual se define una extensión del esquema de aceleración Richardson-PR2 para matrices rectangulares. Para construir el algoritmo es necesario buscar una expresión para calcular Ck+1r(k+1) = Mk+1r (k+1) en función de los valores de la iteración k para k = 0, 1, . . . .Esta expresión puede hallarse combinando la igualdad r(k+1) = r(k) − λkAMkr(k+1), con la ecuación (5.4) siendo Pk ≡ (In −M0A)2 k , de donde se obtiene Mk+1r (k+1) = Mkr (k+1) + PkMkr (k+1) = Mk ( r(k) − λkAMkr(k) ) + PkMk ( r(k) − λkAMkr(k) ) = Mkr (k) − λkMkr(k) + PkMkr(k) + λkP 2kMkr (k) = (1− λk)Mkr(k) + PkMkr(k) + λkP 2kMkr (k) (5.8) Obsérvese que Pk+1 = P 2k , y por lo tanto se obtiene una relación de recurrencia que permite calcular Mk+1r(k+1) en función de los valores obtenidos en la iteración k. Utilizando la relación de recurrencia de la ecuación (5.8) en el algoritmo del esquema de aceleración Richardson-PR2 en su forma cruda, se obtiene una extensión para matrices rectangulares usando {Mk} como sucesión de precondicionadores. La implementación de este esquema de aceleracion Richardson-PR2 extendido se presenta en el algoritmo (5.5). Nótese que para este algoritmo, no se puede usar el valor de la norma del residual como condición de parada, pues en el caso del PMCL, no se garantiza que el residual converja a 0. No obstante, la condición de ortogonalidad de las soluciones del PMCL ofrece una condición para detener la iteración, que es la norma del vector AT r(k), la cual sí converge a 0 cuando x(k) converge a la solución. 59 5. Precondicionamiento del PMCL Algoritmo 5.5 Algoritmo PMCL-Richardson-PR2 2 Algoritmo PCMLRichardsonPR2_2 Descripción Resuelve el PMCL asociado a A y b a través del método Richardson-PR2, usando como precondicionador una matriz Mk. Entradas • A Matriz de rango completo de m filas por n columnas con m > n. • b Vector columna de m filas. • x(0) Vector columna de n filas, iterado inicial. • Nmax Entero positivo. Número máximo de iteraciones. • � Real positivo. Tolerancia mínima para detener la iteración. Salidas • x Solución al sistema Ax = b. 1: función PCMLRichardsonPR2_2(A, b, x(0), Nmax, �) 2: r(0) ← b−Ax(0) 3: M ← AT /‖A‖22 4: d←Mr(0) 5: C ← In −MA 6: para i← 0, 1 . . . , Nmax hacer 7: u← Ad 8: p1 ← uT r(k) 9: p2 ← uTu 10: λk ← p1/p2 11: x(k+1) ← x(k) + λkd 12: r(k+1) ← r(k) − λku 13: si ‖AT r(k+1)‖2 < � entonces 14: ← x(k+1) 15: fin si 16: d′ ← (1− λk) d+ Cd 17: C ← C2 18: d← d′ + λkCd 19: fin para 20: Error (Máximo de iteraciones alcanzado) 21: fin función 60 Capítulo 6 Experimentación Numérica Un algoritmo debe verse para creerse Donald Knuth Si no fallas al menos el noventa porciento de las veces, entonces no estás pensando en algo realmente grande Alan Kay En este capítulo se presentarán los resultados de varios experimentos numéricos realizados sobre la técnica de precondicionamiento para el PMCL desarrollada a lo largo de este trabajo. Las pruebas numéricas se dividen en dos grupos. En primer lugar, se estudiarán las propiedades de las matricesMk como precondicionadores. En los resultados numéricos se verificará que cuando k → ∞, la matriz Mk hace que los autovalores de MkA se agrupe en torno a x = 1, tal como lo sugiere el teorema (3.3.4). Así mismo, se mostrará que los autovalores de MkA se corresponden a los iterados del método de Newton en una variable para hallar el cero de la función f(x) = 1− 1/x usando como iterados iniciales los autovalores de ATA/‖A‖22, tal como lo establece dicho teorema. Adicionalmente, se analizará el comportamiento de κ(MkA) en función de k. En segundo lugar, se harán experimentos numéricos de los algoritmos planteados en el capítulo 5 para la resolución del PMCL. Se analizará el número de iteraciones y tiempo de ejecución para el sistema precondicionado, para diversos valores de Mk tanto en matrices bien condicionadas, como en matrices mal condicionadas. Además, estos métodos se compararán con el esquema de aceleración de convergencia Richardson-PR2 aplicado directamente a las ecuaciones normales, como representante de los métodos clásicos para la resolución del PMCL. Tanto el código fuente de la implementación de todos los experimentos numéricos, así como los archivos de datos contentivos de las matrices de prueba, se encontrarán en el material suplementario de este trabajo. 6.1. Plataforma computacional La plataforma computacional utilizada para la realización de los experimentos numéricos fue la siguiente: 61 6. Experimentación Numérica Equipo: Computador con procesador INTEL c© Core 2 DUO@1.8 GHz con 2Gb. de Me- moria RAM. Sistema Operativo: Microsoft Windows c© Vista Home Premium. Lenguaje y Entorno de programación: MATLAB c© R2008.a (Versión 7.6.0) paraWindows c©. 6.2. Matrices de prueba En esta sección se presentarán los grupos de matrices que serán utilizados en los experimen- tos numéricos. Se harán pruebas numéricas para matrices densas y para matrices sparse. Aún cuando el método tiene su campo de aplicación en matrices densas, debido a que la iteración de Schulz tiende a rellenar las matrices sparse luego de unas pocas iteraciones, se verificará, que aún aquellas, se observa que el precondicionador acelera notablemente la convergencia. 6.2.1. Matrices densas de prueba Las matrices densas a utilizar, se generan con diversas funciones provistas por MATLAB c©y son las siguientes. Matriz NormRand(µ, σ,m, n) Esta es una matriz densa de dimensión m × n con valores aleatorios distribuídos normalmente con promedio µ y desviación estándar σ. Esta matriz se genera con ayuda de la función randn de MATLAB c©, a través de la siguiente fórmula: randn(m,m). ∗ σ + µ El condicionamiento de estas matrices depende, en el caso general, de la desviación estándar elegida. Para valores muy pequeños de la desviación estándar, los valores singulares tieden a agruparse en torno a valores cercanos a 0, por lo cual la matriz generada es muy mal condicionada, mientras que para valores grandes de la desviación estándar, la matriz tiene un número de condición pequeño. Matriz RandSingV al((σ1, . . . , σn),m) La matriz RandSingV al es una matriz densa de m× n con valores singulares σ1, σ2, . . . σn positivos con valores aleatorios uniformemente dis- tribuidos. Para generar la matriz, se hacen los siguientes pasos, basados en la idea de la descomposición en valores singulares Sean U ∈ Rm×m y V ∈ Rn×n matrices densas con valores aleatorios uniformemente distribuidos, la cual se genera con la función rand de MATLAB c©. Sean Q1, R1 y Q2, R2 las matrices de las descomposiciones QR de U y V respectivamente, las cuales se obtienen vía la función qr de MATLAB c©. La matriz final viene dada por la expresión Q1diag(σ1, σ2, . . . , σn, 0, . . . , 0)QT2 62 6. Experimentación Numérica El condicionamiento de estas matrices depende de la distribución de sus valores singulares. Si los valores singulares están agrupados en torno a unos pocos valores, la matriz estará bien condicionada, mientras que si los valores singulares se encuentran muy dispersos, o bien se tienen un grupo de valores singulares muy pequeños y otro grupo de valores singulares muy grandes la matriz estará mal condicionada. Matriz RandSV D(m,n) La matrizRandSV D es una de las matrices de prueba de MATLAB c©, obtenidas a través de la función gallery. Esta es una matriz densa de m × n con n valores singulares distintos, distribuídos geométricamente entre 0 y 1. Estas matrices son muy mal condicionadas con el número de condición del orden de 1/ √ � donde � es el valor epsilon del computador. Matriz V andermonde((v1, . . . vm), n) La matriz de Vandermonde de orden n para un con- junto de valores reales v1, v2, . . . vn es una matriz, cuya i-ésima fila tiene la forma (1, vi, v2i , . . . , v n−1 i ) para i = 1, 2, . . . n. Esta matriz tiene varias aplicaciones en estadística y otras áreas de ciencias aplicadas, usándose principalmente para representar modelos de regresión polinomial. Es una matriz muy mal condicionada. 6.2.2. Matrices sparse de prueba Las matrices sparse de prueba, se tomaron del conjunto de matrices de problemas de míni- mos cuadrados de la Colección de matrices sparse Harwell-Boeing (Ver [9]). Más información sobre el formato de las matrices Harwell-Boeing se puede obtener en http://math.nist. gov/MatrixMarket/formats.html. Para este trabajo se utilizaron las matrices en formato MATLAB c©, las cuales se pueden obtener en http://www.cise.ufl.edu/research/sparse/ mat/HB/. A continuación se dará el código de las matrices de la colección Harwell-Boeing utilizadas en este trabajo Matriz ILLC1033 del grupo LSQ Matriz de 1033× 320 mal condicionada. Matriz ILLC185 del grupo LSQ Matriz de 1850× 712 mal condicionada. Matriz WELL1033 del grupo LSQ Matriz de 1033× 320 bien condicionada. Matriz WELL1850 del grupo LSQ Matriz de 1850× 712 bien condicionada. 6.3. Estudio numérico de Mk como precondicionador En esta sección se presentarán algunos experimentos numéricos que permitirán verificar en la práctica que Mk es un buen condicionador para el PMCL. Los experimentos se dividirán en dos grupos: en el primer grupo de experimentos se analizará el comportamiento del número de condición κ(MkA) en función de k donde k es el k-ésimo iterado de Schulz para A. En el segundo grupo de experimento se analizará el comportamiento de los autovalores de MkA, y se verificará la propiedad de clustering que tiene la aproximación a A†. En este último grupo de experimentos se usarán matrices con pocas columnas, con fines de que sea fácil visualizar las gráficas del espectro de MkA en función de k. 63 6. Experimentación Numérica 6.3.1. Grupo de Experimentos A Este grupo de experimentos, se divide en 3 subgrupos de matrices de la siguiente manera. 1. Subgrupo de matrices A DA11 = NormRand(100, 5, 200, 20). Matriz de 200×20 con κ(DA11) = 1,26×102 DA12 = NormRand(300, 50, 500, 400). Matriz de 500×400 con κ(DA12) = 1,18× 103 DA13 = RandSingV al((5, 10, 20, . . . 500), 300). Matriz de 300×51 con κ(DA13) = 0,99× 102 2. Subgrupo de matrices B DB11 = RandSV D(500, 10). Matriz de 500× 10 con κ(DB11) = 6,71× 107 DB12 = NormRand(300, 50, 500, 400). Matriz de 500×400 con κ(DB12) = 4,50× 109 DB13 = RandSingV al((100, 500, 5000, 50000, 500000, 5000000), 100). Matriz de 100× 6 con κ(DB13) = 5× 104 3. Subgrupo de matrices C DC11 = WELL1033. Matriz de 1033× 320 con κ(DC11) = 1,66× 107 DC12 = WELL1850. Matriz de 1850× 712 con κ(DC12) = 1,13× 102 DC13 = ILL1033. Matriz de 1033× 320 con κ(DC13) = 1,8× 104 DC14 = ILL1850. Matriz de 1850× 712 con κ(DC14) = 1,40× 103 Para las matrices anteriores se compararán los valores de κ(MkA) en función de k (agre- gando inicialmente el valor de κ(A)) con el comportamiento del error en la iteración de Schulz. Las figuras (6.1), (6.2) y (6.3) muestran gráficamente el comportamiento del error en la iteración de Schulz para las matrices de los grupos A, B y C respectivamente, mientras que las figuras (6.4), (6.5) y (6.6) muestran el valor del número de condición de las matrices MkA en función de k, para cada subgrupo. Además, el primer punto de dichas gráficas corresponde a κ(A). A partir de dichos resultados pueden determinarse algunas características del precondi- cionador dado por Mk. En primer lugar nótese que en las primeras iteraciones del método de Schulz, se evidencia que Mk empeora el condicionamiento del sistema original; la razón de este hecho es que el iterado inicial del método de Schulz es de la forma αATA, lo cual implica que κ(M0A) ≈ κ(A)2, así, en el caso de matrices muy mal condicionadas, se tiene un aumento considerable del número de condición en el sistema precondicionado. Este resultado puede observarse fácilmente en las gráfica κ(MkA) vs. k: en todos los casos se observa un salto grande entre los dos primeros puntos de esta (recuérdese que el primer punto de la gráfica corresponde a κ(A)). Este hecho puede suponer alguna desventaja, pues se tiene una cierta cantidad de iteraciones de Schulz que van a ser inútiles para el precondicionamiento; en efecto, sólo después de algún valor de k > 1 es que se conseguirá que las matrices MkA estén mejor condicionadas que la matriz original A. No obstante, una vez que se alcanza tal valor de k las 64 6. Experimentación Numérica siguientes iteraciones de Schulz hacen que el número de condición converja con gran velocidad a 1. En segundo lugar, el comportamiento del número de condición, presenta una correspon- dencia con la convergencia del método de Schulz. En las gráficas de error de la iteración de Schulz, puede observarse, de manera análoga a lo anterior, que en las primeras iteraciones el error puede ser incluso creciente, o bien tener un comportamiento errático: mantenerse más o menos constante o fluctuar, hasta que se alcanza cierto valor de k en el cual, en unas pocas iteraciones (en algunos casos 2 o 3) el método converge explosivamente. Y precisamente en ese punto donde inicia la rápida convergencia de la iteración de Schulz, también se acelera la convergencia del número de condición del sistema precondicionado MkA hacia 1. Como ejemplo, considérese la figura (6.1) correspondiente a la convergencia de Schulz para las matrices del subgrupo A: En particular, obsérvese el patrón de convergencia de la matriz DA11 (línea roja), el cual es un patrón típico de convergencia del método de Schulz en una matriz bien condicionada. En las primeras 7 iteraciones el método parece diverger, posteriormente entre las iteraciones 8 y 15 el error de los iterados se estabiliza en torno a un valor de error, y entre la iteración 16 y la iteración 19, el error decae desde aproximadamente 10−1 hasta 10−8, es decir, alrededor de siete órdenes de magnitud, superando inclusive el valor de tolerancia dado como entrada en el algoritmo que era de 10−5. Ahora obsérvese la figura 6.4 correspondiente a la gráfica κ(MkA) vs. k; como para la gráfica anterior, considérese el comportamiento del número de condición para el caso de la matriz DA11 (línea roja). Puede observarse que en las 7 primeras iteraciones, el número de condición de MkA se mantiene por encima del número de condición original de A, y justo luego de la iteración 8, que se corresponde a la iteración en la cual el error en el método de Schulz deja de ser creciente, el número de condición de MkA es mejor que el número de condición de A. Además se puede observar en dicha gráfica, que la pendiente a partir de la iteración 15 se hace mayor, lo cual indica una mayor velocidad de convergencia del número de condición, justo en la misma iteración en donde el método de Schulz comienza su rápida convergencia. En el resto de los experimentos de este grupo se observa un comportamiento semejante. 65 6. Experimentación Numérica 0 5 10 15 20 25 10 −8 10 −7 10 −6 10 −5 10 −4 10 −3 10 −2 10 −1 10 0 k E rr o r( S ch u lz ) A = DA11 A = DA12 A = DA13 Figura 6.1: Comportamiento de la iteración de Schulz para las matrices del subgrupo A 0 10 20 30 40 50 60 70 10 −7 10 −6 10 −5 10 −4 10 −3 10 −2 10 −1 10 0 k E rr o r( S ch u lz ) A = DB11 A = DB12 A = DB13 Figura 6.2: Comportamiento de la iteración de Schulz para las matrices del subgrupo B 66 6. Experimentación Numérica 0 5 10 15 20 25 30 35 40 10 −10 10 −8 10 −6 10 −4 10 −2 10 0 k e rr o r( S ch u lz ) A = DC11 A =DC12 A = DC13 A = DC14 Figura 6.3: Comportamiento de la iteración de Schulz para las matrices del subgrupo C 0 5 10 15 20 25 10 0 10 1 10 2 10 3 10 4 10 5 10 6 k co n d (M kA ) A = DA11 A = DA12 A = DA13 Figura 6.4: Gráfico MkA en función de k para las matrices del subgrupo A 67 6. Experimentación Numérica 0 10 20 30 40 50 10 0 10 5 10 10 10 15 10 20 k co n d (M kA ) A = DB11 A = DB12 A = DB13 Figura 6.5: Gráfico MkA en función de k para las matrices del subgrupo B 0 5 10 15 20 25 30 35 40 10 0 10 2 10 4 10 6 10 8 10 10 k co n d (M kA ) A = DC11 A = DC12 A = DC13 A = DC14 Figura 6.6: Gráfico MkA en función de k para las matrices del subgrupo C 68 6. Experimentación Numérica 6.3.2. Grupo de Experimentos B En este grupo de experimentos, se utilizaron las siguientes matrices densas: DA21 = NormRand(10, 4, 500, 5). Matriz de 500× 5 con κ(DA21) = 6,53 DA22 = NormRand(5, 0,1, 20, 5). Matriz de 20× 5 con κ(DA22) = 4,97× 102 DA23 = RandSV D(20, 5). Matriz de 20× 5 con κ(DA23) = 6,71× 107 DA24 = V andermonde((5, 10, 15, . . . 400), 4). Matriz de 40×4 con κ(DA24) = 1,03×108 Para cada una de estas matrices se realizó la iteración de Schulz, para obtener las sucesiones de autovalores de las matrices MkA y se realizó la iteración de Newton en una variable de la función f(x) = 1− 1/x, tomando como iterado inicial los autovalores de la matriz M0A. Los autovalores de las matrices fueron calculados con la función eig de MATLAB c©. El comportamiento del espectro de autovalores aproximados vía los iterados de Newton en una variable MkA se sumariza en los cuadros (6.1), (6.2), (6.3) y (6.4) para A = DA21, A = DA22, A = DA23 y A = DA24 respectivmente. En todos los casos, el error relativo de la la iteración de Newton respecto a los autovalores de las matrices MkA es menor a 102. Por otro lado las figuras (6.7), (6.8), (6.9) y (6.10) muestran gráficamente el comportamiento de los autovalores de las matrices precondicionadas MkA. A partir de las gráficas, puede observarse, que de forma análoga al comportamiento del número de condición de MkA en función d k, los autovalores mantienen la tendencia de con- verger de manera estable hacia 1; convergiendo de manera lenta en las primeras iteraciones, y luego de cierto valor de k > 1 se acelera notablemente la convergencia y el agrupamiento de todos los autovalores en torno a x = 1. Esto confirma el resultado teórico del teorema (3.3.4) el cual establecía como consecuencia, que los precondicionadoresMk comprimen el espectro de valores singulares de la matriz original en torno a 1, lo cual verifica finalmente que los iterados de Schulz son un buen precondicionador para el PMCL. Además, en los experimentos se observó una propiedad adicional sobre el espectro de autovalores de las matricesMkA: en todos los experimentos, para el iterado inicialM0 siempre se tiene un autovalor muy cercano a 1 y otro mucho menor a 1 en la matriz M0A. En efecto comoM0 = AT /‖A‖22 entonces todos los autovalores deM0A son menores a 1. Por otro lado se sabe que el radio espectral de una matriz es una cota inferior de todas las normas matriciales, por lo tanto, si para una matriz se tiene que ρ(A) ≈ ‖A‖2, entonces la matriz ATA/‖A‖22 siempre tendrá un autovalor muy cercano a 1, el cual va a ser justamente ρ(ATA). 69 6. Experimentación Numérica k λ1 λ2 λ3 λ4 λ5 1 8, 91× 10−01 3, 32× 10−02 2, 08× 10−02 2, 87× 10−02 2, 62× 10−02 2 9, 88× 10−01 6, 53× 10−02 4, 12× 10−02 5, 65× 10−02 5, 18× 10−02 3 1, 00× 10+00 1, 26× 10−01 8, 08× 10−02 1, 10× 10−01 1, 01× 10−01 4 1, 00× 10+00 2, 37× 10−01 1, 55× 10−01 2, 07× 10−01 1, 92× 10−01 4 1, 00× 10+00 4, 17× 10−01 2, 86× 10−01 3, 72× 10−01 3, 46× 10−01 5 1, 00× 10+00 6, 61× 10−01 4, 90× 10−01 6, 06× 10−01 5, 73× 10−01 6 1, 00× 10+00 8, 85× 10−01 7, 40× 10−01 8, 44× 10−01 8, 18× 10−01 7 1, 00× 10+00 9, 87× 10−01 9, 32× 10−01 9, 76× 10−01 9, 67× 10−01 8 1, 00× 10+00 1, 00× 10+00 9, 95× 10−01 9, 99× 10−01 9, 99× 10−01 9 1, 00× 10+00 1, 00× 10+00 1, 00× 10+00 1, 00× 10+00 1, 00× 10+00 10 1, 00× 10+00 1, 00× 10+00 1, 00× 10+00 1, 00× 10+00 1, 00× 10+00 11 1, 00× 10+00 1, 00× 10+00 1, 00× 10+00 1, 00× 10+00 1, 00× 10+00 12 1, 00× 10+00 1, 00× 10+00 1, 00× 10+00 1, 00× 10+00 1, 00× 10+00 13 1, 00× 10+00 1, 00× 10+00 1, 00× 10+00 1, 00× 10+00 1, 00× 10+00 14 1, 00× 10+00 1, 00× 10+00 1, 00× 10+00 1, 00× 10+00 1, 00× 10+00 Cuadro 6.1: Aproximación a los autovalores de las matrices MkA para A = DA21, obtenidos a través de la iteración de Newton de f(x) = 1− 1/x 0 5 10 15 10 −2 10 −1 10 0 k a p ro x λ n e w to n aprox λ1 aprox λ2 aprox λ3 aprox λ4 aprox λ5 Figura 6.7: Gráfica del espectro de autovalores de la matriz MkA en función de k para A = DA21 70 6. Experimentación Numérica 0 5 10 15 20 25 10 −6 10 −5 10 −4 10 −3 10 −2 10 −1 10 0 k a p ro x λ n e w to n aprox λ1 aprox λ2 aprox λ3 aprox λ4 aprox λ5 Figura 6.8: Gráfica del espectro de autovalores de la matriz MkA en función de k para A = DA22 71 6. Experimentación Numérica 0 10 20 30 40 50 60 70 10 −16 10 −14 10 −12 10 −10 10 −8 10 −6 10 −4 10 −2 10 0 k a p ro x λ n e w to n aprox λ1 aprox λ2 aprox λ3 aprox λ4 aprox λ5 Figura 6.9: Gráfica del espectro de autovalores de la matriz MkA en función de k para A = DA23 72 6. Experimentación Numérica 0 10 20 30 40 50 60 70 10 −20 10 −15 10 −10 10 −5 10 0 k a p ro x λ N e w to n aprox λ 1 aprox λ 2 aprox λ 3 aprox λ 4 Figura 6.10: Gráfica del espectro de autovalores de la matriz MkA en función de k para A = DA24 73 6. Experimentación Numérica k λ1 λ2 λ3 λ4 λ5 1 9, 90× 10−01 4, 05× 10−06 5, 78× 10−06 8, 55× 10−06 1, 34× 10−05 2 9, 90× 10−01 8, 09× 10−06 1, 16× 10−05 1, 71× 10−05 2, 68× 10−05 3 1, 00× 10+00 1, 62× 10−05 2, 31× 10−05 3, 42× 10−05 5, 35× 10−05 4 1, 00× 10+00 3, 24× 10−05 4, 62× 10−05 6, 84× 10−05 1, 07× 10−04 4 1, 00× 10+00 6, 47× 10−05 9, 24× 10−05 1, 37× 10−04 2, 14× 10−04 5 1, 00× 10+00 1, 29× 10−04 1, 85× 10−04 2, 74× 10−04 4, 28× 10−04 6 1, 00× 10+00 2, 59× 10−04 3, 70× 10−04 5, 47× 10−04 8, 56× 10−04 7 1, 00× 10+00 5, 18× 10−04 7, 39× 10−04 1, 09× 10−03 1, 71× 10−03 8 1, 00× 10+00 1, 04× 10−03 1, 48× 10−03 2, 19× 10−03 3, 42× 10−03 9 1, 00× 10+00 2, 07× 10−03 2, 95× 10−03 4, 37× 10−03 6, 83× 10−03 10 1, 00× 10+00 4, 13× 10−03 5, 90× 10−03 8, 72× 10−03 1, 36× 10−02 11 1, 00× 10+00 8, 25× 10−03 1, 18× 10−02 1, 74× 10−02 2, 70× 10−02 12 1, 00× 10+00 1, 64× 10−02 2, 34× 10−02 3, 44× 10−02 5, 33× 10−02 13 1, 00× 10+00 3, 26× 10−02 4, 62× 10−02 6, 77× 10−02 1, 04× 10−01 14 1, 00× 10+00 6, 41× 10−02 9, 03× 10−02 1, 31× 10−01 1, 97× 10−01 16 1, 00× 10+00 1, 24× 10−01 1, 72× 10−01 2, 44× 10−01 3, 55× 10−01 17 1, 00× 10+00 2, 33× 10−01 3, 15× 10−01 4, 29× 10−01 5, 84× 10−01 18 1, 00× 10+00 4, 12× 10−01 5, 31× 10−01 6, 74× 10−01 8, 27× 10−01 19 1, 00× 10+00 6, 54× 10−01 7, 80× 10−01 8, 94× 10−01 9, 70× 10−01 20 1, 00× 10+00 8, 80× 10−01 9, 52× 10−01 9, 89× 10−01 9, 99× 10−01 21 1, 00× 10+00 9, 86× 10−01 9, 98× 10−01 1, 00× 10+00 1, 00× 10+00 22 1, 00× 10+00 1, 00× 10+00 1, 00× 10+00 1, 00× 10+00 1, 00× 10+00 23 1, 00× 10+00 1, 00× 10+00 1, 00× 10+00 1, 00× 10+00 1, 00× 10+00 24 1, 00× 10+00 1, 00× 10+00 1, 00× 10+00 1, 00× 10+00 1, 00× 10+00 25 1, 00× 10+00 1, 00× 10+00 1, 00× 10+00 1, 00× 10+00 1, 00× 10+00 Cuadro 6.2: Aproximación a los autovalores de las matrices MkA para A = DA22, obtenidos a través de la iteración de Newton de f(x) = 1− 1/x 74 6. Experimentación Numérica k λ1 λ2 λ3 λ4 λ5 1 1, 22× 10−04 1, 22× 10−04 1, 49× 10−08 1, 82× 10−12 0, 00× 10+00 2 2, 44× 10−04 2, 44× 10−04 2, 98× 10−08 3, 64× 10−12 0, 00× 10+00 3 4, 88× 10−04 4, 88× 10−04 5, 96× 10−08 7, 28× 10−12 1, 00× 10−15 4 9, 76× 10−04 9, 76× 10−04 1, 19× 10−07 1, 46× 10−11 2, 00× 10−15 5 1, 95× 10−03 1, 95× 10−03 2, 38× 10−07 2, 91× 10−11 3, 00× 10−15 6 3, 90× 10−03 3, 90× 10−03 4, 77× 10−07 5, 82× 10−11 7, 00× 10−15 7 7, 78× 10−03 7, 78× 10−03 9, 54× 10−07 1, 16× 10−10 1, 30× 10−14 8 1, 55× 10−02 1, 55× 10−02 1, 91× 10−06 2, 33× 10−10 2, 70× 10−14 9 3, 08× 10−02 3, 08× 10−02 3, 81× 10−06 4, 66× 10−10 5, 40× 10−14 10 6, 06× 10−02 6, 06× 10−02 7, 63× 10−06 9, 31× 10−10 1, 07× 10−13 11 1, 17× 10−01 1, 17× 10−01 1, 53× 10−05 1, 86× 10−09 2, 14× 10−13 12 2, 21× 10−01 2, 21× 10−01 3, 05× 10−05 3, 72× 10−09 4, 29× 10−13 13 3, 93× 10−01 3, 93× 10−01 6, 10× 10−05 7, 45× 10−09 8, 58× 10−13 14 6, 32× 10−01 6, 32× 10−01 1, 22× 10−04 1, 49× 10−08 1, 72× 10−12 15 8, 65× 10−01 8, 65× 10−01 2, 44× 10−04 2, 98× 10−08 3, 43× 10−12 16 9, 82× 10−01 9, 82× 10−01 4, 88× 10−04 5, 96× 10−08 6, 86× 10−12 17 1, 00× 10+00 1, 00× 10+00 9, 76× 10−04 1, 19× 10−07 1, 37× 10−11 18 1, 00× 10+00 1, 00× 10+00 1, 95× 10−03 2, 38× 10−07 2, 74× 10−11 19 1, 00× 10+00 1, 00× 10+00 3, 90× 10−03 4, 77× 10−07 5, 49× 10−11 20 1, 00× 10+00 1, 00× 10+00 7, 78× 10−03 9, 54× 10−07 1, 10× 10−10 21 1, 00× 10+00 1, 00× 10+00 1, 55× 10−02 1, 91× 10−06 2, 20× 10−10 22 1, 00× 10+00 1, 00× 10+00 3, 08× 10−02 3, 81× 10−06 4, 39× 10−10 23 1, 00× 10+00 1, 00× 10+00 6, 06× 10−02 7, 63× 10−06 8, 78× 10−10 24 1, 00× 10+00 1, 00× 10+00 1, 17× 10−01 1, 53× 10−05 1, 76× 10−09 25 1, 00× 10+00 1, 00× 10+00 2, 21× 10−01 3, 05× 10−05 3, 51× 10−09 Cuadro 6.3: Aproximación a los autovalores de las matrices MkA para A = DA23, obtenidos a través de la iteración de Newton de f(x) = 1− 1/x 75 6. Experimentación Numérica k λ1 λ2 λ3 λ4 1 9, 90× 10−01 2, 40× 10−07 0, 00× 10+00 8, 89× 10−13 2 9, 90× 10−01 4, 80× 10−07 0, 00× 10+00 1, 78× 10−12 3 9, 90× 10−01 9, 60× 10−07 0, 00× 10+00 3, 56× 10−12 4 1, 00× 10+00 1, 92× 10−06 1, 00× 10−15 7, 11× 10−12 5 1, 00× 10+00 3, 84× 10−06 1, 00× 10−15 1, 42× 10−11 6 1, 00× 10+00 7, 68× 10−06 3, 00× 10−15 2, 85× 10−11 7 1, 00× 10+00 1, 54× 10−05 6, 00× 10−15 5, 69× 10−11 8 1, 00× 10+00 3, 07× 10−05 1, 20× 10−14 1, 14× 10−10 9 1, 00× 10+00 6, 14× 10−05 2, 40× 10−14 2, 28× 10−10 10 1, 00× 10+00 1, 23× 10−04 4, 80× 10−14 4, 55× 10−10 11 1, 00× 10+00 2, 46× 10−04 9, 50× 10−14 9, 10× 10−10 12 1, 00× 10+00 4, 91× 10−04 1, 90× 10−13 1, 82× 10−09 13 1, 00× 10+00 9, 82× 10−04 3, 81× 10−13 3, 64× 10−09 14 1, 00× 10+00 1, 96× 10−03 7, 62× 10−13 7, 28× 10−09 15 1, 00× 10+00 3, 92× 10−03 1, 52× 10−12 1, 46× 10−08 16 1, 00× 10+00 7, 83× 10−03 3, 05× 10−12 2, 91× 10−08 17 1, 00× 10+00 1, 56× 10−02 6, 09× 10−12 5, 83× 10−08 18 1, 00× 10+00 3, 10× 10−02 1, 22× 10−11 1, 17× 10−07 19 1, 00× 10+00 6, 10× 10−02 2, 44× 10−11 2, 33× 10−07 20 1, 00× 10+00 1, 18× 10−01 4, 87× 10−11 4, 66× 10−07 21 1, 00× 10+00 2, 22× 10−01 9, 75× 10−11 9, 32× 10−07 22 1, 00× 10+00 3, 95× 10−01 1, 95× 10−10 1, 86× 10−06 23 1, 00× 10+00 6, 35× 10−01 3, 90× 10−10 3, 73× 10−06 24 1, 00× 10+00 8, 66× 10−01 7, 80× 10−10 7, 46× 10−06 25 1, 00× 10+00 9, 82× 10−01 1, 56× 10−09 1, 49× 10−05 26 1, 00× 10+00 1, 00× 10+00 3, 12× 10−09 2, 98× 10−05 27 1, 00× 10+00 1, 00× 10+00 6, 24× 10−09 5, 97× 10−05 28 1, 00× 10+00 1, 00× 10+00 1, 25× 10−08 1, 19× 10−04 29 1, 00× 10+00 1, 00× 10+00 2, 50× 10−08 2, 39× 10−04 30 1, 00× 10+00 1, 00× 10+00 4, 99× 10−08 4, 77× 10−04 Cuadro 6.4: Aproximación a los autovalores de las matrices MkA para A = DA24, obtenidos a través de la iteración de Newton de f(x) = 1− 1/x 76 6. Experimentación Numérica 6.4. Experimentos numéricos de la resolución del PMCL En este segundo conjunto de experimentos, se mostrarán resultados de diversas ejecuciones de los algoritmos (5.4) y (5.5) desarrollados en el capítulo 5 tanto para matrices densas, como para matrices sparse. Se presentarán resultados comparativos tanto de las iteraciones del método de Richardson- PR2, como del tiempo de ejecución de cada uno de los algoritmos para todas las matrices de prueba. Además se comparará con la aceleración de Richardson-PR2 aplicada a las ecuaciones normales, lo cual es equivalente al método iterativo de Richardson de primer orden. 6.4.1. Formato de los experimentos numéricos de la resolución del PMCL La experimentación se dividirá en los siguientes dos grupos de matrices. Grupo de matrices A: Matrices densas, se ejecutará el algoritmo para 5 matrices de prueba generadas con las funciones especificadas al inicio del capítulo. Grupo de matrices B: Matrices sparse. Se utilizarán las matrices de la colección Harwell- Boeing presentadas anteriormente. Para cada matriz en los grupos se harán varias ejecuciones de los algoritmos, y se mostrará un cuadro comparativo con los siguientes resultados Alg: Algoritmo utilizado, alg1 corresponde al algoritmo (5.4), alg2 corresponde al algo- ritmo (5.5) y alg3 corresponde al algoritmo (4.1) aplicado a las ecuaciones normales. ksch: para alg1 este valor indica el número de iteraciones de Schulz realizadas para generar el precondicionador Mk. nit: Número de iteraciones realizadas por el método. Se denota con ∞ en el caso de que el algoritmo no haya alcanzado la convergencia luego del número máximo de iteraciones establecido. tpre: Tiempo para generar la matriz precondicionadora Mk en segundos para alg1. trich: Tiempo que tarda la iteración de Richardson-PR2 en segundos. ttot: Tiempo total de ejecución del algoritmo en segundos. err: Error relativo del resultado de la ejecución del algoritmo, respecto una solución de referencia al PMCL para cada matriz, calculado con las funciones de MATLAB c©.. En todas las matrices del grupo A, para la resolución de Ax = b se toma b = (1, 1, 1, . . . , 1)T con la dimensión adecuada para cada caso. Para los casos de estudio del grupo B, se usa el vector b proveído por la misma colección Harwell-Boeing. En ambos grupos, se utiliza como iterado inicial el vector x0 = (1, 1, 1, . . . 1)T con la dimensión adecuada para cada caso. En todos los casos de prueba, se ejecutó la iteración con una tolerancia � = 10−7 y una cantidad máxima de iteraciones Mmax = 300. Para la medición de los tiempos de ejecución se toma el tiempo promedio de 1000 ejecu- ciones de cada algoritmo. 77 6. Experimentación Numérica La solución de referencia utilizada en ambos grupos, se obtiene aplicando el operador backslash (\) de MATLAB c©. En el caso de un sistema sobredeterminado Ax = b, la función A\b retorna la solución al PMCL asociado a A y b la cual se calcula a través de una variante de la descomposición en valores singulares (SVD). Para el caso del algoritmo (5.5) se observó que en la práctica, el valor de ‖AT r(k))‖2 se estanca, aun cuando los iterados x(k) siguen convergiendo, por lo tanto, para los experimentos, se utilizó como condición de parada para dicho algoritmo, el valor ‖r(k+1)−r(k)‖2. Esta elección de condición de parada, en la práctica funcionó para conseguir el resultado con la tolerancia deseada en todos los casos. 6.4.2. Grupo de Experimentos A Las matrices de prueba de este grupo son las siguientes: DD11 = NormRand(500, 300, 1500, 300). Matriz de 1500× 300 con κ(DD11) = 15,61 DD12 = NormRand(4500, 800, 2000, 800). Matriz de 2000×800 con κ(DD12) = 1,16× 103 DD13 = RandSingV al((10, 20, . . . 5000), 1500). Matriz de 1500 × 500 con κ(DD13) = 4,99× 102 DD14 = RandSingV al(100, 50). Matriz de 100× 50 con κ(DD14) = 6,71× 107 DD15 = V andermonde((1/2, 1, 3/2, . . . 20), 5). Matriz de 40× 5 con κ(DD15) = 3,88× 107 Las figuras (6.11) a la (6.4.2) muestran el comportamiento del residual de las iteraciones de Richardson-PR2 para cada matriz. En cada gráfica se muestra el valor del residual en función de la iteración, i, para cada uno de los algoritmos. En el caso de alg1 se muestra el comportamiento del método para distintos valores de Mk. Los resultados se resumen en los cuadros (6.5) al (6.9). En general, para todas las matrices, precondicionando con Mk para k menor a los valores reportados, no mejora la convergencia del método. A partir de estos resultados, se observa, que en principio, el precondicionador tiene una desventaja en lo relativo al alto tiempo que se requiere para el precondicionamiento del sistema, el cual es mucho mayor al tiempo empleado por el método iterativo Richardson-PR2, llegando a ser en algunos casos hasta 200 veces mayor. No obstante, para las matrices de prueba utilizadas, no se obtuvo convergencia del método Richardson-PR2 aplicado a las ecuaciones normales, para el número máximo de iteraciones usando en todos los experimentos (NMAX = 300), más aún en todos los casos, para las ecua- ciones normales, el error relativo es mayor a 1 respecto a la solución de referencia obtenida con las funciones de MATLAB c©, lo cual implica que para la resolución del PMCL el precon- dicionamiento es un paso necesario aun cuando este sea de un alto costo computacional. Respecto a la mejora de la convergencia, para el caso del algoritmo 1 (precondicionamiento explícito), puede observarse que la sucesión de precondicionadoresMk ofrecen una gran mejora a la convergencia del PMCL originales, una vez que se consigue el valor de k para el cual el método de Schulz inicia su rápida convergencia. A partir de cierto valor de k crítico que depende de la matriz, se observa que cada precondicionadorMk reduce drásticamente el número 78 6. Experimentación Numérica de iteraciones que se necesitan para la convergencia. Por ejemplo, en el caso de la matriz DD11, una matriz bien condicionada, para k = 5, el sistema precondicionado conMk requiere de 106 iteraciones, y para k = 9 ya el sistema precondicionado con Mk requiere de apenas 12 iteraciones, casi 10 veces menos. Igualmente el comportamiento se observa, incluso en matrices muy mal condicionadas. Por ejemplo, para la matriz DD14 el sistema precondicionado conMk para k = 50 necesita de 98 iteraciones para la convergencia, y con k = 55, apenas 5 iteraciones de Schulz más, hacen que el sistema precondicionado requiera de apenas 6 iteraciones para la convergencia. Este mismo comportamiento se puede observar en las gráficas del residual en función de las iterciones para todas las matrices de prueba. El algoritmo 2, en el cual se utilizan las matrices Mk como sucesión de precondicionadores en el esquema de aceleración de Richardson-PR2, se puede observar un mayor paralelismo del comportamiento del residual en cada iteración del esquema de aceleración de Richardson-PR2, respecto al comportamiento de la convergencia en el método de Shculz, esto es, en las primeras iteraciones, se observa que el residual alcanza valores muy altos, llegando a ser para algunas matrices del orden de 1010, lo cual se debe a ese empeoramiento inicial del número de condición de la matriz que produce la iteración de Schulz. Para el caso de matrices bien condicionadas, como son las matrices DD11 y DD12, en las primeras iteraciones hay un descenso fuerte del residual, pero aun siendo mucho mayor a 1, mientras que en las matrices mal condicionadas como DD14 y DD15 se observa que en las primeras iteraciones se mantiene más estable el residual. Posteriormente unas pocas iteraciones en donde hay un aparente estancamiento del residual, y finalmente en las últimas 2 o 3 iteraciones, hay una caída muy rápida del residual, llegando incluso a un nivel varios órdenes menor a la tolerancia escogida para el algoritmo. Sin embargo, para el caso del algoritmo 2, el tiempo de ejecución es mucho mayor que el utilizado con la combinación de precondicionamiento directo más iteración de Richardson del algoritmo 1, por lo cual hace que no aporte nada significativo la elección del algoritmo 2. Alg ksch nit tpre × 103 trich × 103 ttot × 103 err alg1 5 106 110,93 17,71 128,64 4,65 alg1 7 36 124,08 5,66 129,74 6,6× 10−3 alg1 9 12 146,14 1,76 147,90 1,1× 10−3 alg1 12 3 181,19 0,44 181,63 9,7× 10−5 alg2 N/A 16 N/A 264,45 264,45 2,2× 10−8 alg3 N/A 237 N/A 54,04 54,04 1,3× 10−5 Cuadro 6.5: Resultados de la experimentación numérica para la resolución del PMCL para la matriz A = DD11. 79 6. Experimentación Numérica 0 20 40 60 80 100 120 10 −10 10 −5 10 0 10 5 10 10 10 15 iteracion re si d u a l alg 1 M k , k = 5 alg 1 M k , k = 7 alg 1 M k , k = 9 alg 1 M k , k = 12 alg 3 Figura 6.11: Convergencia de los algoritmos para la resolución del PMCL para A = DD11 80 6. Experimentación Numérica 0 5 10 15 20 25 30 35 40 45 10 −10 10 −5 10 0 10 5 10 10 iteración re si d u a l alg 1, M k , k = 10 alg 1, M k , k = 12 alg 1, M k , k = 14 alg 1, M k , k = 16 alg 2 Figura 6.12: Convergencia de los algoritmos para la resolución del PMCL para A = DD12 0 10 20 30 40 50 10 −15 10 −10 10 −5 10 0 10 5 10 10 iteración re si d u a l alg 1, M k , k = 23 alg 1, M k = 25 alg 1, M k = 27 alg 3 Figura 6.13: Convergencia de los algoritmos para la resolución del PMCL para A = DD13 81 6. Experimentación Numérica 0 20 40 60 80 100 10 −10 10 −5 10 0 10 5 10 10 iteracion re si d u a l alg 1, M k , k = 50 alg 1, M k , k = 52 alg 1, M k , k = 54 alg 1, M k , k = 55 alg 2 Figura 6.14: Convergencia de los algoritmos para la resolución del PMCL para A = DD14 0 5 10 15 20 25 30 35 40 45 10 −10 10 −5 10 0 10 5 10 10 10 15 iteración re si d u a l Figura 6.15: Convergencia de los algoritmos para la resolución del PMCL para A = DD15 82 6. Experimentación Numérica Alg ksch nit tpre × 103 trich × 103 ttot × 103 err alg1 7 101 63,73 5,18 68,91 0,55 alg1 10 48 66,24 2,48 68,72 0,07 alg1 12 16 71,19 0,86 72,05 0,01 alg1 14 7 76,09 0,42 76,51 1,2× 10−3 alg1 16 3 80,45 0,21 80,66 1,9× 10−5 alg2 N/A 20 N/A 119,83 119,83 6,9× 10−8 alg3 N/A ∞ N/A 54,48 54,48 > ×102 Cuadro 6.6: Resultados de la experimentación numérica para la resolución del PMCL para la matriz A = DD12. Alg ksch nit tpre × 103 trich × 103 ttot × 103 err alg1 21 165 3013,75 40,90 3054,65 5,8× 10−7 alg1 23 47 3078,32 11,63 3089,95 1,4× 10−7 alg1 25 14 3356,05 4,04 3360,09 3,6× 10−8 alg1 27 5 3562,03 1,33 3563,36 1,5× 10−8 alg2 N/A 32 N/A 4931,93 4931,93 1,1× 10−9 alg3 N/A ∞ N/A 54,48 54,48 5,6 Cuadro 6.7: Resultados de la experimentación numérica para la resolución del PMCL para la matriz A = DD13. Alg ksch nit tpre × 103 trich × 103 ttot × 103 err alg1 50 98 20,05 2,34 22,39 0,18 alg1 52 27 21,54 0,84 22,38 0,18 alg1 54 11 23,06 0,32 23,38 0,18 alg1 55 6 24,34 0,11 24,45 0,18 alg2 N/A 59 N/A 16,89 19,89 0,18 alg3 N/A ∞ N/A 7,23 7,23 5,6 Cuadro 6.8: Resultados de la experimentación numérica para la resolución del PMCL para la matriz A = DD14. Alg ksch nit tpre trich ttot err alg1 34 35 0,65 0,26 0,91 2,4× 10−6 alg1 35 11 0,79 0,21 1,00 2,2× 10−6 alg1 36 6 0,81 0,14 0,95 1,9× 10−6 alg2 N/A 43 N/A 1,71 1,71 7,2× 10−7 alg3 N/A ∞ N/A 5,42 5,42 > 10× 102 Cuadro 6.9: Resultados de la experimentación numérica para la resolución del PMCL para la matriz A = DD15. 83 6. Experimentación Numérica 6.4.3. Grupo de Experimentos B Las matrices de prueba de este grupo son las siguientes: DD21 = WELL1033. Matriz de 1033× 320 con κ(DD21) = 1,66× 107 DD22 = WELL1850. Matriz de 1850× 712 con κ(DD22) = 1,13× 102 DD23 = ILL1033. Matriz de 1033× 320 con κ(DD23) = 1,8× 104 DD24 = ILL1850. Matriz de 1850× 712 con κ(DD24) = 1,40× 103 Las figuras (6.16) a la (6.4.3) muestran el comportamiento del residual de las iteraciones de Richardson-PR2 para cada matriz. En cada gráfica se muestra el valor del residual en función de la iteración, i, para cada uno de los algoritmos. En el caso de alg1 se muestra el comportamiento del método para distintos valores de Mk. Los resultados se resumen en los cuadros (6.10) al (6.13). Para el caso de matrices sparse, se puede observar un comportamiento análogo al caso de matrices densas en cuanto a la mejora de la convergencia del método de Richardson-PR2 con el precondicionamiento como en el algoritmo 1, así como para el algoritmo 2. No obstante, puede verse que la diferencia de tiempo entre la generación del precondicionador es mayor en el caso general. Esto se debe a que en el caso de matrices sparse evidentemente, la iteración de Richardson-PR2 es más rápida pues se puede aprovechar el hecho de que no es necesario construir explícitamente, la matriz ATA para el caso de las ecuaciones normales, mientras que el precondicionamiento con Mk hace que la matriz se vuelva densa. Para el grupo de prueba de matrices sparse igualmente se observa que el algoritmo 2 no ofrece una mejora sustancial al precondicionaiento explícito del algoritmo 1. Alg ksch nit tpre × 103 trich × 103 ttot × 103 err alg1 18 120 211,28 12,04 223,32 1,3× 10−10 alg1 20 33 227,04 3,26 230,30 5,7× 10−11 alg1 22 11 254,64 1,15 255,79 4,2× 10−11 alg1 24 4 284,69 0,48 285,17 3,9× 10−11 alg2 N/A 27 N/A 360,34 360,34 2,6× 10−11 alg3 N/A ∞ N/A 319,52 319,52 1,5 Cuadro 6.10: Resultados de la experimentación numérica para la resolución del PMCL para la matriz A = DD21. 84 6. Experimentación Numérica 0 20 40 60 80 100 120 10 −12 10 −10 10 −8 10 −6 10 −4 10 −2 10 0 10 2 10 4 iteración re si d u a l alg 1, M k , k = 18 alg 1, M k , k = 20 alg 1, M k , k = 22 alg 1, M k , k = 24 alg 2 Figura 6.16: Convergencia de los algoritmos para la resolución del PMCL para A = DD21 0 20 40 60 80 100 120 140 10 −12 10 −10 10 −8 10 −6 10 −4 10 −2 10 0 10 2 10 4 iteración re si d u a l alg 1, M k , k = 18 alg 1, M k , k = 20 alg 1, M k , k = 22 alg 1, M k , k = 24 alg 2 Figura 6.17: Convergencia de los algoritmos para la resolución del PMCL para A = DD22 85 6. Experimentación Numérica Alg ksch nit tpre × 103 trich × 103 ttot × 103 err alg1 18 121 2127,39 63,84 2191,23 1,5× 10−10 alg1 20 34 2516,00 24,65 2540,65 6,8× 10−11 alg1 22 12 2799,13 7,81 2806,94 6,1× 10−11 alg1 24 4 2856,82 2,90 2859,72 6,1× 10−11 alg2 N/A 27 N/A 3508,61 3508,61 3,8× 10−11 alg3 N/A ∞ N/A 347,72 347,72 0,7 Cuadro 6.11: Resultados de la experimentación numérica para la resolución del PMCL para la matriz A = DD22. Alg ksch nit tpre × 103 trich × 103 ttot × 103 err alg1 30 165 361,22 2,43 363,65 1,5× 10−7 alg1 32 67 408,54 6,68 415,22 1,5× 10−7 alg1 34 11 413,59 2,25 415,84 1,5× 10−8 alg1 36 6 429,62 0,70 430,32 1,5× 10−8 alg2 N/A 40 N/A 592,08 592,08 1,1× 10−9 alg3 N/A ∞ N/A 57,64 57,64 0,9 Cuadro 6.12: Resultados de la experimentación numérica para la resolución del PMCL para la matriz A = DD23. 0 50 100 150 200 250 10 −12 10 −10 10 −8 10 −6 10 −4 10 −2 10 0 10 2 10 4 iteración re si d u a l alg 1, M k , k = 30 alg 1, M k , k = 32 alg 1, M k , k = 35 alg 1, M k , k = 36 alg 2 Figura 6.18: Convergencia de los algoritmos para la resolución del PMCL para A = DD23 86 6. Experimentación Numérica Alg ksch nit tpre × 103 trich × 103 ttot × 103 err alg1 18 181 2870,18 108,84 2929,02 9,5× 10−10 alg1 20 54 2931,49 30,91 2962,40 9,5× 10−10 alg1 22 18 3034,97 10,27 3045,24 9,5× 10−10 alg1 24 6 3216,58 4,08 3219,66 9,5× 10−10 alg2 N/A 34 N/A 4088,09 4088,09 9,5× 10−10 alg3 N/A ∞ N/A 314,21 314,21 0,79 Cuadro 6.13: Resultados de la experimentación numérica para la resolución del PMCL para la matriz A = DD24. 0 20 40 60 80 100 120 140 160 180 200 10 −14 10 −12 10 −10 10 −8 10 −6 10 −4 10 −2 10 0 10 2 10 4 iteración re si d u a l alg1 M k , k = 24 alg1, M k , k = 26 alg1, M k , k = 28 alg1, M k , k = 30 alg2 Figura 6.19: Convergencia de los algoritmos para la resolución del PMCL para A = DD24 87 6. Experimentación Numérica 88 Capítulo 7 Conclusiones Ten siempre a Itaca en tu mente. Llegar allí es tu destino. Mas no apresures nunca el viaje. Mejor que dure muchos años y atracar, viejo ya, en la isla, enriquecido de cuanto ganaste en el camino sin aguantar a que Itaca te enriquezca. Itaca te brindó tan hermoso viaje. Sin ella no habrías emprendido el camino. Pero no tiene ya nada que darte. Aunque la halles pobre, Itaca no te ha engañado. Así, sabio como te has vuelto, con tanta experiencia, entenderás ya qué significan las Itacas. Constantino Kavafis En este último capítulo se presentarán algunas conclusiones obtenidas luego de la realiza- ción del trabajo y a partir de la experimentación numérica. El principal aporte del trabajo, es el estudio y aplicación del precondicionamiento a sistemas lineales en matrices rectangulares. Si bien la tendencia actual del análisis numérico en el área de resolución de sistemas lineales es la búsqueda de técnicas de precondicionamiento y precondicionadores para el caso general; en la literatura no se encontró ninguna referencia a precondicionadores rectangulares en el contexto del PMCL. En efecto, y tal como se vio en el capitulo 2, los precondicionadores genericos como la factorización de Cholesky incompleta o la factorización LU incompleta, se aplican al sistema de ecuaciones normales, el cual es un sistema de n ecuaciones con n incógnitas. Sin embargo, para el caso de las ecuaciones normales, estos precondicionadores presentan serios problemas de estabilidad numérica (Ver [34]) Por consiguiente, esta nueva idea de un precondicionador rectangular que no presenta problemas de estabilidad numérica de precondicionadores clásicos aplicados a las ecuaciones normales adquiere relevancia, y se deja como un punto de inicio para la investigación de una nueva línea de precondicionadores rectangulares para el PMCL. Por otro lado, se planteó una extensión de un método iterativo, que fue concebido en principio para matrices cuadradas, como lo es el esquema de acelereación de convergencia Richardson-PR22, a matrices rectangulares. A pesar de que los resultados de la convergencia de este método extendido no presentaron una mejora sustancial en tiempo para las matrices de 89 7. Conclusiones prueba, debido a que no se consiguió una expresión que eliminara los productos matriz-matriz asociados a la iteración de Schulz en el algoritmo (5.5) (apenas se logro reducir el cálculo de dos productos matriz-matriz a uno solo) sí se observó una mejora interesante en cuanto a la convergencia a la solución del problema: el método produce inicialmente una serie de iteraciones en apariencia inútiles, pero finalmente, al conseguir corregir el condicionamiento, el método alcanza una convergencia muy acelerada en pocas iteraciones, usualmente en 2 o 3 iteraciones, el residual se reduce en varios órdenes de magnitud. Respecto al precondicionamiento explícito al PMCL propuesto en el algoritmo (5.4), se observa que la aproximación a la matriz pseudoinversa obtenida por el método de Schulz es un precondicionador muy efectivo, aun cuando su cálculo es costoso en tiempo, debido, nuevamente a la necesidad de efectuar productos matriz-matriz. No obstante, aun cuando el tiempo invertido en precondicionar el problema sea varias veces mayor al tiempo empleado por el método iterativo, debe observarse, que para el caso de las ecuaciones normales, no se observó convergencia para ninguna de las matrices de prueba utilizadas al intentar resolver el sistema de ecuaciones normales sin precondicionar. Más aun, se observó que en dicho sistema, el método de aceleración de Richardson-PR2 tiende a estancarse luego de algunas iteraciones. De lo anterior puede concluirse que a pesar de que el precondicionamiento de un sistema lineal puede implicar un alto costo computacional, en muchas ocasiones es un paso indispen- sable para obtener la solución a dicho sistema; máxime en el caso de problemas que son mal condicionados y típicamente inestables como el caso de las ecuaciones normales aplicadas al PMCL. Además, el precondicionamiento evita la necesidad de elegir métodos iterativos que tengan asociado un alto costo computacional para el cálculo de direcciones de búsqueda en cada iteración, tal como los métodos de proyección. En efecto, el método iterativo utilizado en este trabajo: el esquema de aceleración de convergencia Richardson-PR2, utiliza como direc- ción de búsqueda en la iteración n+1 el vector residual de la iteración n escalado con un valor real adecuado, con lo cual, se puede utilizar la mayor parte de la capacidad computacional en la obtención de un buen precondicionador, que generalmente garantizará la convergencia a la solución correcta del problema. 90 Bibliografía [1] Akritas, A.G., Malaschonok, G.I., y Vigklas, P.S. The SVD-Fundamental theorem of linear algebra. Nonlinear Analysis: Modelling and Control Vol. 11 No 2, 2006. [2] Barrett, R., Berry, M., Chan, T. F., Demmel, J., Donato, J., Dongarra, J., Eijkhout, V., Pozo, R., Romine, C., y Van der Vorst, H. Templates for the Solution of Linear Systems: Building Blocks for Iterative Methods, 2nd Edition. SIAM, Philadelphia, PA, 1994. [3] Benzi, M. Preconditioning techniques for large linear systems: a survey. Journal of computational physics, páginas 418–477, 2002. [4] Benzi, M., Giraud, L., y All, G. Sparse approximate inverse preconditioning for dense linear systems arising in computational electromagnetics. Numerical Algorithms, 16:1–15, 1997. [5] Björck, A. Numerical methods for least squares problem. Society for Industrial and Applied Mathematics, 1996. [6] Bourden, R. y Douglas-Faires, J. Análisis Numérico. International Thompson Editores, sexta edición, 1998. [7] Brezinski, C. Variations on Richardson’s method and acceleration. Bull. Soc. Math. Belg., páginas 33–44, 1996. [8] Brezinski, C. Projection methods for systems of equations. Elsevier Science Publishers B. V., 1997. [9] Duff, I., Grimes, R., y Lewis, J. Users’ guide for the Harwell-Boeing sparse matrix collec- tion (Release I). Informe técnico, European Centre for Rresearch and Advanced Training in Scientific Computation (CERFACS), 1992. [10] Forsman, K., Gropp, W., Kettunen, L., Levine, D., y Salonen, J. Solution of dense systems of linear equations arising from integral equation formulations. Antennas and propagation magazine, 37:96–100, 1995. [11] Freud, R. W., Golub, G. H., y Nachtigal, N. Iterative solution of linear systems. Acta Numerica, 1992. [12] Golub, G. Numerical methods for solving least squares problems. Numer. Math, 7:206– 216, 1965. 91 [13] Golub, G., Klema, V., y Stewart, G. Rank degeneracy and least squares problems. Informe técnico, Computer Sciences Department, Stanford University, 1976. [14] Golub, G. y Plemmons, R. Large scale geodetic least squares adjustment by dissection and orthogonal decomposition. Informe técnico, Computer Sciences Department, Stanford University, 1979. [15] Golub, G. y Van Loan, C. Matrix Calculations. Johns Hopkings University Press, tercera edición, 1996. [16] Golub, G. y Varga, R. Chebyshev semi-iterative methods, successive overrelaxation ite- rative methods, and the second order Richardson iterative methods part I. Numer. Math, 3:147–156, 1961. [17] Golub, G. y Varga, R. Chebyshev semi-iterative methods, successive overrelaxation itera- tive methods, and the second order Richardson iterative methods part II. Numer. Math, 3:157–168, 1961. [18] Grossman, S. Álgebra Lineal. Mac Graw Hill Latinoamericana, quinta edición, 1999. [19] Herón, B., Issard-Rorch, F., y Picard, C. Analyse numérique: Exercises et problèmes corrigés. Dunod, París, 1999. [20] Hestenes, M. y Stiefel, E. Methods of conjugate gradients for solving linear systems. Journal of Research of the National Bureau of Standards, 49:409–436, 1952. [21] Kariya, T. y Kurata, H. Generalized Least Squares. John Wiley and Sons Ltd., 2004. [22] Kincaid, D. y Cheney, W. Numerical Analysis, Mathematics of scientific computing. Brooks/Cole Publishing Company, 1991. [23] La Cruz, W. y Raydán, M. Residual iterative schemes for large-scale nonsymmetric positive definite linear systems. Computational and applied mathematics, 49:151–173, 2008. [24] Leach, S. Singular value decomposition, a primer. Informe técnico, Department of Com- puter Science, Brown University, Providence, USA, 1997. [25] Lin, C. y Saigal, R. An incomplete Cholesky factorization for dense matrices. Applied Numerical Mathematics, 536:536–558, 2000. [26] Marsden, J. y Hoffman, M. Elementary Classical Analisys. W. H. Freeman, 1974. [27] Molina, B. y Raydán, M. Métodos iterativos tipo Krylov para sistemas lineales. Centro de Estudios Avanzados, Instituto Venezolano de Investigaciones Científicas (IVIC) Caracas - Venezuela, 2004. ISBN 980-261-078-X. [28] Nievergelt, Y. A tutorial history of least squares with applications to astronomy and geodesy. J. Comput. Appl. Math., 121(1–2):37–72, 2000. [29] Opfer, G. y Schober, G. Richardson’s iteration for nonsymmetric matrices. Linear Algebra Appl, 58:343–361, 1984. 92 [30] Ortega, J. Numerical Analysis: A second course. Classics in applied mathematics. Society for Industrial and Applied Mathematics, Philadelphia, PA, USA, 1990. [31] Ortega, J. y Rheinboldt, W. Iterative solution of nonlinear equations in several varia- bles. Classics in applied mathematics. Society for Industrial and Applied Mathematics, Philadelphia, PA, USA, 2000. [32] Richardson, L.F. The approximate arithmethical solution by finite differences of physical problems involving differential equations, with application to the stress in a masonry dam. Philos. Trans. Roy. Soc. London Series A, páginas 307–357, 1910. [33] Saad, Y. Iterative methods for sparse linear systems. Y. Saad, segunda edición, 2000. [34] Saad, Y. y Sosonkina, M. Enhanced preconditioners for large sparse least squares problem. Informe técnico, Minnesota Supercomputer Institute, 2001. [35] Saad, Y. y Van der Vorst, H.A. Iterative solutions of linear systems in the 20th century. Informe técnico, Minnesota Supercomputer Institute, 1999. [36] Schmidt, E. y Stewart, G. W. On the early history of singular value decomposition. Informe técnico, Institute of Advanced Computer Studies, University of Maryland, 1992. [37] Schulz, G. Iterative berechung der reziproken matrix. ZAMM - Zeitschrift für Angewandte Mathematik und Mechanik, páginas 57–59, 1933. [38] Sheynin, O. On the history of the principle of least squares. Archive for history of exact sciences, 49:39–54, 1993. [39] Stoer, J. y Bulirsch, R. Introduction to numerical analysis. Springer-Verlag, 1993. [40] Wolberg, J. Data analysis using the method of the least squares. Springer-Verlag, 2006. [41] Wolf, P. y Ghilani, C. Adjustment Computations: Statistics and Least Squares in Surveying and GIS. John Wiley and Sons Ltd., 1997. [42] Yan, Y. Sparse preconditioned iterative methods for dense linear systems. SIAM J. Sci. Comp, 15:1190–1200, 1994. [43] Young, D.M. On Richardson’s method for solving linear systems with positive definite matrices. J. Math Phys, 32:243–255, 1954. [44] Young, D.M. Iterative solution of large linear systems. Academic Press, New York, 1971. 93Universidad Central de Venezuela Facultad de Ciencias Escuela de Computación Cálculo Científico y Tecnológico Precondicionamiento por aproximaciones a la matriz pseudoinversa para el problema de mínimos cuadrados lineales Trabajo Especial de Grado presentado ante la Ilustre Universidad Central de Venezuela Por el Bachiller Oskar Raúl Cahueñas Camargo para optar al título de Licenciado en Computación Tutores Prof. Marcos Raydán y Prof. Luis Manuel Hernández Caracas, Julio de 2009 ACTA Quienes suscriben, miembros del jurado designado por el Consejo de Escuela de Computación de la Facultad de Ciencias, para examinar el Trabajo Especial de Grado presentado por el bachiller Oskar Raúl Cahueñas Camargo, portador de la cédula de identidad V-12.422.793, con el título “Precondicionamiento por aproximaciones a la matriz pseudoinversa para el problema de mínimos cuadrados lineales”, para optar al título de Licenciado en Computación, dejan constancia de lo siguiente: Leído como fue este trabajo, por cada uno de los miembros del jurado, se fijó el día 06 de julio de 2009 a las 11:00 am, para que su autor lo defendiera en forma pública en el Salón Leandro Aristiguieta de la Facultad de Ciencias de la Universidad Central de Venezuela, mediante una presentación oral de su contenido, luego de lo cual respondió a las preguntas formuladas. Finalizada la defensa pública del Trabajo Especial de Grado, el jurado decidió aprobarlo. En fe de lo cual se levanta la presente Acta, en Caracas el día seis de julio del año dos mil nueve, dejándose también constancia de que actuó como Coordinador del Jurado el profesor Luis Manuel Hernández. ________________________________________________ Profa. Joalí Moreno (Jurado Principal) ________________________________________________ Prof. Otilio Rojas (Jurado Principal) ________________________________________________ Prof. Luis Manuel Hernández (Tutor) Resumen Dada una matriz A ∈ Rm×n con m > n de rango completo y un vector b ∈ Rn, se propone utilizar una aproximación a la matriz pseudoinversa de A, A† ∈ Rn×m como precondicionador para el problema de mínimos cuadrados lineales asociado a A y b. Dicha aproximación a A† se obtendrá a partir del método iterativo de Schulz, un esquema iterativo basado en el método de Newton en espacios de matrices para el cálculo de matrices inversas y pseudoinversas. El problema de mínimos cuadrados precondicionado se resolverá a través del esquema de aceleración de convergencia Richardson-PR2, un método iterativo-residual obtenido como una generalización del método de Richardson de primer orden para resolver sistemas de ecuaciones lineales. Se estudiarán las propiedades del precondicionador y se realizarán pruebas numéricas con diversas matrices. iii iv Índice general Introducción xiii 1. Mínimos Cuadrados Lineales (PMCL) 1 1.1. Las ecuaciones normales . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 1.2. La condición de ortogonalidad . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 1.3. Propiedades geométricas de la solución del PMCL . . . . . . . . . . . . . . . . . 3 1.4. La matriz pseudoinversa . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 1.4.1. Definición de la pseudoinversa . . . . . . . . . . . . . . . . . . . . . . . . 5 1.4.2. La Descomposición en Valores Singulares . . . . . . . . . . . . . . . . . 6 2. Métodos Clásicos para el PMCL 11 2.1. Métodos directos para el PMCL . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 2.1.1. Factorización de Cholesky . . . . . . . . . . . . . . . . . . . . . . . . . . 12 2.1.2. Descomposición QR . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 2.1.3. Descomposición QR vía transformaciones ortogonales . . . . . . . . . . . 14 2.1.4. Descomposición QR vía ortogonalización . . . . . . . . . . . . . . . . . . 16 2.2. Métodos iterativos para el PMCL . . . . . . . . . . . . . . . . . . . . . . . . . . 17 2.2.1. Métodos Estacionarios . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 2.2.2. El método del Gradiente Conjugado para las Ecuaciones Normales . . . 25 2.3. Precondicionamiento para las ecuaciones normales . . . . . . . . . . . . . . . . 28 2.3.1. Gradiente Conjugado Precondicionado para las ecuaciones normales . . 29 2.3.2. Algunos precondicionadores conocidos . . . . . . . . . . . . . . . . . . . 29 3. El Método de Schulz 35 3.1. El método de Newton . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35 3.2. El método de Schulz . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38 3.3. Propiedades del Método de Schulz para matrices rectangulares . . . . . . . . . . 42 4. El Método Richardson-PR2 45 4.1. El esquema de aceleración de convergencia Richardson-PR2 . . . . . . . . . . . 46 4.2. Elección del tamaño de paso para el esquema Richardson-PR2 . . . . . . . . . . 46 4.3. Elección de la dirección de búsqueda para Richardson-PR2 . . . . . . . . . . . . 48 4.4. El método iterativo de Richardson-PR2 . . . . . . . . . . . . . . . . . . . . . . 48 4.5. Algunas estrategias de precondicionamiento para Richardson-PR2 . . . . . . . . 50 4.5.1. Precondicionador constante . . . . . . . . . . . . . . . . . . . . . . . . . 50 4.5.2. Precondicionador lineal iterativo . . . . . . . . . . . . . . . . . . . . . . 50 v 4.5.3. Precondicionador cuadrático iterativo . . . . . . . . . . . . . . . . . . . 50 5. Precondicionamiento del PMCL 53 5.1. Mk como precondicionador al PMCL . . . . . . . . . . . . . . . . . . . . . . . . 53 5.2. Cálculo de MkA y Mkb . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54 5.3. Algoritmo PMCL-Richardson-PR2 1 . . . . . . . . . . . . . . . . . . . . . . . . 56 5.4. Algoritmo PMCL-Richardson-PR2 2 . . . . . . . . . . . . . . . . . . . . . . . . 59 6. Experimentación Numérica 61 6.1. Plataforma computacional . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61 6.2. Matrices de prueba . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62 6.2.1. Matrices densas de prueba . . . . . . . . . . . . . . . . . . . . . . . . . . 62 6.2.2. Matrices sparse de prueba . . . . . . . . . . . . . . . . . . . . . . . . . . 63 6.3. Estudio numérico de Mk como precondicionador . . . . . . . . . . . . . . . . . . 63 6.3.1. Grupo de Experimentos A . . . . . . . . . . . . . . . . . . . . . . . . . . 64 6.3.2. Grupo de Experimentos B . . . . . . . . . . . . . . . . . . . . . . . . . . 69 6.4. Experimentos numéricos de la resolución del PMCL . . . . . . . . . . . . . . . . 77 6.4.1. Formato de los experimentos numéricos de la resolución del PMCL . . . 77 6.4.2. Grupo de Experimentos A . . . . . . . . . . . . . . . . . . . . . . . . . . 78 6.4.3. Grupo de Experimentos B . . . . . . . . . . . . . . . . . . . . . . . . . . 84 7. Conclusiones 89 vi Índice de Algoritmos 2.1. Factorización de Cholesky de la matriz A . . . . . . . . . . . . . . . . . . . . . 13 2.2. Solución del PMCL vía factorización de Cholesky . . . . . . . . . . . . . . . . . 14 2.3. Método de Golub para la solución del PMCL vía descomposición QR . . . . . . 16 2.4. Método de Richardson de primer orden para las ecuaciones normales . . . . . . 19 2.5. Método de Jacobi para las ecuaciones normales . . . . . . . . . . . . . . . . . . 21 2.6. Método de Gauss-Seidel para las ecuaciones normales . . . . . . . . . . . . . . . 22 2.7. Método SOR para las ecuaciones normales . . . . . . . . . . . . . . . . . . . . . 24 2.8. Gradiente Conjugado Genérico para una matriz SPD . . . . . . . . . . . . . . . 26 2.9. GC aplicado a las ecuaciones normales . . . . . . . . . . . . . . . . . . . . . . . 27 2.10. GC Precondicionado . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 2.11. Factorización de Cholesky incompleta . . . . . . . . . . . . . . . . . . . . . . . 31 2.12. GC Precondicionado con Factorización LU . . . . . . . . . . . . . . . . . . . . . 34 3.1. Método de Newton en varias variables . . . . . . . . . . . . . . . . . . . . . . . 37 3.2. Iteración de Schulz . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41 4.1. Iteración de Richardson-PR2 cruda . . . . . . . . . . . . . . . . . . . . . . . . . 49 5.1. Cálculo de la potencia A2 k para una matriz A . . . . . . . . . . . . . . . . . . . 55 5.2. Cálculo de la matriz MkA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56 5.3. Cálculo de la matriz MkA y el vector Mkb . . . . . . . . . . . . . . . . . . . . . 57 5.4. Algoritmo PMCL-Richardson-PR2 1 . . . . . . . . . . . . . . . . . . . . . . . . 58 5.5. Algoritmo PMCL-Richardson-PR2 2 . . . . . . . . . . . . . . . . . . . . . . . . 60 vii viii Dedicatoria A mi madre y mi hermana. ix x Agradecimientos A mis tutores, los Profesores Marcos Raydan y Luis Manuel Hernández por su infinita paciencia y apoyo. xi La precisión numérica es el alma de toda ciencia. Sir D’ Arcy Wentworth Thompson En mi teoría del movimiento de los cuerpos celestes, mostré cómo calcular los valores más probables de las incógnitas, dado que las leyes probabilísticas que rigen los errores observacionales son bien conocidas. Sin embargo, en la mayoría de los casos dichas leyes son meras hipótesis. En virtud de ello apliqué a esa teoría a la ley más plausible: que la probabilidad de un error x es proporcional a exxhh. A partir de esta suposición, nació un método que usé durante algún tiempo, especialmente para cálculos astronómicos. Actualmente es utilizado por muchas personas y es conocido como el método de mínimos cuadrados Carl Gauss xii Introducción Si una idea no es absurda al principio, entonces no merece la pena Albert Einstein Uno de los problemas históricamente más relevantes en las matemáticas aplicadas, es el de la búsqueda de métodos numéricos para el ajuste de datos a alguna función. Este problema es una necesidad fundamental en las ciencias básicas y aplicadas, pues el objetivo último de las disciplinas científicas es la explicación de fenómenos naturales a través de modelos matemáticos que deben obtenerse o ser validados a partir del ajuste de datos provenientes de la observación o experimentación de dichos fenómenos. En particular, en una gran cantidad de aplicaciones, se busca establecer la existencia de relaciones lineales entre varias variables que caracterizan determinado fenómeno. Esto es, supóngase que se tienen variables x1, x2, . . . xn y se desea verificar que alguna variable bk satisface la relación bk = i=n∑ i=1 cixi para un conjunto de parámetros reales c1, c2, . . . cn. La ecuación anterior se conoce como modelo de ajuste lineal para n valores. Si se tienen m conjuntos de datos experimentales (x1k, x2k, . . . xnk, bk) para k = 1, . . . ,m con m > n, el problema anterior conlleva a un sistema de ecuaciones lineales sobredeterminado de la forma Ax = b con A ∈ Rm×n, x ∈ Rn y b ∈ Rm. Debido a que este sistema es sobredeterminado, no se garantiza la existencia de una solución en el caso general. No obstante, se desea poder hallar al menos una aproximación a la solución de dicho problema que sea la mejor posible en algún sentido. El problema anterior se conoce como problema de mínimos cuadrados lineales o simple- mente problema de mínimos cuadrados. Históricamente, se tienen referencias de J. Kepler (1571 - 1630) quien usó modelos lineales para ajustar cálculos de órbitas planetarias. R. Cotes (1682 - 1716) y T. Simpson (1710 - 1761) utilizaron ajustes lineales en donde se buscaba minimizar la suma de los cuadrados de los errores de los datos experimentales, aplicados a problemas de astronomía. También S. Laplace (1749-1827) propuso diversas técnicas de ajuste lineal de datos basados en la minimización xiii del error promedio y del error promedio al cuadrado en aplicaciones de geodesia y mediciones terrestres. No obstante, fue C. Gauss (1777 - 1855) quién postuló el modelo de ajuste por mínimos cuadrados tal como se conoce en la actualidad, y finalmente, A. Legendre (1752- 1833) quien acuñó el término Mínimos cuadrados al publicar en 1805 el clásico método de las ecuaciones normales para la resolución de dicho problema. Para el lector interesado en los aspectos históricos del problema de mínimos cuadrados se sugiere consultar [28] y [38]. Por otro lado, el desarrollo de métodos numéricos para el problema de mínimos cuadrados se inició plenamente después de la segunda mitad del siglo XX. En 1965 G. Golub (1932-2007) publica un artículo donde utiliza la descomposición QR vía transformaciones de Householder a la resolución del problema de mínimos cuadrados [16], el cual es uno de los primeros trabajos de métodos numéricos para mínimos cuadrados. Diversos métodos para la resolución del problema a través de métodos como la eliminación Gaussiana, de la descomposición en valores singulares (SVD) entre otros, fueron propuestos por Golub, Kahan y Wilkinson, entre otros, en las décadas de 1960 y 1970. Así mismo en los últimos años se han hecho grandes progresos en los métodos directos para resolver problemas de mínimos cuadrados, así como métodos iterativos para el caso de matrices sparse de grandes dimensiones. En este trabajo se desarrollará un método numérico iterativo para resolver el problema de mínimos cuadrados para matrices densas de grandes dimensiones. En primer lugar, la matriz A será precondicionada usando una aproximación a A†, la matriz pseudoinversa de A. La aproximación a A† proviene de la iteración de Schulz, que es un caso particular del método de Newton en espacios de matrices, aplicado al cálculo de la inversa de una matriz. Se plantea, por consiguiente, el precondicionamiento de una matriz rectangular, un tópico del cual no se encontró ninguna referencia en la literatura, por lo cual representa el aporte original de este trabajo. En segundo lugar, el sistema precondicionado, el cual se convierte en un sistema de n ecuaciones con n incógnitas, se resolverá con un método iterativo que proviene de un esquema de aceleración de convergencia del método de Richardson de primer orden desarrollado en [7]. En este esquema, las iteraciones son de la forma r(0) = b−Ax(0) x(k+1) = x(k) + λkCkr(k) k ≥ 0 r(k+1) = r(k) − λkACkr(k) k ≥ 0 Donde en cada iteración, la solución x avanza por una dirección de búsqueda dada por el residual de la iteración anterior premultiplicado por una matriz precondicionadora Ck, la cual puede variar en cada iteración y un tamaño de paso λk Además, se definirá una extensión de dicho esquema de aceleración de convergencia para iterar sobre la matriz rectangular, usando como familia de matrices precondicionadoras las aproximaciones aA† generadas por la iteración de Schulz. Debe observarse que históricamente, los métodos iterativos suelen ser utilizados princi- palmente en matrices sparse, ya que en el caso de matrices densas, estos tienen un costo computacional más elevado respecto a los métodos directos. Sin embargo, en los últimos años, debido a la creciente popularización del procesamiento paralelo, está surgiendo un interés por el uso de métodos iterativos para la resolución de sistemas lineales con matrices densas de grandes dimensiones, pues estos métodos permiten explotar el paralelismo de una manera mu- cho más eficiente que los métodos directos. Algunas referencias en esta dirección son [10] y [42]. xiv Entre aplicaciones de problemas de mínimos cuadrados que involucran matrices densas se pueden encontrar en áreas como astronomía, geodesia, fotogrametría, estadística entre otras (Ver [11], [21] y [14]) El resto del trabajo se organiza de la siguiente manera. En el capítulo 1 se presentarán los fundamentos teóricos del problema de mínimos cuadrados lineales sobre matrices de rango completo. En el capítulo 2 se dará una breve introducción de los métodos numéricos clásicos encontrados en la literatura para la resolución del problema de mínimos cuadrados. En el capítulo 3 se presentará el método de Schulz como un caso especial del método de Newton en varias variables y se probarán algunas propiedades de la iteración de Schulz para el caso de matrices rectangulares, que implican que dichas matrices son una buena elección para precondicionar el problema de mínimos cuadrados. En el capítulo 4 se introducirá el esquema de aceleración de convergencia de Richardson-PR2. En el capítulo 5 se presentarán los algoritmos para la resolución del problema de mínimos cuadrados, obtenidos a partir de la iteración de Richardson-PR2 combinada con el precondicionamiento por aproximaciones a la matriz pseudoinversa. Los dos últimos capítulos estarán dedicados a presentar los resultados de las pruebas numéricas realizadas y las conclusiones del trabajo. xv xvi Capítulo 1 El Problema de Mínimos Cuadrados Lineales (PMCL) La verdad es demasiado complicada como para permitir nada más allá de meras aproximaciones John von Neumann Considere el sistema de ecuaciones lineales: Ax = b, A ∈ Rm×n, b ∈ Rm m > n (1.1) Como este sistema es sobredeterminado, esto es, con más ecuaciones que incógnitas, no se puede garantizar la existencia de una solución x para una matriz A y un vector b arbitrarios. Por lo tanto, es deseable hallar un vector x, tal que Ax sea la mejor aproximación a b. Una posible escogencia de x, para que Ax sea la mejor aproximación a b, consiste en hallar un vector x que minimice la distancia entre los vectores Ax y b. En otras palabras, hallar el vector x que minimice el vector residual Ax − b bajo la norma euclídea. Esto conlleva al siguiente problema de optimización: min x ‖Ax− b‖2 (1.2) donde ‖ ‖2 es la norma euclídea. Este problema es conocido como el Problema de Mínimos Cuadrados Lineales (PMCL) asociado a la matriz A y el vector b. Esta elección de una mejor aproximación a una solución de la ecuación (1.1) está motivada por las aplicaciones estadísticas que dieron origen al estudio de los mínimos cuadrados. Para una introducción a los problemas de mínimos cuadrados desde el enfoque de la estadística, ver [40], [21] y [41]. En el resto de este capítulo, se estudiarán los fundamentos teóricos del PMCL y su solucio- nes. El trabajo se restringirá al PMCL en el caso de matrices A ∈ Rm×n de rango completo, esto es, rango(A) = n. El lector interesado en métodos numéricos para la resolución del PMCL para matrices con rango(A) < n (conocidas en la literatura como matrices de rango deficiente) puede consultar [13] y [5]. 1 1. Mínimos Cuadrados Lineales (PMCL) 1.1. Las ecuaciones normales A partir de la resolución problema de optimización (1.2) obtiene un sistema de ecuacio- nes lineales asociado al PMCL, el cual se conoce como sistema de ecuaciones normales o simplemente ecuaciones normales, tal como se define a continuación. Definición 1.1.1. Sea A ∈ Rm×n y b ∈ Rm. El sistema de n ecuaciones y n incógnitas ATAx = AT b (1.3) Se conoce como sistema de ecuaciones normales asociado al PMCL. Obsérvese que para el caso de una matriz A de rango completo, la matriz ATA es una matriz simétrico positivo definida (SPD), lo cual implica que todos sus autovalores son positivos y por lo tanto el sistema (1.3) tiene solución única. La importancia de las ecuaciones normales radica en que la solución de (1.3) es la solución del PMCL asociado a A y b, tal como se establece en el siguiente teorema. Teorema 1.1.1. Si x ∈ Rn es solución de las ecuaciones normales, para A ∈ Rm×n de rango completo y b ∈ Rm, entonces, x es solución del PMCL asociado a A y b. Demostración. Ver [5]. Este último teorema, permite obtener una solución cerrada para el PMCL en el caso de que A sea de rango completo: el vector x que satisface la ecuación (1.2) viene dado por x = ( ATA )−1 AT b (1.4) 1.2. La condición de ortogonalidad A través de las ecuaciones normales, presentadas en la sección anterior, puede deducirse una importante propiedad que caracteriza a las soluciones del PMCL asociado a A ∈ Rm×n y b ∈ Rm. Dicha propiedad se conoce como la condición de ortogonalidad de las soluciones del PMCL y se establece en el siguiente teorema. Teorema 1.2.1. Sea S = {x ∈ Rn : ‖Ax− b‖2 es mínima} El conjunto de soluciones al PMCL asociado a A y b. Entonces, x ∈ S si y solo si x satisface la siguiente condición de ortogonalidad: AT (b−Ax) = 0 Demostración. (⇒) Suponga que x̂ ∈ S y que x̂ no satisface la condición de ortogonalidad. Esto es, AT r̂ = z 6= 0, donde r̂ = b−Ax̂. Considere x = x̂+ �z para algún � > 0. Así 2 1. Mínimos Cuadrados Lineales (PMCL) r = b−Ax = b−A (x̂+ �z) = (b−Ax̂) + �Az = r̂ − �Az y, por otro lado ‖Ax− b‖2 = rT r = r̂T r̂ − 2�zT z + �2(Az)T (Az) Para �� 1 el término que tiene �2 es despreciable y por lo tanto ‖Ax− b‖2 ≈ r̂T r̂ − 2�zT z < r̂r̂T = ‖Ax̂− b‖2 Esto es, x̂ no minimiza ‖Ax̂−b‖2, lo cual es una contradicción. Por lo tanto, queda probado, que x ∈ S satisface la condición de ortogonalidad. (⇐) Suponga que x̂ satisface la condición de ortogonalidad AT r̂ = 0 con r̂ = b− Ax̂. Sea x ∈ Rn arbitrario, se tiene: r = b−Ax = b−Ax+Ax̂−Ax̂ = (b−Ax̂) +A (x̂− x) = r̂ +Ae Calculando la norma de r, a partir de la expresión anterior se tiene ‖b−Ax‖2 = rT r = (r̂ +Ae)T (r̂ +Ae) = r̂T r + ‖Ae‖2 de donde se concluye que ‖b−Ax‖2 se minimiza cuando e = 0, esto es, x̂− x = 0. Luego x̂ es solución del PMCL. Con este resultado, se tiene una importante herramienta teórica para el estudio de los métodos numéricos para resolver el PMCL, pues provee una condición necesaria y suficiente para verificar que un vector x ∈ Rm sea solución de la ecuación (1.2). 1.3. Propiedades geométricas de la solución del PMCL Para ilustrar la interpretación geométrica de la solución al PMCL, considere los subespacios fundamentales asociados a la matriz A ∈ Rm×n: 3 1. Mínimos Cuadrados Lineales (PMCL) Ax b b−Ax Figura 1.1: Significado geométrico de la solución del PMCL R(A) = {z ∈ Rn : para algún x ∈ Rm Ax = z}, el recorrido o espacio columna de A N (A) = {x ∈ Rm : Ax = 0}, la nulidad o kernel de A Obsérvese que si x es una solución del PMCL, entonces la condición de ortogonalidad implica que r = b−Ax ∈ N (AT ) Por otro lado, es evidente que Ax ∈ R(A) Así, se tiene que x, una solución al PMCL descompone al vector b en dos componentes ortogonales de la forma: b = Ax+ r, Ax ∈ R(A), r ∈ N (AT ) (1.5) En la figura (1.1) se ilustra esta descomposición ortogonal. Además, a partir de la solución al PMCL de la ecuación (1.4), se obtiene una transformación lineal que proyecta cualquier vector de Rm en R(A). La definición formal de proyector se presenta a continuación. Definición 1.3.1. Sea S un subespacio de Rn, una matriz PS ∈ Rn×n se denomina proyector ortogonal sobre S si: PS es simétrica. PS es idempotente, esto es P 2S = PS Para todo x ∈ Rn, PSx ∈ S 4 1. Mínimos Cuadrados Lineales (PMCL) Puede verificarse fácilmente que si PS es un proyector ortogonal sobre el subespacio S, entonces, Im − PS es un proyector ortogonal sobre S⊥, el complemento ortogonal de S. Con la definición anterior, se tiene que si x es la solución de la ecuación (1.2), entonces Ax es la proyección ortogonal de b sobre R(A), así, r = (Im − PR(A)), y usando la ecuación (1.4) se obtiene que PR(A) = A ( ATA )−1 AT (1.6) 1.4. La matriz pseudoinversa La matriz pseudoinversa es una extensión del concepto de matriz inversa para el caso de matrices rectangulares. Esta matriz es un caso particular de las Matrices inversas generaliza- das, las cuales se definen a continuación. Definición 1.4.1. Sea A ∈ Rm×n. Una matriz A− ∈ Rn×m es una inversa generalizada de A si satisface: AA−A = A En esta sección se definirá la matriz pseudoinversa y se presentarán varias de sus propieda- des, que serán de utilidad posteriormente cuando se analice el precondicionamiento del PMCL por aproximaciones a la pseudoinversa. Finalmente, se estudiará la Descomposición en Valores Singulares (SVD). Esta es una factorización que existe para cualquier matriz A ∈ Rm×n. 1.4.1. Definición de la pseudoinversa A continuación se definirá la matriz pseudoinversa de A en función de los proyectores ortogonales asociados a los subespacios R (A) y N (A) introducidos en la sección anterior. Definición 1.4.2. Sea A ∈ Rm×n la matriz pseudoinversa de A o matriz de Moore-Penrose A† ∈ Rn×m es una matriz que satisface: 1. P ≡ A†A es el proyector ortogonal sobre N (A)⊥ 2. P̄ ≡ AA† es el proyector ortogonal sobre R (A)⊥ La matriz pseudoinversa satisface una serie de propiedades análogas a las de la matriz inversa, también conocidas como Condiciones de Penrose. El siguiente teorema establece dichas propiedades. Teorema 1.4.1. Sea A ∈ Rm×n. La pseudoinversa A† satisface las siguientes propiedades (Condiciones de Penrose): 1. A†A es simétrica. 2. AA† es simétrica. 3. AA†A = A. 4. A†AA† = A. 5 1. Mínimos Cuadrados Lineales (PMCL) Demostración. Ver [39] Existen otras propiedades relevantes de la pseudoinversa, que pueden deducirse de las con- diciones dadas en el teorema (1.4.1) y que también mantienen una analogía con las propiedades de la inversa para el caso de matrices cuadradas. Teorema 1.4.2. Sea A ∈ Rm×n. La pseudoinversa A† satisface las siguientes propiedades: 1. ( A† )† = A. 2. ( A† )T = ( AT )† . 3. (αA)† = α−1A† Para todo α ∈ R tal que α 6= 0 4. ( ATA )† = A† ( A† )T . 5. Si U ∈ Rn×m y V ∈ Rm×n son matrices unitarias, esto es U−1 = UT y V −1 = V T ,( UAV T )† = V A†UT . 6. Si AAT = ATA entonces A†A = AA† y (An)† = ( A† )n para n ∈ Z. 7. rango (A) = rango ( AT ) = rango ( A† ) = rango ( A†A ) = tr ( A†A ) . Demostración. Ver [5] 1.4.2. La Descomposición en Valores Singulares La Descomposición en Valores Singulares (SVD) de una matriz A ∈ Rm×n es una herra- mienta de gran importancia teórica para el estudio del PMCL. La SVD provee una extensión al concepto de Diagonalización de matrices cuadradas, esto es, una transformación ortogonal de A a una matriz diagonal de n×n que preserva la norma ‖ ‖2. El siguiente resultado establece la existencia de la SVD para cualquier matriz rectangular. Teorema 1.4.3. Sea A ∈ Rm×n con rango (A) = r ≤ n, entonces existen matrices unitarias U ∈ Rm×m y V ∈ Rn×n y una matriz Σ ∈ Rm×n tales que: A = UΣV H , Σ = ( Σ1 0 0 0 ) , Σ1 =   σ1 0 . . . 0 0 σ2 . . . 0 . . . 0 0 . . . σr   ∈ Rr×r Los elementos no nulos en la diagonal principal del bloque Σ1 satisfacen σ1 ≥ σ2 ≥ · · · ≥ σn > 0 y se denominan valores singulares de A. Demostración. Ver [15]. Obsérvese que si U = (u1 · · ·ur) y V = (v1, · · · vr) donde ui y vi son las columnas de U y V respectivamente, SVD de A se puede reescribir de la forma: A = UΣH = r∑ i=1 σiuiv H i (1.7) 6 1. Mínimos Cuadrados Lineales (PMCL) Así, se tiene que la SVD de A induce a una descomposición de A como la suma de r = rango(A) matrices de rango 1 (estas son las matrices σiuivi) La SVD de A, es de interés, ya que permite definir la solución al PMCL en función de las matrices U , V y Σ mencionadas en el teorema (1.4.3) Teorema 1.4.4. Considere el problema general de mínimos cuadrados lineales: min x∈S ‖x‖2, S = {x ∈ Rn : ‖Ax− b‖2 es mínima} donde A ∈ Cm×n y rango(A) = r ≤ min(m,n). Este problema siempre tiene una solución única x, la cual se puede escribir en términos de las matrices resultantes de la SVD de A como: x = V ( Σ−1r 0 0 0 ) UHb Donde UH denota la matriz traspuesta conjugada de U . Demostración. En vista de que V y U son matrices unitarias, se tiene que V V H = In. Así b−Ax = b−AV V Hx Por otro lado ‖b−Ax‖2 = ‖UH (b−Ax) ‖2 = ‖UH ( b−AV V H ) ‖2 Ahora considere: z = V Hx = ( z1 z2 ) , c = UHb = ( c1 c2 ) = ( [ UHb ] r[ UHb ] m−r ) Donde z1, c1 ∈ Cr. Con lo anterior, podemos reescribir la norma del residual b−Ax como: ‖b−Ax‖2 = ‖UH ( b−AV V Hx ) ‖2 = ‖UHb− ( UHAV ) V Hx‖2 = ∥∥∥∥ ( c1 c2 ) − ( Σ1 0 0 0 )( z1 z2 )∥∥∥∥ 2 = ∥∥∥∥ ( c1 − Σ1z1 c2 )∥∥∥∥ 2 Puede observarse que para c1 y c2 fijos, el residual se minimiza cuando c1 − Σ1z1 = 0, esto es, z1 = Σ −1 1 c1, mientras que z2 puede ser cualquier valor. En particular, si se escoge z2 = 0, también se minimiza ‖z‖2, lo cual implica que se minimiza ‖x‖2, siendo el valor de x que 7 1. Mínimos Cuadrados Lineales (PMCL) minimiza la norma igual a: x = V z = V ( z1 z2 ) = V ( Σ−11 c1 0 ) = V ( Σ−11 [ UHb ] r + 0 [ UHb ] m−r 0 [ UHb ] r + 0 [ UHb ] m−r ) = V ( Σ−11 0 0 0 )( [ UHb ] r[ UHb ] m−r ) = V ( Σ−11 0 0 0 ) UHb La construcción de la solución del problema de mínimos cuadrados generalizados en la demostración del teorema (1.4.4), define la matriz C = V ( Σ−11 0 0 0 ) UH (1.8) La cual resulta ser A†, tal como se deduce del siguiente resultado: Teorema 1.4.5. La matriz C = V ( Σ−11 0 0 0 ) UH Satisface las condiciones de Penrose establecidas en el teorema (1.4.1) y por consiguiente, C = A†. Demostración. Ver [5] Con este último resultado, y a partir del teorema (1.4.4) se deriva que la solución del PMCL se puede expresar, en términos de A† como: x = A†b (1.9) Para el caso de A de rango completo, usando la ecuación (1.4) se obtiene una expresión para A†. A† = ( ATA )−1 AT (1.10) Obsérvese que la cálculo de la SVD provee desde el punto de vista numérico de un método directo para la resolución del PMCL. Sin embargo,el cálculo de la SVD tiene un altísimo costo computacional, lo cual hace que el enfoque de la SVD sea poco atractivo para la resolución del PMCL en el caso general. No obstante, para el caso de matrices densas de rango deficiente, la SVD es la primera elección para la resolución del PMCL. Ver [15] para un estudio de los métodos clásicos para el cálculo de la SVD y sus aplicaciones al PMCL en matrices de rango deficiente. 8 1. Mínimos Cuadrados Lineales (PMCL) Cabe destacar, además, que la SVD por sí sola, es un tópico de gran interés en el álgebra lineal numérica debido a la cantidad de aplicaciones existentes en áreas como el tratamiento digital de imágenes, filtros digitales, reconocimiento de patrones y series temporales, entre otras. Para un estudio más detallado de la SVD y referencias a sus distintas aplicaciones ver [24] y [1]. Para una introducción al estudio de los orígenes y desarrollo histórico de la SVD ver [36]. Para finalizar este capítulo, se demostrará una de las propiedades más importantes de la matriz pseudoinversa: AA† ≈ In en el sentido de que la distancia entre AA† e In es la mínima posible respecto a AX con X ∈ Rn×m. Teorema 1.4.6. X = A† minimiza la norma ‖AX − Im‖F , donde ‖ ‖F es la norma de Frobenious. Demostración. Sea X la matriz que minimiza la norma. Observe que: ‖AX − I‖F = m∑ i=1 ‖Axi − ei‖22 Donde xi es la i-ésima columna de X y ei es el i-ésimo vector de la base canónica de Rm. Para todo i en 1 . . . n, el término ‖Axi − ei‖2 se minimiza cuando xi es solución al PMCL Ax = ei. Por la ecuación (1.10), la solución del i-ésimo PMCL es xi = A†ei, es decir, xi es la i-ésima columna de A†. Luego, X = A†. 9 1. Mínimos Cuadrados Lineales (PMCL) 10 Capítulo 2 Métodos Numéricos Clásicos para el PMCL Donde quiera que haya un número está la belleza Proclo En este capítulo se dará una breve introducción a los métodos numéricos más conocidos para la resolución del PMCL que se encuentranen la literatura. Los métodos numéricos para el PMCL, se dividen en dos grandes familias: métodos directos, los cuales se aplican principal- mente a matrices densas y los métodos iterativos usados principalmente en matrices sparse. También, se revisarán algunas ideas del precondicionamiento de las ecuaciones normales apli- cado al PMCL. El principal compendio de métodos numéricos para el PMCL es [5]; los métodos directos clásicos como la factorización QR para matrices rectagulares son también cubiertos por [15]. La teoría general para los métodos iterativos estacionarios así como del método del Gradiente Conjugado puede consultarse en [33]. Esta obra igualmente aborda los métodos iterativos para la resolución del sistema (1.3) en particular. Un artículo introductorio al tema del precondicio- namiento de matrices es [3]; para el caso particular del precondicionamiento de las ecuaciones normales, puede consultarse [34] y [5]. Un bosquejo histórico del desarrollo de los métodos iterativos para la resolución de sistemas lineales, desde un punto de vista computacional, se encuentra en [35]. Previo a la presentación de los métodos para el PMCL, se introducirá un concepto de gran importancia para el estudio de los métodos numéricos: el condicionamiento numérico. El condicionamiento numérico de un problema, se refiere a una medida de la sensibilidad que puede tener la solución de dicho problema a pequeñas perturbaciones en los datos de entrada. Para el caso de los sistemas de ecuaciones lineales, sea M ∈ Rn×n y b ∈ Rn y supóngase que se desea resolver el sistema Mx = b (2.1) si los valores numéricos de M y b provienen, por ejemplo, de algún experimento, estos serán susceptibles de tener alguna perturbación debido al redondedo u otros errores experimentales como instrumentos de medición mal calibrados, una lectura incorrecta por parte del experi- mentador, o la propagación de errores en mediciones indirectas, entre otros. Por lo tanto, en 11 2. Métodos Clásicos para el PMCL la práctica, en lugar de resolver el sistema (2.1) se tendrá que resolver el siguiente sistema asociado (M + ∆M)x = b+ ∆b (2.2) donde ∆M y ∆b son una matriz y un vector que contienen los errores experimentales o perturbaciones asociados a M y b respectivamente. Entonces, se dice que el sistema (2.1) está bien condicionado si el error relativo de la solución del sistema perturbado (2.2) respecto a la solución del sistema original es pequeño. En caso contrario, se dice que el problema está mal condicionado. Una medida para el condicionamiento de una matriz en el caso general, es el número de condición, el cual se define a continuación Definición 2.0.3. Sea A ∈ Rn×n, no sigular, y sea ‖ ‖ una norma matricial inducida. El número de condición de A, el cual se denota como κ(A) se define como κ(A) ≡ ‖A‖‖A−1‖ Para el caso de una matriz rectangular A ∈ Rm×n el número de condición se define como κ(A) ≡ ‖A‖‖A†‖ Obsérvese que para toda matriz A, se cumple que κ(A) ≥ 1. El número de condición da una medida del condicionamiento de un sistema de ecuaciones lineales que involucre a la matriz A. Si κ(A) ≈ 1 (esto es, un número de condición pequeño) entonces la matriz está bien condicionada, mientras que un número de condición que sea mucho mayor a 1 (esto es, un número de condición grande) indica que la matriz está mal condicionada, y por lo tanto el resultado de un método numérico que involucre a dicha matriz podría ser poco confiable. Además, debe destacarse que en el caso del PMCL asociado a una matriz A y un vector b, el condicionamiento del problema no depende sólo de A, sino que depende además del valor de b, por lo cual se puede obtener una extensión del número de condición para el PMCL (Ver [5]) tal como se define a continuación Definición 2.0.4. Sea A ∈ Rm×n y b ∈ Rn. Si xLS es una solución al PMCL asociado a A y b, entonces el número de condición del PMCL, usando la norma euclídea, viene dado por κLS(A, b) ≡ κ(A) ( 1 + κ(A) ‖r‖2 ‖A‖2‖xLS‖2 ) donde r = b−AxLS. 2.1. Métodos directos para el PMCL 2.1.1. Factorización de Cholesky El método directo más simple para la resolución de las ecuaciones normales se basa en la factorización de Cholesky, la cual existe para cualquier matriz SPD, tal como se establece en el siguiente teorema. 12 2. Métodos Clásicos para el PMCL Teorema 2.1.1. Sea C ∈ Rn×n una matriz SPD, entonces existe una única matriz R ∈ Rn×n triangular superior tal que: C = RTR La matriz R se denomina Factor de Cholesky asociado a C. Demostración. Ver [22] Para el caso de una matriz C almacenada por filas, la factorización de Cholesky se puede calcular con el algoritmo (2.1). Algoritmo 2.1 Factorización de Cholesky de la matriz A Algoritmo FactChol Descripción Calcula la factorización de Cholesky de una matriz A SPD. Entradas • A Matriz de n filas por n columnas SPD. Salidas • R Matriz triangular de n filas por n columnas tal que A = RTR. 1: función FactChol(A) 2: R← 0 3: para j ← 1 . . . n hacer 4: para i← 1 . . . j − 1 hacer 5: rij ← cij − ∑i−1 k=1 rkirkj 6: rij ← rij/rii 7: fin para 8: rjj ← ( cjj − ∑j−1 k=1 r 2 kj )1/2 9: fin para 10: ← R 11: fin función Finalmente, conociendo la factorización de Cholesky de A, el algoritmo (2.2) define un método para la resolución del PMCL. 2.1.2. Descomposición QR Uno de los métodos directos más utilizados para la resolución del PMCL en sistemas sobredeterminados, es el de la descomposición ortogonal de la matriz A. Esta estrategia es factible ya que, de que si Q ∈ Rm×m es una matriz ortogonal, esta preserva la norma euclídea y por consiguiente, el problema de optimización min x ∥∥QT (Ax− b)∥∥ 2 (2.3) 13 2. Métodos Clásicos para el PMCL Algoritmo 2.2 Solución del PMCL vía factorización de Cholesky Algoritmo CholPMCL Descripción Resuelve el PMCL asociado a A y b usando la factorización de Cholesky. Entradas • A Matriz de rango completo de m filas por n columnas con m > n. • b Vector columna de n filas. Salidas • xLS Solución al PMCL asociado a A y b. 1: función CholPMCL(A, b) 2: R← FactChol(C) 3: d← AT b 4: Resolver el sistema Ry = d 5: Resolver el sistema RTxLS = y 6: ← xLS 7: fin función es equivalente al planteado en la ecuación (1.2). El objetivo es, entonces, hallar alguna matriz Q ∈ Rm×m tal que la ecuación (2.3) sea más fácil de resolver que la ecuación (1.2). En general, para cualquier matriz rectangular, existe una descomposición ortogonal cono- cida como descomposición QR cuya existencia se garantiza por el siguiente teorema Teorema 2.1.2. Sea A ∈ Rm×n con m ≥ n, entonces existe una matriz ortogonal Q ∈ Rm×m y una matriz triangular R ∈ Rn×n tales que A = Q ( R 0 ) esta factorización se conoce como descomposición QR de A y la matriz R se conoce como factor R asociado a A. Demostración. Ver [5] En los siguientes apartados se estudiarán algunos de los principales métodos numéricos para la descomposición QR de A y su respectiva aplicación al PMCL 2.1.3. Descomposición QR vía transformaciones ortogonales Una estrategia para hallar la descomposición QR deA, consiste en hallar matrices P1, P2, . . . , Pn de dimensión m×m, tales que Q = PnPn−1 . . . P1, donde Pi es un proyector ortogonal sobre algún subespacio de R(A), tal como se describe a continuación. 14 2. Métodos Clásicos para el PMCL Para comenzar, suponga que A se puede descomponer a partir de la siguiente relación de recurrencia A(1) = A A(k+1) = PkA (k−1), k = 1 . . . n (2.4) donde Pk, k = 1, 2, . . . es una matriz ortogonal que debe construirse de tal forma que A(k+1) tenga una estructura triangular superior sobre las primeras k columnas, es decir, A(k+1) debe tener la estructura A(k+1) = PkPk−1 . . . P1A = ( R11 R12 0 Ã(k+1) ) (2.5) donde R11 ∈ Rk×k es una matriz triangular superior y la matriz Ã(k+1) ∈ R(m−k)×(n−k−1) es ortogonal. Si la estructura de Ã(k) por columnas es: Ã(k) = (ã(k)k , ã (k) k+1, . . . , ã (k) n ) entonces, la matriz Pk debe tener la siguiente forma Pk = ( Ik−1 0 0 P̃k ) (2.6) donde P̃k es una matriz ortogonal de tamaño (m− k + 1)× (m− k + 1) tal que Pkã (k) k = σke1, σk = rkk = ‖ã k k‖ 2 2, e1 = (1, 0, . . . 0) T Obsérvese que en el k-ésimo paso de la recurrencia de la ecuación (2.5) la transformación ortogonal sólo afecta a la submatriz Ã(k+1) de A y (R11, R12) son las primeras k − 1 filas de R. Después de n pasos se obtiene A(n+1) = PnPn−1 . . . P1A = ( R 0 ) (2.7) de donde se deduce que Q = (PnPn−1 . . . P1) −1 = (PnPn−1 . . . P1) T = P T1 . . . P T n (2.8) Una vez calculada la descomposición QR a través de transformaciones ortogonales, el PMCL se resuelve a través del método de Golub [16] cuya implementación (restringida al caso de matrices de rango completo) se presenta en el algoritmo (2.3). Para efectuar la descomposición QR de A falta conocer algíun método que permita calcular la colección de matrices P̃k. Los métodos numéricos más populares para la obteción de matrices de transformación ortogonal son los siguientes: Transformaciones ortogonales de Householder. Matrices de rotación de Givens El lector interesado en los diversos algoritmos existentes para la obtención de la descompo- sición ortogonal de una matriz rectangular a través de los métodos anteriores puede consultar [15] y [5]. 15 2. Métodos Clásicos para el PMCL Algoritmo 2.3 Método de Golub para la solución del PMCL vía descomposición QR Algoritmo GolubPMCL Descripción Resuelve el PMCL asociado a A y b usando la factorización QR. Entradas • A Matriz de rango completo de m filas por n columnas con m > n. • b Vector columna de n filas. Salidas • xLS Solución al PMCL asociado a A y b. 1: función GolubPMCL(A, b) 2: [Q,R]← FactQR(A) 3: d← QT b 4: c← d(1 : n) . n < m 5: Resolver el sistema RxLS = c 6: ← xLS 7: fin función 2.1.4. Descomposición QR vía ortogonalización Un proceso de ortogonalización es un problema clásico del álgebra lineal, el cual tiene como objetivo, dado un conjunto de vectores V = {v1, v2, . . . vr} en Rn, con r ≤ n, hallar un nuevo conjunto U = {u1, u2, . . . ur} tal que span(V ) = span(U) y los vectores de U son ortogonales entre sí. Si además, los vectores resultantes de U son unitarios, el proceso se denomina proceso de ortonormalización El algoritmo clásico para la ortonormalización de un conjunto de vectores es el método de Gram Schmidt (Ver [18]). Dada una matriz A ∈ Rm×n con m ≥ n, se puede utilizar un proceso de ortonormalización sobre las columnas de A para obtener su descomposición QR, de la siguiente manera: si A = (a1, a2, . . . , an), donde ai es la i-ésima columna de A, entonces A = (q1, q2, . . . qn)R (2.9) donde Q = (q1, q2, . . . qn) se obtiene a partir de la ortonormalización de las columnas de A. Evidentemente Q será una matriz ortogonal. En esta sección se presentará el método de Gram Schmidt modificado para obtener la descomposición QR de A a través de a ortogonalización de sus columnas. La estrategia del método de Gram Schmidt modificado, consiste en construir una sucesión de matrices A = A(1), A(2), . . . A(n) = Q donde A(k) tiene la forma A(k) = ( q1, q2, . . . qk−1, a (k) k , . . . , a (k) n ) Así, las k primeras columnas de A(k) correponden a los k primeros pasos del proceso de 16 2. Métodos Clásicos para el PMCL ortonormalización de las columnas de A. En el k-ésimo paso, en primer lugar se obtiene el vector qk normalizando el vector a (k) k , esto es q̃k = a (k) k , rkk = ‖q̃k‖2, qk = q̃k/rkk (2.10) y luego se ortogonalizan a(k)k+1, . . . , a (k) n respecto a qk como sigue a (k+1) j = a (k) j − rkjqk, rkj = q T k a (k) j , j = k + 1, . . . n (2.11) Existen varias implementaciones algoritmicas del método de Gram Schmidt modificado para la descomposición QR de una matriz rectangular. Los detalles de estos algoritmos pueden consultarse en [5] y [22]. 2.2. Métodos iterativos para el PMCL En esta sección se considerarán varios métodos iterativos clásicos, con algunas modifica- ciones para la solución de la ecuación (1.3). El objetivo de los métodos iterativos es generar una sucesión de soluciones aproximadas { x(0), x(1), · · · } donde x(0) es una aproximación inicial dada a la solución o iterado inicial y x(k) para k ≥ 1 puede calcularse a través de una relación de recurrencia bien definida, en función de x(0), x(1), · · · , x(k−1). Otra característica deseable es que el cálculo de x(n) sea de bajo costo computacional. Una condición necesaria para que un método iterativo pueda dar la solución de un sistema de ecuaciones lineales, es que el método sea convergente. La definición formal de convergencia es la siguiente Definición 2.2.1. Un método iterativo, con un iterado inicial x(0) y una relación de recurren- cia bien definida para el cálculo de x(k) para k ≥ 1 es convergente si la sucesión { x(k) }∞ k=0 es convergente para cualquier iterado inicial x(0). Una de las propiedades de los métodos iterativos, es que el proceso de generación de los x(k) puede detenerse en cualquier momento, así se tiene la ventaja respecto a los métodos directos, en el hecho de que el método iterativo tenga una menor complejidad en tiempo en el caso de que se deseen soluciones de poca precisión numérica, lo cual implica un número reducido de iteraciones, o bien, para el caso de matrices de dimensión grande el número de iteraciones para observar convergencia puede ser mucho menor que el tamaño de la matriz. Para el caso de los métodos iterativos aplicados a las ecuaciones normales, es deseable que estos eviten la formación explícita de la matriz ATA, y para ello se suele trabajar con las ecuaciones normales en su forma factorizada que es AT (Ax− b) (2.12) Trabajar con AT y A de manera separada, tiene varias ventajas: En primer lugar, la matriz ATA suele tener un número de condición mucho mayor a A (κ ( ATA ) = κ (A)2 en el caso de que se tome el número de condición usando la norma euclídea), por lo cual una pequeña perturbación en ATA podría producir un error considerable en la resolución del problema. En segundo lugar, para el caso de matrices sparse se evita el relleno que aparece al formar la matriz ATA. 17 2. Métodos Clásicos para el PMCL 2.2.1. Métodos Estacionarios Los métodos iterativos más sencillos para la resolución de sistemas de ecuaciones lineales son los métodos estacionarios. Un método estacionario para la resolución de (1.3) se construye definiendo un par de matrices M y N asociadas a ATA y la iteración: Mx(k+1) = Nx(k) + b, k = 0, 1, . . . (2.13) Las matricesM y N de la ecuación (2.13) son tales que ATA = M−N y además se impone la condición de que M sea no singular. Este esquema de definición de M y N propio de los métodos estacionarios también es conocido como particionamiento o splitting regular de las ecuaciones normales. Para el análisis de la convergencia de los métodos estacionarios, se define: G = M−1N = I −M−1ATA (2.14) c = M−1b (2.15) Así, (2.13) puede reescribirse como: x(k+1) = Gx(k) + c, k = 0, 1, . . . (2.16) La matriz G se conoce como matriz de iteración asociada al método estacionario. Usando dicha matriz de iteración, se puede establecer una condición necesaria y suficiente para la convergencia de un método estacionario, la cual se presenta en el siguiente teorema Teorema 2.2.1. El método estacionario de la ecuación (2.16) es convergente si y sólo si ρ (G) < 1. Demostración. Ver [22] A continuación se presentarán algunos de los métodos estacionarios que se consiguen con mayor frecuencia en la literatura, cada uno de estos métodos se define a partir de un splitting distinto para la matriz ATA. El método de Richardson de primer orden El método de Richardson de primer orden se define a partir del siguiente splitting ATA = 1 α In − ( 1 α In −ATA ) (2.17) M = 1 α In N = 1 α In −ATA Donde α > 0 es un parámetro fijo. Para este método se tiene que G = I − αATA y c = αAT b, así, luego de una manipulación algebraica de la ecuación (2.16) se obtiene la siguiente iteración: 18 2. Métodos Clásicos para el PMCL x(k+1) = x(k) + αAT ( b−Ax(k) ) (2.18) En donde no hace falta realizar el producto ATA de manera explícita, el algoritmo (2.4) sumariza el método. Algoritmo 2.4 Método de Richardson de primer orden para las ecuaciones normales Algoritmo Richardson1ordPMCL Descripción Resuelve el PMCL asociado a A y b usando la iteración de Richardsond de primer orden para las ecuaciones normales. Entradas • x(0) Vector columna de n filas, iterado inicial. • A Matriz de rango completo de m filas por n columnas con m > n. • b Vector columna de n filas. • α Real positivo que proviene del splitting de A. • Nmax Entero positivo. Número máximo de iteraciones. • � Real positivo. Tolerancia mínima para detener la iteración. Salidas • x Solución al PMCL asociado a A y b. 1: función Richardson1ordPMCL(x(0), A, b, α,Nmax, �) 2: para i← 1, . . . , Nmax hacer 3: x(i) ← x(i−1) + αAT ( b−Ax(i−1) ) 4: r(i) ← b−Ax(i) 5: si ‖r(i)‖2 < � entonces 6: ← x(i) 7: fin si 8: fin para 9: Error (Máximo de iteraciones alcanzado) 10: fin función Para garantizar que el método de Richardson de primer orden converge a la solución del PMCL, esto es, converge a x = A†b se establece la siguiente condición Teorema 2.2.2. El método de Richardson de primer orden converge a la solución del PMCL si x(0) ∈ R ( AT ) y 0 < α < 2/σ21 donde σ1 es el máximo de los valores singulares de A. Demostración. Ver [22] 19 2. Métodos Clásicos para el PMCL El método de Jacobi El método de Jacobi se define a partir del siguiente splitting ATA = LA +DA + L T A (2.19) M = DA N = − ( LA + L T A ) Donde DA es la matriz con la diagonal principal de ATA, LA es la matriz que contiene la porción triangular inferior de ATA y LTA es la matriz que contiene la porción triangular superior de ATA. Para este método se tiene que G = In−D−1A A TA y c = D−1A A T b. Reescribiendo la ecuación (2.16) para evitar el producto explícito de ATA, la iteración de Jacobi es la siguiente: x(k+1) = x(k) +D−1A A T ( b−Ax(k) ) (2.20) Finalmente el método se presenta en el algoritmo (2.5) El método de Gauss-Seidel El método de Gauss-Seidel se define a partir del siguiente splitting ATA = LA +DA + L T A (2.21) M = LA +DA (2.22) N = −LA (2.23) A partir de las definiciones anteriores, se tiene que G = −(LA + DA)−1LTA y c = (LA + DA)−1b, y así la iteración de Gauss-Seidel queda como: x(k+1) = x(k) +D−1A ( AT b− LAx(k+1) + ( DA + L T A ) x(k) ) (2.24) Obsérvese que en este caso, la iteración para x(k+1) incluye el propio término en la relación de recurrencia, por lo cual no es posible manipular algebraicamente la expresión (2.24) para evitar la formación explícita de ATA. No obstante, se presentará una alternativa para la construcción de la iteración de Gauss-Seidel sin necesidad de realizar produtos matriciales, conocida como método de reducción de residual ya que genera una sucesión de aproximaciones x(j) tales que ‖r(j+1)‖22 ≤ ‖r (j)‖22 donde r (j) = b−Ax(j). Sea {pj} , j = 1, 2, · · · una sucesión de vectores no nulos tales que pj /∈ N (A) , para j = 1, 2, · · · y considérese una sucesión de aproximaciones de la forma x(k+1) = x(j) + αjpj , αj = pTj A T ( b−Ax(j) ) ‖Apj‖22 (2.25) Dada una aproximación inicial x(0), puede verificarse que (Ver [5]) ‖r(j+1)‖22 = |αj | 2‖Apj‖22 ≤ ‖r (j)‖22 (2.26) En particular, si A ∈ Rm×n es de rango completo, el método de Gauss-Seidel para las ecuaciones normales puede derivarse de la ecuación (2.26) escogiendo la sucesión p1, p2, . . . 20 2. Métodos Clásicos para el PMCL Algoritmo 2.5 Método de Jacobi para las ecuaciones normales Algoritmo JacobiPMCL Descripción Resuelve el PMCL asociado a A y b usando la iteración de Jacobi para las ecuaciones normales. Entradas • x(0) Vector columna de n filas, iterado inicial. • A Matriz de rango completo de m filas por n columnas con m > n. • b Vector columna de n filas. • Nmax Entero positivo. Número máximo de iteraciones. • � Real positivo. Tolerancia mínima para detener la iteración. Salidas • x Solución al PMCL asociado a A y b. 1: función JacobiPMCL(x(0), A, b,Nmax, �) 2: r(0) ← b−Ax(0) 3: para i← 1, . . . , Nmax hacer 4: x(i) = x(i−1) +D−1A A T r(i−1) 5: r(i) ← b−Ax(i) 6: si ‖r(i)‖2 < � entonces 7: ← x(i) 8: fin si 9: fin para 10: Error (Máximo de iteraciones alcanzado) 11: fin función 21 2. Métodos Clásicos para el PMCL Algoritmo 2.6 Método de Gauss-Seidel para las ecuaciones normales Algoritmo GaussSeidelPMCL Descripción Resuelve el PMCL asociado a A y b usando la iteración de Gauss-Seidel para las ecua- ciones normales. Entradas • x(0) Vector columna de n filas, iterado inicial. • A Matriz de rango completo de m filas por n columnas con m > n. • b Vector columna de n filas. • Nmax Entero positivo. Número máximo de iteraciones. • � Real positivo. Tolerancia mínima para detener la iteración. Salidas • x Solución al PMCL asociado a A y b. 1: función GaussSeidelPMCL(x(0), A, b,Nmax, �) 2: r(0) = b−Ax(0) 3: para i← 1, . . . , Nmax hacer 4: z(1) ← x(i−1) . Iterado inicial para el barrido de n subiteraciones 5: r̃(1) ← r(0) 6: para j ← 1, 2, · · · , n hacer 7: a← Aej . j-ésima columna de A 8: d← ‖a‖22 9: δj ← ( aT r̃(j) ) /d 10: z(j+1) ← z(j) + δjej 11: r̃(j+1) ← r̃(j) − δja 12: fin para 13: x(i) ← z(n+1) 14: r(i) ← r̃(n+1) 15: si ‖r(i)‖2 < � entonces 16: ← x(i) 17: fin si 18: fin para 19: Error (Máximo de iteraciones alcanzado) 20: fin función 22 2. Métodos Clásicos para el PMCL como e1, e2, . . . , donde ei es el i-ésimo vector de la base canónica en orden cíclico. Con esta elección, se tiene que Apj = Aej = aj donde aj es la j-ésima columna de A. A partir de la elección anterior, una iteración de Gauss-Seidel se realiza a través de un barrido de n sub-iteraciones. Los pasos para obtener la aproximación x(k) son los siguientes: Sea z(1) = x(k), y r̃(1) = b−Ax(k) Calcular las n− 1 subiteraciones restantes z(2), · · · , z(n+1) usando la recurrencia: δj = aTj r̃ (j) ‖aj‖22 z(j+1) = z(j) + δjej r̃(j+1) = r̃(j) − δjaj Sea x(k+1) = z(n+1) Finalmente, se presenta el método de Gauss-Seidel en el algoritmo (2.6). El Método de Sobrerrelajación sucesiva (SOR) El método SOR es una mejora del método de Gauss-Seidel, la cual consiste en introducir un parámetro ω ∈ R a las sub-iteraciones de un barrido en la iteración de Gauss-Seidel con la finalidad de acelerar la convergencia. Dicho factor se conoce como factor de relajación. Con la introducción del factor de relajación, las recurrencias (2.27) se convierten en: δj = ω aTj r̃ (j) ‖aj‖22 (2.27) z(j+1) = z(j) + δjej (2.28) r̃(j+1) = r̃(j) − δjaj (2.29) La principal desventaja que presenta este método es que para el caso general no es po- sible determinar el factor de relajación óptimo para garantizar la máxima aceleración de la convergencia, no obstante, en vista de que la matriz ATA es SPD, es condición necesaria que 0 < ω < 2. El método sor se presenta en el algoritmo (2.7). Para cierta clase de matrices puede hallarse un factor de relajación óptimo ωopt que garan- tiza la máxima velocidad de convergencia del de convergencia del método SOR, estas son las matrices de orden consistente cuya definición es la siguiente Definición 2.2.2. Sea una matriz de la forma M = ATA donde A ∈ Rm×n y considérese una descomposición M = DA(Im − L− U) donde: DA es una matriz no singular. L es una matriz estrictamente triangular inferior. U es una matriz estrictamente triangular superior. 23 2. Métodos Clásicos para el PMCL Algoritmo 2.7 Método SOR para las ecuaciones normales Algoritmo SOR_PMCL Descripción Resuelve el PMCL asociado a A y b usando la iteración SOR para las ecuaciones nor- males. Entradas • x(0) Vector columna de n filas, iterado inicial. • A Matriz de rango completo de m filas por n columnas con m > n. • b Vector columna de n filas. • ω Real positivo, factor de relajación de SOR. • Nmax Entero positivo. Número máximo de iteraciones. • � Real positivo. Tolerancia mínima para detener la iteración. Salidas • x Solución al PMCL asociado a A y b. 1: función SOR_PMCL(x(0), A, b, ω,Nmax, �) 2: r(0) = b−Ax(0) 3: para i← 1, . . . , Nmax hacer 4: z(1) ← x(i−1) . Iterado inicial para el barrido de n subiteraciones 5: r̃(1) ← r(0) 6: para j ← 1, 2, · · · , n hacer 7: a← Aej . j-ésima columna de A 8: d← ‖a‖22 9: δj ← ω((aT r̃(j))/d) 10: z(j+1) ← z(j) + δjej 11: r̃(j+1) ← r̃(j) − δja 12: fin para 13: x(i) ← z(n+1) 14: r(i) ← r̃(j+1) 15: si ‖r(i)‖2 < � entonces 16: ← x(i) 17: fin si 18: fin para 19: Error (Máximo de iteraciones alcanzado) 20: fin función 24 2. Métodos Clásicos para el PMCL Se dice que M es una matriz de orden consistente si los autovalores del funcional J(α) = αL+ α−1U, α ∈ R, α 6= 0 Teorema 2.2.3. Sea A ∈ Rm×n, si AAT una matriz de orden consistente con AAT = DA(Im − L − U) tal como en la definicion (2.2.2), entonces si B = L + U tiene todos sus autovalores reales y ρ(B) < 1, entonces el factor de relajación óptimo para el método SOR aplicado a la resolución de las ecuaciones normales es ωopt = 2 1 + √ 1− ρ(B)2 Demostración. Ver [44] 2.2.2. El método del Gradiente Conjugado para las Ecuaciones Normales El método del Gradiente Conjugado (GC) fue propuesto originalmente en [20] como un método directo para la resolución de sistemas de ecuaciones lineales de la forma Cx = d, C ∈ Rn×n y C es SPD. No obstante, GC adquirió relevancia al ser reformulado como un método iterativo, en este ámbito el método es muy eficiente sobre todo para el caso de matrices sparse. Para el caso de matrices SPD, el GC proviene a partir de un problema de optimización de funcionales cuadráticas. En efecto paraM ∈ Rn×n, una matriz SPD, el problema de minimizar el funcional cuadrático q(x) = 1 2 xTMx− xT b+ c (2.30) Es equivalente a resolver el sistema Mx = b. El objetivo del método es generar un conjunto de direcciones de búsqueda que cumplen una propiedad conocida como M-Conjugancia respecto a una matriz M , la cual se define a continuación Definición 2.2.3. Sea U = {u1, u2, . . . un} un conjunto de vectores en Rn. Dada una matriz M ∈ Rn×n SPD, se dice que U posee la propiedad de M-conjugancia si para todo 1 ≤ i, j ≤ n se cumple uTj Mui = 0 si i 6= j Así, dado un conjunto U de vectores que poseen la propiedad de M-Conjugancia, existe un método iterativo tal que, usando los vectores de dicho conjunto como direcciones de búsqueda, obtiene la solución al sistema de ecuaciones Mx = b en un número finito de iteraciones, tal como se establece en el siguiente resultado Teorema 2.2.4. Sea M ∈ Rn×n SPD, b ∈ Rn y U = {u1, u2, . . . un} un conjunto de vectores no nulos con la propiedad de M-Conjugancia, entonces, para todo iterado inicial x0 ∈ Rn la sucesión xk = xk−1 + rTk−1uk uTkMuk uk, (rk = b−Mxk) satisface Mxn = b 25 2. Métodos Clásicos para el PMCL Demostración. Ver [27] La conexión entre el método iterativo definido por el teorema anterior y la optimización de funcionales cuadráticas está en el hecho de que el factor (rTk−1uk/u T kMuk)uk es el argumento que minimiza a q(x + αu) donde α es un parámetro real, x y u son valores fijos y q es el funcional definido en la ecuación (2.30) (Ver [27]) En el algoritmo (2.8) presenta el método GC en su forma genérica para una matriz M SPD. Finalmente el algoritmo (2.9) presenta el método GC aplicado a las ecuaciones normales para una matriz A ∈ Rm×n, en donde no es necesario formar explícitamente la matriz ATA. Algoritmo 2.8 Gradiente Conjugado Genérico para una matriz SPD Algoritmo GC_Generico Descripción Resuelve el sistema Mx = b usando GC. Entradas • x(0) Vector columna de n filas, iterado inicial. • M Matriz SPD de n filas por n columnas. • b Vector columna de n filas. iteración. Salidas • x solución del sistema de ecuaciones. 1: función GC_Generico(x(0),M, b) 2: k ← 0 3: r(0) ← b−Mx(0) 4: mientras ‖rk‖2 6= 0 hacer 5: k ← k + 1 6: si k = 1 entonces 7: u1 ← r(0) 8: si no 9: βk ← r(k−1)T r(k−1)/r(k−2)T r(k−2) 10: uk ← r(k−1) − βkuk−1 11: fin si 12: αk ← r(k−1)T r(k−1)/(uTkM Tuk) 13: x(k) ← x(k−1) + αkuk 14: r(k) ← r(k−1) − αkAuk 15: fin mientras 16: ← x(k) 17: fin función 26 2. Métodos Clásicos para el PMCL Algoritmo 2.9 GC aplicado a las ecuaciones normales Algoritmo GC_PMCL Descripción Resuelve el PMCL asociado a A y b usando GC para las ecuaciones normales. Entradas • x(0) Vector columna de n filas, iterado inicial. • A Matriz de rango completo de m filas por n columnas con m > n. • b Vector columna de n filas. • Nmax Entero positivo. Número máximo de iteraciones. • � Real positivo. Tolerancia mínima para detener la iteración. Salidas • x Solución al PMCL asociado a A y b. 1: función GC_PMCL(x(0), A, b,Nmax, �) 2: p(0) ← AT (b−Ax(0)) 3: s(0) ← p(0) 4: γ(0) ← ‖s(0)‖22 5: para k = 0, 1, . . . Nmax hacer 6: q(k) ← Ap(k) 7: αk ← γ(k)/‖q(k)‖22 8: x(k+1) ← x(k) + αkp(k) 9: s(k+1) ← s(k) − αk(AT q(k)) 10: γ(k+1) ← ‖s(k+1)‖22 11: si γ(k+1) < � entonces 12: ← x(k+1) 13: fin si 14: βk ← γ(k+1)/γ(k) 15: p(k+1) ← s(k+1) + βkp(k) 16: fin para 17: Error (Máximo de iteraciones alcanzado) 18: fin función 27 2. Métodos Clásicos para el PMCL 2.3. Precondicionamiento para las ecuaciones normales En muchos casos, la convergencia de los métodos iterativos puede resultar lenta, principal- mente en los casos de matrices mal condicionadas. Es por ello, que en los últimos años, se ha puesto mayor énfasis en precondicionamiento de la matriz de entrada más que en los propios métodos iterativos. Dada A ∈ Rn×n y b ∈ Rn, el precondicionamiento es una técnica que tiene como objetivo transformar el sistema de ecuaciones original Ax = b en otro sistema equivalente Ãx = b̃ tal que Ã sea una matriz mejor condicionada, y por consiguiente un método iterativo aplicado a este último sistema tenga una mayor velocidad de convergencia que para el primer sistema. Existen dos maneras fundamentales para precondicionar el sistema Ax = b: la primera es premultiplicar el sistema por una matriz M ∈ Rn×n no singular, así se obtiene el sistema equivalente MAx = Mb (2.31) Esta forma de precondicionamiento se denomina precondicionamiento por la izquierda. La otra forma de precondicionamiento, el precondicionamiento por la derecha proviene de postmultiplicar A por M−1, obteniendo el problema equivalente: AM−1y = b, Mx = y (2.32) En ambos casos la matriz M se denomina precondicionador del sistema de ecuaciones. Para el caso particular de las ecuaciones normales, en la literatura se encuentran técnicas de precondicionamiento por la derecha. En este caso, sea M ∈ Rn×n no singular, entonces el problema de optimización de la ecuación (1.2) es equivalente a min y ‖(AS−1)y − b‖2, Sx = y (2.33) Donde se elige S tal que la matriz AS−1 tenga un espectro más favorable que la matriz A. En general, es deseable que la matriz S tenga las siguientes características: AS−1 debe tener un número de condición más pequeño que A y debe tener pocos valores singulares distintos. En el caso de matrices sparse, S debe tener el mismo patrón de sparsidad de A, esto es, las posiciones de los elementos no nulos de S debería ser igual a las de A. Debe ser de bajo costo computacional resolver sistemas de ecuaciones que involucren a S y a ST . Una vez hallado un precondicionador S, se procede a resolver el sistema de ecuaciones normales precondicionadas (AS−1)T (AS−1)x = (AS−1)T b (2.34) 28 2. Métodos Clásicos para el PMCL 2.3.1. Gradiente Conjugado Precondicionado para las ecuaciones normales Dado un precondicionador S ∈ Rn×n, puede reformularse el algoritmo (2.8) para resolver el sistema de ecuaciones (2.34). El GC precondicionado se presenta en el algoritmo (2.10) 2.3.2. Algunos precondicionadores conocidos Uno de los problemas actuales en el cálculo numérico es conseguir buenos precondiciona- dores. Lamentablemente no existe una teoría general para hallar precondicionadores que sean aplicables para cualquier sistema de ecuaciones, sino que lo usual es hallar precondicionadores aplicables a algún problema o familia de problemas particulares, basándose en información adicional del contexto de dichos problemas. No obstante, existen unos pocos precondicionadores que pueden aplicarse a una amplia gama de sistemas de ecuaciones. En este trabajo se presentarán los precondicionadores más populares para el PMCL. Precondicionador basado en la Factorización de Cholesky incompleta Sea S = R, donde R es el factor de Cholesky asociado a AAT , puede verificarse que (Ver [5]) κ(AS−1) = 1, lo cual implica que, en teoría, el sistema de ecuaciones normales precondicionado con S necesitaría de sólo una iteración para converger a la solución, para cualquier método iterativo. Este hecho teórico hace intuir que una aproximación al factor de Cholesky de AAT puede ser un buen precondicionador para el PMCL. Una forma de aproximación al factor de Cholesky, es la que se conoce como factorización de Cholesky incompleta, la cual consiste en una matriz R̃ ≡ R tal que C = ATA = RTR− E Donde E es una matriz de error. Se desea que ‖E‖ sea pequeña y R̃ sea sparse con un patrón de sparsidad similar a A. Para calcular dicha R̃ puede usarse el mismo algoritmo (2.1) de la factorización de Cholesky, pero calculando sólo aquellos elementos r̃ij tales que Aij 6= 0. El algoritmo (2.11) presenta la factorización de Cholesky incompleta. Para el algoritmo C = AAT y se define el patrón de sparsidad de A como el conjunto P = {(i, j), Aij 6= 0} y es una de las entradas del algoritmo. Obsérvese que no hace falta calcular de manera explícita la matriz AAT , sino que sólo hace falta acceder a las filas i y j cada vez que se desee calcular cij . Precondicionador basado en la factorización LU Una opción para el precondiciona- miento del PMCL en sistemas sobredeterminados, es usar la factorización de un subbloque cuadrado de A. Para una matriz de rango completo A ∈ Rm×n, si luego de permutaciones de filas sobre A se puede llegar a A = ( A1 A2 ) , A1 ∈ Rn×n (2.35) Tal que A1 sea no singular, entonces, A −1 1 se puede usar como precondicionador derecho, obteniendo el siguiente problema de optimización, equivalente al PMCL. 29 2. Métodos Clásicos para el PMCL Algoritmo 2.10 GC Precondicionado Algoritmo GCPrecondicionadoPMCL Descripción Resuelve el PMCL asociado a A y b usando GC precondicionado para las ecuaciones normales. Entradas • x(0) Vector columna de n filas, iterado inicial. • A Matriz de rango completo de m filas por n columnas con m > n. • b Vector columna de n filas. • S Matriz precondicionadora de n filas por n columnas. • Nmax Entero positivo. Número máximo de iteraciones. • � Real positivo. Tolerancia mínima para detener la iteración. Salidas • x Solución al PMCL asociado a A y b. 1: función GCPrecondicionadoPMCL(x(0), A, b, S,Nmax, �) 2: r(0) ← (b−Ax(0)) 3: Resolver el sistema ST y = AT r(0) 4: p(0) ← y 5: s(0) ← p(0) 6: γ(0) ← ‖s(0)‖22 7: para k ← 0, 1, . . . Nmax hacer 8: Resolver el sistema Sw = p(k) 9: t(k) ← w 10: q(k) ← At(k) 11: αk ← γ(k)/‖q(k)‖22 12: x(k+1) ← x(k) + αkt(k) 13: r(k+1) ← r(k) + αkq(k) 14: u← A(t)r(k+1) 15: Resolver el sistema ST z = u 16: s(k+1) ← z 17: γ(k+1) ← ‖s(k+1)‖22 18: si γ(k+1) < � entonces 19: ← x(k+1) 20: fin si 21: βk ← γ(k+1)/γ(k) 22: p(k+1) ← s(k+1) + βkp(k) 23: fin para 24: Error (Máximo de iteraciones alcanzado) 25: fin función 30 2. Métodos Clásicos para el PMCL Algoritmo 2.11 Factorización de Cholesky incompleta Algoritmo CholeskyIncompleto Descripción Calcula la factorización de Cholesky incompleta de una matriz C SPD con un patrón de sparsidad P . Entradas • C Matriz sparse n filas por n columnas SPD. • P Patrón de sparsidad para la factorización incompleta. Salidas • R Aproximación al factor de Cholesky de C con patrón de sparsidad P . 1: función CholeskyIncompleto(C,P ) 2: para i← 1 . . . n hacer 3: rii ← ( cii − ∑i−1 k=1 r 2 ki )1/2 4: para j ← 1 . . . n hacer 5: si (i, j) ∈ P entonces 6: rij ← cij − ∑i−1 k=1 rkirkj 7: rij ← rij/rii 8: fin si 9: fin para 10: fin para 11: ← C 12: fin función 31 2. Métodos Clásicos para el PMCL min y ‖AA−11 y − b‖2, y = A1x (2.36) En donde el PMCL asociado a AA−11 y b está mejor condicionado que el problema original y se puede resolver con cualquiera de los métodos iterativos cubiertos en las secciones anteriores. Una vez resuelto el PMCL precondicionado, la solución al PMCL original es x = A−11 y. En el caso general para evitar el cálculo explícito de la inversa, se prefiere trabajar con la factorización LU de A1. Precondicionador para GC basado en la factorización LU Un esquema de precondi- cionamiento basado en la factorización LU combinado con GC, y que puede usarse tanto para matrices sparse, como para matrices densas, se presenta en [5]. Si la ecuación (2.35) se sustituye en el problema de optimización (1.2), se obtiene el problema de optimización equivalente min x ∥∥∥∥ ( In C ) y − ( b1 b2 )∥∥∥∥ 2 , b = ( b1 b2 ) , (2.37) donde b1 ∈ Rn, b2 ∈ Rm−n, C = A2A−11 En este caso, con r = b−Ax, las ecuaciones normales pueden ser reescritas como  In 0 In0 Im−n 0 In C T 0     r1r2 y   =   b1b2 0   , r = ( r1 r2 ) (2.38) donde r1 ∈ Rn, r2 ∈ Rm−n Haciendo r2 = 0 se obtiene el siguiente sistema SPD( In + C TC ) r1 = C T (b2 − Cb1) (2.39) Con la solución del sistema de ecuaciones (2.39), se obtiene, finalmente, la solución al PMCL resolviendo el sistema A1x = b1 − r1 (2.40) El algoritmo (2.12) muestra cómo se adapta el GC al precondicionamiento LU explicado anteriormente. Sin embargo, se requiere del cálculo explícito de la matriz C = A2A −1 1 . Una mejora de dicho algoritmo para evitar el cálculo de C, se logra observando que, para cada iteración s = r + CT r = r + (LT )−1((UT )−1(AT2 r)) y q = Cp = A2(U −1(L−1p)) Por consiguiente, sólo basta conocer la factorización LU de A1, y en cada paso del algoritmo (2.12) en donde esté involucrada la matriz C, será preciso resolver varios sistemas de ecuaciones 32 2. Métodos Clásicos para el PMCL lineales donde existan productos matriz-vector de las formas L−1v, U−1v, (LT )−1v y (UT )−1v. Estos sistemas de ecuaciones son de bajo costo computacional, debido a que las matrices U y L son triangulares y sparse. 33 2. Métodos Clásicos para el PMCL Algoritmo 2.12 GC Precondicionado con Factorización LU Algoritmo GC_LU_PMCL Descripción Resuelve el PMCL asociado a A y b usando la GC precondicionado por factorización LU incompleta de un subbloque cuadrado de A. Entradas • x(0) Vector columna de n filas, iterado inicial. • A Matriz de rango completo de m filas por n columnas con m > n. • C Matriz precondicionadora de la forma C = A2A−11 donde A1 y A2 son subbloques de A luego de permutaciones de filas. • b Vector columna de n filas. • Nmax Entero positivo. Número máximo de iteraciones. • � Real positivo. Tolerancia mínima para detener la iteración. Salidas • x Solución al PMCL asociado a A y b. 1: función GC_LU_PMCL(x(0), A,C, b,Nmax, �) 2: r(0) ← b−Ax(0) 3: ρ (0) 1 ← r (0)(1 : n) 4: ρ (0) 2 ← r (0)((n+ 1) : m) 5: p(0) ← ρ(0)1 + C Tρ (0) 2 6: s(0) ← p(0) 7: γ(0) ← ‖s(0)‖22 8: para k = 0, 1, . . . Nmax hacer 9: q(k) ← Cp(k) 10: αk ← γ(k)/‖q(k)‖22 11: ρ (k+1) 1 ← ρ (k) 1 − αkp (k) 12: ρ (k+1) 2 ← ρ (k) 2 − αkq (k) 13: s(k+1) ← ρ(k+1)1 + C Tρ (k+1) 2 14: γ(k+1) ← ‖s(k+1)‖22 15: si γk+1 < � entonces 16: Resolver el sistema A1x = b1 − ρ1 17: ← x 18: fin si 19: βk ← γ(k+1)/γ(k) 20: p(k+1) ← s(k+1) + βkp(k) 21: fin para 22: Error (Máximo de iteraciones alcanzado) 23: fin función 34 Capítulo 3 El método de Schulz El centro no es un punto Si lo fuera, resultaría fácil acertarlo Roberto Juarroz - Segunda poesía vertical Como se vio en el capítulo 1, x = A†b, es la solución al PMCL para el caso de una matriz A de rango completo. Esto permite pensar que una matriz B ≈ A† podría ser un buen precondicionador para la ecuación (1). Por esto, es de interés hallar algún método iterativo que permita obtener una aproximación a A† con la precisión que se desee, y además, sería deseable que dicho método posea una alta velocidad de convergencia. Un método que puede utilizarse para calcular una aproximación a la matriz pseudoinversa de A es el Método iterativo de Schulz [37], un método que se deriva a partir de la iteración de Newton en espacios de matrices. Originalmente, este método fue ideado para el cálculo de la inversa de una matriz cuadrada; no obstante, en este trabajo se reutilizará para calcular una aproximación a la pseudoinversa de una matriz rectangular de rango completo. El resto del capítulo se organiza así: en la primera sección se dará una breve introducción al método de Newton en espacios de matrices. Posteriormente se presentará el método iterativo de Schultz y se estudiarán sus propiedades en cuanto a la convergencia y otros aspectos relevantes. Finalmente, el capítulo concluirá con el estudio de algunas propiedades interesantes de la iteración de Schultz para el caso de matrices rectangulares, las cuales dan una pista teórica de que la aproximación de Schultz es un buen precondicionador. 3.1. El método de Newton Uno de los problemas clásicos del cálculo numérico es hallar el cero de una función; esto es, dada F : Rn → Rn hallar ξ tal que F (ξ) = 0. Para el caso general de funciones no lineales diferenciables, elMétodo iterativo de Newton representa una de las alternativas más porderosas para el cálculo de los ceros de una función no lineal. Sea F : Rn → Rn, si para h ∈ Rn F es diferenciable en algún entorno de h, entonces, el teorema de Taylor [26], implica que para toda matriz X en un subconjunto convexo de dicho entorno de h se tiene la siguiente aproximación a F (X) F (X) ≈ F (h) + (X − h)DF (h) (3.1) 35 3. El Método de Schulz donde DF (h) es la derivada de F en h. En particular, si ξ es un cero de F , para alguna matriz X0 en un entorno de ξ, usando la ecuación (3.1) se tiene 0 = F (ξ) ≈ F (X0) + (ξ −X0)DF (X0) (3.2) Con la información anterior, puede deducirse la iteración de Newton en espacios de matrices a través de un modelo general para el estudio de los métodos iterativos para funciones no lineales: la iteración funcional de punto fijo. En el esquema de iteración funcional de punto fijo, un método iterativo para la función F : Rn → Rn se define a partir de una función Φ asociada a F de la siguiente manera xk+1 = Φ(xk) (3.3) esto es, que cualquier método iterativo para F puede redefinirse como una iteración que con- verja al punto fijo de una función Φ. Para un estudio general de las propiedades y condiciones de convergencia de los métodos de punto fijo, ver [22] y [39]. Así, a partir de la ecuación (3.2), si DF es no singular en un entorno de ξ puede obtenerse una iteración de punto fijo definiendo Φ de la siguiente manera Φ(x) ≡ x− (DF (x)) −1 F (x) (3.4) de donde se deduce la iteración de Newton en espacios de matrices para un iterado inicial X0 cercano al cero de F xk+1 = xk − (DF (xk)) −1 F (xk) (3.5) El algoritmo (3.1) muestra una implementación del esquema iterativo (3.5). En este caso, para evitar el cálculo explícito de la matriz inversa a DF , es necesario resolver un sistema de ecuaciones lineales en cada iteración. La convergencia del método de Newton en espacios de matrices queda establecida a partir del siguiente resultado. Teorema 3.1.1. Sea C ⊆ Rn un conjunto abierto y tal que F : C → Rn y sea C0 ⊆ C un conjunto convexo tal que F es diferenciable en C0 y continua en C. Dado x0 ∈ C0, sea r > 0 tal que {x, ‖x− x0‖2 < r} ⊆ C0 y sean constantes h, α, β y γ tales que r = α/(1 − h) y h = αβγ/2 < 1. Si F satisface las siguientes condiciones: 1. ‖DF (x)−DF (y)‖2 ≤ γ ‖x− y‖2 para todo x, y ∈ C0. 2. (DF (x))−1 existe para todo x ∈ C0 y ∥∥(DF (x))−1∥∥2 ≤ β. 3. ∥∥(DF (x0))−1F (x0)∥∥2 ≤ α. Entonces: 1. La sucesión xk+1 = xk − (DF (xk))−1, k = 0, 1, . . . está bien definida, y además satisface xk ∈ {x, ‖x− x0‖2 < r} para k = 1, 2, . . .. 2. limx→∞ xk = ξ <∞ y F (ξ) = 0. Demostración. Ver [31] 36 3. El Método de Schulz Algoritmo 3.1 Método de Newton en varias variables Algoritmo Newton_Varias_Variables Descripción Calcula el cero de la función F usando el método de Newton en espacios de matrices. Entradas • X0 Vector columna de n filas, iterado inicial. • Nmax Entero positivo. Número máximo de iteraciones. • � Real positivo. Tolerancia mínima para detener la iteración. Salidas • X cero de la función F . 1: función Newton_Varias_Variables(X0, Nmax, �) 2: para i← 1, . . . , Nmax hacer 3: v ← F (X0) 4: M ← DF (X0) 5: Resolver el sistema My = v 6: X1 ← X0 − y 7: si ‖X0 −X1‖2/‖X1‖2 < � entonces 8: ← X1 9: fin si 10: X0 ← X1 11: fin para 12: Error (Máximo de iteraciones alcanzado) 13: fin función 37 3. El Método de Schulz 3.2. El método de Schulz Sea A ∈ Rn×n no singular. En esta sección se construirá la iteración de Schulz como un caso particular del del método de Newton en espacios de matrices. Lo primero que se necesita es la definir F : Rn → Rn tal que F (A) = 0. Una función que satistace esta condición es F (X) = X−1 −A. El próximo paso será calcular la derivada de la función F definida anteriormente. Para esto se usará la expansión en series de Newman para una matriz, la cual queda establecida en el siguiente teorema Teorema 3.2.1. Sea A ∈ Rn×n tal que ‖A‖2 < 1, entonces, la matriz In − A es no singular y además: (In −A)−1 = ∞∑ k=0 Ak Demostración. Ver [22] Con F = X−1−A, escójase alguna matrizB ∈ Rn×n tal que ∥∥A−1B∥∥ 2 ≤ ∥∥A−1∥∥ 2 ‖B‖2 < 1, esto es, B es una matriz cercana a 0. Se desea acotar el diferencial F (A + B) − F (B) −DB para alguna matriz D. En primera instancia, la expresión para F (A+B)− F (B) es F (A+B)− F (B) = ( (A+B)−1 −A ) − ( A−1 −A ) = (A+B)−1 −A−1 = ( A ( In +A −1B ))−1 −A−1 = A−1 ( In +A −1B )−1 −A−1 = (( In +A −1B )−1 − In)A−1 Sustituyendo la expansión del teorema (3.2.1) en la expresión anterior F (B +A)− F (A) = ( ∞∑ k=0 (−1)k(A−1B)kA−1 − In ) A−1 = −A−1BA−1 + ∞∑ k=2 (−1)k(A−1B)kA−1 Finalmente, la expresión para el diferencial viene dada por F (A+B)− F (B)− (−A−1BA−1) = ∞∑ k=2 (−1)k(A−1B)kA−1 (3.6) Por otro lado, se obtendrá una cota para la norma del miembro derecho de la ecuación (3.6) 38 3. El Método de Schulz ∥∥∥∥∥ ∞∑ k=2 (−1)k(A−1B)kA−1 ∥∥∥∥∥ 2 ≤ ∥∥∥∥∥ ∞∑ k=2 (−1)k(A−1B)k ∥∥∥∥∥ 2 ∥∥A−1∥∥ 2 ≤ ∥∥A−1B∥∥2 2 ∥∥A−1∥∥ 2 ∥∥∥∥∥ ∞∑ k=0 (−1)k(A−1B)k ∥∥∥∥∥ 2 ≤ ∥∥A−1∥∥3 2 ‖B‖22 ∞∑ k=0 ‖(A−1B)k‖2︸ ︷︷ ︸ Serie Geométrica (3.7) = ∥∥A−1∥∥3 2 ‖B‖22 1− ‖A−1B‖2 (3.8) Así, con la cota anterior∥∥F (A+B)− F (B)− (−A−1BA−1)∥∥ 2 ‖B‖2 ≤ ∥∥A−1∥∥3 2 1− ‖A−1B‖2 ‖B‖2 (3.9) Luego, el límite cuando la matriz B tiende a 0 es lim ‖B‖2→0 ∥∥F (A+B)− F (B)− (−A−1BA−1)∥∥ 2 ‖B‖2 = 0 (3.10) Lo cual implica que F es diferenciable en X = A y además, por la definición de derivada, DF (A)B = −A−1BA−1. Esta expresión se puede sustituir en la función de iteración del método de Newton, obteniendo: Mk+1 = Φ(Mk) ⇒Mk+1 = Mk − (DF (Mk))−1F (Mk) ⇒DF (Mk)(Mk+1 −Mk)M−1k = F (Mk) = M −1 k −A ⇒M−1k (Mk+1 −Mk)M −1 k = M −1 k −A ⇒Mk+1 −Mk = Mk −MkAMk (3.11) De donde se puede obtener la iteración del método de Schulz Mk+1 = 2Mk −MkAMk (3.12) La condición de convergencia para el método de Schulz se presenta en el siguiente teorema Teorema 3.2.2. La sucesión {Mk} definida por la ecuación (3.12) converge a A−1 si y sólo si ρ(I −AM0) < 1. Demostración. Para k = 1,2,. . . se tiene que In −AMk+1 =In −A(2Mk −MkAMk) =(AMk)2 − 2AMk + In =(In −AMk)2 39 3. El Método de Schulz Con esta relación, puede deducirse indutivamente que In −AMk =(In −AMk−1)2 =((In −AMk−2)2)2 =(((In −AMk−3)2)2)2 = · · · =(In −AM0)2 k Sea la sucesión Bk = AMk, combinándola con el resultado anterior, se puede reescribir como Bk = In − (In −B0)2 k (3.13) Nótese que limk→∞(I − B0)2 k = 0 o equivalentemente, limk→∞Bk = In si y solo si ρ(I −B0) = ρ(I −AM0) < 1. Por otro lado, limk→∞Bk = limk→∞AMk = In de donde se puede deducir que limk→∞Mk = A−1 lo cual concluye la prueba. Para cerrar esta sección, se presentará un resultado que da una elección de iterado inicial del método de Schulz que garantiza la convergencia para cualquier matriz A no singular. Teorema 3.2.3. Sea A ∈ Rn×n y sea p(x) = a0 + a1x+ · · ·+ anxn un polinomio. Si λ es un autovalor de A, entonces p(λ) es un autovalor de P (A). Demostración. Ver [22] Teorema 3.2.4. La elección del iterado inicial M0 = AT ‖A‖2 ‖AT ‖2 garantiza la convergencia del método de Schulz. Demostración. Con tal elección de M0, se tiene que In − AM0 = I − (1/‖A‖2‖AT ‖2)AAT . Considerando el polinomio p(x) = (1 − x)(‖A‖2‖AT ‖2), puede verificarse que AAT = p(In − AM0). Así, por el teorema (3.2.3) se cumple que para todo λ, autovalor de In −AM0, p(λ) = (1− λ)(‖A‖2‖AT ‖2) es autovalor de AAT . Como A es no singular, AAT es SPD, por lo cual todos sus autovalores son positivos. En particular, para todo λ, autovalor de In −AM0 se cumple (1− λ)(‖A‖2‖AT ‖2) > 0⇒ 1− λ > 0⇒ λ < 1 Luego ρ(In −AM0) < 1 y se asegura la convergencia del método. Usando la el iterado inicial del teorema anterior el algoritmo (3.2) presenta el método de Schulz para una matriz A ∈ Rn×n 40 3. El Método de Schulz Algoritmo 3.2 Iteración de Schulz Algoritmo IterSchulz Descripción Dada una matriz A, calcula una aproximación a A† a través del método de Schulz. Entradas • A Matriz de rango completo de m filas por n columnas con m > n. • Nmax Entero positivo. Número máximo de iteraciones. • � Real positivo. Tolerancia mínima para detener la iteración. Salidas • M aproximación a A†. 1: función IterSchulz(A, �,Nmax) 2: M0 ← AT /‖A‖22 3: para i← 1, . . . , Nmax hacer 4: U ←M0A 5: V ← 2In − U 6: M1 ← VM0 7: si ‖M0 −M1‖2/‖M1‖2 < � entonces 8: ← M1 9: fin si 10: M0 ←M1 11: fin para 12: Error (Máximo de iteraciones alcanzado) 13: fin función 41 3. El Método de Schulz 3.3. Propiedades del Método de Schulz para matrices rectan- gulares En la sección anterior, se presentó el método de Schulz para matrices cuadradas A ∈ Rn×n. Este método puede ser aplicado también para matrices rectangulares A ∈ Rm×n de rango completo, en cuyo caso, converge a A†, tal como lo establece el siguiente resultado. Teorema 3.3.1. Sea A ∈ Rm×n de rango completo. La iteración de Schulz (3.12) converge a A † si y sólo si ρ(I −AM0) < 1. Demostración. En primer lugar, es necesario verificar que la expresión de la iteración de Schulz esté bien definida respecto a las dimensiones de la matriz. Para ello es necesario que M0 ∈ Rn×m. Se sabe que A ∈ Rm×n y M0 = AT /‖A‖22 ∈ R n×m. Así, para todo k > 0 se cumple que Mk ∈ Rn×m y por lo tanto MkA ∈ Rn×n. Puede observarse que de esta forma también se cumple la relación de recurrencia para (In−AMk) establecida en la prueba del teorema (3.2.2), luego, sustituyendo A −1 por A† en la prueba de dicho teorema se verifica este resultado. Para finalizar el capítulo, se probarán dos propiedades relevantes de la iteración de Schulz para el caso de matrices rectangulares: en primer lugar, para cualquier iterado Mk si x es la solución al PMCL asociado a A, entonces se cumple una condición de ortogonalidad sobre Mk(b−Ax) análoga a la condición de ortogonalidad para el PMCL. Este es uno de los resultados teóricos centrales de este trabajo, pues da una indicación, en la teoría, de que la elección de Mk para precondicionar el PMCL es correcta en el sentido que el sistema precondicionado (que será de n ecuaciones con n incógnitas, a diferencia del sistema original que es de m ecuaciones con n incógnitas) tendrá la misma solución del sistema original, esto es la solución al PMCL. En segundo lugar, se probará que si λki es el i-ésimo autovalor de MkA, entonces λki es la k-ésima iteración de Newton para hallar el cero de la función f(x) = 1− 1/x (evidentemente el cero de f es x = 1) con iterado inicial λ0i , el i-ésimo autovalor de la matriz M0A. Este es el segundo resultado central de este trabajo, pues muestra en teoría, que los iterados Mk tienden a hacer un agrupamiento (clustering) de los autovalores del sistema inicial, en torno a λ = 1, lo cual es una de las propiedades ideales de un buen precondicionador: agrupar el espectro de autovalores de la matriz original en torno a un único valor. Durante la realización de este trabajo, no se encontró ninguna referencia en la literatura de la demostración de estas propiedades. Antes de demostrar estos teoremas, se probará un resultado preliminar: que los autovalores de la matriz MkA para k = 0, 1, . . . están acotados, tal como se establece a continuación. Teorema 3.3.2. Sea A ∈ Rm×n de rango completo con m ≥ n y sea Mk la k-ésima iteración de Schulz para k = 0, 1, . . . , entonces para todo autovalor λ de MkA, 0 < λ ≤ 1. Demostración. Para k = 0 se tiene que M0 = AT /‖A‖22, así M0A = A TA/‖A‖22. Como A es de rango completo, entonces M0 es SPD y por consiguiente todos sus autovalores son reales y positivos. Por otro lado, se tiene que para cualquier matriz M , el radio espectral ρ(M) es el ínfimo de todas las normas matriciales (Ver [22]), así ρ(M0) = ρ ( ATA ‖A‖22 ) ≤ ∥∥∥∥ATA‖A‖22 ∥∥∥∥ 2 < 1 42 3. El Método de Schulz Lo cual concluye la prueba para k = 0. Ahora, sea k ≥ 1, con la recurrencia obtenida en la prueba del teorema (3.2.2), se tiene que MkA = In − (In −M0A) 2k Si se considera el polinomio p(x) = 1 − (1 − x)2 k , entonces MkA se puede reescribir como MkA = p(M0A) y por el teorema (3.2.3) los autovalores de MkA son de la forma Λ(λ) = 1 − (1 − λ)2 k para λ autovalor de M0A. En vista de que 0 < λ < 1, se sigue que 0 < Λ(λ) < 1, lo cual prueba el teorema para k ≥ 1. Con el teorema anterior, también se puede establecer una cota para los autovalores de la matriz 2In −MkA, a través del siguiente cololario Corolario 3.3.1. Para Mk, k = 0, 1, . . . se verifica que para todo autovalor λ de 2In −MkA, 1 < λ < 2. Demostración. Basta considerar el polinomio p(x) = 1 + x, así 2In −MkA = p(In −MkA), como los autovalores de In −MkA están en el intervalo (0, 1) se verifica el resultado. Teorema 3.3.3. Sea A ∈ Rm×n de rango completo con m ≥ n, sea b ∈ Rn y sea Mk la k-ésima iteración de Schulz para k = 0, 1, . . . , entonces, xM es la solución al PMCL asociado a A y b, si y solo si cumple la siguiente condición de ortogonalidad: Mk(b−AxM ) = 0, k = 0, 1, . . . Demostración. La demostración de hará por inducción sobre k. Para k = 0 se tiene que M0 = AT /‖A‖22, por la condición de ortogonalidad del teorema (1.2.1) para el PMCL se satisface M0(b−AxM ) = 1 ‖A‖22 ( AT (b−Ax) ) = 0 si y solo si xM es la solución al PMCL, lo cual prueba el resultado en este caso. Para k = i, por hipótesis inductiva se satisface la condición de ortogonalidad Mi(b − AxM ) = 0. Falta verificar que el resultado se cumple para k = i + 1. En este caso, la (i + 1)-ésima iteración de Schulz es de la forma Mi+1 = 2Mi −MiAMi = (2In −MiA)Mi así Mi+1(b−AxM ) = (2In −MiA)Mi(b−AxM ) Obsérvese que usando la hipótesis inductivaMi(b−AxM ) = 0 si y solo si xM es la solución al PMCL, y por consiguiente (2In −MiA)Mi(b − AxM ) = 0 si y solo si xM es la solución al PMCL. No obstante, para garantizar que no puede haber alguna cancelación trivial del producto (2In −MiA) por algún vector, es necesario verificar que si b′ ∈ Rn entonces (2In −MiA) b′ = 0 si y solo si b′ = 0. Esto se cumple cuando la matriz (2In −MiA) es no singular, lo cual 43 3. El Método de Schulz está garantizado pues todos los valores de esta matriz son distintos a 0. En efecto, por el corolario (3.3.1) todos los autovalores de (2In −MiA) se encuentran en el intervalo (1, 2). Luego, haciendo b′ = Mi(b − AxM ) se verifica el teorema para k = i + 1, lo cual finaliza la prueba. Teorema 3.3.4. Sea A ∈ Rm×n de rango completo con m ≥ n, sea Mk la k-ésima iteración de Schulz y sea λki el i-ésimo autovalor de la matriz MkA, entonces λki es igual a la k-ésima iteración del método de Newton en una variable para f(x) = 1 − 1/x con iterado inicial λ0i , el i-ésimo autovalor de la matriz M0A, con M0 = AT /‖A‖22. Demostración. Para f(x) = 1 − 1/x se tiene que f ′(x) = 1/x2, así, la iteración de Newton para f(x) es de la forma. xk+1 =xk − f(xk) f ′(xk) =xk − xk (1− xk) =2xk − x2k =1− (1− xk) 2 Esta iteración puede reescribirse como 1− xk+1 = (1− xk) 2 Y así, puede deducirse inductivamente la siguiente relación de recurrencia 1− xk = (1− xk−1) 2 = ( (1− xk−2) 2 )2 = (( (1− xk−3) 2 )2)2 = · · · =1− (1− x0)2 k por lo tanto, la iteración de Newton de f(x) es igual a xk = 1− (1− x0) 2k Por otro lado, para k ≥ 1, se sabe que MkA = In − (In −M0A) 2k . Si se considera el polinomio p(x) = 1−(1− x)2 k , por el teorema (3.2.3), como p(M0A) = MkA entonces p(λ0i) = 1− (1− λ0i) = λki , lo cual completa la prueba. 44 Capítulo 4 El Método de Aceleración de Convergencia Richardson-PR2 Lo supieron los arduos alumnos de Pitágoras los astros y los hombres vuelven cíclicamente Jorge Luis Borges - El otro, el mismo El método de Richardson de primer orden, presentado en la sección (2.2.1) para resolver las ecuaciones normales, fue propuesto originalmente en 1910 (Ver [32]) como un método iterativo para sistemas de ecuaciones lineales de la forma Ax = b donde A ∈ Rn×n y b ∈ Rn. Este método iterativo, obtenido en principio a partir de un splitting regular de A, puede generalizarse de la siguiente manera x(k+1) = x(k) + λkr(k) r(k+1) = b−Ax(k) (4.1) Así, el método de Richardson puede generalizarse como un esquema iterativo donde la dirección de búsqueda viene dada por el residual r(k) = b − Ax(k), mientras que el tamaño de paso λk, que ahora puede variar en cada iteración, se calcula de distintas formas. Algunas estrategias para la construcción de la sucesión {λk} pueden encontrarse en [44], [43] y [29], entre otros. Hay que hacer notar que la investigación en torno a métodos iterativos basados en la iteración general de Richardson no fue muy prolija a lo largo del siglo XX, debido a que estos métodos se consideraban poco competitivos frente a otras familias de métodos iterativos por algunas características poco favorables, tales como su lenta velocidad de convergencia e inestabilidada numérica (Ver [35]) Sin embargo, en los últimos años, ha resurgido algún interés académico en métodos tipo Richardson, motivado principalmente al hecho de que las técnicas de precondicionamiento re- legan a un segundo plano la elección del método iterativo. En la práctica computacional, se ha determinado que si se logra elegir un precondicionador eficiente para un sistema de ecuaciones lineales, la rápida convergencia a su solución se logra independientemente del método iterativo utilizado. Por lo tanto, en ciertos escenarios no merece la pena invertir recursos computacio- nales en estrategias complejas para generar direcciones de búsqueda, cuando la simple elección del vector residual como dirección de búsqueda puede garantizar una muy buena velocidad de convergencia. En [27] se pueden encontrar varias referencias de investigaciones recientes de 45 4. El Método Richardson-PR2 métodos iterativos que usan el vector residual como dirección de búsqueda (también conocidos como métodos iterativo-residuales). Otra línea de pensamiento bastante novedosa busca apli- car esquemas iterativo-residuales ideados incialmente para ecuaciones no lineales en espacios de matrices, a la resolución de sistemas de ecuaciones lineales [23]. En este orden de ideas, en lo que resta del capítulo se presentará un esquema iterativo de aceleración de convergencia basado en la iteración de Richardson, el cual fue propuesto en [7]; conocido como Método de aceleración de Richardson PR2 (Richardson-PR2). El objetivo es utilizar este método iterativo sobre el PMCL precondicionado con la apro- ximación de Schultz. 4.1. El esquema de aceleración de convergencia Richardson- PR2 Supóngase que se tiene un método iterativo que sigue el esquema de la ecuación (4.1), el cual genera una sucesión de iterados { x(k) } que converge a la solución del sistema Ax = b. Se puede definir una nueva sucesión { y(k) } de iterados asociados al problema inicial, de la siguiente manera y(k) = x(k) − λkz(k) ρ(k) = r(k) + λkAz(k) (4.2) Donde ρ(k) = b − Ay(k) es el nuevo residual asociado a y(k) y z(k) es una nueva dirección de búsqueda, que en principio, puede elegirse de manera arbitraria. Si se logra que a partir de este esquema, la sucesión { y(k) } converja más rápidamente a la solución del sistema de ecuaciones que la sucesión { x(k) } , entonces se dice que el esquema iterativo de la ecuación (4.2) es un esquema de aceleración de convergencia del método iterativo (4.1). Para conseguir un esquema de aceleración de convergencia, una elección usual para el tamaño de paso λk es el valor que minimice algún funcional asociado a ρ(k) para k = 1, 2, . . . , mientras que z(k) puede escogerse, entre otras formas, como la proyección de r(k) sobre algún subespacio de Rn. En las siguientes secciones se mostrarán las estrategias de elección del tamaño de paso y la dirección de búsqueda en Richardson-PR2. 4.2. Elección del tamaño de paso para el esquema Richardson- PR2 Para el esquema Richardson-PR2, el tamaño de paso λk se elige de tal forma que minimice el funcional F (λk) ≡ ‖ρ(k)‖2. El argumento que minimiza al funcional anterior, se obtiene a partir del siguiente teorema Teorema 4.2.1. El argumento que minimiza a F (λk) ≡ ‖ρ(k)‖2 para k = 1, 2, . . . con ρ(k) definido como en la ecuación (4.2) es λkmin = − ( Az(k) )T r(k) ‖Az(k)‖22 46 4. El Método Richardson-PR2 Demostración. Ver [20] Evidentemente, tal argumento también minimiza a ‖ρ(k)‖22 = ρ (k)Tρ(k), así, sustituyendo λkmin en la expresión de ρ (k)Tρ(k) min λk ( ‖ρ(k)‖22 ) = ρ(k)Tρ(k) = ( r(k)T + λk(Az (k))T )( r(k) + λk(Az (k)) ) = r(k)T r(k) + 2λk(Az (k))T r(k) + λ2k(Az (k))T (Az(k)) = ‖r(k)‖22 − ( (Az(k))T r(k) )2( (Az(k))T (Az(k)) )2 = ‖r(k)‖22 − ( (Az(k))T r(k) )2 ‖Az(k)‖22‖r(k)‖ 2 2 ‖r(k)‖22 = ( 1− cos2(θk) ) ‖r(k)‖22 Luego min λk ( ‖ρ(k)‖22 ) = ‖r(k)‖22 sen(θk) (4.3) Donde θk es el ángulo entre los vectores r(k) y Az(k). Lo anterior implica que con dicha elección de λk se cumple que ‖ρ(k)‖2 ≤ ‖r(k)‖2 Y así, en vista de que r(k) → 0 cuando k →∞, entonces se deduce que lim k→∞ ‖ρ(k)‖2 ‖r(k)‖2 = 0 ⇐⇒ lim k→∞ θk = 0 ó lim k→∞ θk = π (4.4) Esto es, para la elección de λk del teorema (4.2.1), es condición necesaria y suficiente para la convergencia de ρ(k) que θk → 0 o bien θk → π. Por otro lado, si se define Pk = Az(k)z(k)T ‖Az(k)‖22 (4.5) Se tiene que ρ(k) = (In − Pk)r(k) (4.6) Obsérvese que P 2k = Pk y P T k = Pk, lo que implica que la matriz In − Pk es un proyector ortogonal. Esto permite obtener una interpretación geométrica de la elección del tamaño de paso para el esquema de aceleración Richardson-PR2: en este esquema, el residual ρ(k) es la proyección ortogonal de r(k) sobre Az(k). 47 4. El Método Richardson-PR2 4.3. Elección de la dirección de búsqueda para Richardson-PR2 A partir de la condición (4.4) para la convergencia de ρ(k) se deduce que el residual del esquema de aceleración de convergencia tiende a 0 si y sólo si los vectores r(k) y Az(k) son colineales, esto es, z(k) = ±αA−1r(k) con α ∈ R y α 6= 0. En particular, se puede elegir α = 1. Sin embargo, tal valor para z(k) no es de utilidad en la práctica, pues se tendría que encarar el problema de obtener A−1. Una alternativa más plausible es sustituir A−1 por una sucesión de matrices {Ck} tal que {Ck} converja a A−1 y así, la dirección de búsqueda utilizada, será z(k) = Ckr (k) (4.7) 4.4. El método iterativo de Richardson-PR2 Con la dirección de búsqueda y el tamaño de paso hallados tal como se explicó en las dos secciones precedentes, el esquema de aceleración Richardson-PR2 queda de la siguiente manera y(k) = x(k) − λkCkr(k) ρ(k) = r(k) + λkACkr(k) λk = − (ACkr(k)) T r(k) ‖ACkr(k)‖22 (4.8) Como puede observarse, este esquema de aceleración de convergencia, se obtuvo como una combinación de precondicionamiento por la izquierda y proyección. Finalmente, aplicando la estrategia de reinicio o cycling (Ver [8]), sea x(k+1) = y(k) y r(k+1) = ρ(k), entonces se obtiene el método iterativo Richardson-PR2, el cual, dado un iterado inicial x(0) y haciendo r(0) = b−Ax(0) queda así x(k+1) = x(k) + λkCkr(k) r(k+1) = r(k) − λkACkr(k) λk = (ACkr(k)) T r(k) ‖ACkr(k)‖22 (4.9) Nótese que para k = 1, 2, . . . el residual puede calcularse en su forma explícita r(k) = b−Ax(k), no obstante, en la práctica se prefiere utilizar la fórmula del residual de la ecuación (4.9) la cual se conoce como residual iterativo o residual implícito. En efecto, el residual iterativo suele ser más estable desde el punto de vista numérico que el residual explícito, incluso, en la mayoría de los problemas numéricos, sucede que el residual explícito puede quedarse estancado luego de cierto número de iteraciones, mientras que el residual iterativo sigue decreciendo en norma. Una observación final: Si se hace Ck = In para todo k, se obtiene el método iterativo de Richardson clásico. En el algoritmo (4.1) se presenta el método de Richardson-PR2 en su forma cruda. Se asume que existe algún procedimiento para obtener el precondicionador Ck en cada iteración, dado que se conoce un precondicionador inicial C0. 48 4. El Método Richardson-PR2 Algoritmo 4.1 Iteración de Richardson-PR2 cruda Algoritmo RichardsonPR2Crudo Descripción Resuelve el sistema Ax = b usando el método iterativo Richardson-PR2. Entradas • A Matriz de n filas por n columnas • b Vector columna de n filas. • x(0) Vector columna de n filas, iterado inicial. • C(0) Matriz de n filas por n columnas, precondicionador inicial. • Nmax Entero positivo. Número máximo de iteraciones. • � Real positivo. Tolerancia mínima para detener la iteración. Salidas • x Solución al sistema Ax = b. 1: función RichardsonPR2Crudo(A, b, x(0), C0, Nmax, �) 2: r(0) ← b−Ax(0) 3: para i← 0, 1 . . . , Nmax hacer 4: v ← Ckr(k) 5: u← Av 6: p1 ← uT r(k) 7: p2 ← uTu 8: λk ← p1/p2 9: x(k+1) ← x(k) + λkv 10: r(k+1) ← r(k) − λku 11: si ‖r(k+1)‖2 < � entonces 12: ← x(k+1) 13: fin si 14: fin para 15: Error (Máximo de iteraciones alcanzado) 16: fin función 49 4. El Método Richardson-PR2 4.5. Algunas estrategias de precondicionamiento para Richardson- PR2 En esta sección se presentarán tres posibles estrategias para la elección de la sucesión de precondicionadores {Ck} para la iteración Richardson-PR2; estas son: Precondicionador constante, Precondicionador lineal iteratvo y Precondicionador cuadrático iterativo. Un estu- dio detallado y referencias bibliográficas de estas estrategias de precondicionamiento pueden encontrarse en [8]. El caso particular de los precondicionadores lineal y cuadrático iterativos son de escaso interés en este trabajo y se presentan con propósitos informativos. Estas dos estrategias de precondicionamiento requieren el cálculo explícito de la sucesión de matrices {Ck}, algo que para el caso del precondicionamiento del PMCL se hará a través de la sucesión de iteraciones de Schulz. 4.5.1. Precondicionador constante La estrategia más simple de precondicionamiento para Richardson-PR2 se basa en selec- cionar Ck = C0 para k = 0, 1, 2, . . . una matriz precondicionadora constante. En este caso, lo usual es buscar alguna matriz que se aproxime a A−1 y que sea simple de calcular. Dos de las elecciones más usuales para un precondicionador constante son Ck = diag(A)−1 En este caso ni siquiera es necesario formar explícitamente las matrices Ck, basta con almacenar los valores a −1 ii de la inversa de la diagonal de A Ck = L−1 donde L es la matriz que contiene la porción triangular inferior de A, in- cluyendo la diagonal principal. Este precondicionador, además de dar una velocidad de convergencia bastante aceptable, no requiere el cálculo explícito de L−1. En efecto, el producto L−1r(k) se puede obtener a través el sistema de ecuaciones Lx = r(k), el cual se puede resolver por una simple sustitución hacia adelante, por ser L una matriz triangular. 4.5.2. Precondicionador lineal iterativo Si se conoce un splitting regular de A de la forma A = M − N , donde M es una matriz no singular, se puede definir un esquema iterativo para Ck. Sustituyendo A = M − N en la relación In = AA−1 se obtiene A−1 = (M−1N)A−1 +M−1 (4.10) Lo cual se puede convertir en el procedimiento iterativo Ck+1 = (M −1N)Ck +M −1 (4.11) Con C0 una matriz arbitraria. Esta sucesión converge linealmente a A−1. 4.5.3. Precondicionador cuadrático iterativo Si las matrices de la sucesión {Ck} se obtienen a través de una relación recursiva de la forma 50 4. El Método Richardson-PR2 Ck+1 = CkUk +Dk (4.12) Donde {Uk} y {Dk} son un par de sucesiones de matrices arbitrarias. Considérese la matriz residual Rk definida como Rk = In −ACk (4.13) Entonces, puede hacerse un refinamiento iterativo de la sucesión (4.12) haciendo Dk = Ck y Uk = Rk y así se obtiene una nueva relación de recurrencia para las matrices Ck Ck+1 = CkRk + Ck = Ck (In +Rk) (4.14) La cual converge cuadráticamente a A−1. 51 4. El Método Richardson-PR2 52 Capítulo 5 Precondicionamiento del PMCL por la Aproximación del Método de Schultz La programación es una de las disciplinas más complejas en las matemáticas aplicadas. Es preferible que los malos matemáticos se queden haciendo matemáticas puras Edsger W. Dijkstra En este capítulo se presentarán los algoritmos para la resolución del PMCL a través del mé- todo ierativo Richardson-PR2 , usando como precondicionador la aproximación a A† obtenida por el método de Schultz. Se proponen las siguientes variantes sobre el método Richardson- PR2: Algoritmo 1: Precondicionamiento por la izquierda explícito para la matriz A, usandoMj para algún j > 1. De esta forma se resolverá el sistema MjA = Mjb usando Richardson- PR2 con Ck = In para k = 0, 1, 2, . . . la k-ésima iteración de Richardson-PR2, lo cual es equivalente al método de Richardson de primer orden. Algoritmo 2: Se extenderá el método Richardson-PR2 para iterar directamente sobre la matriz rectangular A, siendo Ck = Mk para k = 0, 1, 2, . . . En la próxima sección se analizará cómo la matriz Mk, mejora el condicionamiento del PMCL. Posteriormente se presentarán los algoritmos para el cálculo del producto matriz- matriz MkA y el producto matriz-vector Mkb. El resto del capítulo muestra los algoritmos mencionados en el párrafo anterior. 5.1. Mk como precondicionador al PMCL Al utilizar la matriz Mk para el precondicionamiento por la izquierda del PMCL, se tiene que el número de condición asociado al PMCL dado por la definición (2.0.4) se convierte en κLS(MkA,Mkb) = κ(MkA) + κ 2(MkA) ‖Mk(b−AxLS)‖2 ‖MkA‖2‖xLS‖2 (5.1) donde xLS es la solución al PMCL. 53 5. Precondicionamiento del PMCL Obsérvese que por el teorema (3.3.3) AT (b −MkxLS) = 0. Por lo tanto, se deduce que el precondicionamiento por la izquierda del PMCL, usando la matriz Mk elimina el término de orden κ2(A) en el número de condición del PMCL. Así, finalmente, el número de condición del PMCL precondicionado por Mk es igual a κLS(MkA,Mkb) = κ(MkA) (5.2) Luego, en vista de queMk → A†, a partir de cierto valor de k, se garantiza que κLS(Mk,Mkb) ≈ 1. Lo anterior muestra que los iterados de Schultz introducen una gran mejora en el condi- cionamiento del PMCL, garantizando que cualquier método iterativo pueda converger rápida- mente a la solución xLS . 5.2. Cálculo de MkA y Mkb Como se vio en la demostración del teorema (3.2.2), el iterado de Schultz Mk satisface la siguiente relación de recurrencia In −MkA = (I −M0A) 2k (5.3) De donde se obtiene MkA = In − (I −M0A) 2k (5.4) Esta relación de recurrencia puede utilizarse en el cálculo de MkA, observando que dada una matriz C ∈ Rn×n la potencia Cp con p ∈ N y p = 2k para algún k ≥ 0 puede expresarse de forma recursiva como sigue Cp =   C p 2C p 2 = ( C p 2 )2 si p > 1 C si p = 1 In si p = 0 (5.5) De esta forma, la potencia C2 k se puede calcular iterativamente realizando k productos matriz-matriz. En el algoritmo (5.1) se presenta el procedimiento para el cálculo de dicha potencia. Usando la estrategia del algoritmo (5.1), el algoritmo (5.2) calcula la matriz MkA para A ∈ rmn y k ≥ 1 Para el cálculo del producto matriz-vector Mkb con b ∈ Rm, debe observarse que a partir de iteración de Schulz se tiene, para k ≥ 1 Mkb = (2Mk−1 −Mk−1AMk−1) b = 2Mk−1b− (Mk−1A)Mk−1 (5.6) Sustituyendo el valor de Mk−1A obtenido a partir de la ecuación (3.13) se obtiene 54 5. Precondicionamiento del PMCL Algoritmo 5.1 Cálculo de la potencia A2 k para una matriz A Algoritmo Potencia Descripción Dada una matriz cuadrada A, calcula la potencia A2 k para k > 0 entero. Entradas • A Matriz de n filas por n columnas. Salidas • P Matriz, potencia 2k de A. 1: función Potencia(A, k) 2: si k = 0 entonces 3: ← In 4: fin si 5: P ← A 6: para i← 0, 1 . . . , k hacer 7: P ← P 2 8: fin para 9: ← P 10: fin función 55 5. Precondicionamiento del PMCL Algoritmo 5.2 Cálculo de la matriz MkA Algoritmo PrecondMkA Descripción Dada una matriz A rectangular, calcula la matrizMkA dondeMk es la k-ésima iteración de Schulz, para k > 0. Entradas • A Matriz de rango completo de m filas por n columnas con m > n. • k Entero positivo. Salidas • Matriz MkA. 1: función PrecondMkA(A, k) 2: P ← In − ( ATA/‖A‖22 ) 3: para i← 0, 1 . . . , k hacer 4: P ← P 2 5: fin para 6: ← In − P 7: fin función Mkb = 2Mk−1b− ( In − (In −M0A)2 k−1 ) Mk−1b = Mk−1b+ (In −M0A) 2k−1 b (5.7) Finalmente, combinando la ecuación (5.6) con el algoritmo (5.2), se puede hallar un nuevo algoritmo que calcula de una sola vez la matriz MkA y el vector Mkb. Basta con calcular el vector M0b antes de entrar en el ciclo principal del algoritmo, y así, en la k-ésima iteración se tendrá el producto matriz vector Mk−1b necesario para el cálculo de Mkb. Esta estrategia se ilustra en el algoritmo (5.3). 5.3. Algoritmo PMCL-Richardson-PR2 1 Para este algoritmo se precondiciona de manera explícita el PMCL y se resuelve el sistema precondicionado de n ecuaciones con n incógnitas MkA = Mkb usando el esquema de aceleracion Richardson-PR2 con Ck = In para k = 0, 1, . . . lo cual es equivalente al método de Richardson de primer orden. En el algoritmo (5.4) se presenta esta estrategia. 56 5. Precondicionamiento del PMCL Algoritmo 5.3 Cálculo de la matriz MkA y el vector Mkb Algoritmo PrecondMkAb Descripción Dada una matriz A rectangular y un vector b, calcula la matriz MkA y el vector Mkb donde Mk es la k-ésima iteración de Schulz, para k > 0. Entradas • A Matriz de rango completo de m filas por n columnas con m > n. • b Vector columna de m filas. • k Entero positivo. Salidas • Matriz MkA. • Vector Mkb. 1: función PrecondMkAb(A, b, k) 2: M ← AT /‖A‖22 3: v ←Mb 4: P ← In −MA 5: para i = 1, . . . k hacer 6: v ← v + Pv 7: P ← P 2 8: fin para 9: ← [In − P, v] 10: fin función 57 5. Precondicionamiento del PMCL Algoritmo 5.4 Algoritmo PMCL-Richardson-PR2 1 Algoritmo PCMLRichardsonPR2_1 Descripción Resuelve el PMCL asociado a A y b a través del método Richardson-PR2, usando como precondicionador una matriz Mk. Entradas • A Matriz de rango completo de m filas por n columnas con m > n. • b Vector columna de m filas. • x(0) Vector columna de n filas, iterado inicial. • k Entero positivo que indica el número de iteraciones de Schulz para generar el precondicionador. • Nmax Entero positivo. Número máximo de iteraciones. • � Real positivo. Tolerancia mínima para detener la iteración. Salidas • x Solución al sistema Ax = b. 1: función PCMLRichardsonPR2_1(A, b, x(0), k,Nmax, �) 2: [b′, C]← PrecondMkAb(A) 3: r(0) ← b′ − Cx(0) 4: para i← 0, 1, . . . , Nmax hacer 5: u← Cr(k) 6: p1 ← uT r(k) 7: p2 ← uTu 8: λk ← p1/p2 9: x(k+1) ← x(k) + λkr(k) 10: r(k+1) ← r(k) − λku 11: si ‖r(k+1)‖2 < � entonces 12: ← x(k+1) 13: fin si 14: fin para 15: fin función 58 5. Precondicionamiento del PMCL 5.4. Algoritmo PMCL-Richardson-PR2 2 El algoritmo PMCL-Richardson-PR2 2 es una adaptación del esquema de aceleración Richardson-PR2 para una matriz rectangular A ∈ Rm×n con m > n. De la ecuación (4.9) se observa que en una iteración de Richardson-PR2 es necesario realizar los siguientes produc- tos matriz-vector: Ckr (k). ACkr (k). Si la matriz A ∈ Rm×n es rectangular deben verificarse que los productos anteriores están bien definidos. Como r(k) = b − Ax(k) para k = 0, 1, . . . , entonces r(k) ∈ Rm; por otro lado, para que el producto A ( Ckr k ) esté bien definido, es necesario que Ckr(k) sea un vector en Rn, lo cual se cumple si Ck ∈ Rn×m. Así, si se tiene una sucesión {Ck} de matrices en Rn×m, entonces la iteración de Richardson-PR2 queda bien definida para una matriz rectangular. Por otro lado, para el caso de matrices cuadradas, en el esquema de aceleración Richardson-PR2 se escoge una sucesión de matrices Ck que converja a A−1, luego, una elección heurística para el caso rectangular, puede ser una sucesión de matrices Ck que converja a A†. A partir de lo anterior, se propone la elección de Ck = Mk donde Mk es el k-ésimo iterado del método de Schulz para A, con lo cual se define una extensión del esquema de aceleración Richardson-PR2 para matrices rectangulares. Para construir el algoritmo es necesario buscar una expresión para calcular Ck+1r(k+1) = Mk+1r (k+1) en función de los valores de la iteración k para k = 0, 1, . . . .Esta expresión puede hallarse combinando la igualdad r(k+1) = r(k) − λkAMkr(k+1), con la ecuación (5.4) siendo Pk ≡ (In −M0A)2 k , de donde se obtiene Mk+1r (k+1) = Mkr (k+1) + PkMkr (k+1) = Mk ( r(k) − λkAMkr(k) ) + PkMk ( r(k) − λkAMkr(k) ) = Mkr (k) − λkMkr(k) + PkMkr(k) + λkP 2kMkr (k) = (1− λk)Mkr(k) + PkMkr(k) + λkP 2kMkr (k) (5.8) Obsérvese que Pk+1 = P 2k , y por lo tanto se obtiene una relación de recurrencia que permite calcular Mk+1r(k+1) en función de los valores obtenidos en la iteración k. Utilizando la relación de recurrencia de la ecuación (5.8) en el algoritmo del esquema de aceleración Richardson-PR2 en su forma cruda, se obtiene una extensión para matrices rectangulares usando {Mk} como sucesión de precondicionadores. La implementación de este esquema de aceleracion Richardson-PR2 extendido se presenta en el algoritmo (5.5). Nótese que para este algoritmo, no se puede usar el valor de la norma del residual como condición de parada, pues en el caso del PMCL, no se garantiza que el residual converja a 0. No obstante, la condición de ortogonalidad de las soluciones del PMCL ofrece una condición para detener la iteración, que es la norma del vector AT r(k), la cual sí converge a 0 cuando x(k) converge a la solución. 59 5. Precondicionamiento del PMCL Algoritmo 5.5 Algoritmo PMCL-Richardson-PR2 2 Algoritmo PCMLRichardsonPR2_2 Descripción Resuelve el PMCL asociado a A y b a través del método Richardson-PR2, usando como precondicionador una matriz Mk. Entradas • A Matriz de rango completo de m filas por n columnas con m > n. • b Vector columna de m filas. • x(0) Vector columna de n filas, iterado inicial. • Nmax Entero positivo. Número máximo de iteraciones. • � Real positivo. Tolerancia mínima para detener la iteración. Salidas • x Solución al sistema Ax = b. 1: función PCMLRichardsonPR2_2(A, b, x(0), Nmax, �) 2: r(0) ← b−Ax(0) 3: M ← AT /‖A‖22 4: d←Mr(0) 5: C ← In −MA 6: para i← 0, 1 . . . , Nmax hacer 7: u← Ad 8: p1 ← uT r(k) 9: p2 ← uTu 10: λk ← p1/p2 11: x(k+1) ← x(k) + λkd 12: r(k+1) ← r(k) − λku 13: si ‖AT r(k+1)‖2 < � entonces 14: ← x(k+1) 15: fin si 16: d′ ← (1− λk) d+ Cd 17: C ← C2 18: d← d′ + λkCd 19: fin para 20: Error (Máximo de iteraciones alcanzado) 21: fin función 60 Capítulo 6 Experimentación Numérica Un algoritmo debe verse para creerse Donald Knuth Si no fallas al menos el noventa porciento de las veces, entonces no estás pensando en algo realmente grande Alan Kay En este capítulo se presentarán los resultados de varios experimentos numéricos realizados sobre la técnica de precondicionamiento para el PMCL desarrollada a lo largo de este trabajo. Las pruebas numéricas se dividen en dos grupos. En primer lugar, se estudiarán las propiedades de las matricesMk como precondicionadores. En los resultados numéricos se verificará que cuando k → ∞, la matriz Mk hace que los autovalores de MkA se agrupe en torno a x = 1, tal como lo sugiere el teorema (3.3.4). Así mismo, se mostrará que los autovalores de MkA se corresponden a los iterados del método de Newton en una variable para hallar el cero de la función f(x) = 1− 1/x usando como iterados iniciales los autovalores de ATA/‖A‖22, tal como lo establece dicho teorema. Adicionalmente, se analizará el comportamiento de κ(MkA) en función de k. En segundo lugar, se harán experimentos numéricos de los algoritmos planteados en el capítulo 5 para la resolución del PMCL. Se analizará el número de iteraciones y tiempo de ejecución para el sistema precondicionado, para diversos valores de Mk tanto en matrices bien condicionadas, como en matrices mal condicionadas. Además, estos métodos se compararán con el esquema de aceleración de convergencia Richardson-PR2 aplicado directamente a las ecuaciones normales, como representante de los métodos clásicos para la resolución del PMCL. Tanto el código fuente de la implementación de todos los experimentos numéricos, así como los archivos de datos contentivos de las matrices de prueba, se encontrarán en el material suplementario de este trabajo. 6.1. Plataforma computacional La plataforma computacional utilizada para la realización de los experimentos numéricos fue la siguiente: 61 6. Experimentación Numérica Equipo: Computador con procesador INTEL c© Core 2 DUO@1.8 GHz con 2Gb. de Me- moria RAM. Sistema Operativo: Microsoft Windows c© Vista Home Premium. Lenguaje y Entorno de programación: MATLAB c© R2008.a (Versión 7.6.0) paraWindows c©. 6.2. Matrices de prueba En esta sección se presentarán los grupos de matrices que serán utilizados en los experimen- tos numéricos. Se harán pruebas numéricas para matrices densas y para matrices sparse. Aún cuando el método tiene su campo de aplicación en matrices densas, debido a que la iteración de Schulz tiende a rellenar las matrices sparse luego de unas pocas iteraciones, se verificará, que aún aquellas, se observa que el precondicionador acelera notablemente la convergencia. 6.2.1. Matrices densas de prueba Las matrices densas a utilizar, se generan con diversas funciones provistas por MATLAB c©y son las siguientes. Matriz NormRand(µ, σ,m, n) Esta es una matriz densa de dimensión m × n con valores aleatorios distribuídos normalmente con promedio µ y desviación estándar σ. Esta matriz se genera con ayuda de la función randn de MATLAB c©, a través de la siguiente fórmula: randn(m,m). ∗ σ + µ El condicionamiento de estas matrices depende, en el caso general, de la desviación estándar elegida. Para valores muy pequeños de la desviación estándar, los valores singulares tieden a agruparse en torno a valores cercanos a 0, por lo cual la matriz generada es muy mal condicionada, mientras que para valores grandes de la desviación estándar, la matriz tiene un número de condición pequeño. Matriz RandSingV al((σ1, . . . , σn),m) La matriz RandSingV al es una matriz densa de m× n con valores singulares σ1, σ2, . . . σn positivos con valores aleatorios uniformemente dis- tribuidos. Para generar la matriz, se hacen los siguientes pasos, basados en la idea de la descomposición en valores singulares Sean U ∈ Rm×m y V ∈ Rn×n matrices densas con valores aleatorios uniformemente distribuidos, la cual se genera con la función rand de MATLAB c©. Sean Q1, R1 y Q2, R2 las matrices de las descomposiciones QR de U y V respectivamente, las cuales se obtienen vía la función qr de MATLAB c©. La matriz final viene dada por la expresión Q1diag(σ1, σ2, . . . , σn, 0, . . . , 0)QT2 62 6. Experimentación Numérica El condicionamiento de estas matrices depende de la distribución de sus valores singulares. Si los valores singulares están agrupados en torno a unos pocos valores, la matriz estará bien condicionada, mientras que si los valores singulares se encuentran muy dispersos, o bien se tienen un grupo de valores singulares muy pequeños y otro grupo de valores singulares muy grandes la matriz estará mal condicionada. Matriz RandSV D(m,n) La matrizRandSV D es una de las matrices de prueba de MATLAB c©, obtenidas a través de la función gallery. Esta es una matriz densa de m × n con n valores singulares distintos, distribuídos geométricamente entre 0 y 1. Estas matrices son muy mal condicionadas con el número de condición del orden de 1/ √ � donde � es el valor epsilon del computador. Matriz V andermonde((v1, . . . vm), n) La matriz de Vandermonde de orden n para un con- junto de valores reales v1, v2, . . . vn es una matriz, cuya i-ésima fila tiene la forma (1, vi, v2i , . . . , v n−1 i ) para i = 1, 2, . . . n. Esta matriz tiene varias aplicaciones en estadística y otras áreas de ciencias aplicadas, usándose principalmente para representar modelos de regresión polinomial. Es una matriz muy mal condicionada. 6.2.2. Matrices sparse de prueba Las matrices sparse de prueba, se tomaron del conjunto de matrices de problemas de míni- mos cuadrados de la Colección de matrices sparse Harwell-Boeing (Ver [9]). Más información sobre el formato de las matrices Harwell-Boeing se puede obtener en http://math.nist. gov/MatrixMarket/formats.html. Para este trabajo se utilizaron las matrices en formato MATLAB c©, las cuales se pueden obtener en http://www.cise.ufl.edu/research/sparse/ mat/HB/. A continuación se dará el código de las matrices de la colección Harwell-Boeing utilizadas en este trabajo Matriz ILLC1033 del grupo LSQ Matriz de 1033× 320 mal condicionada. Matriz ILLC185 del grupo LSQ Matriz de 1850× 712 mal condicionada. Matriz WELL1033 del grupo LSQ Matriz de 1033× 320 bien condicionada. Matriz WELL1850 del grupo LSQ Matriz de 1850× 712 bien condicionada. 6.3. Estudio numérico de Mk como precondicionador En esta sección se presentarán algunos experimentos numéricos que permitirán verificar en la práctica que Mk es un buen condicionador para el PMCL. Los experimentos se dividirán en dos grupos: en el primer grupo de experimentos se analizará el comportamiento del número de condición κ(MkA) en función de k donde k es el k-ésimo iterado de Schulz para A. En el segundo grupo de experimento se analizará el comportamiento de los autovalores de MkA, y se verificará la propiedad de clustering que tiene la aproximación a A†. En este último grupo de experimentos se usarán matrices con pocas columnas, con fines de que sea fácil visualizar las gráficas del espectro de MkA en función de k. 63 6. Experimentación Numérica 6.3.1. Grupo de Experimentos A Este grupo de experimentos, se divide en 3 subgrupos de matrices de la siguiente manera. 1. Subgrupo de matrices A DA11 = NormRand(100, 5, 200, 20). Matriz de 200×20 con κ(DA11) = 1,26×102 DA12 = NormRand(300, 50, 500, 400). Matriz de 500×400 con κ(DA12) = 1,18× 103 DA13 = RandSingV al((5, 10, 20, . . . 500), 300). Matriz de 300×51 con κ(DA13) = 0,99× 102 2. Subgrupo de matrices B DB11 = RandSV D(500, 10). Matriz de 500× 10 con κ(DB11) = 6,71× 107 DB12 = NormRand(300, 50, 500, 400). Matriz de 500×400 con κ(DB12) = 4,50× 109 DB13 = RandSingV al((100, 500, 5000, 50000, 500000, 5000000), 100). Matriz de 100× 6 con κ(DB13) = 5× 104 3. Subgrupo de matrices C DC11 = WELL1033. Matriz de 1033× 320 con κ(DC11) = 1,66× 107 DC12 = WELL1850. Matriz de 1850× 712 con κ(DC12) = 1,13× 102 DC13 = ILL1033. Matriz de 1033× 320 con κ(DC13) = 1,8× 104 DC14 = ILL1850. Matriz de 1850× 712 con κ(DC14) = 1,40× 103 Para las matrices anteriores se compararán los valores de κ(MkA) en función de k (agre- gando inicialmente el valor de κ(A)) con el comportamiento del error en la iteración de Schulz. Las figuras (6.1), (6.2) y (6.3) muestran gráficamente el comportamiento del error en la iteración de Schulz para las matrices de los grupos A, B y C respectivamente, mientras que las figuras (6.4), (6.5) y (6.6) muestran el valor del número de condición de las matrices MkA en función de k, para cada subgrupo. Además, el primer punto de dichas gráficas corresponde a κ(A). A partir de dichos resultados pueden determinarse algunas características del precondi- cionador dado por Mk. En primer lugar nótese que en las primeras iteraciones del método de Schulz, se evidencia que Mk empeora el condicionamiento del sistema original; la razón de este hecho es que el iterado inicial del método de Schulz es de la forma αATA, lo cual implica que κ(M0A) ≈ κ(A)2, así, en el caso de matrices muy mal condicionadas, se tiene un aumento considerable del número de condición en el sistema precondicionado. Este resultado puede observarse fácilmente en las gráfica κ(MkA) vs. k: en todos los casos se observa un salto grande entre los dos primeros puntos de esta (recuérdese que el primer punto de la gráfica corresponde a κ(A)). Este hecho puede suponer alguna desventaja, pues se tiene una cierta cantidad de iteraciones de Schulz que van a ser inútiles para el precondicionamiento; en efecto, sólo después de algún valor de k > 1 es que se conseguirá que las matrices MkA estén mejor condicionadas que la matriz original A. No obstante, una vez que se alcanza tal valor de k las 64 6. Experimentación Numérica siguientes iteraciones de Schulz hacen que el número de condición converja con gran velocidad a 1. En segundo lugar, el comportamiento del número de condición, presenta una correspon- dencia con la convergencia del método de Schulz. En las gráficas de error de la iteración de Schulz, puede observarse, de manera análoga a lo anterior, que en las primeras iteraciones el error puede ser incluso creciente, o bien tener un comportamiento errático: mantenerse más o menos constante o fluctuar, hasta que se alcanza cierto valor de k en el cual, en unas pocas iteraciones (en algunos casos 2 o 3) el método converge explosivamente. Y precisamente en ese punto donde inicia la rápida convergencia de la iteración de Schulz, también se acelera la convergencia del número de condición del sistema precondicionado MkA hacia 1. Como ejemplo, considérese la figura (6.1) correspondiente a la convergencia de Schulz para las matrices del subgrupo A: En particular, obsérvese el patrón de convergencia de la matriz DA11 (línea roja), el cual es un patrón típico de convergencia del método de Schulz en una matriz bien condicionada. En las primeras 7 iteraciones el método parece diverger, posteriormente entre las iteraciones 8 y 15 el error de los iterados se estabiliza en torno a un valor de error, y entre la iteración 16 y la iteración 19, el error decae desde aproximadamente 10−1 hasta 10−8, es decir, alrededor de siete órdenes de magnitud, superando inclusive el valor de tolerancia dado como entrada en el algoritmo que era de 10−5. Ahora obsérvese la figura 6.4 correspondiente a la gráfica κ(MkA) vs. k; como para la gráfica anterior, considérese el comportamiento del número de condición para el caso de la matriz DA11 (línea roja). Puede observarse que en las 7 primeras iteraciones, el número de condición de MkA se mantiene por encima del número de condición original de A, y justo luego de la iteración 8, que se corresponde a la iteración en la cual el error en el método de Schulz deja de ser creciente, el número de condición de MkA es mejor que el número de condición de A. Además se puede observar en dicha gráfica, que la pendiente a partir de la iteración 15 se hace mayor, lo cual indica una mayor velocidad de convergencia del número de condición, justo en la misma iteración en donde el método de Schulz comienza su rápida convergencia. En el resto de los experimentos de este grupo se observa un comportamiento semejante. 65 6. Experimentación Numérica 0 5 10 15 20 25 10 −8 10 −7 10 −6 10 −5 10 −4 10 −3 10 −2 10 −1 10 0 k E rr o r( S ch u lz ) A = DA11 A = DA12 A = DA13 Figura 6.1: Comportamiento de la iteración de Schulz para las matrices del subgrupo A 0 10 20 30 40 50 60 70 10 −7 10 −6 10 −5 10 −4 10 −3 10 −2 10 −1 10 0 k E rr o r( S ch u lz ) A = DB11 A = DB12 A = DB13 Figura 6.2: Comportamiento de la iteración de Schulz para las matrices del subgrupo B 66 6. Experimentación Numérica 0 5 10 15 20 25 30 35 40 10 −10 10 −8 10 −6 10 −4 10 −2 10 0 k e rr o r( S ch u lz ) A = DC11 A =DC12 A = DC13 A = DC14 Figura 6.3: Comportamiento de la iteración de Schulz para las matrices del subgrupo C 0 5 10 15 20 25 10 0 10 1 10 2 10 3 10 4 10 5 10 6 k co n d (M kA ) A = DA11 A = DA12 A = DA13 Figura 6.4: Gráfico MkA en función de k para las matrices del subgrupo A 67 6. Experimentación Numérica 0 10 20 30 40 50 10 0 10 5 10 10 10 15 10 20 k co n d (M kA ) A = DB11 A = DB12 A = DB13 Figura 6.5: Gráfico MkA en función de k para las matrices del subgrupo B 0 5 10 15 20 25 30 35 40 10 0 10 2 10 4 10 6 10 8 10 10 k co n d (M kA ) A = DC11 A = DC12 A = DC13 A = DC14 Figura 6.6: Gráfico MkA en función de k para las matrices del subgrupo C 68 6. Experimentación Numérica 6.3.2. Grupo de Experimentos B En este grupo de experimentos, se utilizaron las siguientes matrices densas: DA21 = NormRand(10, 4, 500, 5). Matriz de 500× 5 con κ(DA21) = 6,53 DA22 = NormRand(5, 0,1, 20, 5). Matriz de 20× 5 con κ(DA22) = 4,97× 102 DA23 = RandSV D(20, 5). Matriz de 20× 5 con κ(DA23) = 6,71× 107 DA24 = V andermonde((5, 10, 15, . . . 400), 4). Matriz de 40×4 con κ(DA24) = 1,03×108 Para cada una de estas matrices se realizó la iteración de Schulz, para obtener las sucesiones de autovalores de las matrices MkA y se realizó la iteración de Newton en una variable de la función f(x) = 1− 1/x, tomando como iterado inicial los autovalores de la matriz M0A. Los autovalores de las matrices fueron calculados con la función eig de MATLAB c©. El comportamiento del espectro de autovalores aproximados vía los iterados de Newton en una variable MkA se sumariza en los cuadros (6.1), (6.2), (6.3) y (6.4) para A = DA21, A = DA22, A = DA23 y A = DA24 respectivmente. En todos los casos, el error relativo de la la iteración de Newton respecto a los autovalores de las matrices MkA es menor a 102. Por otro lado las figuras (6.7), (6.8), (6.9) y (6.10) muestran gráficamente el comportamiento de los autovalores de las matrices precondicionadas MkA. A partir de las gráficas, puede observarse, que de forma análoga al comportamiento del número de condición de MkA en función d k, los autovalores mantienen la tendencia de con- verger de manera estable hacia 1; convergiendo de manera lenta en las primeras iteraciones, y luego de cierto valor de k > 1 se acelera notablemente la convergencia y el agrupamiento de todos los autovalores en torno a x = 1. Esto confirma el resultado teórico del teorema (3.3.4) el cual establecía como consecuencia, que los precondicionadoresMk comprimen el espectro de valores singulares de la matriz original en torno a 1, lo cual verifica finalmente que los iterados de Schulz son un buen precondicionador para el PMCL. Además, en los experimentos se observó una propiedad adicional sobre el espectro de autovalores de las matricesMkA: en todos los experimentos, para el iterado inicialM0 siempre se tiene un autovalor muy cercano a 1 y otro mucho menor a 1 en la matriz M0A. En efecto comoM0 = AT /‖A‖22 entonces todos los autovalores deM0A son menores a 1. Por otro lado se sabe que el radio espectral de una matriz es una cota inferior de todas las normas matriciales, por lo tanto, si para una matriz se tiene que ρ(A) ≈ ‖A‖2, entonces la matriz ATA/‖A‖22 siempre tendrá un autovalor muy cercano a 1, el cual va a ser justamente ρ(ATA). 69 6. Experimentación Numérica k λ1 λ2 λ3 λ4 λ5 1 8, 91× 10−01 3, 32× 10−02 2, 08× 10−02 2, 87× 10−02 2, 62× 10−02 2 9, 88× 10−01 6, 53× 10−02 4, 12× 10−02 5, 65× 10−02 5, 18× 10−02 3 1, 00× 10+00 1, 26× 10−01 8, 08× 10−02 1, 10× 10−01 1, 01× 10−01 4 1, 00× 10+00 2, 37× 10−01 1, 55× 10−01 2, 07× 10−01 1, 92× 10−01 4 1, 00× 10+00 4, 17× 10−01 2, 86× 10−01 3, 72× 10−01 3, 46× 10−01 5 1, 00× 10+00 6, 61× 10−01 4, 90× 10−01 6, 06× 10−01 5, 73× 10−01 6 1, 00× 10+00 8, 85× 10−01 7, 40× 10−01 8, 44× 10−01 8, 18× 10−01 7 1, 00× 10+00 9, 87× 10−01 9, 32× 10−01 9, 76× 10−01 9, 67× 10−01 8 1, 00× 10+00 1, 00× 10+00 9, 95× 10−01 9, 99× 10−01 9, 99× 10−01 9 1, 00× 10+00 1, 00× 10+00 1, 00× 10+00 1, 00× 10+00 1, 00× 10+00 10 1, 00× 10+00 1, 00× 10+00 1, 00× 10+00 1, 00× 10+00 1, 00× 10+00 11 1, 00× 10+00 1, 00× 10+00 1, 00× 10+00 1, 00× 10+00 1, 00× 10+00 12 1, 00× 10+00 1, 00× 10+00 1, 00× 10+00 1, 00× 10+00 1, 00× 10+00 13 1, 00× 10+00 1, 00× 10+00 1, 00× 10+00 1, 00× 10+00 1, 00× 10+00 14 1, 00× 10+00 1, 00× 10+00 1, 00× 10+00 1, 00× 10+00 1, 00× 10+00 Cuadro 6.1: Aproximación a los autovalores de las matrices MkA para A = DA21, obtenidos a través de la iteración de Newton de f(x) = 1− 1/x 0 5 10 15 10 −2 10 −1 10 0 k a p ro x λ n e w to n aprox λ1 aprox λ2 aprox λ3 aprox λ4 aprox λ5 Figura 6.7: Gráfica del espectro de autovalores de la matriz MkA en función de k para A = DA21 70 6. Experimentación Numérica 0 5 10 15 20 25 10 −6 10 −5 10 −4 10 −3 10 −2 10 −1 10 0 k a p ro x λ n e w to n aprox λ1 aprox λ2 aprox λ3 aprox λ4 aprox λ5 Figura 6.8: Gráfica del espectro de autovalores de la matriz MkA en función de k para A = DA22 71 6. Experimentación Numérica 0 10 20 30 40 50 60 70 10 −16 10 −14 10 −12 10 −10 10 −8 10 −6 10 −4 10 −2 10 0 k a p ro x λ n e w to n aprox λ1 aprox λ2 aprox λ3 aprox λ4 aprox λ5 Figura 6.9: Gráfica del espectro de autovalores de la matriz MkA en función de k para A = DA23 72 6. Experimentación Numérica 0 10 20 30 40 50 60 70 10 −20 10 −15 10 −10 10 −5 10 0 k a p ro x λ N e w to n aprox λ 1 aprox λ 2 aprox λ 3 aprox λ 4 Figura 6.10: Gráfica del espectro de autovalores de la matriz MkA en función de k para A = DA24 73 6. Experimentación Numérica k λ1 λ2 λ3 λ4 λ5 1 9, 90× 10−01 4, 05× 10−06 5, 78× 10−06 8, 55× 10−06 1, 34× 10−05 2 9, 90× 10−01 8, 09× 10−06 1, 16× 10−05 1, 71× 10−05 2, 68× 10−05 3 1, 00× 10+00 1, 62× 10−05 2, 31× 10−05 3, 42× 10−05 5, 35× 10−05 4 1, 00× 10+00 3, 24× 10−05 4, 62× 10−05 6, 84× 10−05 1, 07× 10−04 4 1, 00× 10+00 6, 47× 10−05 9, 24× 10−05 1, 37× 10−04 2, 14× 10−04 5 1, 00× 10+00 1, 29× 10−04 1, 85× 10−04 2, 74× 10−04 4, 28× 10−04 6 1, 00× 10+00 2, 59× 10−04 3, 70× 10−04 5, 47× 10−04 8, 56× 10−04 7 1, 00× 10+00 5, 18× 10−04 7, 39× 10−04 1, 09× 10−03 1, 71× 10−03 8 1, 00× 10+00 1, 04× 10−03 1, 48× 10−03 2, 19× 10−03 3, 42× 10−03 9 1, 00× 10+00 2, 07× 10−03 2, 95× 10−03 4, 37× 10−03 6, 83× 10−03 10 1, 00× 10+00 4, 13× 10−03 5, 90× 10−03 8, 72× 10−03 1, 36× 10−02 11 1, 00× 10+00 8, 25× 10−03 1, 18× 10−02 1, 74× 10−02 2, 70× 10−02 12 1, 00× 10+00 1, 64× 10−02 2, 34× 10−02 3, 44× 10−02 5, 33× 10−02 13 1, 00× 10+00 3, 26× 10−02 4, 62× 10−02 6, 77× 10−02 1, 04× 10−01 14 1, 00× 10+00 6, 41× 10−02 9, 03× 10−02 1, 31× 10−01 1, 97× 10−01 16 1, 00× 10+00 1, 24× 10−01 1, 72× 10−01 2, 44× 10−01 3, 55× 10−01 17 1, 00× 10+00 2, 33× 10−01 3, 15× 10−01 4, 29× 10−01 5, 84× 10−01 18 1, 00× 10+00 4, 12× 10−01 5, 31× 10−01 6, 74× 10−01 8, 27× 10−01 19 1, 00× 10+00 6, 54× 10−01 7, 80× 10−01 8, 94× 10−01 9, 70× 10−01 20 1, 00× 10+00 8, 80× 10−01 9, 52× 10−01 9, 89× 10−01 9, 99× 10−01 21 1, 00× 10+00 9, 86× 10−01 9, 98× 10−01 1, 00× 10+00 1, 00× 10+00 22 1, 00× 10+00 1, 00× 10+00 1, 00× 10+00 1, 00× 10+00 1, 00× 10+00 23 1, 00× 10+00 1, 00× 10+00 1, 00× 10+00 1, 00× 10+00 1, 00× 10+00 24 1, 00× 10+00 1, 00× 10+00 1, 00× 10+00 1, 00× 10+00 1, 00× 10+00 25 1, 00× 10+00 1, 00× 10+00 1, 00× 10+00 1, 00× 10+00 1, 00× 10+00 Cuadro 6.2: Aproximación a los autovalores de las matrices MkA para A = DA22, obtenidos a través de la iteración de Newton de f(x) = 1− 1/x 74 6. Experimentación Numérica k λ1 λ2 λ3 λ4 λ5 1 1, 22× 10−04 1, 22× 10−04 1, 49× 10−08 1, 82× 10−12 0, 00× 10+00 2 2, 44× 10−04 2, 44× 10−04 2, 98× 10−08 3, 64× 10−12 0, 00× 10+00 3 4, 88× 10−04 4, 88× 10−04 5, 96× 10−08 7, 28× 10−12 1, 00× 10−15 4 9, 76× 10−04 9, 76× 10−04 1, 19× 10−07 1, 46× 10−11 2, 00× 10−15 5 1, 95× 10−03 1, 95× 10−03 2, 38× 10−07 2, 91× 10−11 3, 00× 10−15 6 3, 90× 10−03 3, 90× 10−03 4, 77× 10−07 5, 82× 10−11 7, 00× 10−15 7 7, 78× 10−03 7, 78× 10−03 9, 54× 10−07 1, 16× 10−10 1, 30× 10−14 8 1, 55× 10−02 1, 55× 10−02 1, 91× 10−06 2, 33× 10−10 2, 70× 10−14 9 3, 08× 10−02 3, 08× 10−02 3, 81× 10−06 4, 66× 10−10 5, 40× 10−14 10 6, 06× 10−02 6, 06× 10−02 7, 63× 10−06 9, 31× 10−10 1, 07× 10−13 11 1, 17× 10−01 1, 17× 10−01 1, 53× 10−05 1, 86× 10−09 2, 14× 10−13 12 2, 21× 10−01 2, 21× 10−01 3, 05× 10−05 3, 72× 10−09 4, 29× 10−13 13 3, 93× 10−01 3, 93× 10−01 6, 10× 10−05 7, 45× 10−09 8, 58× 10−13 14 6, 32× 10−01 6, 32× 10−01 1, 22× 10−04 1, 49× 10−08 1, 72× 10−12 15 8, 65× 10−01 8, 65× 10−01 2, 44× 10−04 2, 98× 10−08 3, 43× 10−12 16 9, 82× 10−01 9, 82× 10−01 4, 88× 10−04 5, 96× 10−08 6, 86× 10−12 17 1, 00× 10+00 1, 00× 10+00 9, 76× 10−04 1, 19× 10−07 1, 37× 10−11 18 1, 00× 10+00 1, 00× 10+00 1, 95× 10−03 2, 38× 10−07 2, 74× 10−11 19 1, 00× 10+00 1, 00× 10+00 3, 90× 10−03 4, 77× 10−07 5, 49× 10−11 20 1, 00× 10+00 1, 00× 10+00 7, 78× 10−03 9, 54× 10−07 1, 10× 10−10 21 1, 00× 10+00 1, 00× 10+00 1, 55× 10−02 1, 91× 10−06 2, 20× 10−10 22 1, 00× 10+00 1, 00× 10+00 3, 08× 10−02 3, 81× 10−06 4, 39× 10−10 23 1, 00× 10+00 1, 00× 10+00 6, 06× 10−02 7, 63× 10−06 8, 78× 10−10 24 1, 00× 10+00 1, 00× 10+00 1, 17× 10−01 1, 53× 10−05 1, 76× 10−09 25 1, 00× 10+00 1, 00× 10+00 2, 21× 10−01 3, 05× 10−05 3, 51× 10−09 Cuadro 6.3: Aproximación a los autovalores de las matrices MkA para A = DA23, obtenidos a través de la iteración de Newton de f(x) = 1− 1/x 75 6. Experimentación Numérica k λ1 λ2 λ3 λ4 1 9, 90× 10−01 2, 40× 10−07 0, 00× 10+00 8, 89× 10−13 2 9, 90× 10−01 4, 80× 10−07 0, 00× 10+00 1, 78× 10−12 3 9, 90× 10−01 9, 60× 10−07 0, 00× 10+00 3, 56× 10−12 4 1, 00× 10+00 1, 92× 10−06 1, 00× 10−15 7, 11× 10−12 5 1, 00× 10+00 3, 84× 10−06 1, 00× 10−15 1, 42× 10−11 6 1, 00× 10+00 7, 68× 10−06 3, 00× 10−15 2, 85× 10−11 7 1, 00× 10+00 1, 54× 10−05 6, 00× 10−15 5, 69× 10−11 8 1, 00× 10+00 3, 07× 10−05 1, 20× 10−14 1, 14× 10−10 9 1, 00× 10+00 6, 14× 10−05 2, 40× 10−14 2, 28× 10−10 10 1, 00× 10+00 1, 23× 10−04 4, 80× 10−14 4, 55× 10−10 11 1, 00× 10+00 2, 46× 10−04 9, 50× 10−14 9, 10× 10−10 12 1, 00× 10+00 4, 91× 10−04 1, 90× 10−13 1, 82× 10−09 13 1, 00× 10+00 9, 82× 10−04 3, 81× 10−13 3, 64× 10−09 14 1, 00× 10+00 1, 96× 10−03 7, 62× 10−13 7, 28× 10−09 15 1, 00× 10+00 3, 92× 10−03 1, 52× 10−12 1, 46× 10−08 16 1, 00× 10+00 7, 83× 10−03 3, 05× 10−12 2, 91× 10−08 17 1, 00× 10+00 1, 56× 10−02 6, 09× 10−12 5, 83× 10−08 18 1, 00× 10+00 3, 10× 10−02 1, 22× 10−11 1, 17× 10−07 19 1, 00× 10+00 6, 10× 10−02 2, 44× 10−11 2, 33× 10−07 20 1, 00× 10+00 1, 18× 10−01 4, 87× 10−11 4, 66× 10−07 21 1, 00× 10+00 2, 22× 10−01 9, 75× 10−11 9, 32× 10−07 22 1, 00× 10+00 3, 95× 10−01 1, 95× 10−10 1, 86× 10−06 23 1, 00× 10+00 6, 35× 10−01 3, 90× 10−10 3, 73× 10−06 24 1, 00× 10+00 8, 66× 10−01 7, 80× 10−10 7, 46× 10−06 25 1, 00× 10+00 9, 82× 10−01 1, 56× 10−09 1, 49× 10−05 26 1, 00× 10+00 1, 00× 10+00 3, 12× 10−09 2, 98× 10−05 27 1, 00× 10+00 1, 00× 10+00 6, 24× 10−09 5, 97× 10−05 28 1, 00× 10+00 1, 00× 10+00 1, 25× 10−08 1, 19× 10−04 29 1, 00× 10+00 1, 00× 10+00 2, 50× 10−08 2, 39× 10−04 30 1, 00× 10+00 1, 00× 10+00 4, 99× 10−08 4, 77× 10−04 Cuadro 6.4: Aproximación a los autovalores de las matrices MkA para A = DA24, obtenidos a través de la iteración de Newton de f(x) = 1− 1/x 76 6. Experimentación Numérica 6.4. Experimentos numéricos de la resolución del PMCL En este segundo conjunto de experimentos, se mostrarán resultados de diversas ejecuciones de los algoritmos (5.4) y (5.5) desarrollados en el capítulo 5 tanto para matrices densas, como para matrices sparse. Se presentarán resultados comparativos tanto de las iteraciones del método de Richardson- PR2, como del tiempo de ejecución de cada uno de los algoritmos para todas las matrices de prueba. Además se comparará con la aceleración de Richardson-PR2 aplicada a las ecuaciones normales, lo cual es equivalente al método iterativo de Richardson de primer orden. 6.4.1. Formato de los experimentos numéricos de la resolución del PMCL La experimentación se dividirá en los siguientes dos grupos de matrices. Grupo de matrices A: Matrices densas, se ejecutará el algoritmo para 5 matrices de prueba generadas con las funciones especificadas al inicio del capítulo. Grupo de matrices B: Matrices sparse. Se utilizarán las matrices de la colección Harwell- Boeing presentadas anteriormente. Para cada matriz en los grupos se harán varias ejecuciones de los algoritmos, y se mostrará un cuadro comparativo con los siguientes resultados Alg: Algoritmo utilizado, alg1 corresponde al algoritmo (5.4), alg2 corresponde al algo- ritmo (5.5) y alg3 corresponde al algoritmo (4.1) aplicado a las ecuaciones normales. ksch: para alg1 este valor indica el número de iteraciones de Schulz realizadas para generar el precondicionador Mk. nit: Número de iteraciones realizadas por el método. Se denota con ∞ en el caso de que el algoritmo no haya alcanzado la convergencia luego del número máximo de iteraciones establecido. tpre: Tiempo para generar la matriz precondicionadora Mk en segundos para alg1. trich: Tiempo que tarda la iteración de Richardson-PR2 en segundos. ttot: Tiempo total de ejecución del algoritmo en segundos. err: Error relativo del resultado de la ejecución del algoritmo, respecto una solución de referencia al PMCL para cada matriz, calculado con las funciones de MATLAB c©.. En todas las matrices del grupo A, para la resolución de Ax = b se toma b = (1, 1, 1, . . . , 1)T con la dimensión adecuada para cada caso. Para los casos de estudio del grupo B, se usa el vector b proveído por la misma colección Harwell-Boeing. En ambos grupos, se utiliza como iterado inicial el vector x0 = (1, 1, 1, . . . 1)T con la dimensión adecuada para cada caso. En todos los casos de prueba, se ejecutó la iteración con una tolerancia � = 10−7 y una cantidad máxima de iteraciones Mmax = 300. Para la medición de los tiempos de ejecución se toma el tiempo promedio de 1000 ejecu- ciones de cada algoritmo. 77 6. Experimentación Numérica La solución de referencia utilizada en ambos grupos, se obtiene aplicando el operador backslash (\) de MATLAB c©. En el caso de un sistema sobredeterminado Ax = b, la función A\b retorna la solución al PMCL asociado a A y b la cual se calcula a través de una variante de la descomposición en valores singulares (SVD). Para el caso del algoritmo (5.5) se observó que en la práctica, el valor de ‖AT r(k))‖2 se estanca, aun cuando los iterados x(k) siguen convergiendo, por lo tanto, para los experimentos, se utilizó como condición de parada para dicho algoritmo, el valor ‖r(k+1)−r(k)‖2. Esta elección de condición de parada, en la práctica funcionó para conseguir el resultado con la tolerancia deseada en todos los casos. 6.4.2. Grupo de Experimentos A Las matrices de prueba de este grupo son las siguientes: DD11 = NormRand(500, 300, 1500, 300). Matriz de 1500× 300 con κ(DD11) = 15,61 DD12 = NormRand(4500, 800, 2000, 800). Matriz de 2000×800 con κ(DD12) = 1,16× 103 DD13 = RandSingV al((10, 20, . . . 5000), 1500). Matriz de 1500 × 500 con κ(DD13) = 4,99× 102 DD14 = RandSingV al(100, 50). Matriz de 100× 50 con κ(DD14) = 6,71× 107 DD15 = V andermonde((1/2, 1, 3/2, . . . 20), 5). Matriz de 40× 5 con κ(DD15) = 3,88× 107 Las figuras (6.11) a la (6.4.2) muestran el comportamiento del residual de las iteraciones de Richardson-PR2 para cada matriz. En cada gráfica se muestra el valor del residual en función de la iteración, i, para cada uno de los algoritmos. En el caso de alg1 se muestra el comportamiento del método para distintos valores de Mk. Los resultados se resumen en los cuadros (6.5) al (6.9). En general, para todas las matrices, precondicionando con Mk para k menor a los valores reportados, no mejora la convergencia del método. A partir de estos resultados, se observa, que en principio, el precondicionador tiene una desventaja en lo relativo al alto tiempo que se requiere para el precondicionamiento del sistema, el cual es mucho mayor al tiempo empleado por el método iterativo Richardson-PR2, llegando a ser en algunos casos hasta 200 veces mayor. No obstante, para las matrices de prueba utilizadas, no se obtuvo convergencia del método Richardson-PR2 aplicado a las ecuaciones normales, para el número máximo de iteraciones usando en todos los experimentos (NMAX = 300), más aún en todos los casos, para las ecua- ciones normales, el error relativo es mayor a 1 respecto a la solución de referencia obtenida con las funciones de MATLAB c©, lo cual implica que para la resolución del PMCL el precon- dicionamiento es un paso necesario aun cuando este sea de un alto costo computacional. Respecto a la mejora de la convergencia, para el caso del algoritmo 1 (precondicionamiento explícito), puede observarse que la sucesión de precondicionadoresMk ofrecen una gran mejora a la convergencia del PMCL originales, una vez que se consigue el valor de k para el cual el método de Schulz inicia su rápida convergencia. A partir de cierto valor de k crítico que depende de la matriz, se observa que cada precondicionadorMk reduce drásticamente el número 78 6. Experimentación Numérica de iteraciones que se necesitan para la convergencia. Por ejemplo, en el caso de la matriz DD11, una matriz bien condicionada, para k = 5, el sistema precondicionado conMk requiere de 106 iteraciones, y para k = 9 ya el sistema precondicionado con Mk requiere de apenas 12 iteraciones, casi 10 veces menos. Igualmente el comportamiento se observa, incluso en matrices muy mal condicionadas. Por ejemplo, para la matriz DD14 el sistema precondicionado conMk para k = 50 necesita de 98 iteraciones para la convergencia, y con k = 55, apenas 5 iteraciones de Schulz más, hacen que el sistema precondicionado requiera de apenas 6 iteraciones para la convergencia. Este mismo comportamiento se puede observar en las gráficas del residual en función de las iterciones para todas las matrices de prueba. El algoritmo 2, en el cual se utilizan las matrices Mk como sucesión de precondicionadores en el esquema de aceleración de Richardson-PR2, se puede observar un mayor paralelismo del comportamiento del residual en cada iteración del esquema de aceleración de Richardson-PR2, respecto al comportamiento de la convergencia en el método de Shculz, esto es, en las primeras iteraciones, se observa que el residual alcanza valores muy altos, llegando a ser para algunas matrices del orden de 1010, lo cual se debe a ese empeoramiento inicial del número de condición de la matriz que produce la iteración de Schulz. Para el caso de matrices bien condicionadas, como son las matrices DD11 y DD12, en las primeras iteraciones hay un descenso fuerte del residual, pero aun siendo mucho mayor a 1, mientras que en las matrices mal condicionadas como DD14 y DD15 se observa que en las primeras iteraciones se mantiene más estable el residual. Posteriormente unas pocas iteraciones en donde hay un aparente estancamiento del residual, y finalmente en las últimas 2 o 3 iteraciones, hay una caída muy rápida del residual, llegando incluso a un nivel varios órdenes menor a la tolerancia escogida para el algoritmo. Sin embargo, para el caso del algoritmo 2, el tiempo de ejecución es mucho mayor que el utilizado con la combinación de precondicionamiento directo más iteración de Richardson del algoritmo 1, por lo cual hace que no aporte nada significativo la elección del algoritmo 2. Alg ksch nit tpre × 103 trich × 103 ttot × 103 err alg1 5 106 110,93 17,71 128,64 4,65 alg1 7 36 124,08 5,66 129,74 6,6× 10−3 alg1 9 12 146,14 1,76 147,90 1,1× 10−3 alg1 12 3 181,19 0,44 181,63 9,7× 10−5 alg2 N/A 16 N/A 264,45 264,45 2,2× 10−8 alg3 N/A 237 N/A 54,04 54,04 1,3× 10−5 Cuadro 6.5: Resultados de la experimentación numérica para la resolución del PMCL para la matriz A = DD11. 79 6. Experimentación Numérica 0 20 40 60 80 100 120 10 −10 10 −5 10 0 10 5 10 10 10 15 iteracion re si d u a l alg 1 M k , k = 5 alg 1 M k , k = 7 alg 1 M k , k = 9 alg 1 M k , k = 12 alg 3 Figura 6.11: Convergencia de los algoritmos para la resolución del PMCL para A = DD11 80 6. Experimentación Numérica 0 5 10 15 20 25 30 35 40 45 10 −10 10 −5 10 0 10 5 10 10 iteración re si d u a l alg 1, M k , k = 10 alg 1, M k , k = 12 alg 1, M k , k = 14 alg 1, M k , k = 16 alg 2 Figura 6.12: Convergencia de los algoritmos para la resolución del PMCL para A = DD12 0 10 20 30 40 50 10 −15 10 −10 10 −5 10 0 10 5 10 10 iteración re si d u a l alg 1, M k , k = 23 alg 1, M k = 25 alg 1, M k = 27 alg 3 Figura 6.13: Convergencia de los algoritmos para la resolución del PMCL para A = DD13 81 6. Experimentación Numérica 0 20 40 60 80 100 10 −10 10 −5 10 0 10 5 10 10 iteracion re si d u a l alg 1, M k , k = 50 alg 1, M k , k = 52 alg 1, M k , k = 54 alg 1, M k , k = 55 alg 2 Figura 6.14: Convergencia de los algoritmos para la resolución del PMCL para A = DD14 0 5 10 15 20 25 30 35 40 45 10 −10 10 −5 10 0 10 5 10 10 10 15 iteración re si d u a l Figura 6.15: Convergencia de los algoritmos para la resolución del PMCL para A = DD15 82 6. Experimentación Numérica Alg ksch nit tpre × 103 trich × 103 ttot × 103 err alg1 7 101 63,73 5,18 68,91 0,55 alg1 10 48 66,24 2,48 68,72 0,07 alg1 12 16 71,19 0,86 72,05 0,01 alg1 14 7 76,09 0,42 76,51 1,2× 10−3 alg1 16 3 80,45 0,21 80,66 1,9× 10−5 alg2 N/A 20 N/A 119,83 119,83 6,9× 10−8 alg3 N/A ∞ N/A 54,48 54,48 > ×102 Cuadro 6.6: Resultados de la experimentación numérica para la resolución del PMCL para la matriz A = DD12. Alg ksch nit tpre × 103 trich × 103 ttot × 103 err alg1 21 165 3013,75 40,90 3054,65 5,8× 10−7 alg1 23 47 3078,32 11,63 3089,95 1,4× 10−7 alg1 25 14 3356,05 4,04 3360,09 3,6× 10−8 alg1 27 5 3562,03 1,33 3563,36 1,5× 10−8 alg2 N/A 32 N/A 4931,93 4931,93 1,1× 10−9 alg3 N/A ∞ N/A 54,48 54,48 5,6 Cuadro 6.7: Resultados de la experimentación numérica para la resolución del PMCL para la matriz A = DD13. Alg ksch nit tpre × 103 trich × 103 ttot × 103 err alg1 50 98 20,05 2,34 22,39 0,18 alg1 52 27 21,54 0,84 22,38 0,18 alg1 54 11 23,06 0,32 23,38 0,18 alg1 55 6 24,34 0,11 24,45 0,18 alg2 N/A 59 N/A 16,89 19,89 0,18 alg3 N/A ∞ N/A 7,23 7,23 5,6 Cuadro 6.8: Resultados de la experimentación numérica para la resolución del PMCL para la matriz A = DD14. Alg ksch nit tpre trich ttot err alg1 34 35 0,65 0,26 0,91 2,4× 10−6 alg1 35 11 0,79 0,21 1,00 2,2× 10−6 alg1 36 6 0,81 0,14 0,95 1,9× 10−6 alg2 N/A 43 N/A 1,71 1,71 7,2× 10−7 alg3 N/A ∞ N/A 5,42 5,42 > 10× 102 Cuadro 6.9: Resultados de la experimentación numérica para la resolución del PMCL para la matriz A = DD15. 83 6. Experimentación Numérica 6.4.3. Grupo de Experimentos B Las matrices de prueba de este grupo son las siguientes: DD21 = WELL1033. Matriz de 1033× 320 con κ(DD21) = 1,66× 107 DD22 = WELL1850. Matriz de 1850× 712 con κ(DD22) = 1,13× 102 DD23 = ILL1033. Matriz de 1033× 320 con κ(DD23) = 1,8× 104 DD24 = ILL1850. Matriz de 1850× 712 con κ(DD24) = 1,40× 103 Las figuras (6.16) a la (6.4.3) muestran el comportamiento del residual de las iteraciones de Richardson-PR2 para cada matriz. En cada gráfica se muestra el valor del residual en función de la iteración, i, para cada uno de los algoritmos. En el caso de alg1 se muestra el comportamiento del método para distintos valores de Mk. Los resultados se resumen en los cuadros (6.10) al (6.13). Para el caso de matrices sparse, se puede observar un comportamiento análogo al caso de matrices densas en cuanto a la mejora de la convergencia del método de Richardson-PR2 con el precondicionamiento como en el algoritmo 1, así como para el algoritmo 2. No obstante, puede verse que la diferencia de tiempo entre la generación del precondicionador es mayor en el caso general. Esto se debe a que en el caso de matrices sparse evidentemente, la iteración de Richardson-PR2 es más rápida pues se puede aprovechar el hecho de que no es necesario construir explícitamente, la matriz ATA para el caso de las ecuaciones normales, mientras que el precondicionamiento con Mk hace que la matriz se vuelva densa. Para el grupo de prueba de matrices sparse igualmente se observa que el algoritmo 2 no ofrece una mejora sustancial al precondicionaiento explícito del algoritmo 1. Alg ksch nit tpre × 103 trich × 103 ttot × 103 err alg1 18 120 211,28 12,04 223,32 1,3× 10−10 alg1 20 33 227,04 3,26 230,30 5,7× 10−11 alg1 22 11 254,64 1,15 255,79 4,2× 10−11 alg1 24 4 284,69 0,48 285,17 3,9× 10−11 alg2 N/A 27 N/A 360,34 360,34 2,6× 10−11 alg3 N/A ∞ N/A 319,52 319,52 1,5 Cuadro 6.10: Resultados de la experimentación numérica para la resolución del PMCL para la matriz A = DD21. 84 6. Experimentación Numérica 0 20 40 60 80 100 120 10 −12 10 −10 10 −8 10 −6 10 −4 10 −2 10 0 10 2 10 4 iteración re si d u a l alg 1, M k , k = 18 alg 1, M k , k = 20 alg 1, M k , k = 22 alg 1, M k , k = 24 alg 2 Figura 6.16: Convergencia de los algoritmos para la resolución del PMCL para A = DD21 0 20 40 60 80 100 120 140 10 −12 10 −10 10 −8 10 −6 10 −4 10 −2 10 0 10 2 10 4 iteración re si d u a l alg 1, M k , k = 18 alg 1, M k , k = 20 alg 1, M k , k = 22 alg 1, M k , k = 24 alg 2 Figura 6.17: Convergencia de los algoritmos para la resolución del PMCL para A = DD22 85 6. Experimentación Numérica Alg ksch nit tpre × 103 trich × 103 ttot × 103 err alg1 18 121 2127,39 63,84 2191,23 1,5× 10−10 alg1 20 34 2516,00 24,65 2540,65 6,8× 10−11 alg1 22 12 2799,13 7,81 2806,94 6,1× 10−11 alg1 24 4 2856,82 2,90 2859,72 6,1× 10−11 alg2 N/A 27 N/A 3508,61 3508,61 3,8× 10−11 alg3 N/A ∞ N/A 347,72 347,72 0,7 Cuadro 6.11: Resultados de la experimentación numérica para la resolución del PMCL para la matriz A = DD22. Alg ksch nit tpre × 103 trich × 103 ttot × 103 err alg1 30 165 361,22 2,43 363,65 1,5× 10−7 alg1 32 67 408,54 6,68 415,22 1,5× 10−7 alg1 34 11 413,59 2,25 415,84 1,5× 10−8 alg1 36 6 429,62 0,70 430,32 1,5× 10−8 alg2 N/A 40 N/A 592,08 592,08 1,1× 10−9 alg3 N/A ∞ N/A 57,64 57,64 0,9 Cuadro 6.12: Resultados de la experimentación numérica para la resolución del PMCL para la matriz A = DD23. 0 50 100 150 200 250 10 −12 10 −10 10 −8 10 −6 10 −4 10 −2 10 0 10 2 10 4 iteración re si d u a l alg 1, M k , k = 30 alg 1, M k , k = 32 alg 1, M k , k = 35 alg 1, M k , k = 36 alg 2 Figura 6.18: Convergencia de los algoritmos para la resolución del PMCL para A = DD23 86 6. Experimentación Numérica Alg ksch nit tpre × 103 trich × 103 ttot × 103 err alg1 18 181 2870,18 108,84 2929,02 9,5× 10−10 alg1 20 54 2931,49 30,91 2962,40 9,5× 10−10 alg1 22 18 3034,97 10,27 3045,24 9,5× 10−10 alg1 24 6 3216,58 4,08 3219,66 9,5× 10−10 alg2 N/A 34 N/A 4088,09 4088,09 9,5× 10−10 alg3 N/A ∞ N/A 314,21 314,21 0,79 Cuadro 6.13: Resultados de la experimentación numérica para la resolución del PMCL para la matriz A = DD24. 0 20 40 60 80 100 120 140 160 180 200 10 −14 10 −12 10 −10 10 −8 10 −6 10 −4 10 −2 10 0 10 2 10 4 iteración re si d u a l alg1 M k , k = 24 alg1, M k , k = 26 alg1, M k , k = 28 alg1, M k , k = 30 alg2 Figura 6.19: Convergencia de los algoritmos para la resolución del PMCL para A = DD24 87 6. Experimentación Numérica 88 Capítulo 7 Conclusiones Ten siempre a Itaca en tu mente. Llegar allí es tu destino. Mas no apresures nunca el viaje. Mejor que dure muchos años y atracar, viejo ya, en la isla, enriquecido de cuanto ganaste en el camino sin aguantar a que Itaca te enriquezca. Itaca te brindó tan hermoso viaje. Sin ella no habrías emprendido el camino. Pero no tiene ya nada que darte. Aunque la halles pobre, Itaca no te ha engañado. Así, sabio como te has vuelto, con tanta experiencia, entenderás ya qué significan las Itacas. Constantino Kavafis En este último capítulo se presentarán algunas conclusiones obtenidas luego de la realiza- ción del trabajo y a partir de la experimentación numérica. El principal aporte del trabajo, es el estudio y aplicación del precondicionamiento a sistemas lineales en matrices rectangulares. Si bien la tendencia actual del análisis numérico en el área de resolución de sistemas lineales es la búsqueda de técnicas de precondicionamiento y precondicionadores para el caso general; en la literatura no se encontró ninguna referencia a precondicionadores rectangulares en el contexto del PMCL. En efecto, y tal como se vio en el capitulo 2, los precondicionadores genericos como la factorización de Cholesky incompleta o la factorización LU incompleta, se aplican al sistema de ecuaciones normales, el cual es un sistema de n ecuaciones con n incógnitas. Sin embargo, para el caso de las ecuaciones normales, estos precondicionadores presentan serios problemas de estabilidad numérica (Ver [34]) Por consiguiente, esta nueva idea de un precondicionador rectangular que no presenta problemas de estabilidad numérica de precondicionadores clásicos aplicados a las ecuaciones normales adquiere relevancia, y se deja como un punto de inicio para la investigación de una nueva línea de precondicionadores rectangulares para el PMCL. Por otro lado, se planteó una extensión de un método iterativo, que fue concebido en principio para matrices cuadradas, como lo es el esquema de acelereación de convergencia Richardson-PR22, a matrices rectangulares. A pesar de que los resultados de la convergencia de este método extendido no presentaron una mejora sustancial en tiempo para las matrices de 89 7. Conclusiones prueba, debido a que no se consiguió una expresión que eliminara los productos matriz-matriz asociados a la iteración de Schulz en el algoritmo (5.5) (apenas se logro reducir el cálculo de dos productos matriz-matriz a uno solo) sí se observó una mejora interesante en cuanto a la convergencia a la solución del problema: el método produce inicialmente una serie de iteraciones en apariencia inútiles, pero finalmente, al conseguir corregir el condicionamiento, el método alcanza una convergencia muy acelerada en pocas iteraciones, usualmente en 2 o 3 iteraciones, el residual se reduce en varios órdenes de magnitud. Respecto al precondicionamiento explícito al PMCL propuesto en el algoritmo (5.4), se observa que la aproximación a la matriz pseudoinversa obtenida por el método de Schulz es un precondicionador muy efectivo, aun cuando su cálculo es costoso en tiempo, debido, nuevamente a la necesidad de efectuar productos matriz-matriz. No obstante, aun cuando el tiempo invertido en precondicionar el problema sea varias veces mayor al tiempo empleado por el método iterativo, debe observarse, que para el caso de las ecuaciones normales, no se observó convergencia para ninguna de las matrices de prueba utilizadas al intentar resolver el sistema de ecuaciones normales sin precondicionar. Más aun, se observó que en dicho sistema, el método de aceleración de Richardson-PR2 tiende a estancarse luego de algunas iteraciones. De lo anterior puede concluirse que a pesar de que el precondicionamiento de un sistema lineal puede implicar un alto costo computacional, en muchas ocasiones es un paso indispen- sable para obtener la solución a dicho sistema; máxime en el caso de problemas que son mal condicionados y típicamente inestables como el caso de las ecuaciones normales aplicadas al PMCL. Además, el precondicionamiento evita la necesidad de elegir métodos iterativos que tengan asociado un alto costo computacional para el cálculo de direcciones de búsqueda en cada iteración, tal como los métodos de proyección. En efecto, el método iterativo utilizado en este trabajo: el esquema de aceleración de convergencia Richardson-PR2, utiliza como direc- ción de búsqueda en la iteración n+1 el vector residual de la iteración n escalado con un valor real adecuado, con lo cual, se puede utilizar la mayor parte de la capacidad computacional en la obtención de un buen precondicionador, que generalmente garantizará la convergencia a la solución correcta del problema. 90 Bibliografía [1] Akritas, A.G., Malaschonok, G.I., y Vigklas, P.S. The SVD-Fundamental theorem of linear algebra. Nonlinear Analysis: Modelling and Control Vol. 11 No 2, 2006. [2] Barrett, R., Berry, M., Chan, T. F., Demmel, J., Donato, J., Dongarra, J., Eijkhout, V., Pozo, R., Romine, C., y Van der Vorst, H. Templates for the Solution of Linear Systems: Building Blocks for Iterative Methods, 2nd Edition. SIAM, Philadelphia, PA, 1994. [3] Benzi, M. Preconditioning techniques for large linear systems: a survey. Journal of computational physics, páginas 418–477, 2002. [4] Benzi, M., Giraud, L., y All, G. Sparse approximate inverse preconditioning for dense linear systems arising in computational electromagnetics. Numerical Algorithms, 16:1–15, 1997. [5] Björck, A. Numerical methods for least squares problem. Society for Industrial and Applied Mathematics, 1996. [6] Bourden, R. y Douglas-Faires, J. Análisis Numérico. International Thompson Editores, sexta edición, 1998. [7] Brezinski, C. Variations on Richardson’s method and acceleration. Bull. Soc. Math. Belg., páginas 33–44, 1996. [8] Brezinski, C. Projection methods for systems of equations. Elsevier Science Publishers B. V., 1997. [9] Duff, I., Grimes, R., y Lewis, J. Users’ guide for the Harwell-Boeing sparse matrix collec- tion (Release I). Informe técnico, European Centre for Rresearch and Advanced Training in Scientific Computation (CERFACS), 1992. [10] Forsman, K., Gropp, W., Kettunen, L., Levine, D., y Salonen, J. Solution of dense systems of linear equations arising from integral equation formulations. Antennas and propagation magazine, 37:96–100, 1995. [11] Freud, R. W., Golub, G. H., y Nachtigal, N. Iterative solution of linear systems. Acta Numerica, 1992. [12] Golub, G. Numerical methods for solving least squares problems. Numer. Math, 7:206– 216, 1965. 91 [13] Golub, G., Klema, V., y Stewart, G. Rank degeneracy and least squares problems. Informe técnico, Computer Sciences Department, Stanford University, 1976. [14] Golub, G. y Plemmons, R. Large scale geodetic least squares adjustment by dissection and orthogonal decomposition. Informe técnico, Computer Sciences Department, Stanford University, 1979. [15] Golub, G. y Van Loan, C. Matrix Calculations. Johns Hopkings University Press, tercera edición, 1996. [16] Golub, G. y Varga, R. Chebyshev semi-iterative methods, successive overrelaxation ite- rative methods, and the second order Richardson iterative methods part I. Numer. Math, 3:147–156, 1961. [17] Golub, G. y Varga, R. Chebyshev semi-iterative methods, successive overrelaxation itera- tive methods, and the second order Richardson iterative methods part II. Numer. Math, 3:157–168, 1961. [18] Grossman, S. Álgebra Lineal. Mac Graw Hill Latinoamericana, quinta edición, 1999. [19] Herón, B., Issard-Rorch, F., y Picard, C. Analyse numérique: Exercises et problèmes corrigés. Dunod, París, 1999. [20] Hestenes, M. y Stiefel, E. Methods of conjugate gradients for solving linear systems. Journal of Research of the National Bureau of Standards, 49:409–436, 1952. [21] Kariya, T. y Kurata, H. Generalized Least Squares. John Wiley and Sons Ltd., 2004. [22] Kincaid, D. y Cheney, W. Numerical Analysis, Mathematics of scientific computing. Brooks/Cole Publishing Company, 1991. [23] La Cruz, W. y Raydán, M. Residual iterative schemes for large-scale nonsymmetric positive definite linear systems. Computational and applied mathematics, 49:151–173, 2008. [24] Leach, S. Singular value decomposition, a primer. Informe técnico, Department of Com- puter Science, Brown University, Providence, USA, 1997. [25] Lin, C. y Saigal, R. An incomplete Cholesky factorization for dense matrices. Applied Numerical Mathematics, 536:536–558, 2000. [26] Marsden, J. y Hoffman, M. Elementary Classical Analisys. W. H. Freeman, 1974. [27] Molina, B. y Raydán, M. Métodos iterativos tipo Krylov para sistemas lineales. Centro de Estudios Avanzados, Instituto Venezolano de Investigaciones Científicas (IVIC) Caracas - Venezuela, 2004. ISBN 980-261-078-X. [28] Nievergelt, Y. A tutorial history of least squares with applications to astronomy and geodesy. J. Comput. Appl. Math., 121(1–2):37–72, 2000. [29] Opfer, G. y Schober, G. Richardson’s iteration for nonsymmetric matrices. Linear Algebra Appl, 58:343–361, 1984. 92 [30] Ortega, J. Numerical Analysis: A second course. Classics in applied mathematics. Society for Industrial and Applied Mathematics, Philadelphia, PA, USA, 1990. [31] Ortega, J. y Rheinboldt, W. Iterative solution of nonlinear equations in several varia- bles. Classics in applied mathematics. Society for Industrial and Applied Mathematics, Philadelphia, PA, USA, 2000. [32] Richardson, L.F. The approximate arithmethical solution by finite differences of physical problems involving differential equations, with application to the stress in a masonry dam. Philos. Trans. Roy. Soc. London Series A, páginas 307–357, 1910. [33] Saad, Y. Iterative methods for sparse linear systems. Y. Saad, segunda edición, 2000. [34] Saad, Y. y Sosonkina, M. Enhanced preconditioners for large sparse least squares problem. Informe técnico, Minnesota Supercomputer Institute, 2001. [35] Saad, Y. y Van der Vorst, H.A. Iterative solutions of linear systems in the 20th century. Informe técnico, Minnesota Supercomputer Institute, 1999. [36] Schmidt, E. y Stewart, G. W. On the early history of singular value decomposition. Informe técnico, Institute of Advanced Computer Studies, University of Maryland, 1992. [37] Schulz, G. Iterative berechung der reziproken matrix. ZAMM - Zeitschrift für Angewandte Mathematik und Mechanik, páginas 57–59, 1933. [38] Sheynin, O. On the history of the principle of least squares. Archive for history of exact sciences, 49:39–54, 1993. [39] Stoer, J. y Bulirsch, R. Introduction to numerical analysis. Springer-Verlag, 1993. [40] Wolberg, J. Data analysis using the method of the least squares. Springer-Verlag, 2006. [41] Wolf, P. y Ghilani, C. Adjustment Computations: Statistics and Least Squares in Surveying and GIS. John Wiley and Sons Ltd., 1997. [42] Yan, Y. Sparse preconditioned iterative methods for dense linear systems. SIAM J. Sci. Comp, 15:1190–1200, 1994. [43] Young, D.M. On Richardson’s method for solving linear systems with positive definite matrices. J. Math Phys, 32:243–255, 1954. [44] Young, D.M. Iterative solution of large linear systems. Academic Press, New York, 1971. 93