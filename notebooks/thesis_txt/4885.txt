UNIVERSIDAD CENTRAL DE VENEZUELA FACULTAD DE CIENCIAS ESCUELA DE COMPUTACIÓN ACELERACIONES DE LOS MÉTODOS DE PROYECCIONES ALTERNANTES Y SIMULTÁNEAS EN EL PROBLEMA DE ASIGNACIÓN DE AUTOVALORES Trabajo Especial de Grado presenta- do ante la ilustre Universidad Central de Venezuela por Gledys Sulbarán para optar al t́ıtulo de Licenciada en Computación. Tutores: Dr. Luis Manuel Her- nandez y Dra. Joali Moreno Caracas, Venezuela Febrero 2014 2 Nosotros, los abajo firmantes, designados por la Universidad Central de Venezue- la como integrantes del Jurado Examinador del Trabajo Especial de Grado titulado “Aceleraciones de los Métodos de Proyecciones Alternantes y Simultáneas en el Problema de Asignación de Autovalores”, presentado por Gledys Sulbarán, titular de la Cédula de Identidad 17.313.160, certificamos que este trabajo cumple con los requisitos exigidos por nuestra Magna Casa de Estudios para optar al t́ıtulo de Li- cenciada en Computación. Dr. Luis Manuel Hernández. Tutor Dra. Joali Moreno. Tutora Dr. Marcos Raydan. Jurado Dr. Otilio Rojas. Jurado Agradecimientos A Dios por darme salud y fuerzas. A mi tutores, el profesor Luis Manuel Hernández Ramos y el profesora Joali Moreno por toda la atención, la paciencia y el conocimiento que siempre me brindaron. Muchas gracias por creer en mi. A mi madre Gladys Goyo, por estar a mi lado en todo momento. A mi hermanita Glendy por apoyarme y ayudarme cuando la necesito. A mis queridos amigos Gisell, Freddy, Yoel, Mariolys, Adolfo, Adelis, Lilibeth, Nelson, Augusto. 3 Índice general Introducción 1 1. Introducción 2 2. Métodos de Proyecciones Alternantes 4 2.1. Introducción . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 2.2. Proyecciones Alternantes . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 2.2.1. Algoritmo MAP . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 2.3. Los Métodos de Kaczmarz y Cimmino . . . . . . . . . . . . . . . . . . . 7 2.3.1. Método de Kaczmarz . . . . . . . . . . . . . . . . . . . . . . . . . 7 2.3.2. Método de Cimmino . . . . . . . . . . . . . . . . . . . . . . . . . 8 2.4. Velocidad de Convergencia . . . . . . . . . . . . . . . . . . . . . . . . . . 9 2.4.1. Ángulo entre Subespacios . . . . . . . . . . . . . . . . . . . . . . 9 2.4.2. Velocidad de Convergencia para Proyecciones Alternantes . . . . . 9 3. Proyecciones Alternantes para el PAMM 12 3.1. Introducción . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 3.2. Sistema matricial de segundo orden en problemas de control . . . . . . . 13 3.3. Asignación parcial de autovalores en problemas de control . . . . . . . . 14 3.4. Formulación del problema . . . . . . . . . . . . . . . . . . . . . . . . . . 15 3.5. Exigir una estructura dada . . . . . . . . . . . . . . . . . . . . . . . . . . 19 3.6. Aceleración del MAP con ideas Appleby y Smorlarski . . . . . . . . . . . 22 i 1 4. Métodos tipo residual para sistemas no lineales aplicado al PAMM 27 4.1. Introducción . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 4.2. Aceleración para variedades lineales . . . . . . . . . . . . . . . . . . . . . 28 4.3. Métodos tipo residual para sistemas no lineales . . . . . . . . . . . . . . 29 4.4. Aceleración del Método de Cimmino en variedades lineales para el PAMM usando DF-SANE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 4.5. Aceleración del Método de Proyecciones Alternantes en variedades lineales para el PAMM usando DF-SANE . . . . . . . . . . . . . . . . . . . . . . 33 4.5.1. El método DF-SANE manteniendo un patrón de raleza dado . . . 35 5. Resultados numéricos 36 5.1. Experimento 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37 5.2. Experimento 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39 5.3. Experimento 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41 5.4. Experimento 4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43 6. Conclusiones y Recomendaciones 45 Bibliograf́ıa 46 Caṕıtulo 1 Introducción El estudio y análisis de vibraciones tiene como propósito combatir el fenómeno de la resonancia en estructuras. Los sistemas estructurales son representados por ecuaciones diferenciales de segundo orden. Las vibraciones están relacionadas con los autovalores que surgen del modelo. En este análisis los sistemas estructurales son representados por un sistema matricial de segundo orden de la siguiente forma: Mẍ(t) +Dẋ(t) +Kx(t) = 0. (1.1) donde M,D,K ∈ Rn×n representan la masa, la amortiguación y la rigidez respectiva- mente. Los vectores ẍ(t), ẋ(t), x(t) ∈ Rn son los vectores de aceleración, velocidad y desplazamiento respectivamente (ver detalle en [19]). Además ẋ(t) y ẍ(t) representan la primera y segunda derivada de x(t) con respecto al tiempo. Una solución fundamental para el sistema (1.1) es x(t) = veλt donde el escalar λ y el vector v satisfacen la siguiente ecuación cuadrática de autovalores (Quadratic Eigenvalue Problem - QEP) (λ2M + λD +K)v = 0, (1.2) la cual posee 2n autovalores y 2n autovectores. Estos autovalores están relacionados con las frecuencias naturales del sistema homogéneo. Sólo algunos autovalores causan un fenómeno no deseado que es conocido como reso- nancia. De alĺı la importancia de resolver el problema cuadrático de asignación parcial 2 Introducción 3 de autovalores, que consiste en reemplazar los autovalores “no deseados” por autovalores “deseados”, manteniendo invariante el resto del espectro, evitando aśı el fenómeno de resonancia. El Problema de Actualización en el Modelo Matricial (PAMM) consiste en actualizar algunas matrices que surgen en el modelo matricial de segundo orden, de manera que los autovalores asociados al modelo sean los deseados y al mismo tiempo preservar la estructura y simetŕıa de las matrices involucradas. En el trabajo doctoral [18], se plantea el PAMM como un problema de optimización no lineal con restricciones, y en dicho trabajo se propone el uso del método de proyecciones alternantes (MAP) y sus variantes para resolver dicho problema, además se plantea una estrategia de aceleración basada en una técnica propuesta por Appleby y Smorlarski [1]. Los métodos de proyecciones alternantes y simultáneas suelen tener una convergencia lenta. En [13] se estudia el problema de mejor aceleración en variedades lineales. Los autores proponen mejorar a los algoritmos de proyecciones alternantes y de Cimmino utilizando técnicas de optimización. En el caso de variedades lineales se usan técnicas ba- sadas en métodos tipo residual, como por ejemplo el SANE y el DF-SANE, estudiadas y analizadas por la Cruz et al. [5][6], para acelerar los métodos basados en proyecciones. Es- tas técnicas pueden igualmente ser empleados para resolver el Problema de Actualización en el Modelo Matricial. En este trabajo proponemos la aplicación y comparación de estas técnicas recientes propuestas en [13] para resolver el PAMM con métodos basados en proyeciones como propone [18] y además se realizan experimentos numéricos con una nueva propuesta basada en la aceleración del MAP mediante el DF-SANE. Caṕıtulo 2 Métodos de Proyecciones Alternantes 2.1. Introducción En este caṕıtulo se resumen algunos conceptos acerca el Método de Proyecciones Alternantes (MAP, por sus siglas en inglés). Este caṕıtulo se basa principalmente en el libro de Escalante y Raydan [11] y en la tesis doctoral de Hernández-Ramos [13]. 2.2. Proyecciones Alternantes El Método de Proyecciones Alternantes en su versión original fue desarrollado por Von Neumann en 1933 [20], quien trató el problema de encontrar la proyección de un punto dado en un espacio de Hilbert en la intersección de dos subespacios cerrados. Posteriormente, el método de Von Neumann fue extendido por Halperin [12] a una familia finita de subespacio cerrados de H. Igualmente, en 1937, Kaczmarz [15] hab́ıa propuesto un método análogo al método de Halperin, pero solo en el caso particular de sistemas de ecuaciones lineales. Sea H un espacio de Hilbert y sea M un subespacio cerrado de H. Denotemos por PM 4 Métodos de Proyecciones Alternantes 5 el operador de Proyección ortogonal sobre M. Este es un operador lineal, autoadjunto (〈Px, y〉 = 〈x, Py〉) e idempotente (P 2 ≡ P ). PM es tal que: ‖ x− PM(x) ‖= d(x,M) donde d(x,M) = inf{‖ x− y ‖, y ∈ M}, es decir PM(x) es el punto más cercano a x en M. Una propiedad que lo caracteriza en el caso de subespacios es: 〈x− PM(x), y〉 = 0,∀y ∈M. Una definición que utilizaremos luego es la del complemento ortogonal de un conjunto. El Complemento ortogonal de M (denotado por M⊥) se define como: M⊥ := {y ∈ H : 〈x, y〉 = 0,∀x ∈M}. Es decir, x− PM(x) es ortogonal a M . Sean A y B subespacios cerrados de H. Entonces PAPB = PBPA si y sólo si PAPB = PA∩B. Es decir, PA y PB conmutan si y solo si su composición es también una proyección ortogonal. Von Neumann estuvo interesado en el caso donde PA y PB no conmutan, probando el siguiente resultado: Teorema 2.1. (Von Neumann [1933]) [20] Sean A y B subespacios cerrados en un es- pacio de Hilbert H. Entonces, para cada x ∈ H ĺım n→∞ (PBPA) n(x) = PA∩B(x). 2.2.1. Algoritmo MAP El Teorema de Von Neumann induce un algoritmo iterativo (Algoritmo 1) para cal- cular la proyección en la intersección de dos subespacios cerrados. La interpretación geométrica de este método nos dice que para encontrar la mejor aproximación a un punto x desde A∩B, primero se proyecta x sobre A, el vector obtenido se proyecta sobre B, y aśı se continua proyectando de forma alternada sobre A y B. La Métodos de Proyecciones Alternantes 6 Figura 2.1: Proyecciones Alternantes sucesión de los elementos generados de esta manera converge a PA∩B(x). La utilidad práctica de este procedimiento proviene del hecho de que es más sencillo proyectar sobre cada subespacio individualmente que proyectar sobre la intersección. Algoritmo 1 MAP 1: Dado x0 2: for n = 1, ..., do 3: xn = PBPAxn−1 = (PBPA) nx 4: end for El resultado obtenido por Von Neumann fue extendido posteriormente por Halperin [12] a una familia finita de subespacio cerrados. Teorema 2.2. (Halperin 1962) . Sean M1,M2, . . . ,Mr subespacios cerrados en H y M = ∩ri=1Mi. Entonces ĺım n→∞ (PMrPMr−1 . . . PM2PM1) n(x) = P∩ri=1Mi(x) para cada x ∈ H. Demostración. Ver [11]. Es importante mencionar que la demostración del teorema 2.2 cuando r ≥ 3 es dife- rente a la dada por Von Neumann para el teorema 2.1 cuando r = 2. Métodos de Proyecciones Alternantes 7 En 1983, Frank Deutsch [8] hace una generalización del teorema 2.2 al caso donde los conjuntos involucrados son un número finito de variedades lineales. Teorema 2.3. (Deutsch, 1983 [8]) Si M1,M2, . . . ,Mr son variedades lineales cerradas en un espacio de Hilbert H. Entonces, para cada x ∈ H. ĺım n→∞ (PMrPMr−1 . . . PM2PM1) n(x) = P⋂r i=1Mi (x). 2.3. Los Métodos de Kaczmarz y Cimmino 2.3.1. Método de Kaczmarz Este método fue propuesto originalmente por Kaczmarz [15] quien estableció su con- vergencia para resolver sistemas de ecuaciones lineales Ax = b. Dicho método es un caso particular del método propuesto posteriormente por Halperin para el caso espećıfico de resolución de sistemas de ecuaciones lineales. Consideremos el problema de resolver los sistemas de ecuaciones lineales Ax = b. Donde A es una matriz real m× n, x ∈ Rn y b ∈ Rm. Sea Hi = {x ∈ Rn : 〈ai, x〉 = bi}. Aqúı ai = (ai1, ai2, . . . , ain) ∈ Rn, x ∈ Rn y bi ∈ R i = 1, · · · ,m. Encontrar una solución de Ax = b es equivalente a encontrar un punto en la inter- sección de todos los hiperplanos Hi, es decir en ∩mi=1Hi. Para encontrar tal punto, por el método de Kaczmarz, para un iterado inicial x0 ∈ Rn arbitrario, se define la iteración: xn = (PHmPHm−1 . . . PH2PH1)xn−1 (n = 1, 2, . . .) Entonces por el teorema 2.3 se tiene que Métodos de Proyecciones Alternantes 8 {xn}n∈N −→ P⋂mi=1Hi(x0) =: y0. Es decir, la iteración del método de Kaczmarz converge a un punto y0 ∈ Rn que satisface la ecuación Ax = b. Note que para cada i, i = 1, ...,m, ai es ortogonal a Hi. Si z 6∈ Hi, la proyección sobre Hi viene dada por la siguiente expresión: PHi(z) = z + bi − 〈ai, z〉 〈ai, ai〉 ai. Es importante resaltar que si z ∈ Hi la proyección sobre Hi es el mismo z. 2.3.2. Método de Cimmino Este método fue propuesto por Cimmino [4], quien estableció la convergencia para la solución de sistemas de ecuaciones lineales Ax = b, con A ∈ Rnxn y b ∈ Rn Para un x0 ∈ Rn, un paso t́ıpico del método de Cimmino es el siguiente, xk+1 = 1 n n∑ i=1 PHi(xk) donde Hi = {x ∈ Rn :< ai, x >= bi}. El método de Cimmino ha sido extendido y generalizado para variedades lineales. Kam- merer y Nashed en [16] generalizan el método de la siguiente manera: Sean V1, V2, ..., Vn variedades lineales cerradas de un espacio de Hilbert H, y dado x0 ∈ H. El método de Cimmino viene dado entonces por la secuencia, xk+1 = 1 n n∑ i=1 PVi(xk) Tal como en el método de Kaczmarz, las iteraciones de este método convergen a PV (x0), donde V = ⋂n i=0 Vi. Métodos de Proyecciones Alternantes 9 2.4. Velocidad de Convergencia La velocidad de convergencia del Método de Proyecciones Alternantes ha sido estu- diada por varios autores, entre ellos: Smith, Solmon y Wagner [21], Kayalar y Weinert [17] y F. Deutsch [10]. En general, la velocidad de convergencia del MAP es r-lineal y depende del ángulo entre los subespacios o variedades lineales involucrados. 2.4.1. Ángulo entre Subespacios Recordemos que si x, y ∈ H, el ángulo θ entre x e y está definido como el ángulo cuyo coseno está dado por: cosθ = 〈x, y〉 ‖x‖‖y‖ La siguiente definición introducida originalmente por Friedrichs en 1937, es la más utili- zada en la literatura del MAP para trabajar con el ángulo entre subespacios. Definición 2.4. (Friedricks, [1937]) [9] El ángulo θ(M,N) entre dos subespacios cerra- dos M y N de H es el ángulo en [0, π/2] cuyo coseno C(M,N) = cos θ(M,N) esta dado por: C(M,N) := sup{|〈x, y〉| : x ∈M ∩ (M ∩N)⊥, ‖ x ‖≤ 1, y ∈ N ∩ (N ∩M)⊥, ‖ y ‖≤ 1} 2.4.2. Velocidad de Convergencia para Proyecciones Alternan- tes Caso dos subespacios En el siguiente teorema se demuestra que la velocidad de convergencia del Método de Proyecciones Alternantes depende del ángulo entre los subespacios involucrados. De dicho teorema tenemos que si el ángulo entre los subespacios es pequeño el método se hace lento en el sentido de que se requerirán muchas iteraciones para su convergencia. [10] Métodos de Proyecciones Alternantes 10 Teorema 2.5. (Aronszajn 1950)[2]: Sean M1 y M2 subespacios cerrados en H cuyo coseno C(M1,M2) = cos θ(M1,M2). Entonces, para cada x ∈ H ‖ (PM2PM1) n(x)− PM1∩M2(x) ‖≤ C 2n−1 ‖ x− PM1∩M2(x) ‖≤ C 2n−1 ‖ x ‖ para n = 1, 2, ... por otra parte, la constante C2n−1 es la más pequeña posible. En la siguiente figura se puede apreciar la dependencia del ángulo en la velocidad de convergencia para el caso de dos subespacios A y B. En el ejemplo de la figura 2.2, el método realiza menos iteraciones para llegar a la solución mientras que el ejemplo 2.3 converge más lentamente ya que realiza más iteraciones para llegar a la solución, es decir, el método es más rápido cuando el ángulo entre los subespacios está más cerca de la ortogonalidad Figura 2.2: Método MAP: ángulo grande Figura 2.3: Método MAP: ángulo pequeño Métodos de Proyecciones Alternantes 11 Caso n subespacios Smith, Solmon, and Wagner(1977) determinaron una cota análoga en el caso de más de dos subespacios. Teorema 2.6. (Smith, Solmon, Wagner 1977 [21]): Sean M1,M2, ...,Mk subespacios cerrados en H y Sea M = ∩ki=1Mi. Entonces, para cada x ∈ H y un entero n ≥ 1 ‖ (PMkPMk−1 · · ·PM1) n(x)− P∩ki=1Mi(x) ‖≤ C n ‖ x ‖ Donde C = [1− k−1∏ i=1 sen2 θi] 1/2 Y θi es el Ángulo entre el subespacio Mi y ∩kj=i+1Aj Caṕıtulo 3 Proyecciones Alternantes para el PAMM 3.1. Introducción En este caṕıtulo nos apoyamos en el trabajo doctoral [18] y en el art́ıculo [19], en donde se plantea un nuevo enfoque para resolver el Problema de Actualización en el Modelo Matricial (PAMM), el cual consiste en actualizar algunas matrices que surgen en el modelo matricial de segundo orden, de manera tal que se sustituyen los autovalores y autovectores “malos” por los “deseados”, planteando el PAMM como un problema de optimización no lineal con restricciones se propone aplicar el método de proyecciones alternantes (MAP) y además se muestra una variación de aceleración de Appleby y Smorlarski [1] planteada en [18] para resolver el problema no lineal, conservando las propiedades de las matrices involucradas. 12 Proyecciones Alternantes para el PAMM 13 3.2. Sistema matricial de segundo orden en proble- mas de control Se plantea el siguiente sistema matricial de segundo orden, el cual es obtenido a través de una semidiscretización de la ecuación de movimiento mediante el modelo de elemento finito aplicado a un problema de control: Mẍ(t) +Dẋ(t) +Kx(t) = F (t), donde ẋ(t) y ẍ(t) denotan la primera y segunda derivada con respecto al tiempo t. Las matrices M , D y K de n × n son llamadas matriz de masa, amortiguación y de rigidez respectivamente cuyas caracteŕısticas son; M es simétrica positiva definida (M = M t > 0), D y K son simétricas (D = Dt, K = Kt). Los vectores ẍ(t), ẋ(t), x(t) ∈ Rn son los vectores de aceleración, velocidad, desplazamiento respectivamente, y F (t) es un vector que representa la fuerza externa. Estamos interesados en estudiar el caso homogéneo, lo que significa que no se toma en cuenta la fuerza externa F (t) = 0, es decir estudiareamos sistemas modelados por la siguiente ecuación: Mẍ(t) +Dẋ(t) +Kx(t) = 0. (3.1) La solución fundamental para el sistema 3.1 es de la forma x(t) = veλt, suponiendo que el sistema obedece a un movimiento armónico, por lo cual el escalar λ y el vector v deben ser la solución del siguiente problema cuadrático de autovalores (Quadratic Eigenvalue Problem - QEP), (λ2M + λD +K)v = 0, (3.2) el cual posee 2n autovalores y 2n autovectores. Los autovalores son las ráıces de la ecuación no lineal det(P (λ)) = 0 donde P (λ) = λ2M + λD +K. (3.3) Proyecciones Alternantes para el PAMM 14 Estamos interesados en cambiar únicamente los autovalores “no deseados” o “malos”, mientras que el resto permanecen invariantes, puesto que estos autovalores malos son los responsable de la resonancia del sistema. 3.3. Asignación parcial de autovalores en problemas de control El problema consiste en reemplazar parte del espectro (conjunto de autovalores) para mantener y/o controlar la estabilidad del sistema. Este proceso es llamado problema cuadrático de asignación parcial de autovalores, y puede ser definido de la siguiente manera: Dados: Las matrices M,K,D ∈ Rn×n, que deben ser: M = M t > 0, K = Kt, D = Dt Un subconjunto de los autovalores {λ1, . . . , λp} con p < n, del conjunto de autova- lores de 3.2 y sus correspondientes autovectores {x1, . . . , xp}. Los autovalores {µ1, . . . , µp} y autovectores {y1, . . . , yp} deseados. Encontrar las matrices K̃, D̃ ∈ Rn×n, tales que el espectro de λ2M + λD̃ + K̃ sea {µ1, . . . , µp, λp+1, . . . , λ2n} y el conjunto de autovectores sea {y1, . . . , yp, xp+1, . . . , x2n}, donde {xp+1, . . . , x2n} son los autovectores de 3.2 correspondientes a {λp+1, . . . , λ2n}, es decir, se cambian p de los 2n autovalores y autovectores denominados “no deseados” {λ1, . . . , λp} y {x1, . . . , xp}, por los deseados {µ1, . . . , µp} y {y1, . . . , yp}. El resto de los 2n− p autovalores y sus correspondientes autovectores permanecen sin cambio. El Problema de Actualización en el Modelo Matricial (PAMM) reemplaza los auto- valores y autovectores “no deseados” por medio de la actualización de las matrices del modelo, exigiendo que las matrices a encontrar D̃ y K̃ sean simétricas. Proyecciones Alternantes para el PAMM 15 Se debe mencionar que el espacio de Hilbert que trabajaremos es el de las matrices reales de tamaño Rn×n, con la norma de Frobenius definida como ‖A‖F = (traza(AtA)) 1 2 y el producto interno 〈A,B〉F = traza(AtB). 3.4. Formulación del problema El Problema de Actualización en el Modelo Matricial (PAMM), puede ser reformulado como un problema de optimización de la siguiente manera [18]: Encontrar las matrices D̃ y K̃ de modo que: min ‖K − K̃‖2F + ‖D − D̃‖ 2 F (3.4) sujeto a: K̃ = K̃t, D̃ = D̃t (3.5) MY1(Λ ∗ 1) 2 + D̃Y1(Λ ∗ 1) + K̃Y1 = 0 (3.6) donde: Λ∗1 = diag(µ1, . . . , µp), es una matriz con los autovalores “deseados”. Y ∗1 = {y1, y2, . . . , yp}, es una matriz cuyas columnas son los autovectores “deseados”. El problema se reduce a encontrar las matrices K̃ y D̃, más cercanas a K y D tal que sean simétricas y actualicen los autovalores y autovectores no deseados. Una opción para resolver este problema es usar el MAP, puesto que nuestro objetivo es encontrar la mejor aproximación en la intersección de subespacios y variedades lineales. En este caso el vector solución es una matriz. La técnica que se va a describir fue estudiada en [19], en ella se utiliza el Método de Proyecciones Alternantes (MAP). Observando que las restricciones del problema pueden Proyecciones Alternantes para el PAMM 16 ser vistas como subespacios o variedades lineales, cuya solución está en la intersección de los mismos. Para simplificar el problema 3.4 se reescribe el sistema de la siguiente manera: min ‖K − K̃‖2F + ‖D − D̃‖ 2 F (3.7) Sujeto a: K̃ = K̃t, D̃ = D̃t A+ D̃B + K̃C = 0 Donde: A = MY1(Λ ∗ 1) 2, B = Y1(Λ ∗ 1) 2, C = Y1. Con A,B,C ∈ Cn×p, n es la dimensión del problema y p la cantidad de autovalores y autovectores que se actualizará. Se reformula el problema 3.7 en función de una matriz, de modo que se definen las matrices en bloque X ∈ R2n×2n y X̃ ∈ R2n×2n como: X =  K 0 0 D  , X̃ =   K̃ 0 0 D̃  . Haciendo manipulaciones algebraicas (ver detalles en [19]) el problema queda reducido a encontrar X̃ tal que: min‖X − X̃‖2F (3.8) sujeto a: X̃ = X̃ t (3.9) A+ Î t ∗ X̃ ∗W = 0. (3.10) Proyecciones Alternantes para el PAMM 17 El operador * representa la multiplicación de matrices, Î y W son matrices por blo- ques, definidas como Î =   In In  , W =  C B  , donde In es la matriz identidad de tamaño n× n. La solución del problema planteado 3.8 se encuentra utilizando el MAP, proyectando sobre cada una de las restricciones. La primera restricción 3.9 es que la matriz X̃ debe pertenecer al espacio de las matrices simétricas, la proyección ortogonal de una matriz X sobre este subespacio viene dada por: PSIM(X) = {X ∈ R2n×2n : X = X t}. El conjunto de las matrices que satisfacen la segunda restricción 3.10 es una varie- dad lineal, aqúı se contempla la asignación de los nuevos autovalores y autovectores, necesitamos proyectar en el siguiente conjunto, V = {X ∈ R2n×2n : A+ ÎXW = 0}. Los vectores que cumplen las restricciones 3.9 y 3.10 son los vectores de la intersección. El siguiente teorema establece la proyección de X sobre la variedad lineal V . La demostración se puede ver con detalle en [19]. Teorema 3.1. Si X ∈ R2n×2n es cualquier matriz dada, entonces la proyección de X sobre la variedad lineal V está dada por PV (X) = X + ÎΣW t, donde Σ satisface W tW ∑ = −1 2 (At +W tX tÎ). Demostración. Ver [19]. El siguiente Algoritmo de Proyecciones Alternantes calcula las matrices D̃ y K̃ tal que sean simétricas y actualicen los autovalores y autovectores no deseados, es decir K̃ y D̃ es solución de la ecuación 3.8. Proyecciones Alternantes para el PAMM 18 Algoritmo 2 Algoritmo de Proyecciones Alternantes para la actualización de las matrices D y K 1: Dadas las matrices M,K,D,Λ∗1, Y1 2: A = MY1(Λ ∗ 1) 2 3: B = Y1(Λ ∗ 1) 4: C = Y1 5: Contruir las matrices y vectores en bloque: X =  K 0 0 D  , Î =   In In  , W =  C B   6: QR=W (Factorización QR de W) 7: whileX /∈ V do 8: RtΓ = −1 2 (At +W tX tÎ) (Sustitución hacia adelante para hallar Γ) 9: RΣt = Γ (Sustitución hacia atrás, para hallar Σ) 10: X = X + Î ∗ ∑ ∗W t (Proyección de X sobre V ) 11: X = X+X t 2 (Proyección de X sobre es subespacio de las matrices simétricas) 12: end while 13: Salida: La matrices D̃ y K̃ solución de 3.8 Las matrices M,K,D ∈ Rn×n cuya propiedad es M = M t > 0, K = Kt y D = Dt La matriz diagonal Λ∗1 = diag(µ1, . . . , µp) que contiene los autovalores “deseados” La matriz Y1 = contiene los autovectores “deseados” en columna {y1, . . . , yp} El algoritmo proyecta sobre la variedad lineal V en las ĺıneas 7 a la 10 del algoritmo 2, y luego proyecta sobre el subespacio de las matrices simétricas encontrando la proyección sobre la intersección. Por el teorema 2.3 el algoritmo 2 converge a la única solución de(3.8) con las restricciones (3.9) y (3.10). Proyecciones Alternantes para el PAMM 19 3.5. Exigir una estructura dada Dependiendo de la naturaleza del problema, las matrices asociadas a la ecuación matricial 3.1 poseen una estructura de raleza que en la mayoŕıa de los casos se desea conservar, las matrices actualizadas D̃ y K̃ deben tener el mismo patrón de raleza que las matrices iniciales D y K, los patrones de raleza encontrados comúnmente corresponden a matrices diagonales, matrices tridiagonales, matrices en banda en general, entre otras. Figura 3.1: Patrones de raleza Al resolver el problema de optimización 3.8-3.10, usando el método de proyecciones alternantes, es probable que en cada iteración las matrices iniciales D y K pierdan su patrón de raleza, por lo tanto es necesario modificar el problema planteado originales en 3.8-3.10 agregando una nueva restricción que permita a las matrices actualizadas Proyecciones Alternantes para el PAMM 20 mantener esta caracteŕıstica. El problema con la nueva restricción seŕıa: min‖X − X̃‖2F (3.11) sujeto a: X̃ = X̃ t (3.12) A+ Î t ∗ X̃ ∗W = 0. (3.13) X̃posee el mismo patrón de raleza que X (3.14) La restricción 3.14 hace referencia a que la matriz X debe pertenecer al siguiente subespacio, Ω = {X ∈ R2n×2n : X posee el mismo patrón de raleza que S}. La matriz S está conformada por elementos unos y ceros que determinan el patrón de raleza, es decir, si sij es igual a uno, entonces se debe mantener el valor ubicado en la posición (i, j) de la matriz a proyectar, en caso contrario debe colocarse un cero en esta posición. La proyección sobre el subespacio Ω viene dada por: PΩ(N) = N ◦ S donde ◦ representa el producto de Hadamard entre las matrices N y S. (N ◦ S) =   Nij si Sij = 1;0 si Sij = 0. El Algoritmo 2 es modificado añadiendo la proyección sobre el subespacio Ω asociado a la restricción 3.14. Esta modificación se puede ver en la ĺınea 13 del algoritmo 3, donde Proyecciones Alternantes para el PAMM 21 se realiza el producto de Hadamard entre las matrices X y S, con la finalidad de que la matriz X recupere el patrón de raleza que poséıa originalmente, el cual está almacenado en la matriz S. Algoritmo 3 Algoritmo actualización D y K con un patrón de raleza 1: Dadas las matrices M,K,D,Λ∗1, Y1 2: A = MY1(Λ ∗ 1) 2 3: B = Y1(Λ ∗ 1) 4: C = Y1 5: Contruir las matrices y vectores en bloque: X =  K 0 0 D  , Î =   In In  , W =  C B   6: Contruir la matriz S con el patrón de raleza de X. S =   Nij si Sij = 1;0 si Sij = 0. 7: QR=W (Factorización QR de W) 8: whileX /∈ V do 9: RtΓ = −1 2 (At +W tX tÎ) (Sustitución hacia adelante para hallar Γ) 10: RΣt = Γ (Sustitución hacia atrás, para hallar Σ) 11: X = X + Î ∗ ∑ ∗W t (Proyección de X sobre V ) 12: X = X+X t 2 (Proyección de X sobre es subespacio de las matrices simétricas) 13: X = X ◦ S (Proyección de X sobre es subespacio Ω) 14: end while 15: Salida: La matrices D̃ y K̃ solución de 3.8 Proyecciones Alternantes para el PAMM 22 3.6. Aceleración del MAP con ideas Appleby y Smor- larski En el caṕıtulo anterior se observó que el MAP tiene una velocidad de convergencia lenta cuando los ángulos entre los subespacios involucrados son pequeños. Los métodos clásicos son lentos y en el trabajo doctoral Moreno [18] se propone un método de ace- leración lineal utilizando las ideas de Appleby y Smolarski [1] calculando dos centroides xc1, xc2 ∈ Rn, como el promedio de todas las proyecciones en una iteración (paso de Cimmino). Este método determina una ĺınea parametrizada a través de estos centroides, y moviendose a lo largo de esta ĺınea se calcula el nuevo iterado x̂ que está más cercano a la solución, dicho iterado puede ser descrito como sigue: x̃ = xc1 + δ(xc2 − xc1), (3.15) Donde δ es una distancia a lo largo de la ĺınea que pasa por el iterado xc1 con dirección xc2 − xc1. Para esto se calcula el nuevo iterado x̃ como el punto de intersección entre la ĺınea y el hiperplano más cercano a esta. En el caso del problema PAMM, los iterados son matrices de Rn×n, por lo que se adapta el método de aceleración lineal a matrices. El cálculo de δ no es directo y éste se convierte en un problema de optimización. Sean X̂ = Xc1 + δ(Xc2 −Xc1) (3.16) Para determinar δ forzando que X̂ sea simétrica, se quiere encontrar δ tal que mı́n δ ‖An − Atn‖ 2 F , donde An = Ac1 + δ(Ac2 − Ac1) y Atn = Atc1 + δ(Atc2 − Atc1). Haciendo manipulaciones algebraicas se obtiene (Para más detalles [18]). Proyecciones Alternantes para el PAMM 23 δ = ‖Ac1 − Atc1‖2F − 〈Ac1 − A t c1, Ac2 − Atc2〉 ‖Ac1 − Atc1‖2F − 2〈Ac1 − A t c1, Ac2 − Atc2〉+ ‖Ac2 − Atc2‖2F (3.17) Se muestra a continuación el algoritmo de aceleración correspondiente al Algoritmo 2 usando el δ que se acaba de calcular. Proyecciones Alternantes para el PAMM 24 Algoritmo 4 Aceleración del MAP con ideas Appleby y Smorlarski para la actualización de las matrices D y K 1: Dadas las matrices M,K,D,Λ∗1, Y1 2: A = MY1(Λ ∗ 1) 2 3: B = Y1(Λ ∗ 1) 4: C = Y1 5: Contruir las matrices y vectores en bloque: X =  K 0 0 D  , Î =   In In  , W =  C B   6: QR=W (Factorización QR de W) 7: whileX /∈ V do 8: RtΓ = −1 2 (At +W tX tÎ) (Sustitución hacia adelante para hallar Γ) 9: RΣt = Γ (Sustitución hacia atrás, para hallar Σ) 10: X1 = X + Î ∗ ∑ ∗W t (Proyección de X sobre V ) 11: X2 = X+Xt 2 (Proyección de X sobre es subespacio de las matrices simétricas) 12: Xc1 = X1+X2 2 (Cálculo del primer centroide) 13: RtΓ = −1 2 (At +W tX tc1Î) (Sustitución hacia adelante para hallar Γ) 14: RΣt = Γ (Sustitución hacia atrás, para hallar Σ) 15: X1 = Xc1 + Î ∗ ∑ ∗W t (Proyección de Xc1 sobre V ) 16: X2 = Xc1+X t c1 2 (Proyección de Xc1 sobre es subespacio de las matrices simétricas) 17: Xc2 = X1+X2 2 (Cálculo del segundo centroide) 18: Cálculo del escalar δ dada en 3.17 19: X = Xc1 + δ(Xc2 −Xc1) (Cálculo el nuevo iterado) 20: end while 21: Salida: La matrices D̃ y K̃ solución de 3.8 Proyecciones Alternantes para el PAMM 25 El Algoritmo 2 se modificó obteniendo aśı el algoritmo acelerado 4, en este caso una vez calculado los centroides en las ĺıneas 12 y 17 se genera un nuevo iterado en la ĺınea 19, pero para el cálculo de cada centroide se debe proyectar sobre una variedad lineal y luego sobre el subespacio. Ahora bien, debido a la necesidad de conservar el patrón de raleza, se plantea el siguiente algoritmo 5 cuya diferencia con respecto al algoritmo 4 es que en lugar de trabajar con un subespacio y una variedad lineal, se tienen dos subespacios y una variedad lineal, para poder conservar dicho patrón de raleza, como se puede observar en la ĺınea 19. Proyecciones Alternantes para el PAMM 26 Algoritmo 5 Aceleración del MAP con ideas Appleby y Smorlarski para la actualización de las matrices D y K con un patrón de raleza 1: Dadas las matrices M,K,D,Λ∗1, Y1 2: A = MY1(Λ ∗ 1) 2 3: B = Y1(Λ ∗ 1) 4: C = Y1 5: Contruir las matrices y vectores en bloque: X =  K 0 0 D  , Î =   In In  , W =  C B   6: Contruir la matriz S con el patrón de raleza de X. S =   Nij si Sij = 1;0 si Sij = 0. 7: QR=W (Factorización QR de W) 8: whileX /∈ V do 9: RtΓ = −1 2 (At +W tX tÎ) (Sustitución hacia adelante para hallar Γ) 10: RΣt = Γ (Sustitución hacia atrás, para hallar Σ) 11: X1 = X + Î ∗ ∑ ∗W t (Proyección de X sobre V ) 12: X2 = X+Xt 2 (Proyección de X sobre es subespacio de las matrices simétricas) 13: X3 = X ◦ S (Proyección de X sobre es subespacio Ω) 14: Xc1 = X1+X2+X3 3 (Cálculo del primer centroide) 15: RtΓ = −1 2 (At +W tX tc1Î) (Sustitución hacia adelante para hallar Γ) 16: RΣt = Γ (Sustitución hacia atrás, para hallar Σ) 17: X1 = Xc1 + Î ∗ ∑ ∗W t (Proyección de Xc1 sobre V ) 18: X2 = Xc1+X t c1 2 (Proyección de Xc1 sobre es subespacio de las matrices simétricas) 19: X3 = Xc1 ◦ S (Proyección de Xc1 sobre es subespacio Ω) 20: Xc2 = X1+X2+X3 3 (Cálculo del segundo centroide) 21: Cálculo del escalar δ dada en 3.17 22: X = Xc1 + δ(Xc2 −Xc1) (Cálculo el nuevo iterado) 23: end while 24: Salida: La matrices D̃ y K̃ solución de 3.8 Caṕıtulo 4 Métodos tipo residual para sistemas no lineales aplicado al PAMM 4.1. Introducción Recientemente se han desarrollado técnicas para acelerar los métodos de proyecciones alternantes y simultáneas. Estas técnicas pudieran aplicarse para resolver el Problema de Actualización en el Modelo Matricial (PAMM). En este trabajo, se implementarán algunas de esas técnicas y las compararemos con las estratégias de aceleración basadas en las ideas Appleby y Smolarski [1], estudiadas y aplicadas para el PAMM en [18] y que fueron descritas en el caṕıtulo anterior. En [13] y [14], se proponen estrategias de aceleración de los métodos de proyecciones alternantes basadas en técnicas de optimización numérica. En el caso que nos concier- ne (variedades lineales), estos autores proponen estrategias de aceleración basadas en métodos que avanzan en la dirección del residual, tales como el DF-SANE, recientemente desarrolladas por La Cruz et al [5][6]. 27 Métodos del residual aplicado al PAMM 28 4.2. Aceleración para variedades lineales Sea H un espacio de Hilbert y sean V1, V2, . . . , Vm variedades lineales cerradas de H, V = ∩mi=1Vi, V 6= ∅ y x ∈ H. Para variedades lineales, los métodos de proyecciones alternantes y simultáneas pue- den escribirse como la iteración siguiente, dado x0 ∈ H xk+1 = Txk. Para el caso del método de Cimmino el operador T = 1 m ∑m i PVi y para el caso de MAP el operador T = PVmPVm−1 ...PV1 . La sucesión {xk} converge a un punto x tal que Tx = x, en ambos casos, el conjunto de puntos fijos del operador Fix T = V . Pero además, en el caso de variedades lineales, los métodos MAP y Cimmino, convergen hacia la proyección de x0 en V , donde V = Fix T . La idea propuesta en [13] y [14] es de encontrar los puntos fijos resolviendo la ecuación: F (x) = (I − T )x = 0. El método a escoger para resolver esta ecuación depende de las caracteŕısticas de F = (I − T ). Por ejemplo, en el caso de que todos los Vi sean subespacios, el operador T asociado al método de Cimmino es lineal, autoadjunto y semipositivo definido; lo mismo para el operador I−T . Por lo tanto es posible usar el método de los gradientes conjugados en la ecuación F (X) = 0 para acelerar el método. En cambio, cuando al menos un Vi es una variedad lineal que no es un subespacio, el operador F deja de ser lineal, y por ello [13] y [14] proponen el uso de métodos tipo residual que no utilizan derivadas, tales como el DF-SANE, estos métodos avanzan en la dirección de ±F (x). A continuación describiremos brevemente el método DF-SANE basados en el art́ıculo [5]. Métodos del residual aplicado al PAMM 29 4.3. Métodos tipo residual para sistemas no lineales Los métodos de residual que usan ±F (x), como dirección de búsqueda para resolver sistemas de ecuaciones no lineales F (x) = 0, han sido desarrollados recientemente por La Cruz et al. [5][6]. Las iteraciones de residual (DF-SANE o SANE) analizadas en [5][6], para problemas no lineales finito dimensionales, están definidas como: xk+1 = xk ± αkF (xk), (4.1) donde αk ≥ 0 es el tamaño del paso y la dirección de búsqueda es ±F (x) dependiendo de cual es la dirección de descenso para la función, f(x) = ‖F (x)‖2 = 〈F (x), F (x)〉. (4.2) La convergencia de 4.1 se alcanza para sistemas no lineales generales, cuando está aso- ciada con las búsquedas lineales no monótonas y libre de derivadas como se explica en [6]. Si el Jacobiano de F (x) es simétrico y positivo semidefinido, entonces habrá con- vergencia para el método puro, es decir, sin estrategias de globalización y usando la dirección −F (x). En [13] se demostró para el caso del método de Cimmino, el operador F (x) tiene Jacobiano autoadjunto y positivo definido lo que es condición suficiente para que el método converja en su versión pura, es decir, sin globalización. Sin embargo en el caso del operador asociado al método MAP la convergencia sin globalización no se ha demostrado.. Para el tamaño de paso αk ≥ 0, se toma la escogencia espectral no monótona, dada por: αk = 〈sk−1, sk−1〉 〈sk−1, yk−1〉 , (4.3) donde sk−1 = xk − xk−1, y yk−1 = F (xk) − F (xk−1). Mediante la aplicación del tamaño de paso 4.3, se acelera la convergencia. Métodos del residual aplicado al PAMM 30 Algoritmo 6 DF-SANE para F (x), en su forma pura 1: Dado x0 ∈ H,α0 ∈ R, α0 6= 0 2: for k = 0, 1, ..., do 3: xk+1 = xk − αkF (xk) 4: sk = xk+1 − xk 5: yk = F (xk+1)− F (xk) 6: αk+1 = 〈sk, sk〉/〈sk, yk〉 7: end for En [13] se demostró que en el caso de variedades lineales, la sucesión {xk} del método DF-SANE (con o sin globalización) produce una sucesión que converge hacia la proyección del iterado inicial x0 en Fix T , es decir, en la intersección de las variedades ∩mi=1Vi. 4.4. Aceleración del Método de Cimmino en varie- dades lineales para el PAMM usando DF-SANE Recordemos que el problema de Actualización en el Modelo Matricial (PAMM), es equivalente a un problema de optimización, en el cual se quiere encontrar X̃ tal que: min‖X − X̃‖2F (4.4) sujeto a: X̃ = X̃ t (4.5) A+ Î t ∗ X̃ ∗W = 0. (4.6) Donde Î y W son matrices por bloques, definidas como Î =   In In  , W =  C B  , donde În es la matriz identidad de tamaño n× n. Métodos del residual aplicado al PAMM 31 Si llamamos F = (I − TV ), donde TV = 12(PV + PS) siendo PS la proyección sobre el subespacio S y PV es la proyección sobre la variedad lineal V , el problema F (x) = 0 puede ser resuelto utilizando métodos tipo residual para hallar ceros de funciones como el método DF-SANE, estudiado en la sección anterior, estos métodos usan la dirección ±F (xk) y las iteraciones son del tipo: xk+1 = xk ± αkF (xk), (4.7) donde αk > 0 es el tamaño del paso y la dirección de búsqueda es F (xk) o −F (xk). En el método de Cimmino, la función F viene dada por: F (x) = x− 1 2 (PV (x) + PS(x)) (4.8) = 1 2 (x− PV (x)) + 1 2 (x− PS(x)) (4.9) Sea X ∈ R2n×2n una matriz en bloque, entonces la función F viene dada como: F (X) = 1 2 (X − PV (X)) + 1 2 (X − PS(X)) (4.10) donde PV es la proyección sobre la variedad lineal V definida como: V = {X ∈ R2n×2× : A+ Î t ∗X ∗W = 0}, y PS es la proyección ortogonal de una matriz X sobre el subespacio de las matrices simétricas, es decir, esta proyección viene dada por: PS = X +X t 2 . El nuevo algoritmo de actualización para el PAMM usando DF-SANE que se mos- trará a continuación, utiliza las mismas matrices de entrada que en el Algoritmo 4. Donde, Las matrices M,K,D ∈ Rn×n cuya propiedad es M = M t > 0, K = Kt y D = Dt La matriz diagonal Λ∗1 = diag(µ1, . . . , µp) que contiene los autovalores “deseados” Métodos del residual aplicado al PAMM 32 Algoritmo 7 Actualización de las matrices D y K usando el método de DF- SANE mediante Cimmino 1: Dadas las matrices M,K,D,Λ∗1, Y1 2: A = MY1(Λ ∗ 1) 2 3: B = Y1(Λ ∗ 1) 4: C = Y1 5: Contruir las matrices y vectores en bloque: X =  K 0 0 D  , Î =   In In  , W =  C B   6: QR=W (Factorización QR de W) 7: whileX /∈ V do 8: RtΓ = −1 2 (At +W tX tkÎ) (Sustitución hacia adelante para hallar Γ) 9: RΣt = Γ (Sustitución hacia atrás, para hallar Σ) 10: PV = Xk + Î ∗ ∑ ∗W t (Proyección de Xk sobre V ) 11: PS = Xk+X t k 2 (Proyección de Xk sobre es subespacio de las matrices simétricas) 12: F (Xk) = 1 2 (Xk − PV ) + 12(Xk − PS) 13: Xk+1 = Xk − αkF (Xk) 14: Sk = Xk+1 −Xk 15: RtΓ = −1 2 (At +W tX tk+1Î) (Sustitución hacia adelante para hallar Γ) 16: RΣt = Γ (Sustitución hacia atrás, para hallar Σ) 17: PV = Xk+1 + Î ∗ ∑ ∗W t (Proyección de Xk+1 sobre V ) 18: PS = Xk+1+X t k+1 2 (Proyección de Xk+1 sobre es subespacio de las matrices simétricas) 19: F (Xk+1) = 1 2 (Xk+1 − PV ) + 12(Xk+1 − PS) 20: Yk = F (Xk+1)− F (Xk) 21: αk+1 = 〈Sk,Sk〉F 〈Sk,Yk〉F 22: end while 23: Salida: La matrices D̂ y K̂ solución de 3.8 Métodos del residual aplicado al PAMM 33 La matriz Y1 = contiene los autovectores “deseados” en columna {y1, . . . , yp} En el siguiente algoritmo 7 se observa: Las ĺıneas del 8 al 10 encuentra la proyección de Xk sobre la variedad lineal V . La ĺınea 11 calcula la proyección ortogonal de Xk sobre el subespacio de las matrices simétricas. La ĺınea 12 calcula la función F (Xk) es el cálculo de la función F (Xk) usando Cimmino (proyecciones simultáneas). La ĺınea 13 calcula el iterado Xk+1. Desde la ĺınea 15 a 19 repite los pasos anteriores para encontrar F (Xk+1). Por último se calcula en la ĺınea 21 el tamaño de paso αk 4.3. Finalmente obtenemos la adaptación del DF-SANE para el problema de Actualización Modelo Matricial. 4.5. Aceleración del Método de Proyecciones Alter- nantes en variedades lineales para el PAMM usando DF-SANE A continuación se muestra otra aceleración del método de proyecciones alternantes usando el DF-SANE, es decir, la función F = I − T viene dada por: F (x) = (x− PSPV (x)) (4.11) En el siguiente algoritmo 8 se observa: Las ĺıneas del 8 al 10 encuentra la proyección de Xk sobre la variedad lineal V . Métodos del residual aplicado al PAMM 34 Algoritmo 8 Actualización de las matrices D y K usando el método de DF- SANE mediante MAP 1: Dadas las matrices M,K,D,Λ∗1, Y1 2: A = MY1(Λ ∗ 1) 2 3: B = Y1(Λ ∗ 1) 4: C = Y1 5: Contruir las matrices y vectores en bloque: X =  K 0 0 D  , Î =   In In  , W =  C B   6: QR=W (Factorización QR de W) 7: whileX /∈ V do 8: RtΓ = −1 2 (At +W tX tkÎ) (Sustitución hacia adelante para hallar Γ) 9: RΣt = Γ (Sustitución hacia atrás, para hallar Σ) 10: PV = Xk + Î ∗ ∑ ∗W t (Proyección de Xk sobre V ) 11: PS = Xk+X t k 2 (Proyección de Xk sobre es subespacio de las matrices simétricas) 12: F (Xk) = Xk − PSPV (Xk) 13: Xk+1 = Xk − αkF (Xk) 14: Sk = Xk+1 −Xk 15: RtΓ = −1 2 (At +W tX tk+1Î) (Sustitución hacia adelante para hallar Γ) 16: RΣt = Γ (Sustitución hacia atrás, para hallar Σ) 17: PV = Xk+1 + Î ∗ ∑ ∗W t (Proyección de Xk+1 sobre V ) 18: PS = Xk+1+X t k+1 2 (Proyección de Xk+1 sobre es subespacio de las matrices simétricas) 19: F (Xk+1) = Xk − PSPV (Xk) 20: Yk = F (Xk+1)− F (Xk) 21: αk+1 = 〈Sk,Sk〉F 〈Sk,Yk〉F 22: end while 23: Salida: La matrices D̂ y K̂ solución de 3.8 Métodos del residual aplicado al PAMM 35 La ĺınea 11 calcula la proyección ortogonal de Xk sobre el subespacio de las matrices simétricas. La ĺınea 12 calcula la función F (Xk) que a diferencia del algoritmo anterior aqúı uti- lizamos la composición o el producto de las proyecciones, es decir, una iteración del método MAP. La ĺınea 13 calcula el iterado Xk+1. Desde la ĺınea 15 a 19 repite los pasos anteriores para encontrar F (Xk+1). Por último se calcula en la ĺınea 21 el tamaño de paso αk. Finalmente obtenemos la adaptación del DF-SANE para el resolver el PAMM usando el MAP. 4.5.1. El método DF-SANE manteniendo un patrón de raleza dado En muchos casos, es importante que la solución al problema PAMM mantenga un patrón de raleza dado [18]. En este caso, la solución debe no solo pertenecer a la intersec- ción del subespacio de las matrices simétricas y la variedad lineal dada en 3.13, sino que debe también pertenecer al subespacio de las matrices que tienen ese patrón de raleza. Es por ello que nuestros métodos de proyecciones trabajarán ahora con 3 conjuntos, y los operadores de la iteración de punto fijo estarán dados por T = 1 3 ∑3 i=1 PVi para el caso de Cimmino y T = PV3PV2PV1 en el caso de MAP, donde: PV3 es la proyección para conservar patrón de raleza. PV2 es la proyección sobre las matrices simétricas. PV1 es la proyección sobre la variedad lineal. Caṕıtulo 5 Resultados numéricos En este caṕıtulo mostramos los resultados de las diferentes aceleraciones aplicadas a la técnica para resolver el problema de actualización modelo matricial (PAMM) estu- diada en [18], para ello se compara el método clásico usando el método de proyecciones alternantes visto en el caṕıtulo 3, las estratégias de aceleración basadas en las ideas Ap- pleby y Smolarski [1] estudiadas en [18], las estrategias de aceleración de los métodos de proyecciones alternantes basadas en técnicas de optimización numérica propuesta por [13] y [14], y una aceleración sugerida por Hernández-Ramos. Todas estas aceleraciones aplicadas a la técnica para resolver el PAMM además de disminuir el número de iteraciones, disminuye también el tiempo de ejecución ya que el costo por iteración es menor. Para realizar los experimentos se seleccionaron cuatro problemas de la tesis doctoral [18] que estan incluidos en la bibliograf́ıa. En los algoritmos desarrollados se observa el siguiente orden de proyecciones: Primero se proyecta sobre la variedad lineal V, esta proyección sustituye los auto- valores y autovectores no deseados por los deseados. Segundo, se proyecta sobre el subespacio de las matrices simétricas. En todo método iterativo es necesario especificar mediante ciertas condiciones cuando 36 Resultados numéricos 37 hay que detener el proceso. El criterio de parada utilizado tiene en cuenta dos factores: 1. La solución encontrada satisface los requerimientos de precisión (tolerancia). Se fijó una tolerancia de 1× 10−8. 2. La cantidad de iteraciones empleadas supera el máximo establecido. Para el primer criterio, el proceso iterativo se detiene cuando ‖A+ Î tXW‖F ≤ 10−8, donde X es la matriz obtenida al finalizar una iteración de los algoritmos, por lo cual la matriz X pertenece al subespacio de las matrices simétricas. Para el segundo criterio el máximo número de iteraciones que establecimos es de 5000, el cual consideramos que el algoritmo no converge si pasa este número de iteraciones. Los algoritmos requeridos para realizar la pruebas numéricas se desarrollaron en Matlab 7.12 (R2011a) y han sido ejecutados con un computador Intel Core 2 Quad 2.10 GHz, 4Gb de RAM con Windows 7. 5.1. Experimento 1 Para este experimento se seleccionó un ejemplo tomado del art́ıculo de Datta y Sar- kissian [7]. Las matrices M,D,K de 4×4, simétricas semidefinidas positivas vienen dadas por: M =   1, 4685 0, 7177 0, 4757 0, 4311 0, 7177 2, 6938 1, 2660 0, 9676 0, 4757 1, 2660 2, 7061 1, 3918 0, 4311 0, 9676 1, 3918 2, 1876   , Resultados numéricos 38 D =   1, 3525 1, 2695 0, 7967 0, 8160 1, 2695 1, 3274 0, 9144 0, 7325 0, 7967 0, 9144 0, 9456 0, 8310 0, 8160 0, 7325 0, 8310 1, 1536   , K =   1, 7824 0, 0076 −0, 1359 −0, 7290 0, 0076 1, 0287 −0, 0101 −0, 0493 −0, 1359 −0, 0101 2, 8360 −0, 2564 −0, 7290 −0, 0493 −0, 2564 1, 9130   Los autovalores de λ2M + λD +K calculados con MATLAB son: −0, 0861± 1, 6242i,−0, 1022± 0, 8876i,−0, 1748± 1, 1922i,−0, 4480± 0, 2465i. Se quiere reasignar solo los autovalores causantes de la inestabilidad. En este caso, es el par −0,0861± 1, 6242i, el cual va a ser sustituido por el par −0, 1000± 1, 6242i. A continuación se muestra la gráfica del desempeño de cada algoritmo con respecto al número de iteraciones, usando como datos de entradas las matrices M,D y K arriba mencionada. Algoritmos iter tiempo MAP para la actualización D y K 96 0,2580 Aceleración Appleby-Smorlarski 43 0,0350 DF-SANE Cimmino 35 0,0270 DF-SANE MAP 17 0,0030 Cuadro 5.1: Comparación entre los diferentes métodos para el Problema de Actualización en el Modelo Matricial en el experimento 1 El costo por iteración de los algoritmos para acelerar el PAMM es menor, esto hace que disminuya también el tiempo de ejecución. Resultados numéricos 39 Figura 5.1: Experimento 1 Se puede observar que para este experimento, el método DF-SANE MAP tiene una ganancia en tiempo y en número de iteraciones. Sin embargo en arquitecturas paralelas, en el método de Cimmino las proyecciones pudieran realizarse simultáneamente, lo que significaŕıa una ganancia en tiempo. 5.2. Experimento 2 Para el segundo experimento seleccionamos las matrices M,D,K de 30×30 simétricas y definidas positivas, cuyas matrices están descritas en Benner, Laud y Mehrmann [3]. Este experimento proviene de la aplicación de un modelo de una secuencia formada por los resortes, los amortiguadores de aire, y las masas unidas, los cuales dan origen a las Resultados numéricos 40 siguientes matrices: M,D = {A ∈ R30×30/a(i, j) = 4 para i = j, a(i, j) = 0 para i 6= j}. Es decir: M,D =   4 0 · · · 0 0 0 4 · · · 0 0 ... · · · ... 0 · · · · · · 0 0 0 · · · 0 4   . K = {A ∈ R30×30/a(i, j) = 2, a(i + 1, j) = a(j, i + 1) = −1 para todo i = j, a(i, j) = 0 para todo i 6= j, a(1, 1) = a(30, 30) = 1}. Es decir: K =   1 −1 0 0 · · · 0 −1 2 −1 0 · · · 0 0 −1 2 −1 ... ... . . . . . . . . . 0 · · · −1 2 −1 0 0 · · · 0 −1 1   . En este caso se empleó el algoritmo 3 y el algoritmo acelerado 5 para encontrar D̃ (matriz diagonal) y K̃ (matriz tridiagonal) y pueda conservar el patrón de raleza. A continuación se muestra la gráfica y un cuadro comparativo del desempeño de cada algoritmo con respecto al número de iteraciones, usando como datos de entradas las matrices M,D y K arriba mencionada. El costo por iteración de los algoritmos para acelerar el PAMM es menor, esto hace que disminuya también el tiempo de ejecución. Sin embargo se puede observar que para este experimento el método DF-SANE MAP tiene una ganancia en tiempo y en número de iteraciones. Resultados numéricos 41 Figura 5.2: Experimento 2 Algoritmos iter tiempo MAP para la actualización D y K con un patrón de raleza 762 0,6090 Aceleración Appleby-Smorlarski con un patrón de raleza 518 0,4840 DF-SANE Cimmino con un patrón de raleza 76 0,0930 DF-SANE MAP con un patrón de raleza 14 0,0010 Cuadro 5.2: Comparación entre los diferentes métodos para el Problema de Actualización en el Modelo Matricial en el experimento 2 5.3. Experimento 3 Sean las matrices M,D y K. Resultados numéricos 42 Las matrices M,K ∈ R66×66 son provenientes de un problema de plataforma pe- trolera. La matriz M es simétrica definida positiva y la matriz K es simétrica semidefinida positiva, ambas matrices son densas. La matriz de amortiguación D está definida por D = ρI66 con ρ = 1, 55. Figura 5.3: Experimento 3 Al igual que en los experimentos anteriores, se observa que las técnicas de aceleración aplicadas al PAMM tiene una disminución en número de iteraciones y en tiempo de eje- cución, ya que el costo por iteración es menor. Sin embargo el algoritmo que el algoritmo que mejor acelera en términos computacionales es el DF-SANE con MAP. Resultados numéricos 43 Algoritmos iter tiempo MAP para la actualización D y K con un patrón de raleza 2574 14,8190 Aceleración Appleby-Smorlarski con un patrón de raleza 2563 13,3590 DF-SANE Cimmino con un patrón de raleza 200 0,8740 DF-SANE MAP con un patrón de raleza 25 0,0460 Cuadro 5.3: Comparación entre los diferentes métodos para el Problema de Actualización en el Modelo Matricial en el experimento 3 5.4. Experimento 4 Sean las matrices M,K ∈ R211×211 simétricas definidas positivas, descritas en Ben- ner, Laudd y Mehrman [3]. Este ejemplo modela un problema que se presenta en centrales eléctricas. Algoritmos iter tiempo MAP para la actualización D y K con un patrón de raleza Núm máx iteraciones * Aceleración Appleby-Smorlarski con un patrón de raleza Núm máx iteraciones * DF-SANE Cimmino con un patrón de raleza Núm máx iteraciones * DF-SANE MAP con un patrón de raleza 168 6,2170 Cuadro 5.4: Comparación entre los diferentes métodos para el Problema de Actualización en el Modelo Matricial en el experimento 4 En este experimento alcanzó el número máximo de iteraciones (5000) excepto para la técnica de aceleración usando DF-SANE MAP, por lo cual esta propuesta sirve para atacar problemas que de otra forma son dif́ıciles de atacar. Resultados numéricos 44 Figura 5.4: Experimento 4 Caṕıtulo 6 Conclusiones y Recomendaciones Basado en las técnicas recientes desarrolladas en [13], las cuales utilizan métodos tipo residual, el presente trabajo propone aplicar dichas técnicas para resolver el Problema de Actualización en el Modelo Matricial (PAMM), estudiado en [18]. El cual es planteado como un problema de optimización no lineal con restricciones, utilizando los métodos de proyecciones alternantes para resolverlo, y el método de Cimmino para acelerarlo. Se pudo observar a través de la experimentación numérica, un aumento de la velocidad de convergencia utilizando las técnicas recientes desarrolladas en [13]. Adicionalmente se hacen pruebas con un operador distinto asociado al método de proyecciones alternantes (DF-SANE MAP). En dichas pruebas esta propuesta fue la que dio mejores resultados computacionales. Todas las aceleraciones aplicadas a la técnica para resolver el PAMM además de disminuir el número de iteraciones, disminuye también el tiempo de ejecución ya que el costo por iteración es menor. La convergencia sin globalización para el caso del método de Cimmino acelerado fue demostrada en [13]. Esa prueba no se ha hecho en el caso del método MAP acelerado (DF- SANE MAP), sin embargo en todos los experimentos realizados se dio la convergencia, aśı que podemos conjeturarla y queda su prueba como un problema abierto para futuras investigaciones. 45 Bibliograf́ıa [1] G. APPLEBY and D. SMOLARSKI. A linear acceleration row action method for projecting onto subspaces. Electronic Transactions on Numerical Analysis (ETNA), 20:253–275, 2005. [2] N. ARONSZAJN. Theory of reproducing kernels. Trans. Amer. Math. Soc., 68:337– 404, 1950. [3] P. BENNER, A.J. LAUD, and V. MEHRMANN. A collection of bench- mark examples for the numerical solutions of algebraic riccati equeations i: Continuous-time case. In Available in electronic form from http://www.tu- chemnitz.de/pester/sfb/spc95pr.html. 1995. [4] G. CIMMINO. Calcolo approsimate per le soluzioni dei sistemi di equazioni lineari. La Ricerca Scientifica. ed. Il progresso tecnico nell’ economia nazionale. Consiglio Nazionale delle Richerche. Ministero dell’ Educatione Nazionale, 9(2):326–333, 1938. [5] W. LA CRUZ, J.M. MARTINEZ, and M. RAYDAN. Spectral residual method without gradient information for solving large-scale nonlinear systems of equations. Mathematics of Computation, 75(255):1429–1448, 2006. [6] W. LA CRUZ and M. RAYDAN. Nonmonotone spectral methods for large-scale nonlinear systems. Optimization Methods and Software, 18(5):583–599, 2003. [7] B.N. DATTA and D.R SARKISSIAN. Theory and computations of some inverse ei- genvalue problems for the quadratic pencil. Contemporary Mathematics, Structured 46 Bibliograf́ıa 47 Matrices in Operator Theory, Control and Signal an Imagen Processing, American Mathematical Society:280:221–240, 2001. [8] F. DEUTSCH. Applications of von Neumann’s alternating projections algorithm. In P. Kenderov, editor, Mathematical Methods in Operations Research, pages 44–51. Sofia, Bulgaria, 1983. [9] F. DEUTSCH. The method of alternating orthogonal projections. In S.P. Singh, editor, Approximation Theory, Spline Functions and Applications, pages 105–121. Kluwer Academic Publishers, Netherlands, 1992. [10] F. DEUTSCH. The angle between subspaces of a Hilbert space. In Approximation theory, wavelets and applications (Maratea, 1994), pages 107–130. Kluwer Acad. Publ., Dordrecht, 1995. [11] R. ESCALANTE and M. RAYDAN. Alternating Projection Methods. SIAM, 2011 (ISBN 978-1-611971-93-4). [12] I. HALPERIN. The product of projection operators. Acta Sci. Math. (Szeged), 23:96–99, 1962. [13] L.M. HERNANDEZ-RAMOS. Método de Proyecciones Alternantes v́ıa optimización numérica. Doctoral dissertation, Universidad Central de Venezuela, 2011. [14] L.M. HERNANDEZ-RAMOS, R. ESCALANTE, and M. RAYDAN. Unconstrai- ned optimization techniques for the acceleration of alternating projection methods. Numerical Functional Analysis and Optimization, 2010. [15] S. KACZMARZ. Angenäherte Auflösung von Systemen linearer Gleichungen. Bull. Internat. Acad. Pol. Sci. Lett., A 35:355–357, 1937. [16] W.J. KAMMERER and M.Z. NASHED. A generalization of a matrix iterative method of G. Cimmino to best approximate solution of linear integral equations of Bibliograf́ıa 48 the first kind. Atti Accad. Naz. Lincei Rend. Cl. Sci. Fis. Mat. Natur.(8), 51:20–25, 1971. [17] S. KAYALAR and H.L. WEINERT. Error bounds for the method of alternating projections. Math. Control Signals Systems, 1:43–59, 1988. [18] J. MORENO. Actualización óptima de matrices en problemas de control. doctoral dissertation, Universidad Central de Venezuela, 2011. [19] J. MORENO, B. DATTA, and M. RAYDAN. A symmetry preserving alternating projection method for matrix model updating. Mechanical Systems and Signal Pro- cessing, pages 23:1784–1791, 2009. [20] J. VON NEUMANN. Functional operators vol. II. The geometry of orthogonal spaces. Annals of Math. Studies, 22, 1950. Princeton University Press. This is a reprint of mimeographed lecture notes first distributed in 1933. [21] K.T. SMITH, D.C. SOLMON, and S.L. WAGNER. Practical and mathematical aspects of the problem of reconstructing objects from radiographs. Bull. Amer. Math. Soc., 83:1227–1270, 1977.UNIVERSIDAD CENTRAL DE VENEZUELA FACULTAD DE CIENCIAS ESCUELA DE COMPUTACIÓN ACELERACIONES DE LOS MÉTODOS DE PROYECCIONES ALTERNANTES Y SIMULTÁNEAS EN EL PROBLEMA DE ASIGNACIÓN DE AUTOVALORES Trabajo Especial de Grado presenta- do ante la ilustre Universidad Central de Venezuela por Gledys Sulbarán para optar al t́ıtulo de Licenciada en Computación. Tutores: Dr. Luis Manuel Her- nandez y Dra. Joali Moreno Caracas, Venezuela Febrero 2014 2 Nosotros, los abajo firmantes, designados por la Universidad Central de Venezue- la como integrantes del Jurado Examinador del Trabajo Especial de Grado titulado “Aceleraciones de los Métodos de Proyecciones Alternantes y Simultáneas en el Problema de Asignación de Autovalores”, presentado por Gledys Sulbarán, titular de la Cédula de Identidad 17.313.160, certificamos que este trabajo cumple con los requisitos exigidos por nuestra Magna Casa de Estudios para optar al t́ıtulo de Li- cenciada en Computación. Dr. Luis Manuel Hernández. Tutor Dra. Joali Moreno. Tutora Dr. Marcos Raydan. Jurado Dr. Otilio Rojas. Jurado Agradecimientos A Dios por darme salud y fuerzas. A mi tutores, el profesor Luis Manuel Hernández Ramos y el profesora Joali Moreno por toda la atención, la paciencia y el conocimiento que siempre me brindaron. Muchas gracias por creer en mi. A mi madre Gladys Goyo, por estar a mi lado en todo momento. A mi hermanita Glendy por apoyarme y ayudarme cuando la necesito. A mis queridos amigos Gisell, Freddy, Yoel, Mariolys, Adolfo, Adelis, Lilibeth, Nelson, Augusto. 3 Índice general Introducción 1 1. Introducción 2 2. Métodos de Proyecciones Alternantes 4 2.1. Introducción . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 2.2. Proyecciones Alternantes . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 2.2.1. Algoritmo MAP . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 2.3. Los Métodos de Kaczmarz y Cimmino . . . . . . . . . . . . . . . . . . . 7 2.3.1. Método de Kaczmarz . . . . . . . . . . . . . . . . . . . . . . . . . 7 2.3.2. Método de Cimmino . . . . . . . . . . . . . . . . . . . . . . . . . 8 2.4. Velocidad de Convergencia . . . . . . . . . . . . . . . . . . . . . . . . . . 9 2.4.1. Ángulo entre Subespacios . . . . . . . . . . . . . . . . . . . . . . 9 2.4.2. Velocidad de Convergencia para Proyecciones Alternantes . . . . . 9 3. Proyecciones Alternantes para el PAMM 12 3.1. Introducción . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 3.2. Sistema matricial de segundo orden en problemas de control . . . . . . . 13 3.3. Asignación parcial de autovalores en problemas de control . . . . . . . . 14 3.4. Formulación del problema . . . . . . . . . . . . . . . . . . . . . . . . . . 15 3.5. Exigir una estructura dada . . . . . . . . . . . . . . . . . . . . . . . . . . 19 3.6. Aceleración del MAP con ideas Appleby y Smorlarski . . . . . . . . . . . 22 i 1 4. Métodos tipo residual para sistemas no lineales aplicado al PAMM 27 4.1. Introducción . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 4.2. Aceleración para variedades lineales . . . . . . . . . . . . . . . . . . . . . 28 4.3. Métodos tipo residual para sistemas no lineales . . . . . . . . . . . . . . 29 4.4. Aceleración del Método de Cimmino en variedades lineales para el PAMM usando DF-SANE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 4.5. Aceleración del Método de Proyecciones Alternantes en variedades lineales para el PAMM usando DF-SANE . . . . . . . . . . . . . . . . . . . . . . 33 4.5.1. El método DF-SANE manteniendo un patrón de raleza dado . . . 35 5. Resultados numéricos 36 5.1. Experimento 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37 5.2. Experimento 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39 5.3. Experimento 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41 5.4. Experimento 4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43 6. Conclusiones y Recomendaciones 45 Bibliograf́ıa 46 Caṕıtulo 1 Introducción El estudio y análisis de vibraciones tiene como propósito combatir el fenómeno de la resonancia en estructuras. Los sistemas estructurales son representados por ecuaciones diferenciales de segundo orden. Las vibraciones están relacionadas con los autovalores que surgen del modelo. En este análisis los sistemas estructurales son representados por un sistema matricial de segundo orden de la siguiente forma: Mẍ(t) +Dẋ(t) +Kx(t) = 0. (1.1) donde M,D,K ∈ Rn×n representan la masa, la amortiguación y la rigidez respectiva- mente. Los vectores ẍ(t), ẋ(t), x(t) ∈ Rn son los vectores de aceleración, velocidad y desplazamiento respectivamente (ver detalle en [19]). Además ẋ(t) y ẍ(t) representan la primera y segunda derivada de x(t) con respecto al tiempo. Una solución fundamental para el sistema (1.1) es x(t) = veλt donde el escalar λ y el vector v satisfacen la siguiente ecuación cuadrática de autovalores (Quadratic Eigenvalue Problem - QEP) (λ2M + λD +K)v = 0, (1.2) la cual posee 2n autovalores y 2n autovectores. Estos autovalores están relacionados con las frecuencias naturales del sistema homogéneo. Sólo algunos autovalores causan un fenómeno no deseado que es conocido como reso- nancia. De alĺı la importancia de resolver el problema cuadrático de asignación parcial 2 Introducción 3 de autovalores, que consiste en reemplazar los autovalores “no deseados” por autovalores “deseados”, manteniendo invariante el resto del espectro, evitando aśı el fenómeno de resonancia. El Problema de Actualización en el Modelo Matricial (PAMM) consiste en actualizar algunas matrices que surgen en el modelo matricial de segundo orden, de manera que los autovalores asociados al modelo sean los deseados y al mismo tiempo preservar la estructura y simetŕıa de las matrices involucradas. En el trabajo doctoral [18], se plantea el PAMM como un problema de optimización no lineal con restricciones, y en dicho trabajo se propone el uso del método de proyecciones alternantes (MAP) y sus variantes para resolver dicho problema, además se plantea una estrategia de aceleración basada en una técnica propuesta por Appleby y Smorlarski [1]. Los métodos de proyecciones alternantes y simultáneas suelen tener una convergencia lenta. En [13] se estudia el problema de mejor aceleración en variedades lineales. Los autores proponen mejorar a los algoritmos de proyecciones alternantes y de Cimmino utilizando técnicas de optimización. En el caso de variedades lineales se usan técnicas ba- sadas en métodos tipo residual, como por ejemplo el SANE y el DF-SANE, estudiadas y analizadas por la Cruz et al. [5][6], para acelerar los métodos basados en proyecciones. Es- tas técnicas pueden igualmente ser empleados para resolver el Problema de Actualización en el Modelo Matricial. En este trabajo proponemos la aplicación y comparación de estas técnicas recientes propuestas en [13] para resolver el PAMM con métodos basados en proyeciones como propone [18] y además se realizan experimentos numéricos con una nueva propuesta basada en la aceleración del MAP mediante el DF-SANE. Caṕıtulo 2 Métodos de Proyecciones Alternantes 2.1. Introducción En este caṕıtulo se resumen algunos conceptos acerca el Método de Proyecciones Alternantes (MAP, por sus siglas en inglés). Este caṕıtulo se basa principalmente en el libro de Escalante y Raydan [11] y en la tesis doctoral de Hernández-Ramos [13]. 2.2. Proyecciones Alternantes El Método de Proyecciones Alternantes en su versión original fue desarrollado por Von Neumann en 1933 [20], quien trató el problema de encontrar la proyección de un punto dado en un espacio de Hilbert en la intersección de dos subespacios cerrados. Posteriormente, el método de Von Neumann fue extendido por Halperin [12] a una familia finita de subespacio cerrados de H. Igualmente, en 1937, Kaczmarz [15] hab́ıa propuesto un método análogo al método de Halperin, pero solo en el caso particular de sistemas de ecuaciones lineales. Sea H un espacio de Hilbert y sea M un subespacio cerrado de H. Denotemos por PM 4 Métodos de Proyecciones Alternantes 5 el operador de Proyección ortogonal sobre M. Este es un operador lineal, autoadjunto (〈Px, y〉 = 〈x, Py〉) e idempotente (P 2 ≡ P ). PM es tal que: ‖ x− PM(x) ‖= d(x,M) donde d(x,M) = inf{‖ x− y ‖, y ∈ M}, es decir PM(x) es el punto más cercano a x en M. Una propiedad que lo caracteriza en el caso de subespacios es: 〈x− PM(x), y〉 = 0,∀y ∈M. Una definición que utilizaremos luego es la del complemento ortogonal de un conjunto. El Complemento ortogonal de M (denotado por M⊥) se define como: M⊥ := {y ∈ H : 〈x, y〉 = 0,∀x ∈M}. Es decir, x− PM(x) es ortogonal a M . Sean A y B subespacios cerrados de H. Entonces PAPB = PBPA si y sólo si PAPB = PA∩B. Es decir, PA y PB conmutan si y solo si su composición es también una proyección ortogonal. Von Neumann estuvo interesado en el caso donde PA y PB no conmutan, probando el siguiente resultado: Teorema 2.1. (Von Neumann [1933]) [20] Sean A y B subespacios cerrados en un es- pacio de Hilbert H. Entonces, para cada x ∈ H ĺım n→∞ (PBPA) n(x) = PA∩B(x). 2.2.1. Algoritmo MAP El Teorema de Von Neumann induce un algoritmo iterativo (Algoritmo 1) para cal- cular la proyección en la intersección de dos subespacios cerrados. La interpretación geométrica de este método nos dice que para encontrar la mejor aproximación a un punto x desde A∩B, primero se proyecta x sobre A, el vector obtenido se proyecta sobre B, y aśı se continua proyectando de forma alternada sobre A y B. La Métodos de Proyecciones Alternantes 6 Figura 2.1: Proyecciones Alternantes sucesión de los elementos generados de esta manera converge a PA∩B(x). La utilidad práctica de este procedimiento proviene del hecho de que es más sencillo proyectar sobre cada subespacio individualmente que proyectar sobre la intersección. Algoritmo 1 MAP 1: Dado x0 2: for n = 1, ..., do 3: xn = PBPAxn−1 = (PBPA) nx 4: end for El resultado obtenido por Von Neumann fue extendido posteriormente por Halperin [12] a una familia finita de subespacio cerrados. Teorema 2.2. (Halperin 1962) . Sean M1,M2, . . . ,Mr subespacios cerrados en H y M = ∩ri=1Mi. Entonces ĺım n→∞ (PMrPMr−1 . . . PM2PM1) n(x) = P∩ri=1Mi(x) para cada x ∈ H. Demostración. Ver [11]. Es importante mencionar que la demostración del teorema 2.2 cuando r ≥ 3 es dife- rente a la dada por Von Neumann para el teorema 2.1 cuando r = 2. Métodos de Proyecciones Alternantes 7 En 1983, Frank Deutsch [8] hace una generalización del teorema 2.2 al caso donde los conjuntos involucrados son un número finito de variedades lineales. Teorema 2.3. (Deutsch, 1983 [8]) Si M1,M2, . . . ,Mr son variedades lineales cerradas en un espacio de Hilbert H. Entonces, para cada x ∈ H. ĺım n→∞ (PMrPMr−1 . . . PM2PM1) n(x) = P⋂r i=1Mi (x). 2.3. Los Métodos de Kaczmarz y Cimmino 2.3.1. Método de Kaczmarz Este método fue propuesto originalmente por Kaczmarz [15] quien estableció su con- vergencia para resolver sistemas de ecuaciones lineales Ax = b. Dicho método es un caso particular del método propuesto posteriormente por Halperin para el caso espećıfico de resolución de sistemas de ecuaciones lineales. Consideremos el problema de resolver los sistemas de ecuaciones lineales Ax = b. Donde A es una matriz real m× n, x ∈ Rn y b ∈ Rm. Sea Hi = {x ∈ Rn : 〈ai, x〉 = bi}. Aqúı ai = (ai1, ai2, . . . , ain) ∈ Rn, x ∈ Rn y bi ∈ R i = 1, · · · ,m. Encontrar una solución de Ax = b es equivalente a encontrar un punto en la inter- sección de todos los hiperplanos Hi, es decir en ∩mi=1Hi. Para encontrar tal punto, por el método de Kaczmarz, para un iterado inicial x0 ∈ Rn arbitrario, se define la iteración: xn = (PHmPHm−1 . . . PH2PH1)xn−1 (n = 1, 2, . . .) Entonces por el teorema 2.3 se tiene que Métodos de Proyecciones Alternantes 8 {xn}n∈N −→ P⋂mi=1Hi(x0) =: y0. Es decir, la iteración del método de Kaczmarz converge a un punto y0 ∈ Rn que satisface la ecuación Ax = b. Note que para cada i, i = 1, ...,m, ai es ortogonal a Hi. Si z 6∈ Hi, la proyección sobre Hi viene dada por la siguiente expresión: PHi(z) = z + bi − 〈ai, z〉 〈ai, ai〉 ai. Es importante resaltar que si z ∈ Hi la proyección sobre Hi es el mismo z. 2.3.2. Método de Cimmino Este método fue propuesto por Cimmino [4], quien estableció la convergencia para la solución de sistemas de ecuaciones lineales Ax = b, con A ∈ Rnxn y b ∈ Rn Para un x0 ∈ Rn, un paso t́ıpico del método de Cimmino es el siguiente, xk+1 = 1 n n∑ i=1 PHi(xk) donde Hi = {x ∈ Rn :< ai, x >= bi}. El método de Cimmino ha sido extendido y generalizado para variedades lineales. Kam- merer y Nashed en [16] generalizan el método de la siguiente manera: Sean V1, V2, ..., Vn variedades lineales cerradas de un espacio de Hilbert H, y dado x0 ∈ H. El método de Cimmino viene dado entonces por la secuencia, xk+1 = 1 n n∑ i=1 PVi(xk) Tal como en el método de Kaczmarz, las iteraciones de este método convergen a PV (x0), donde V = ⋂n i=0 Vi. Métodos de Proyecciones Alternantes 9 2.4. Velocidad de Convergencia La velocidad de convergencia del Método de Proyecciones Alternantes ha sido estu- diada por varios autores, entre ellos: Smith, Solmon y Wagner [21], Kayalar y Weinert [17] y F. Deutsch [10]. En general, la velocidad de convergencia del MAP es r-lineal y depende del ángulo entre los subespacios o variedades lineales involucrados. 2.4.1. Ángulo entre Subespacios Recordemos que si x, y ∈ H, el ángulo θ entre x e y está definido como el ángulo cuyo coseno está dado por: cosθ = 〈x, y〉 ‖x‖‖y‖ La siguiente definición introducida originalmente por Friedrichs en 1937, es la más utili- zada en la literatura del MAP para trabajar con el ángulo entre subespacios. Definición 2.4. (Friedricks, [1937]) [9] El ángulo θ(M,N) entre dos subespacios cerra- dos M y N de H es el ángulo en [0, π/2] cuyo coseno C(M,N) = cos θ(M,N) esta dado por: C(M,N) := sup{|〈x, y〉| : x ∈M ∩ (M ∩N)⊥, ‖ x ‖≤ 1, y ∈ N ∩ (N ∩M)⊥, ‖ y ‖≤ 1} 2.4.2. Velocidad de Convergencia para Proyecciones Alternan- tes Caso dos subespacios En el siguiente teorema se demuestra que la velocidad de convergencia del Método de Proyecciones Alternantes depende del ángulo entre los subespacios involucrados. De dicho teorema tenemos que si el ángulo entre los subespacios es pequeño el método se hace lento en el sentido de que se requerirán muchas iteraciones para su convergencia. [10] Métodos de Proyecciones Alternantes 10 Teorema 2.5. (Aronszajn 1950)[2]: Sean M1 y M2 subespacios cerrados en H cuyo coseno C(M1,M2) = cos θ(M1,M2). Entonces, para cada x ∈ H ‖ (PM2PM1) n(x)− PM1∩M2(x) ‖≤ C 2n−1 ‖ x− PM1∩M2(x) ‖≤ C 2n−1 ‖ x ‖ para n = 1, 2, ... por otra parte, la constante C2n−1 es la más pequeña posible. En la siguiente figura se puede apreciar la dependencia del ángulo en la velocidad de convergencia para el caso de dos subespacios A y B. En el ejemplo de la figura 2.2, el método realiza menos iteraciones para llegar a la solución mientras que el ejemplo 2.3 converge más lentamente ya que realiza más iteraciones para llegar a la solución, es decir, el método es más rápido cuando el ángulo entre los subespacios está más cerca de la ortogonalidad Figura 2.2: Método MAP: ángulo grande Figura 2.3: Método MAP: ángulo pequeño Métodos de Proyecciones Alternantes 11 Caso n subespacios Smith, Solmon, and Wagner(1977) determinaron una cota análoga en el caso de más de dos subespacios. Teorema 2.6. (Smith, Solmon, Wagner 1977 [21]): Sean M1,M2, ...,Mk subespacios cerrados en H y Sea M = ∩ki=1Mi. Entonces, para cada x ∈ H y un entero n ≥ 1 ‖ (PMkPMk−1 · · ·PM1) n(x)− P∩ki=1Mi(x) ‖≤ C n ‖ x ‖ Donde C = [1− k−1∏ i=1 sen2 θi] 1/2 Y θi es el Ángulo entre el subespacio Mi y ∩kj=i+1Aj Caṕıtulo 3 Proyecciones Alternantes para el PAMM 3.1. Introducción En este caṕıtulo nos apoyamos en el trabajo doctoral [18] y en el art́ıculo [19], en donde se plantea un nuevo enfoque para resolver el Problema de Actualización en el Modelo Matricial (PAMM), el cual consiste en actualizar algunas matrices que surgen en el modelo matricial de segundo orden, de manera tal que se sustituyen los autovalores y autovectores “malos” por los “deseados”, planteando el PAMM como un problema de optimización no lineal con restricciones se propone aplicar el método de proyecciones alternantes (MAP) y además se muestra una variación de aceleración de Appleby y Smorlarski [1] planteada en [18] para resolver el problema no lineal, conservando las propiedades de las matrices involucradas. 12 Proyecciones Alternantes para el PAMM 13 3.2. Sistema matricial de segundo orden en proble- mas de control Se plantea el siguiente sistema matricial de segundo orden, el cual es obtenido a través de una semidiscretización de la ecuación de movimiento mediante el modelo de elemento finito aplicado a un problema de control: Mẍ(t) +Dẋ(t) +Kx(t) = F (t), donde ẋ(t) y ẍ(t) denotan la primera y segunda derivada con respecto al tiempo t. Las matrices M , D y K de n × n son llamadas matriz de masa, amortiguación y de rigidez respectivamente cuyas caracteŕısticas son; M es simétrica positiva definida (M = M t > 0), D y K son simétricas (D = Dt, K = Kt). Los vectores ẍ(t), ẋ(t), x(t) ∈ Rn son los vectores de aceleración, velocidad, desplazamiento respectivamente, y F (t) es un vector que representa la fuerza externa. Estamos interesados en estudiar el caso homogéneo, lo que significa que no se toma en cuenta la fuerza externa F (t) = 0, es decir estudiareamos sistemas modelados por la siguiente ecuación: Mẍ(t) +Dẋ(t) +Kx(t) = 0. (3.1) La solución fundamental para el sistema 3.1 es de la forma x(t) = veλt, suponiendo que el sistema obedece a un movimiento armónico, por lo cual el escalar λ y el vector v deben ser la solución del siguiente problema cuadrático de autovalores (Quadratic Eigenvalue Problem - QEP), (λ2M + λD +K)v = 0, (3.2) el cual posee 2n autovalores y 2n autovectores. Los autovalores son las ráıces de la ecuación no lineal det(P (λ)) = 0 donde P (λ) = λ2M + λD +K. (3.3) Proyecciones Alternantes para el PAMM 14 Estamos interesados en cambiar únicamente los autovalores “no deseados” o “malos”, mientras que el resto permanecen invariantes, puesto que estos autovalores malos son los responsable de la resonancia del sistema. 3.3. Asignación parcial de autovalores en problemas de control El problema consiste en reemplazar parte del espectro (conjunto de autovalores) para mantener y/o controlar la estabilidad del sistema. Este proceso es llamado problema cuadrático de asignación parcial de autovalores, y puede ser definido de la siguiente manera: Dados: Las matrices M,K,D ∈ Rn×n, que deben ser: M = M t > 0, K = Kt, D = Dt Un subconjunto de los autovalores {λ1, . . . , λp} con p < n, del conjunto de autova- lores de 3.2 y sus correspondientes autovectores {x1, . . . , xp}. Los autovalores {µ1, . . . , µp} y autovectores {y1, . . . , yp} deseados. Encontrar las matrices K̃, D̃ ∈ Rn×n, tales que el espectro de λ2M + λD̃ + K̃ sea {µ1, . . . , µp, λp+1, . . . , λ2n} y el conjunto de autovectores sea {y1, . . . , yp, xp+1, . . . , x2n}, donde {xp+1, . . . , x2n} son los autovectores de 3.2 correspondientes a {λp+1, . . . , λ2n}, es decir, se cambian p de los 2n autovalores y autovectores denominados “no deseados” {λ1, . . . , λp} y {x1, . . . , xp}, por los deseados {µ1, . . . , µp} y {y1, . . . , yp}. El resto de los 2n− p autovalores y sus correspondientes autovectores permanecen sin cambio. El Problema de Actualización en el Modelo Matricial (PAMM) reemplaza los auto- valores y autovectores “no deseados” por medio de la actualización de las matrices del modelo, exigiendo que las matrices a encontrar D̃ y K̃ sean simétricas. Proyecciones Alternantes para el PAMM 15 Se debe mencionar que el espacio de Hilbert que trabajaremos es el de las matrices reales de tamaño Rn×n, con la norma de Frobenius definida como ‖A‖F = (traza(AtA)) 1 2 y el producto interno 〈A,B〉F = traza(AtB). 3.4. Formulación del problema El Problema de Actualización en el Modelo Matricial (PAMM), puede ser reformulado como un problema de optimización de la siguiente manera [18]: Encontrar las matrices D̃ y K̃ de modo que: min ‖K − K̃‖2F + ‖D − D̃‖ 2 F (3.4) sujeto a: K̃ = K̃t, D̃ = D̃t (3.5) MY1(Λ ∗ 1) 2 + D̃Y1(Λ ∗ 1) + K̃Y1 = 0 (3.6) donde: Λ∗1 = diag(µ1, . . . , µp), es una matriz con los autovalores “deseados”. Y ∗1 = {y1, y2, . . . , yp}, es una matriz cuyas columnas son los autovectores “deseados”. El problema se reduce a encontrar las matrices K̃ y D̃, más cercanas a K y D tal que sean simétricas y actualicen los autovalores y autovectores no deseados. Una opción para resolver este problema es usar el MAP, puesto que nuestro objetivo es encontrar la mejor aproximación en la intersección de subespacios y variedades lineales. En este caso el vector solución es una matriz. La técnica que se va a describir fue estudiada en [19], en ella se utiliza el Método de Proyecciones Alternantes (MAP). Observando que las restricciones del problema pueden Proyecciones Alternantes para el PAMM 16 ser vistas como subespacios o variedades lineales, cuya solución está en la intersección de los mismos. Para simplificar el problema 3.4 se reescribe el sistema de la siguiente manera: min ‖K − K̃‖2F + ‖D − D̃‖ 2 F (3.7) Sujeto a: K̃ = K̃t, D̃ = D̃t A+ D̃B + K̃C = 0 Donde: A = MY1(Λ ∗ 1) 2, B = Y1(Λ ∗ 1) 2, C = Y1. Con A,B,C ∈ Cn×p, n es la dimensión del problema y p la cantidad de autovalores y autovectores que se actualizará. Se reformula el problema 3.7 en función de una matriz, de modo que se definen las matrices en bloque X ∈ R2n×2n y X̃ ∈ R2n×2n como: X =  K 0 0 D  , X̃ =   K̃ 0 0 D̃  . Haciendo manipulaciones algebraicas (ver detalles en [19]) el problema queda reducido a encontrar X̃ tal que: min‖X − X̃‖2F (3.8) sujeto a: X̃ = X̃ t (3.9) A+ Î t ∗ X̃ ∗W = 0. (3.10) Proyecciones Alternantes para el PAMM 17 El operador * representa la multiplicación de matrices, Î y W son matrices por blo- ques, definidas como Î =   In In  , W =  C B  , donde In es la matriz identidad de tamaño n× n. La solución del problema planteado 3.8 se encuentra utilizando el MAP, proyectando sobre cada una de las restricciones. La primera restricción 3.9 es que la matriz X̃ debe pertenecer al espacio de las matrices simétricas, la proyección ortogonal de una matriz X sobre este subespacio viene dada por: PSIM(X) = {X ∈ R2n×2n : X = X t}. El conjunto de las matrices que satisfacen la segunda restricción 3.10 es una varie- dad lineal, aqúı se contempla la asignación de los nuevos autovalores y autovectores, necesitamos proyectar en el siguiente conjunto, V = {X ∈ R2n×2n : A+ ÎXW = 0}. Los vectores que cumplen las restricciones 3.9 y 3.10 son los vectores de la intersección. El siguiente teorema establece la proyección de X sobre la variedad lineal V . La demostración se puede ver con detalle en [19]. Teorema 3.1. Si X ∈ R2n×2n es cualquier matriz dada, entonces la proyección de X sobre la variedad lineal V está dada por PV (X) = X + ÎΣW t, donde Σ satisface W tW ∑ = −1 2 (At +W tX tÎ). Demostración. Ver [19]. El siguiente Algoritmo de Proyecciones Alternantes calcula las matrices D̃ y K̃ tal que sean simétricas y actualicen los autovalores y autovectores no deseados, es decir K̃ y D̃ es solución de la ecuación 3.8. Proyecciones Alternantes para el PAMM 18 Algoritmo 2 Algoritmo de Proyecciones Alternantes para la actualización de las matrices D y K 1: Dadas las matrices M,K,D,Λ∗1, Y1 2: A = MY1(Λ ∗ 1) 2 3: B = Y1(Λ ∗ 1) 4: C = Y1 5: Contruir las matrices y vectores en bloque: X =  K 0 0 D  , Î =   In In  , W =  C B   6: QR=W (Factorización QR de W) 7: whileX /∈ V do 8: RtΓ = −1 2 (At +W tX tÎ) (Sustitución hacia adelante para hallar Γ) 9: RΣt = Γ (Sustitución hacia atrás, para hallar Σ) 10: X = X + Î ∗ ∑ ∗W t (Proyección de X sobre V ) 11: X = X+X t 2 (Proyección de X sobre es subespacio de las matrices simétricas) 12: end while 13: Salida: La matrices D̃ y K̃ solución de 3.8 Las matrices M,K,D ∈ Rn×n cuya propiedad es M = M t > 0, K = Kt y D = Dt La matriz diagonal Λ∗1 = diag(µ1, . . . , µp) que contiene los autovalores “deseados” La matriz Y1 = contiene los autovectores “deseados” en columna {y1, . . . , yp} El algoritmo proyecta sobre la variedad lineal V en las ĺıneas 7 a la 10 del algoritmo 2, y luego proyecta sobre el subespacio de las matrices simétricas encontrando la proyección sobre la intersección. Por el teorema 2.3 el algoritmo 2 converge a la única solución de(3.8) con las restricciones (3.9) y (3.10). Proyecciones Alternantes para el PAMM 19 3.5. Exigir una estructura dada Dependiendo de la naturaleza del problema, las matrices asociadas a la ecuación matricial 3.1 poseen una estructura de raleza que en la mayoŕıa de los casos se desea conservar, las matrices actualizadas D̃ y K̃ deben tener el mismo patrón de raleza que las matrices iniciales D y K, los patrones de raleza encontrados comúnmente corresponden a matrices diagonales, matrices tridiagonales, matrices en banda en general, entre otras. Figura 3.1: Patrones de raleza Al resolver el problema de optimización 3.8-3.10, usando el método de proyecciones alternantes, es probable que en cada iteración las matrices iniciales D y K pierdan su patrón de raleza, por lo tanto es necesario modificar el problema planteado originales en 3.8-3.10 agregando una nueva restricción que permita a las matrices actualizadas Proyecciones Alternantes para el PAMM 20 mantener esta caracteŕıstica. El problema con la nueva restricción seŕıa: min‖X − X̃‖2F (3.11) sujeto a: X̃ = X̃ t (3.12) A+ Î t ∗ X̃ ∗W = 0. (3.13) X̃posee el mismo patrón de raleza que X (3.14) La restricción 3.14 hace referencia a que la matriz X debe pertenecer al siguiente subespacio, Ω = {X ∈ R2n×2n : X posee el mismo patrón de raleza que S}. La matriz S está conformada por elementos unos y ceros que determinan el patrón de raleza, es decir, si sij es igual a uno, entonces se debe mantener el valor ubicado en la posición (i, j) de la matriz a proyectar, en caso contrario debe colocarse un cero en esta posición. La proyección sobre el subespacio Ω viene dada por: PΩ(N) = N ◦ S donde ◦ representa el producto de Hadamard entre las matrices N y S. (N ◦ S) =   Nij si Sij = 1;0 si Sij = 0. El Algoritmo 2 es modificado añadiendo la proyección sobre el subespacio Ω asociado a la restricción 3.14. Esta modificación se puede ver en la ĺınea 13 del algoritmo 3, donde Proyecciones Alternantes para el PAMM 21 se realiza el producto de Hadamard entre las matrices X y S, con la finalidad de que la matriz X recupere el patrón de raleza que poséıa originalmente, el cual está almacenado en la matriz S. Algoritmo 3 Algoritmo actualización D y K con un patrón de raleza 1: Dadas las matrices M,K,D,Λ∗1, Y1 2: A = MY1(Λ ∗ 1) 2 3: B = Y1(Λ ∗ 1) 4: C = Y1 5: Contruir las matrices y vectores en bloque: X =  K 0 0 D  , Î =   In In  , W =  C B   6: Contruir la matriz S con el patrón de raleza de X. S =   Nij si Sij = 1;0 si Sij = 0. 7: QR=W (Factorización QR de W) 8: whileX /∈ V do 9: RtΓ = −1 2 (At +W tX tÎ) (Sustitución hacia adelante para hallar Γ) 10: RΣt = Γ (Sustitución hacia atrás, para hallar Σ) 11: X = X + Î ∗ ∑ ∗W t (Proyección de X sobre V ) 12: X = X+X t 2 (Proyección de X sobre es subespacio de las matrices simétricas) 13: X = X ◦ S (Proyección de X sobre es subespacio Ω) 14: end while 15: Salida: La matrices D̃ y K̃ solución de 3.8 Proyecciones Alternantes para el PAMM 22 3.6. Aceleración del MAP con ideas Appleby y Smor- larski En el caṕıtulo anterior se observó que el MAP tiene una velocidad de convergencia lenta cuando los ángulos entre los subespacios involucrados son pequeños. Los métodos clásicos son lentos y en el trabajo doctoral Moreno [18] se propone un método de ace- leración lineal utilizando las ideas de Appleby y Smolarski [1] calculando dos centroides xc1, xc2 ∈ Rn, como el promedio de todas las proyecciones en una iteración (paso de Cimmino). Este método determina una ĺınea parametrizada a través de estos centroides, y moviendose a lo largo de esta ĺınea se calcula el nuevo iterado x̂ que está más cercano a la solución, dicho iterado puede ser descrito como sigue: x̃ = xc1 + δ(xc2 − xc1), (3.15) Donde δ es una distancia a lo largo de la ĺınea que pasa por el iterado xc1 con dirección xc2 − xc1. Para esto se calcula el nuevo iterado x̃ como el punto de intersección entre la ĺınea y el hiperplano más cercano a esta. En el caso del problema PAMM, los iterados son matrices de Rn×n, por lo que se adapta el método de aceleración lineal a matrices. El cálculo de δ no es directo y éste se convierte en un problema de optimización. Sean X̂ = Xc1 + δ(Xc2 −Xc1) (3.16) Para determinar δ forzando que X̂ sea simétrica, se quiere encontrar δ tal que mı́n δ ‖An − Atn‖ 2 F , donde An = Ac1 + δ(Ac2 − Ac1) y Atn = Atc1 + δ(Atc2 − Atc1). Haciendo manipulaciones algebraicas se obtiene (Para más detalles [18]). Proyecciones Alternantes para el PAMM 23 δ = ‖Ac1 − Atc1‖2F − 〈Ac1 − A t c1, Ac2 − Atc2〉 ‖Ac1 − Atc1‖2F − 2〈Ac1 − A t c1, Ac2 − Atc2〉+ ‖Ac2 − Atc2‖2F (3.17) Se muestra a continuación el algoritmo de aceleración correspondiente al Algoritmo 2 usando el δ que se acaba de calcular. Proyecciones Alternantes para el PAMM 24 Algoritmo 4 Aceleración del MAP con ideas Appleby y Smorlarski para la actualización de las matrices D y K 1: Dadas las matrices M,K,D,Λ∗1, Y1 2: A = MY1(Λ ∗ 1) 2 3: B = Y1(Λ ∗ 1) 4: C = Y1 5: Contruir las matrices y vectores en bloque: X =  K 0 0 D  , Î =   In In  , W =  C B   6: QR=W (Factorización QR de W) 7: whileX /∈ V do 8: RtΓ = −1 2 (At +W tX tÎ) (Sustitución hacia adelante para hallar Γ) 9: RΣt = Γ (Sustitución hacia atrás, para hallar Σ) 10: X1 = X + Î ∗ ∑ ∗W t (Proyección de X sobre V ) 11: X2 = X+Xt 2 (Proyección de X sobre es subespacio de las matrices simétricas) 12: Xc1 = X1+X2 2 (Cálculo del primer centroide) 13: RtΓ = −1 2 (At +W tX tc1Î) (Sustitución hacia adelante para hallar Γ) 14: RΣt = Γ (Sustitución hacia atrás, para hallar Σ) 15: X1 = Xc1 + Î ∗ ∑ ∗W t (Proyección de Xc1 sobre V ) 16: X2 = Xc1+X t c1 2 (Proyección de Xc1 sobre es subespacio de las matrices simétricas) 17: Xc2 = X1+X2 2 (Cálculo del segundo centroide) 18: Cálculo del escalar δ dada en 3.17 19: X = Xc1 + δ(Xc2 −Xc1) (Cálculo el nuevo iterado) 20: end while 21: Salida: La matrices D̃ y K̃ solución de 3.8 Proyecciones Alternantes para el PAMM 25 El Algoritmo 2 se modificó obteniendo aśı el algoritmo acelerado 4, en este caso una vez calculado los centroides en las ĺıneas 12 y 17 se genera un nuevo iterado en la ĺınea 19, pero para el cálculo de cada centroide se debe proyectar sobre una variedad lineal y luego sobre el subespacio. Ahora bien, debido a la necesidad de conservar el patrón de raleza, se plantea el siguiente algoritmo 5 cuya diferencia con respecto al algoritmo 4 es que en lugar de trabajar con un subespacio y una variedad lineal, se tienen dos subespacios y una variedad lineal, para poder conservar dicho patrón de raleza, como se puede observar en la ĺınea 19. Proyecciones Alternantes para el PAMM 26 Algoritmo 5 Aceleración del MAP con ideas Appleby y Smorlarski para la actualización de las matrices D y K con un patrón de raleza 1: Dadas las matrices M,K,D,Λ∗1, Y1 2: A = MY1(Λ ∗ 1) 2 3: B = Y1(Λ ∗ 1) 4: C = Y1 5: Contruir las matrices y vectores en bloque: X =  K 0 0 D  , Î =   In In  , W =  C B   6: Contruir la matriz S con el patrón de raleza de X. S =   Nij si Sij = 1;0 si Sij = 0. 7: QR=W (Factorización QR de W) 8: whileX /∈ V do 9: RtΓ = −1 2 (At +W tX tÎ) (Sustitución hacia adelante para hallar Γ) 10: RΣt = Γ (Sustitución hacia atrás, para hallar Σ) 11: X1 = X + Î ∗ ∑ ∗W t (Proyección de X sobre V ) 12: X2 = X+Xt 2 (Proyección de X sobre es subespacio de las matrices simétricas) 13: X3 = X ◦ S (Proyección de X sobre es subespacio Ω) 14: Xc1 = X1+X2+X3 3 (Cálculo del primer centroide) 15: RtΓ = −1 2 (At +W tX tc1Î) (Sustitución hacia adelante para hallar Γ) 16: RΣt = Γ (Sustitución hacia atrás, para hallar Σ) 17: X1 = Xc1 + Î ∗ ∑ ∗W t (Proyección de Xc1 sobre V ) 18: X2 = Xc1+X t c1 2 (Proyección de Xc1 sobre es subespacio de las matrices simétricas) 19: X3 = Xc1 ◦ S (Proyección de Xc1 sobre es subespacio Ω) 20: Xc2 = X1+X2+X3 3 (Cálculo del segundo centroide) 21: Cálculo del escalar δ dada en 3.17 22: X = Xc1 + δ(Xc2 −Xc1) (Cálculo el nuevo iterado) 23: end while 24: Salida: La matrices D̃ y K̃ solución de 3.8 Caṕıtulo 4 Métodos tipo residual para sistemas no lineales aplicado al PAMM 4.1. Introducción Recientemente se han desarrollado técnicas para acelerar los métodos de proyecciones alternantes y simultáneas. Estas técnicas pudieran aplicarse para resolver el Problema de Actualización en el Modelo Matricial (PAMM). En este trabajo, se implementarán algunas de esas técnicas y las compararemos con las estratégias de aceleración basadas en las ideas Appleby y Smolarski [1], estudiadas y aplicadas para el PAMM en [18] y que fueron descritas en el caṕıtulo anterior. En [13] y [14], se proponen estrategias de aceleración de los métodos de proyecciones alternantes basadas en técnicas de optimización numérica. En el caso que nos concier- ne (variedades lineales), estos autores proponen estrategias de aceleración basadas en métodos que avanzan en la dirección del residual, tales como el DF-SANE, recientemente desarrolladas por La Cruz et al [5][6]. 27 Métodos del residual aplicado al PAMM 28 4.2. Aceleración para variedades lineales Sea H un espacio de Hilbert y sean V1, V2, . . . , Vm variedades lineales cerradas de H, V = ∩mi=1Vi, V 6= ∅ y x ∈ H. Para variedades lineales, los métodos de proyecciones alternantes y simultáneas pue- den escribirse como la iteración siguiente, dado x0 ∈ H xk+1 = Txk. Para el caso del método de Cimmino el operador T = 1 m ∑m i PVi y para el caso de MAP el operador T = PVmPVm−1 ...PV1 . La sucesión {xk} converge a un punto x tal que Tx = x, en ambos casos, el conjunto de puntos fijos del operador Fix T = V . Pero además, en el caso de variedades lineales, los métodos MAP y Cimmino, convergen hacia la proyección de x0 en V , donde V = Fix T . La idea propuesta en [13] y [14] es de encontrar los puntos fijos resolviendo la ecuación: F (x) = (I − T )x = 0. El método a escoger para resolver esta ecuación depende de las caracteŕısticas de F = (I − T ). Por ejemplo, en el caso de que todos los Vi sean subespacios, el operador T asociado al método de Cimmino es lineal, autoadjunto y semipositivo definido; lo mismo para el operador I−T . Por lo tanto es posible usar el método de los gradientes conjugados en la ecuación F (X) = 0 para acelerar el método. En cambio, cuando al menos un Vi es una variedad lineal que no es un subespacio, el operador F deja de ser lineal, y por ello [13] y [14] proponen el uso de métodos tipo residual que no utilizan derivadas, tales como el DF-SANE, estos métodos avanzan en la dirección de ±F (x). A continuación describiremos brevemente el método DF-SANE basados en el art́ıculo [5]. Métodos del residual aplicado al PAMM 29 4.3. Métodos tipo residual para sistemas no lineales Los métodos de residual que usan ±F (x), como dirección de búsqueda para resolver sistemas de ecuaciones no lineales F (x) = 0, han sido desarrollados recientemente por La Cruz et al. [5][6]. Las iteraciones de residual (DF-SANE o SANE) analizadas en [5][6], para problemas no lineales finito dimensionales, están definidas como: xk+1 = xk ± αkF (xk), (4.1) donde αk ≥ 0 es el tamaño del paso y la dirección de búsqueda es ±F (x) dependiendo de cual es la dirección de descenso para la función, f(x) = ‖F (x)‖2 = 〈F (x), F (x)〉. (4.2) La convergencia de 4.1 se alcanza para sistemas no lineales generales, cuando está aso- ciada con las búsquedas lineales no monótonas y libre de derivadas como se explica en [6]. Si el Jacobiano de F (x) es simétrico y positivo semidefinido, entonces habrá con- vergencia para el método puro, es decir, sin estrategias de globalización y usando la dirección −F (x). En [13] se demostró para el caso del método de Cimmino, el operador F (x) tiene Jacobiano autoadjunto y positivo definido lo que es condición suficiente para que el método converja en su versión pura, es decir, sin globalización. Sin embargo en el caso del operador asociado al método MAP la convergencia sin globalización no se ha demostrado.. Para el tamaño de paso αk ≥ 0, se toma la escogencia espectral no monótona, dada por: αk = 〈sk−1, sk−1〉 〈sk−1, yk−1〉 , (4.3) donde sk−1 = xk − xk−1, y yk−1 = F (xk) − F (xk−1). Mediante la aplicación del tamaño de paso 4.3, se acelera la convergencia. Métodos del residual aplicado al PAMM 30 Algoritmo 6 DF-SANE para F (x), en su forma pura 1: Dado x0 ∈ H,α0 ∈ R, α0 6= 0 2: for k = 0, 1, ..., do 3: xk+1 = xk − αkF (xk) 4: sk = xk+1 − xk 5: yk = F (xk+1)− F (xk) 6: αk+1 = 〈sk, sk〉/〈sk, yk〉 7: end for En [13] se demostró que en el caso de variedades lineales, la sucesión {xk} del método DF-SANE (con o sin globalización) produce una sucesión que converge hacia la proyección del iterado inicial x0 en Fix T , es decir, en la intersección de las variedades ∩mi=1Vi. 4.4. Aceleración del Método de Cimmino en varie- dades lineales para el PAMM usando DF-SANE Recordemos que el problema de Actualización en el Modelo Matricial (PAMM), es equivalente a un problema de optimización, en el cual se quiere encontrar X̃ tal que: min‖X − X̃‖2F (4.4) sujeto a: X̃ = X̃ t (4.5) A+ Î t ∗ X̃ ∗W = 0. (4.6) Donde Î y W son matrices por bloques, definidas como Î =   In In  , W =  C B  , donde În es la matriz identidad de tamaño n× n. Métodos del residual aplicado al PAMM 31 Si llamamos F = (I − TV ), donde TV = 12(PV + PS) siendo PS la proyección sobre el subespacio S y PV es la proyección sobre la variedad lineal V , el problema F (x) = 0 puede ser resuelto utilizando métodos tipo residual para hallar ceros de funciones como el método DF-SANE, estudiado en la sección anterior, estos métodos usan la dirección ±F (xk) y las iteraciones son del tipo: xk+1 = xk ± αkF (xk), (4.7) donde αk > 0 es el tamaño del paso y la dirección de búsqueda es F (xk) o −F (xk). En el método de Cimmino, la función F viene dada por: F (x) = x− 1 2 (PV (x) + PS(x)) (4.8) = 1 2 (x− PV (x)) + 1 2 (x− PS(x)) (4.9) Sea X ∈ R2n×2n una matriz en bloque, entonces la función F viene dada como: F (X) = 1 2 (X − PV (X)) + 1 2 (X − PS(X)) (4.10) donde PV es la proyección sobre la variedad lineal V definida como: V = {X ∈ R2n×2× : A+ Î t ∗X ∗W = 0}, y PS es la proyección ortogonal de una matriz X sobre el subespacio de las matrices simétricas, es decir, esta proyección viene dada por: PS = X +X t 2 . El nuevo algoritmo de actualización para el PAMM usando DF-SANE que se mos- trará a continuación, utiliza las mismas matrices de entrada que en el Algoritmo 4. Donde, Las matrices M,K,D ∈ Rn×n cuya propiedad es M = M t > 0, K = Kt y D = Dt La matriz diagonal Λ∗1 = diag(µ1, . . . , µp) que contiene los autovalores “deseados” Métodos del residual aplicado al PAMM 32 Algoritmo 7 Actualización de las matrices D y K usando el método de DF- SANE mediante Cimmino 1: Dadas las matrices M,K,D,Λ∗1, Y1 2: A = MY1(Λ ∗ 1) 2 3: B = Y1(Λ ∗ 1) 4: C = Y1 5: Contruir las matrices y vectores en bloque: X =  K 0 0 D  , Î =   In In  , W =  C B   6: QR=W (Factorización QR de W) 7: whileX /∈ V do 8: RtΓ = −1 2 (At +W tX tkÎ) (Sustitución hacia adelante para hallar Γ) 9: RΣt = Γ (Sustitución hacia atrás, para hallar Σ) 10: PV = Xk + Î ∗ ∑ ∗W t (Proyección de Xk sobre V ) 11: PS = Xk+X t k 2 (Proyección de Xk sobre es subespacio de las matrices simétricas) 12: F (Xk) = 1 2 (Xk − PV ) + 12(Xk − PS) 13: Xk+1 = Xk − αkF (Xk) 14: Sk = Xk+1 −Xk 15: RtΓ = −1 2 (At +W tX tk+1Î) (Sustitución hacia adelante para hallar Γ) 16: RΣt = Γ (Sustitución hacia atrás, para hallar Σ) 17: PV = Xk+1 + Î ∗ ∑ ∗W t (Proyección de Xk+1 sobre V ) 18: PS = Xk+1+X t k+1 2 (Proyección de Xk+1 sobre es subespacio de las matrices simétricas) 19: F (Xk+1) = 1 2 (Xk+1 − PV ) + 12(Xk+1 − PS) 20: Yk = F (Xk+1)− F (Xk) 21: αk+1 = 〈Sk,Sk〉F 〈Sk,Yk〉F 22: end while 23: Salida: La matrices D̂ y K̂ solución de 3.8 Métodos del residual aplicado al PAMM 33 La matriz Y1 = contiene los autovectores “deseados” en columna {y1, . . . , yp} En el siguiente algoritmo 7 se observa: Las ĺıneas del 8 al 10 encuentra la proyección de Xk sobre la variedad lineal V . La ĺınea 11 calcula la proyección ortogonal de Xk sobre el subespacio de las matrices simétricas. La ĺınea 12 calcula la función F (Xk) es el cálculo de la función F (Xk) usando Cimmino (proyecciones simultáneas). La ĺınea 13 calcula el iterado Xk+1. Desde la ĺınea 15 a 19 repite los pasos anteriores para encontrar F (Xk+1). Por último se calcula en la ĺınea 21 el tamaño de paso αk 4.3. Finalmente obtenemos la adaptación del DF-SANE para el problema de Actualización Modelo Matricial. 4.5. Aceleración del Método de Proyecciones Alter- nantes en variedades lineales para el PAMM usando DF-SANE A continuación se muestra otra aceleración del método de proyecciones alternantes usando el DF-SANE, es decir, la función F = I − T viene dada por: F (x) = (x− PSPV (x)) (4.11) En el siguiente algoritmo 8 se observa: Las ĺıneas del 8 al 10 encuentra la proyección de Xk sobre la variedad lineal V . Métodos del residual aplicado al PAMM 34 Algoritmo 8 Actualización de las matrices D y K usando el método de DF- SANE mediante MAP 1: Dadas las matrices M,K,D,Λ∗1, Y1 2: A = MY1(Λ ∗ 1) 2 3: B = Y1(Λ ∗ 1) 4: C = Y1 5: Contruir las matrices y vectores en bloque: X =  K 0 0 D  , Î =   In In  , W =  C B   6: QR=W (Factorización QR de W) 7: whileX /∈ V do 8: RtΓ = −1 2 (At +W tX tkÎ) (Sustitución hacia adelante para hallar Γ) 9: RΣt = Γ (Sustitución hacia atrás, para hallar Σ) 10: PV = Xk + Î ∗ ∑ ∗W t (Proyección de Xk sobre V ) 11: PS = Xk+X t k 2 (Proyección de Xk sobre es subespacio de las matrices simétricas) 12: F (Xk) = Xk − PSPV (Xk) 13: Xk+1 = Xk − αkF (Xk) 14: Sk = Xk+1 −Xk 15: RtΓ = −1 2 (At +W tX tk+1Î) (Sustitución hacia adelante para hallar Γ) 16: RΣt = Γ (Sustitución hacia atrás, para hallar Σ) 17: PV = Xk+1 + Î ∗ ∑ ∗W t (Proyección de Xk+1 sobre V ) 18: PS = Xk+1+X t k+1 2 (Proyección de Xk+1 sobre es subespacio de las matrices simétricas) 19: F (Xk+1) = Xk − PSPV (Xk) 20: Yk = F (Xk+1)− F (Xk) 21: αk+1 = 〈Sk,Sk〉F 〈Sk,Yk〉F 22: end while 23: Salida: La matrices D̂ y K̂ solución de 3.8 Métodos del residual aplicado al PAMM 35 La ĺınea 11 calcula la proyección ortogonal de Xk sobre el subespacio de las matrices simétricas. La ĺınea 12 calcula la función F (Xk) que a diferencia del algoritmo anterior aqúı uti- lizamos la composición o el producto de las proyecciones, es decir, una iteración del método MAP. La ĺınea 13 calcula el iterado Xk+1. Desde la ĺınea 15 a 19 repite los pasos anteriores para encontrar F (Xk+1). Por último se calcula en la ĺınea 21 el tamaño de paso αk. Finalmente obtenemos la adaptación del DF-SANE para el resolver el PAMM usando el MAP. 4.5.1. El método DF-SANE manteniendo un patrón de raleza dado En muchos casos, es importante que la solución al problema PAMM mantenga un patrón de raleza dado [18]. En este caso, la solución debe no solo pertenecer a la intersec- ción del subespacio de las matrices simétricas y la variedad lineal dada en 3.13, sino que debe también pertenecer al subespacio de las matrices que tienen ese patrón de raleza. Es por ello que nuestros métodos de proyecciones trabajarán ahora con 3 conjuntos, y los operadores de la iteración de punto fijo estarán dados por T = 1 3 ∑3 i=1 PVi para el caso de Cimmino y T = PV3PV2PV1 en el caso de MAP, donde: PV3 es la proyección para conservar patrón de raleza. PV2 es la proyección sobre las matrices simétricas. PV1 es la proyección sobre la variedad lineal. Caṕıtulo 5 Resultados numéricos En este caṕıtulo mostramos los resultados de las diferentes aceleraciones aplicadas a la técnica para resolver el problema de actualización modelo matricial (PAMM) estu- diada en [18], para ello se compara el método clásico usando el método de proyecciones alternantes visto en el caṕıtulo 3, las estratégias de aceleración basadas en las ideas Ap- pleby y Smolarski [1] estudiadas en [18], las estrategias de aceleración de los métodos de proyecciones alternantes basadas en técnicas de optimización numérica propuesta por [13] y [14], y una aceleración sugerida por Hernández-Ramos. Todas estas aceleraciones aplicadas a la técnica para resolver el PAMM además de disminuir el número de iteraciones, disminuye también el tiempo de ejecución ya que el costo por iteración es menor. Para realizar los experimentos se seleccionaron cuatro problemas de la tesis doctoral [18] que estan incluidos en la bibliograf́ıa. En los algoritmos desarrollados se observa el siguiente orden de proyecciones: Primero se proyecta sobre la variedad lineal V, esta proyección sustituye los auto- valores y autovectores no deseados por los deseados. Segundo, se proyecta sobre el subespacio de las matrices simétricas. En todo método iterativo es necesario especificar mediante ciertas condiciones cuando 36 Resultados numéricos 37 hay que detener el proceso. El criterio de parada utilizado tiene en cuenta dos factores: 1. La solución encontrada satisface los requerimientos de precisión (tolerancia). Se fijó una tolerancia de 1× 10−8. 2. La cantidad de iteraciones empleadas supera el máximo establecido. Para el primer criterio, el proceso iterativo se detiene cuando ‖A+ Î tXW‖F ≤ 10−8, donde X es la matriz obtenida al finalizar una iteración de los algoritmos, por lo cual la matriz X pertenece al subespacio de las matrices simétricas. Para el segundo criterio el máximo número de iteraciones que establecimos es de 5000, el cual consideramos que el algoritmo no converge si pasa este número de iteraciones. Los algoritmos requeridos para realizar la pruebas numéricas se desarrollaron en Matlab 7.12 (R2011a) y han sido ejecutados con un computador Intel Core 2 Quad 2.10 GHz, 4Gb de RAM con Windows 7. 5.1. Experimento 1 Para este experimento se seleccionó un ejemplo tomado del art́ıculo de Datta y Sar- kissian [7]. Las matrices M,D,K de 4×4, simétricas semidefinidas positivas vienen dadas por: M =   1, 4685 0, 7177 0, 4757 0, 4311 0, 7177 2, 6938 1, 2660 0, 9676 0, 4757 1, 2660 2, 7061 1, 3918 0, 4311 0, 9676 1, 3918 2, 1876   , Resultados numéricos 38 D =   1, 3525 1, 2695 0, 7967 0, 8160 1, 2695 1, 3274 0, 9144 0, 7325 0, 7967 0, 9144 0, 9456 0, 8310 0, 8160 0, 7325 0, 8310 1, 1536   , K =   1, 7824 0, 0076 −0, 1359 −0, 7290 0, 0076 1, 0287 −0, 0101 −0, 0493 −0, 1359 −0, 0101 2, 8360 −0, 2564 −0, 7290 −0, 0493 −0, 2564 1, 9130   Los autovalores de λ2M + λD +K calculados con MATLAB son: −0, 0861± 1, 6242i,−0, 1022± 0, 8876i,−0, 1748± 1, 1922i,−0, 4480± 0, 2465i. Se quiere reasignar solo los autovalores causantes de la inestabilidad. En este caso, es el par −0,0861± 1, 6242i, el cual va a ser sustituido por el par −0, 1000± 1, 6242i. A continuación se muestra la gráfica del desempeño de cada algoritmo con respecto al número de iteraciones, usando como datos de entradas las matrices M,D y K arriba mencionada. Algoritmos iter tiempo MAP para la actualización D y K 96 0,2580 Aceleración Appleby-Smorlarski 43 0,0350 DF-SANE Cimmino 35 0,0270 DF-SANE MAP 17 0,0030 Cuadro 5.1: Comparación entre los diferentes métodos para el Problema de Actualización en el Modelo Matricial en el experimento 1 El costo por iteración de los algoritmos para acelerar el PAMM es menor, esto hace que disminuya también el tiempo de ejecución. Resultados numéricos 39 Figura 5.1: Experimento 1 Se puede observar que para este experimento, el método DF-SANE MAP tiene una ganancia en tiempo y en número de iteraciones. Sin embargo en arquitecturas paralelas, en el método de Cimmino las proyecciones pudieran realizarse simultáneamente, lo que significaŕıa una ganancia en tiempo. 5.2. Experimento 2 Para el segundo experimento seleccionamos las matrices M,D,K de 30×30 simétricas y definidas positivas, cuyas matrices están descritas en Benner, Laud y Mehrmann [3]. Este experimento proviene de la aplicación de un modelo de una secuencia formada por los resortes, los amortiguadores de aire, y las masas unidas, los cuales dan origen a las Resultados numéricos 40 siguientes matrices: M,D = {A ∈ R30×30/a(i, j) = 4 para i = j, a(i, j) = 0 para i 6= j}. Es decir: M,D =   4 0 · · · 0 0 0 4 · · · 0 0 ... · · · ... 0 · · · · · · 0 0 0 · · · 0 4   . K = {A ∈ R30×30/a(i, j) = 2, a(i + 1, j) = a(j, i + 1) = −1 para todo i = j, a(i, j) = 0 para todo i 6= j, a(1, 1) = a(30, 30) = 1}. Es decir: K =   1 −1 0 0 · · · 0 −1 2 −1 0 · · · 0 0 −1 2 −1 ... ... . . . . . . . . . 0 · · · −1 2 −1 0 0 · · · 0 −1 1   . En este caso se empleó el algoritmo 3 y el algoritmo acelerado 5 para encontrar D̃ (matriz diagonal) y K̃ (matriz tridiagonal) y pueda conservar el patrón de raleza. A continuación se muestra la gráfica y un cuadro comparativo del desempeño de cada algoritmo con respecto al número de iteraciones, usando como datos de entradas las matrices M,D y K arriba mencionada. El costo por iteración de los algoritmos para acelerar el PAMM es menor, esto hace que disminuya también el tiempo de ejecución. Sin embargo se puede observar que para este experimento el método DF-SANE MAP tiene una ganancia en tiempo y en número de iteraciones. Resultados numéricos 41 Figura 5.2: Experimento 2 Algoritmos iter tiempo MAP para la actualización D y K con un patrón de raleza 762 0,6090 Aceleración Appleby-Smorlarski con un patrón de raleza 518 0,4840 DF-SANE Cimmino con un patrón de raleza 76 0,0930 DF-SANE MAP con un patrón de raleza 14 0,0010 Cuadro 5.2: Comparación entre los diferentes métodos para el Problema de Actualización en el Modelo Matricial en el experimento 2 5.3. Experimento 3 Sean las matrices M,D y K. Resultados numéricos 42 Las matrices M,K ∈ R66×66 son provenientes de un problema de plataforma pe- trolera. La matriz M es simétrica definida positiva y la matriz K es simétrica semidefinida positiva, ambas matrices son densas. La matriz de amortiguación D está definida por D = ρI66 con ρ = 1, 55. Figura 5.3: Experimento 3 Al igual que en los experimentos anteriores, se observa que las técnicas de aceleración aplicadas al PAMM tiene una disminución en número de iteraciones y en tiempo de eje- cución, ya que el costo por iteración es menor. Sin embargo el algoritmo que el algoritmo que mejor acelera en términos computacionales es el DF-SANE con MAP. Resultados numéricos 43 Algoritmos iter tiempo MAP para la actualización D y K con un patrón de raleza 2574 14,8190 Aceleración Appleby-Smorlarski con un patrón de raleza 2563 13,3590 DF-SANE Cimmino con un patrón de raleza 200 0,8740 DF-SANE MAP con un patrón de raleza 25 0,0460 Cuadro 5.3: Comparación entre los diferentes métodos para el Problema de Actualización en el Modelo Matricial en el experimento 3 5.4. Experimento 4 Sean las matrices M,K ∈ R211×211 simétricas definidas positivas, descritas en Ben- ner, Laudd y Mehrman [3]. Este ejemplo modela un problema que se presenta en centrales eléctricas. Algoritmos iter tiempo MAP para la actualización D y K con un patrón de raleza Núm máx iteraciones * Aceleración Appleby-Smorlarski con un patrón de raleza Núm máx iteraciones * DF-SANE Cimmino con un patrón de raleza Núm máx iteraciones * DF-SANE MAP con un patrón de raleza 168 6,2170 Cuadro 5.4: Comparación entre los diferentes métodos para el Problema de Actualización en el Modelo Matricial en el experimento 4 En este experimento alcanzó el número máximo de iteraciones (5000) excepto para la técnica de aceleración usando DF-SANE MAP, por lo cual esta propuesta sirve para atacar problemas que de otra forma son dif́ıciles de atacar. Resultados numéricos 44 Figura 5.4: Experimento 4 Caṕıtulo 6 Conclusiones y Recomendaciones Basado en las técnicas recientes desarrolladas en [13], las cuales utilizan métodos tipo residual, el presente trabajo propone aplicar dichas técnicas para resolver el Problema de Actualización en el Modelo Matricial (PAMM), estudiado en [18]. El cual es planteado como un problema de optimización no lineal con restricciones, utilizando los métodos de proyecciones alternantes para resolverlo, y el método de Cimmino para acelerarlo. Se pudo observar a través de la experimentación numérica, un aumento de la velocidad de convergencia utilizando las técnicas recientes desarrolladas en [13]. Adicionalmente se hacen pruebas con un operador distinto asociado al método de proyecciones alternantes (DF-SANE MAP). En dichas pruebas esta propuesta fue la que dio mejores resultados computacionales. Todas las aceleraciones aplicadas a la técnica para resolver el PAMM además de disminuir el número de iteraciones, disminuye también el tiempo de ejecución ya que el costo por iteración es menor. La convergencia sin globalización para el caso del método de Cimmino acelerado fue demostrada en [13]. Esa prueba no se ha hecho en el caso del método MAP acelerado (DF- SANE MAP), sin embargo en todos los experimentos realizados se dio la convergencia, aśı que podemos conjeturarla y queda su prueba como un problema abierto para futuras investigaciones. 45 Bibliograf́ıa [1] G. APPLEBY and D. SMOLARSKI. A linear acceleration row action method for projecting onto subspaces. Electronic Transactions on Numerical Analysis (ETNA), 20:253–275, 2005. [2] N. ARONSZAJN. Theory of reproducing kernels. Trans. Amer. Math. Soc., 68:337– 404, 1950. [3] P. BENNER, A.J. LAUD, and V. MEHRMANN. A collection of bench- mark examples for the numerical solutions of algebraic riccati equeations i: Continuous-time case. In Available in electronic form from http://www.tu- chemnitz.de/pester/sfb/spc95pr.html. 1995. [4] G. CIMMINO. Calcolo approsimate per le soluzioni dei sistemi di equazioni lineari. La Ricerca Scientifica. ed. Il progresso tecnico nell’ economia nazionale. Consiglio Nazionale delle Richerche. Ministero dell’ Educatione Nazionale, 9(2):326–333, 1938. [5] W. LA CRUZ, J.M. MARTINEZ, and M. RAYDAN. Spectral residual method without gradient information for solving large-scale nonlinear systems of equations. Mathematics of Computation, 75(255):1429–1448, 2006. [6] W. LA CRUZ and M. RAYDAN. Nonmonotone spectral methods for large-scale nonlinear systems. Optimization Methods and Software, 18(5):583–599, 2003. [7] B.N. DATTA and D.R SARKISSIAN. Theory and computations of some inverse ei- genvalue problems for the quadratic pencil. Contemporary Mathematics, Structured 46 Bibliograf́ıa 47 Matrices in Operator Theory, Control and Signal an Imagen Processing, American Mathematical Society:280:221–240, 2001. [8] F. DEUTSCH. Applications of von Neumann’s alternating projections algorithm. In P. Kenderov, editor, Mathematical Methods in Operations Research, pages 44–51. Sofia, Bulgaria, 1983. [9] F. DEUTSCH. The method of alternating orthogonal projections. In S.P. Singh, editor, Approximation Theory, Spline Functions and Applications, pages 105–121. Kluwer Academic Publishers, Netherlands, 1992. [10] F. DEUTSCH. The angle between subspaces of a Hilbert space. In Approximation theory, wavelets and applications (Maratea, 1994), pages 107–130. Kluwer Acad. Publ., Dordrecht, 1995. [11] R. ESCALANTE and M. RAYDAN. Alternating Projection Methods. SIAM, 2011 (ISBN 978-1-611971-93-4). [12] I. HALPERIN. The product of projection operators. Acta Sci. Math. (Szeged), 23:96–99, 1962. [13] L.M. HERNANDEZ-RAMOS. Método de Proyecciones Alternantes v́ıa optimización numérica. Doctoral dissertation, Universidad Central de Venezuela, 2011. [14] L.M. HERNANDEZ-RAMOS, R. ESCALANTE, and M. RAYDAN. Unconstrai- ned optimization techniques for the acceleration of alternating projection methods. Numerical Functional Analysis and Optimization, 2010. [15] S. KACZMARZ. Angenäherte Auflösung von Systemen linearer Gleichungen. Bull. Internat. Acad. Pol. Sci. Lett., A 35:355–357, 1937. [16] W.J. KAMMERER and M.Z. NASHED. A generalization of a matrix iterative method of G. Cimmino to best approximate solution of linear integral equations of Bibliograf́ıa 48 the first kind. Atti Accad. Naz. Lincei Rend. Cl. Sci. Fis. Mat. Natur.(8), 51:20–25, 1971. [17] S. KAYALAR and H.L. WEINERT. Error bounds for the method of alternating projections. Math. Control Signals Systems, 1:43–59, 1988. [18] J. MORENO. Actualización óptima de matrices en problemas de control. doctoral dissertation, Universidad Central de Venezuela, 2011. [19] J. MORENO, B. DATTA, and M. RAYDAN. A symmetry preserving alternating projection method for matrix model updating. Mechanical Systems and Signal Pro- cessing, pages 23:1784–1791, 2009. [20] J. VON NEUMANN. Functional operators vol. II. The geometry of orthogonal spaces. Annals of Math. Studies, 22, 1950. Princeton University Press. This is a reprint of mimeographed lecture notes first distributed in 1933. [21] K.T. SMITH, D.C. SOLMON, and S.L. WAGNER. Practical and mathematical aspects of the problem of reconstructing objects from radiographs. Bull. Amer. Math. Soc., 83:1227–1270, 1977.