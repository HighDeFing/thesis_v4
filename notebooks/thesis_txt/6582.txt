Universidad Central de Venezuela Facultad de Ciencias Escuela de Computación Centro de Investigación en Sistemas de Información Implementación de una Arquitectura Analítica BIG DATA basada en herramientas de software libre Trabajo Especial de Grado presentado ante la Ilustre Universidad Central de Venezuela por los Bachiller José Luis Prato Zuñiga Hever Alfonso Salas Garcia para optar al título de Licenciado en Computación Tutor: Dr. Pedro N. Bonillo R. Septiembre, 2017 i ii AGRADECIMIENTOS A Dios por darme la oportunidad de vivir y permitirme lograr cada una de las metas propuesta con su apoyo y presencia espiritual. A mis familiares y seres queridos, por su existencia, por su constancia, dedicación y sacrificio, por apoyarme en todo momento y por inculcarme que se puede alcanzar cualquier meta si uno se lo propone. A mi tutor, Prof. Dr. Pedro Bonillo, por haberme dado su apoyo en las consultas y presencia en el desarrollo del presente trabajo. A cada uno de los profesores de la Escuela de Computación quienes impartieron y compartieron sus conocimientos y experiencias en cada una de las etapas vividas durante la carrera. A todas aquellas personas no nombradas, pero que de una u otra manera prestaron su excelente colaboración. A todos estos, solo puedo decir, muchas gracias. iii RESUMEN Cualquier acción que realizamos en internet (la compra de un artículo de ropa, consulta de vuelos, navegar por algunas webs, etc.) deja una huella digital muy valiosa. El auge de las nuevas tecnologías ha provocado que generemos grandes cantidades de estos datos. Estos grandes volúmenes de datos son imposibles de analizar o procesar mediante métodos convencionales. Debido a la limitación en el tratamiento de estas cantidades de datos, nace el concepto Big Data. Debido al gran volumen de datos que comienzan a manejar muchas organizaciones en Venezuela, en muchos casos se requieren de soluciones tecnológicas distintas al modelo relacional, el cual no está diseñado para manejar tantos datos de manera óptima; y mucho menos en el procesamiento y cálculo de indicadores que ayuden a la toma de decisiones sobre dicho volumen de datos. Entre estas organizaciones encontramos el Sistema de Mercado de Alimentos Misión Mercal C.A., el cual maneja una cantidad considerable de datos transaccionales diariamente en sus tiendas encargadas de los registros de inventarios, ventas, compras, cuentas, etc, a nivel nacional. Este Trabajo Especial de Grado tiene como objetivo desarrollar e implementar una arquitectura analítica de Big Data para almacenar, procesar, calcular, visualizar y consultar indicadores originados por grandes volúmenes de datos, en el que actualmente las operaciones de consulta sobre el modelo relacional en más de cien millones de registros, no son capaces de ejecutarse en menos de cinco segundos sobre las tablas del sistema. En el presente TEG se hace uso de la metodología Attribute Driven Design para diseñar la arquitectura, luego se implementa cada uno de sus componentes con herramientas de software libre, haciendo fuerte mención en Apache Hadoop como estacionamiento, Apache Hive como Almacén de Datos sobre Hadoop y motor de consultas y HUE (Hadoop User Experience, por sus siglas en inglés) como herramienta de administración y visualización de reportes y datos, además de fuentes de datos diversas. Palabras claves: Mercado de Alimentos, Big Data, Arquitectura Analítica. iv ABSTRACT Any action we take on the internet (buying an article of clothing, browsing flights, browsing some websites, etc.) leaves a very valuable fingerprint. The rise of new technologies has caused us to generate large amounts of this data. These large volumes of information are impossible to analyze or process using conventional methods. Due to the limitation in the treatment of these amounts of data, the Big Data concept is born. Using Big Data techniques allows you to analyze tastes to offer personalized product offerings, analyze crime trends to know in advance the likelihood of a crime occurring, analyze the success or failure of political or commercial campaigns, etc. Due to the large volume of data that many organizations in Venezuela are beginning to handle, different technological solutions are required to the relational model, which is not designed to handle so much data in an optimal way; Much less in the processing and calculation of indicators that help the decision making on this volume of data. Among these organizations, we find the Misión Mercal (Food Market System), which handles a considerable amount of transactional data daily in its stores in charge of records of inventories, sales, purchases, accounts, etc., at the national level. Aims this work is development and implementation an Analytical Architecture of Big Data. In this work, Attribute Driven Design methodology is used to design the architecture, then it was implemented each of its components with free software tools, making strong mention in Apache Hadoop as stage, Apache Hive as Data Warehouse on Hadoop and query engine, Hadoop User Experience (HUE) as a tool for managing and viewing reports and data. Keywords: Food Market, Big Data, analytics architecture. v INDICE AGRADECIMIENTOS .................................................................................................................... ii RESUMEN ..................................................................................................................................... iii ABSTRACT ................................................................................................................................... iv INDICE .......................................................................................................................................... v INDICE DE TABLAS .................................................................................................................... vii INDICE DE FIGURAS .................................................................................................................. vii INTRODUCCIÓN........................................................................................................................... 1 Capítulo 1: Identificación de la Empresa ....................................................................................... 4 1.1 Reseña Histórica ................................................................................................................. 4 1.2 Misión .................................................................................................................................. 6 1.3 Visión .................................................................................................................................. 6 1.4 Objetivos de la empresa ..................................................................................................... 7 1.5 Organigrama ....................................................................................................................... 7 Capítulo 2: Problema de Investigación ......................................................................................... 8 2.1 Contexto .............................................................................................................................. 8 2.2 Planteamiento del Problema ............................................................................................... 9 2.4 Objetivos ........................................................................................................................... 10 2.4.1 Objetivo General ........................................................................................................ 10 2.4.2 Objetivos Específicos ................................................................................................ 10 2.5 Justificación ....................................................................................................................... 11 2.6 Alcance ............................................................................................................................. 12 2.7 Limitaciones ...................................................................................................................... 12 Capítulo 3: Marco Conceptual ..................................................................................................... 13 3.0 Misión Mercal (Mercado de Alimentos) ..................................................................... 13 3.1 Software Libre ............................................................................................................ 14 3.2 Inteligencia de Negocios (BI) ..................................................................................... 21 3.2.1 Definición ................................................................................................................... 21 3.2.2 Historia ....................................................................................................................... 22 3.2.3 Características ........................................................................................................... 23 3.2.4 Niveles de Realización de BI ..................................................................................... 23 3.2.5 Inteligencia de Empresas ........................................................................................... 24 3.2.6 Inteligencia de Mercados internacionales. ................................................................. 24 3.2.7 Datawarehouse & Business Intelligence ................................................................... 25 3.2.8 La Metodologia de Kimball ......................................................................................... 26 vi 3.3 Big Data ............................................................................................................................ 30 3.3.1 Introducción ............................................................................................................... 30 3.3.2 Historia de Big Data ................................................................................................... 33 3.3.2.1 Big Data Analítica ................................................................................................... 34 3.3.3 Tecnologías involucradas .......................................................................................... 37 3.4 La Inteligencia de Negocio y su evolución con el tiempo a Big Data Analítica ..................... 63 3.5 Ciencia de Datos ............................................................................................................... 67 3.6 Arquitectura de Software .................................................................................................. 69 3.7 Análisis de arquitecturas Big Data .................................................................................... 76 3.8 Diseño de una Arquitectura de Software .......................................................................... 80 Capítulo 4: Marco Metodológico .................................................................................................. 87 4.1 Bases metodológicas de la investigación ......................................................................... 87 4.1.1 Tipo de investigación ................................................................................................. 87 4.1.2 Población y Muestra .................................................................................................. 89 4.1.3 Técnicas e Instrumentos de Recolección de Datos .................................................. 89 4.2 Metodología de Desarrollo ................................................................................................ 90 Capítulo 5: Marco Aplicativo ........................................................................................................ 91 5.1 Entrada del ADD ............................................................................................................... 91 5.1.1 Requerimientos funcionales ...................................................................................... 91 5.1.2 Restricciones de diseño ............................................................................................ 92 5.1.3 Requerimientos de calidad ........................................................................................ 92 5.2 Primera iteración de la metodología ADD ........................................................................ 97 5.2.1 Paso 1: Confirmar que haya suficiente información de los requerimientos .............. 97 5.2.2 Paso 2: Escoger un elemento del sistema a descomponer ...................................... 97 5.2.3 Paso 3: Identificar los drivers de la arquitectura ....................................................... 97 5.2.4 Paso 4: Escoger un patrón que satisfaga los drivers de la arquitectura ................... 98 5.2.5 Paso 5: Instanciar los elementos de la arquitectura y asignar responsabilidades. . 107 5.2.6 Paso 6: Definir las interfaces de los elementos instanciados. ................................ 109 5.2.7 Paso 7: Verificar y refinar los requerimientos para hacer restricciones en los elementos instanciados ...................................................................................................................... 115 5.3 Implementación de la Arquitectura ................................................................................. 124 Conclusiones y Recomendaciones ........................................................................................... 128 Bibliografía ................................................................................................................................. 131 ANEXO 1: Pentaho vs Palo ....................................................................................................... 134 ANEXO 2: Hadoop vs Otros ...................................................................................................... 135 ANEXO 3: Hive vs Pig ............................................................................................................... 136 ANEXO 4: Solr vs Elasticsearch ............................................................................................... 137 ANEXO 5: HUE vs Banano ....................................................................................................... 138 ANEXO 6: Diseño de los mecanismos ETL .............................................................................. 139 vii ANEXO 7: Casos de uso Nivel 1 ............................................................................................... 153 ANEXO 8: Instalación y configuración de Herramientas .......................................................... 157 ANEXO 9: Arquitectura Propuesta de Componentes ............................................................... 182 ANEXO 10: Arquitectura Propuesta con Herramientas Seleccionadas .................................... 183 ANEXO 11: Indicadores Calculados ......................................................................................... 184 ANEXO 12: Juicio de Experto ................................................................................................... 185 INDICE DE TABLAS TABLA 1 ALTERNATIVAS EN TEOREMA DE CAP ................................................................... 56 TABLA 2 ACID VS BASE (BREWER, 2000) ............................................................................... 58 TABLA 3 SEGURIDAD ................................................................................................................ 92 TABLA 4 ESCALABILIDAD ......................................................................................................... 93 TABLA 5 CAPACIDAD DE PRUEBAS ........................................................................................ 94 TABLA 6 TOLERANCIA A FALLOS ............................................................................................ 95 TABLA 7 VISUALIZACIÓN DE DATOS ...................................................................................... 96 TABLA 8 ANÁLISIS DE DATOS ................................................................................................. 96 TABLA 9 DRIVERS DE LA ARQUITECTURA ............................................................................ 97 TABLA 10 PATRONES VS DRIVERS ...................................................................................... 101 TABLA 11 ACTORES - CASOS DE USO NIVEL 0 .................................................................. 121 TABLA 12 FUNCIONALIDADES CASOS DE USO NIVEL 0 .................................................... 122 TABLA 13 FUNCIONALIDADES CASOS DE USO NIVEL 1 .................................................... 153 INDICE DE FIGURAS FIGURA 1 ORGANIGRAMA MERCAL ......................................................................................... 7 FIGURA 2 TAREAS DE LA METODOLOGÍA (CICLO DE VIDA) ............................................... 27 FIGURA 3 BIG DATA EN LAS EMPRESAS FUENTE: HERRAMIENTAS PARA BIG DATA: ENTONO HADOOP (SANCHEZ,2014) .............................................................................. 31 FIGURA 4 TIPOS DE DATOS EN BIG DATA ............................................................................. 32 FIGURA 5 PROCESAMIENTO PARALELO EN MAPREDUCE ................................................. 40 FIGURA 6 ARQUITECTURA BÁSICA DE HADOOP ................................................................. 43 FIGURA 7 CONTEO DE PALABRAS CON MAPREDUCE ........................................................ 44 file:///C:/Users/phd2014/Desktop/Tesis/Tesis_Jose_Prato_Hever_Salas.docx%23_Toc484008555 viii FIGURA 8 HADOOP 1.0 A HADOOP 2.0 ................................................................................... 46 FIGURA 9 NUEVAS APLICACIONES YARN ............................................................................. 46 FIGURA 10 ARQUITECTURA HADOOP 2.0 YARN................................................................... 48 FIGURA 11 ECOSISTEMA HADOOP......................................................................................... 50 FIGURA 12 INDEXACIÓN CON LUCENE .................................................................................. 60 FIGURA 13 ARQUITECTURA SOLR ......................................................................................... 62 FIGURA 14 DIAGRAMA DE VENN CIENCIA DE DATOS ......................................................... 68 FIGURA 15 ESTRUCTURAS DE SOFTWARE .......................................................................... 72 FIGURA 16 ARQUITECTURA BIG DATA IBM ........................................................................... 77 FIGURA 17 ARQUITECTURA BIG DATA CLOUDERA ............................................................. 78 FIGURA 18 ARQUITECTURA BIG DATA HORTONWORKS .................................................... 79 FIGURA 19 ARQUITECTURA BIG DATA DATASTAX-HORTONWORKS ................................ 80 FIGURA 20 CICLO DE VIDA DE ENTREGA EVOLUTIVA ........................................................ 81 FIGURA 21 DIAGRAMA DE COMPONENTES ........................................................................ 119 FIGURA 22 CASO DE USO NIVEL 0; FUENTE: ELABORACIÓN PROPIA ............................ 120 FIGURA 23 INSTALACIÓN JAVA ............................................................................................. 125 FIGURA 24 CLUSTER ESTACIONAMIENTO HDFS 3 NODOS .............................................. 125 FIGURA 25 PROCESOS HADOOP NODO MAESTRO ........................................................... 126 FIGURA 26 EVIDENCIA EJECUCIÓN HIVE ............................................................................ 126 FIGURA 27 EJECUCIÓN SOLR ............................................................................................... 127 FIGURA 28 DISEÑO ETL MISIÓN MERCAL ........................................................................... 139 FIGURA 29 JOB CARGA INICIAL ............................................................................................ 140 FIGURA 30 TRANSF_PDI_HADOOP ....................................................................................... 141 FIGURA 31 PASO DE CONEXIÓN BASE DE DATOS ORACLE ............................................ 142 FIGURA 32 PASO DE CONSULTA SQL SOBRE ORACLE .................................................... 142 FIGURA 33 CALCULATOR ....................................................................................................... 143 FIGURA 34 HADOOP FILE OUTPUT ....................................................................................... 143 FIGURA 35 CAMPOS A CARGAR HDFS ................................................................................ 144 FIGURA 36 LOAD_HIVE ........................................................................................................... 144 FIGURA 37 CONEXIÓN A HIVE ............................................................................................... 145 FIGURA 38 CONSULTA HQL ................................................................................................... 146 FIGURA 39 CARGA DE LOS DATOS EN ESTRUCTURA HIVE ............................................. 146 FIGURA 40 LOAD_HIVE_MAP ................................................................................................. 147 FIGURA 41 CONEXIÓN A HIVE ............................................................................................... 148 FIGURA 42 CREACIÓN DE CUBO HIVE ................................................................................. 148 FIGURA 43 HIVETOSOLR ........................................................................................................ 149 ix FIGURA 44 CONEXIÓN A HIVE ............................................................................................... 150 FIGURA 45 CONSULTA HQL Y CONVERSIÓN A JSON ........................................................ 151 FIGURA 46 SELECCION DE VALORES .................................................................................. 151 FIGURA 47 CAMPOS CONEXIÓN SOLR ................................................................................ 152 FIGURA 48 REST CLIENT ....................................................................................................... 152 FIGURA 49 CASOS DE USO NIVEL 1 ..................................................................................... 153 FIGURA 50 EVIDENCIA DE LA INSTALACIÓN DE JAVA EN AMBIENTE DE DESARROLLO .......................................................................................................................................... 157 FIGURA 51 EVIDENCIA DE SALIDA DEL COMANDO. ENUMERA NAMENODE Y SECONDARYNAMENODE .............................................................................................. 164 FIGURA 52 EVIDENCIA DE SALIDA DEL COMANDO. ENUMERA NAMENODE Y SECONDARYNAMENODE .............................................................................................. 164 FIGURA 53 EVIDENCIA EJECUCIÓN HIVE ............................................................................ 166 FIGURA 54 EVIDENCIA EJECUCIÓN HIVE ............................................................................ 166 FIGURA 55 DESCARGA MAVEN TARBALL ............................................................................ 168 FIGURA 56 DESCOMPRIMIR MAVEN .................................................................................... 168 FIGURA 57 VERIFICANDO VERSIÓN JAVA ........................................................................... 169 FIGURA 58 VERIFICAR INSTALACIÓN MAVEN ..................................................................... 169 FIGURA 59 DESCARGANDO HUE .......................................................................................... 170 FIGURA 60 CONSTRUYENDO HUE........................................................................................ 170 FIGURA 61 CONSTRUYENDO HUE........................................................................................ 171 FIGURA 62 FINALIZANDO INSTALACIÓN HUE ..................................................................... 171 FIGURA 63 INICIANDO HUE .................................................................................................... 171 FIGURA 64 INTERFAZ WEB HUE ........................................................................................... 172 FIGURA 65 ENLAZANDO HUE - HADOOP ............................................................................. 173 FIGURA 66 ENLAZANDO HUE – HADOOP 2 ......................................................................... 174 FIGURA 67 GESTIÓN HDFS CON HUE .................................................................................. 174 FIGURA 68 COMPILAR NGINX ................................................................................................ 176 FIGURA 69 STATUS NGINX .................................................................................................... 177 FIGURA 70 INTERFAZ NGINX ................................................................................................. 178 1 INTRODUCCIÓN El primer cuestionamiento que posiblemente llegue a su mente en este momento es ¿Qué es Big Data y porqué se ha vuelto tan importante? pues bien, en términos generales podríamos referirnos a este concepto, como a la tendencia en el avance de la tecnología que ha abierto las puertas hacia un nuevo enfoque de entendimiento y toma de decisiones, la cual es utilizada para describir enormes cantidades de datos (estructurados, no estructurados y semi estructurados) que tomaría demasiado tiempo y sería muy costoso cargarlos a un base de datos relacional para su análisis. De tal manera que, el concepto de Big Data aplica para toda aquella información que no puede ser procesada o analizada utilizando procesos o herramientas tradicionales. Sin embargo, Big Data no se refiere a alguna cantidad en específico, ya que es usualmente utilizado cuando se habla en términos de petabytes y exabytes de datos. Entonces ¿Cuánto es demasiada información de manera que sea elegible para ser procesada y analizada utilizando Big Data? Analicemos primeramente en términos de bytes: Gigabyte = 109 = 1,000,000,000 Terabyte = 1012 = 1,000,000,000,000 Petabyte = 1015 = 1,000,000,000,000,000 Exabyte = 1018 = 1,000,000,000,000,000,000 Además del gran volumen de información, esta existe en una gran variedad de datos que pueden ser representados de diversas maneras en todo el mundo, por ejemplo de dispositivos móviles, audio, video, sistemas GPS, incontables sensores digitales en equipos industriales, automóviles, medidores eléctricos, veletas, anemómetros, etc., los cuales pueden medir y comunicar el posicionamiento, movimiento, vibración, temperatura, humedad y hasta los cambios químicos que sufre el aire, de tal forma que las aplicaciones que analizan estos datos requieren que la velocidad de respuesta sea lo demasiado rápida para lograr obtener la información correcta en el momento preciso. Estas son las características principales de una oportunidad para Big Data. Es importante entender que las bases de datos convencionales son una parte importante y relevante para una solución analítica. De hecho, se vuelve mucho 2 más vital cuando se usa en conjunto con la plataforma de Big Data. Pensemos en nuestras manos izquierda y derecha, cada una ofrece fortalezas individuales para cada tarea en específico. Por ejemplo, un beisbolista sabe que una de sus manos es mejor para lanzar la pelota y la otra para atraparla; puede ser que cada mano intente hacer la actividad de la otra, mas sin embargo, el resultado no será el más óptimo. El problema surge en situaciones en las que se manejan grandes volúmenes de datos. El modelo relacional propuesto por Edgar Frank Codd es difícil de escalar, por lo que no es aplicable en este tipo de situaciones. La finalidad de este Trabajo Especial de Grado es desarrollar una arquitectura analítica para el manejo de grandes volúmenes de datos y cálculo de indicadores, basada en componentes de software libre. Para cumplir con el objetivo del proyecto, se procedió a analizar cada una de las herramientas utilizadas en una arquitectura Big Data mediante el uso de patrones de diseño Big Data y se encontraron que aquellas que siendo de Software Libre lograron cumplir las expectativas y los objetivos de la organización, utilizando como alcance la base de datos relacional desarrollada bajo Oracle en los Sistemas Logístico, Siga y SAFS, además de archivos xls externos de la Misión Mercal de la República Bolivariana de Venezuela. La estructura del presente TEG se muestra a continuación: Capítulo 1: Identificación y descripción de la organización caso de estudio de este TEG. Destacando la reseña histórica además de la misión, visión y objetivos de la empresa. Capítulo 2: Presentación del problema de investigación, manifestaciones y evidencias necesarias para establecer el objetivo general, los objetivos específicos, la justificación, el alcance y las limitaciones. 3 Capítulo 3: Conceptualización de las bases teóricas que soportan la investigación. Descripción de las tecnologías involucradas en Big Data, breve historia, definición de arquitectura de Software, Software Libre, entre otras, además del diseño de una arquitectura de software utilizando alguna metodología. Capítulo 4: Presentación del marco metodológico, planteamiento de la Metodología Atribute-Driven Design, tipo de investigación, la población y muestra, y de las técnicas e instrumentos de recolección de datos. Capítulo 5: Correspondiente al marco aplicativo, presentando los resultados obtenidos de aplicar la metodología seleccionada, mediante la cual se desarrolló y probó la arquitectura. Por último, las conclusiones y recomendaciones, seguido por la Bibliografía y Anexos. 4 Capítulo 1: Identificación de la Empresa En este capítulo se describirá la empresa Mercado de Alimentos (Misión Mercal C.A.), ya que es el caso de estudio en este trabajo. 1.1 Reseña Histórica La Misión MERCAL tiene su génesis en acontecimientos ocurridos entre diciembre del año 2002 y enero de 2003. En esos meses nuestro país vivió las horas más angustiosas a consecuencia del nefasto sabotaje cometido contra el pueblo venezolano: la paralización de nuestra principal industria, PDVSA. Esa paralización generó un grave problema con la alimentación del pueblo, las principales industrias de producción y comercialización de alimentos se sumaron al vil sabotaje y por esta razón el Estado asume la responsabilidad de garantizar la seguridad alimentaria con la creación del Plan Especial de Seguridad Alimentaria (PESA), donde se conjugó el esfuerzo de empresas como CASA y PROAL, ambas, apoyadas en el hombro inquebrantable de nuestra gloriosa Fuerza Armada Nacional. Aquel esfuerzo mancomunado pronto se vio organizado con la iniciativa del Comandante Hugo Chávez al proponer la creación de un sistema logístico, basado en la planificación de jornadas de ventas de alimentos realizadas al aire libre en las comunidades más desasistidas, a objeto de ofrecer alimentos bajo un esquema de precios accesibles; de este modo, se prevenía cualquier otro intento de vulnerar el derecho de los venezolanos y venezolanas de alimentarse y es por ello que nace Mercados de Alimentos, CA. (MERCAL). El 22 de abril de 2003, MERCAL inicia sus actividades con la inauguración de un Mercal Tipo I realizada en el Sector Ruiz Pineda de la Parroquia Caricuao. Este fue el primer establecimiento en aperturarse y fue comandado por el ciudadano Presidente de la República Bolivariana de Venezuela, Hugo Chávez Frías, quien con orgullo y su acostumbrado furor indicó que se daba inicio a la primera etapa de MERCAL. “Triunfar, triunfar y triunfar, ese es el destino de 5 nuestro pueblo” así lo hizo saber el Comandante Presidente, dando paso a MERCAL que se constituyó en uno de los programas sociales que impulsó el Gobierno Bolivariano para garantizar la cesta alimentaría a los más desposeídos. En sus inicios MERCAL beneficiaba a 55.632 personas y contaba con cinco (5) establecimientos: tres (3) Mercales Tipo I y dos (2) Centros de Acopio; sin embargo, la revolución activó sus mecanismos para la ampliación de los puntos de venta y al cierre del 2003, la red contaba con 1.625 establecimientos, pero la tarea no concluyó allí, se han realizado importantes esfuerzos para que el pueblo pueda acceder a los alimentos y es por ello que al cierre del 2009, MERCAL ha experimentado un gigantesco incremento de beneficiarios elevando la cifra a más de 10 millones de personas. En el 2010, cuenta con más de dieciséis mil ochocientos puntos de venta distribuidos en: 210 Mercales Tipo I, 991 Mercales Tipo II, 36 Supermercales de víveres, 114 centros de acopio, 4 centros frigoríficos, 3 Supermercales de hortalizas, frutas y verduras, 346 Mercalitos móviles, 1.695 Mercalitos comunales y 13.417 Mercalitos. A través del tiempo se ha logrado una conexión entre el pueblo y el Estado, pues esta Misión trabaja de la mano de los Comités de Alimentación de los Consejos Comunales (anteriormente gabinetes de alimentación) para fortalecer la Soberanía y Seguridad Alimentaria de todo el país, siempre basados en la premisa de que un pueblo libre y organizado, debe velar junto al Estado para que la población disfrute, goce, y ejerza su derecho a recibir una alimentación sana, de calidad y a precios justos. En el transcurrir de los años, MERCAL ha reacondicionado sus infraestructuras para elevar la cantidad de toneladas de alimentos según las necesidades del pueblo. También ha celebrado convenios estratégicos con productores locales para arrimar sus productos hasta los puntos de venta más convenientes para los habitantes de las zonas beneficiadas, todo esto, ha incidido significativamente en la creación de empleos directos e indirectos, lo cual se traduce en una mejora del sector productivo regional y en consecuencia, la reactivación de la economía nacional. 6 La implementación de las jornadas de alimentación tales como operativas, mercados a cielo abierto, hallacazos socialistas, operativos especiales de azúcar, entre otros, han logrado canalizar y atender la necesidad de los venezolanos y venezolanas que durante muchos años se vieron olvidados por los gobiernos capitalistas. Otra idea que tiene raíces puramente socialistas, es la creación de la fórmula llamada Mercalitos Comunales cuya estructura busca fortalecer la organización de las comunidades para garantizar la transferencia de poder al pueblo. Hasta el 2010 MERCAL, ha expendido más de 8.4 millones de toneladas y ha garantizado el acceso a una cesta alimentaría balanceada para los sectores de menores recursos, evolucionando con numerosos beneficiarios desde la creación de esta misión, garantizando no sólo el soberano derecho a la alimentación al pueblo venezolano, sino que también se ha transformado en una importante empresa donde laboran de forma directa más de 8 mil trabajadores y de forma indirecta más de 40 mil personas a nivel nacional. Todo esto ha sido el resultado de un exhaustivo trabajo cargado de compromiso y lealtad por parte de su talento humano. (MERCAL, MERCAL, 2017). 1.2 Misión Consolidar en toda la geografía nacional la distribución planificada de alimentos en las zonas de pobreza extrema, bajo los preceptos que impulsa la Revolución y el Gobierno Nacional para erradicar el hambre en nuestro país. 1.3 Visión En Mercal garantizamos la distribución planificada de alimentos en todas las parroquias en las que hacen vida familias en estado de vulnerabilidad, esto como parte del Plan de Ofensiva y Erradicación de la Pobreza Extrema que lleva a cabo el Sistema Nacional de Misiones y Grandes Misiones Socialistas “Hugo Chávez”. 7 1.4 Objetivos de la empresa El objetivo fundamental de la Misión Mercal es contribuir en forma sustancial a mejorar la situación nutricional, la salud y calidad de vida de la población venezolana de manera permanente y sustentable. 1.5 Organigrama La Figura 1 muestra el organigrama de la empresa MERCAL, en ella se destaca La Oficina de Tecnología de la Información, ya que representa el área en la cual se desarrolló este Trabajo Especial de Grado. Figura 1 Organigrama MERCAL Fuente: (MERCAL, Organigrama, 2017) 8 Capítulo 2: Problema de Investigación A continuación, se describe la situación de estudio, ubicándola en el contexto para entender su origen y de esta manera proceder a plantear, justificar y limitar el problema; también se define los objetivos del Trabajo Especial de Grado (TEG). 2.1 Contexto En el mundo, la generación de datos ha incrementado exponencialmente en los últimos años con el auge del internet y la globalización. Estos datos provienen de diferentes fuentes y en diferentes formatos, y pueden estar estructurados o no estructurados, lo cual implica un reto en cuanto a su almacenamiento y procesamiento. El presente trabajo está enfocado en implementar una arquitectura Analítica Big Data para el cálculo de indicadores que ayuden a la toma de decisiones por lo cual se explicará el contexto de la institución Mercado de Alimentos Misión Mercal C.A qué hará uso de la misma. La Misión Mercal C.A. maneja una cantidad de registros numerosa en los diversos orígenes de datos, datos que son actualmente almacenados en diversos formatos y en diversos sistemas, archivos Excel o en sus Sistemas de Bases de Datos Relacional SAF (Sistema Automatizado de Facturación), LOGISTICO y SIGA (Sistema de Inventario y Gestión Administrativa). Dicha organización es la encargada de gestionar el almacenamiento y procesamiento del conjunto de transacciones asociado a las operaciones de inventario, ventas, proveedores y clientes, de cada uno de las tiendas denominadas Centros de Acopio, Mercales, Megamercales, entre otros, y de los cuales se hace de mayor relevancia el cálculo de indicadores de gestión para la toma de decisiones en la empresa. 9 2.2 Planteamiento del Problema La Misión Mercal C.A. maneja una gran cantidad de registros, proveniente en distintos formatos. El 100% de sus datos proviene de transacciones, las cuales se almacenan dependiendo del tipo de transacción en uno de sus diferentes Sistemas de Bases de Datos SAF, LOGISTICO y SIGA, o en su defecto en archivos externos. Los servidores destinados a dichas operaciones no habitan en un ambiente centralizado, además, están alcanzando el máximo de su capacidad, por otra parte las consultas que se llevan a cabo en cada uno de estos sistemas varían en un tiempo considerable de respuesta, llegando a exceder hasta los 10 minutos dependiendo de la complejidad de lo que se desee calcular, lo cual indica que el modelo relacional operativo no se encuentra en la capacidad de soportar por si solo los requerimientos del negocio. Además, es necesario destacar que no disponen de un clúster centralizado actualizado, por lo que la actualización de sus datos se realiza con frecuencia variable y no necesariamente en todas las tiendas. Cabe destacar que los contratos de licenciamiento de Oracle de sus diversos Sistemas de Bases de Datos exceden en inversiones monetarias costosas difíciles de costear. El punto fuerte del problema se acrecienta debido a que los Sistemas Actuales no permiten obtener la información de manera oportuna y necesaria para administrar de forma centralizada los datos y menos aún en el cálculo de las métricas de relevancia para el soporte a la toma de decisiones de la empresa, esto se traduce en pérdidas de dinero y de credibilidad en la institución. Pues así, se plantean las siguientes interrogantes:  ¿Existen arquitecturas alternativas, que permitirían manejar adecuadamente la cantidad de registros que actualmente almacena la Misión Mercal C.A. acorde a sus necesidades de negocio?  ¿Cuáles son estas arquitecturas alternativas?  ¿Cuáles son los componentes de estas arquitecturas alternativas? 10  ¿Cuáles de estos componentes pueden estar en Software Libre?  ¿Cuáles de estos componentes pueden incluirse en la propuesta solución de una nueva arquitectura Analítica Big Data?  ¿Existe alguna empresa en Venezuela que apoye en el mantenimiento, crecimiento, evolución y cambios de la arquitectura solución a fin de garantizar la continuidad? 2.4 Objetivos Se plantean a continuación, el objetivo general y los objetivos específicos para respaldar y solventar la situación planteada: 2.4.1 Objetivo General El objetivo general del presente trabajo es la Implementación de una Arquitectura Analítica BIG DATA basada en herramientas de software libre, específicamente para la Misión Mercal C.A. (Mercado de Alimentos). 2.4.2 Objetivos Específicos Los objetivos específicos del presente trabajo son listados a continuación:  Estudiar las arquitecturas Analítica Big Data disponibles en el mercado  Proponer y diseñar la arquitectura Analítica Big Data.  Seleccionar las herramientas asociadas a cada componente de la arquitectura diseñada.  Implementar la arquitectura con las herramientas seleccionadas. 11 2.5 Justificación Se listan a continuación los hechos que satisfacen el desarrollo del TEG:  Los servidores de la Misión Mercal C.A. alcanzaron su capacidad máxima de almacenamiento y procesamiento acorde a sus requerimientos de negocio por lo que surge la necesidad de comprar más servidores con mayor capacidad o implementar una nueva arquitectura de almacenamiento de la información que se adapte a las necesidades de la empresa.  La Misión Mercal C.A. no dispone de un Sistema centralizada que permita hacer el cálculo de indicadores que garanticen la generación oportuna de reportes asociados a las medidas de interés para la organización.  La licencia de Oracle que utiliza la Misión Mercal C.A alcanza grandes inversiones monetarias, por lo que se necesitó pensar en una solución de Software Libre, además de que el presupuesto aportado a la misma no abarca el pago a empresas que cobren en dólares.  Las consultas sobre las métricas a calcular realizadas en sus diversos sistemas de bases de datos consumen bastante tiempo en responder, además, de presentar una gran complejidad y en la mayoría de los casos imposibilidad de cálculo debido a la no interoperabilidad.  Al disponer de tantos registros en sus bases de datos el sistema relacional comienza a tener bajas de rendimiento, como el caso de las consultas. Esto debido a que los sistemas de bases de datos relacionales no fueron diseñados para almacenar y procesar enormes cantidades de registros y el uso de joins consume muchos recursos del computador.  La depuración y limpieza de los datos ayuda a disminuir los tiempos de lectura y escritura, por lo que una solución utilizando un Sistema de Archivos Distribuido junto con un paradigma Map Reduce es lo más indicado en este caso.  El desarrollo de una arquitectura Analítica de Big Data en la Misión Mercal C.A. implica una solución innovadora a nivel nacional. 12 2.6 Alcance Se planteó fijar el alcance en el desarrollo de la nueva arquitectura, incluyendo las pruebas de carga y volumen, pero sin las pruebas de estrés. El desarrollo e implementación de la arquitectura se realizó a partir de los datos almacenados en los diferentes Sistemas LOGISTICO, SAF y SIGA de Oracle actualmente usados por la organización, que abarca alrededor de unas 300 tiendas a nivel nacional encargadas de los registros transaccionales. La propuesta de la arquitectura se desarrolló en la sede de Altamira de la Misión Mercal C.A., Municipio Chacao en Caracas, Venezuela. 2.7 Limitaciones Listadas a continuación:  La poca información disponible para el diseño e implementación de una arquitectura Analítica Big Data.  La mayor parte de la información disponible se encuentra en el idioma inglés y el investigador tiene como lengua origen el castellano.  La complejidad para el entendimiento de los esquemas de datos debido a que para cada sistema es distinto. Adicionalmente la conexión remota a las oficinas de Mercal se realiza por un enlace intermitente y de muy baja velocidad.  Curva de aprendizaje inclinada en el entendimiento del funcionamiento de los componentes de la arquitectura Analítica Big Data solución. 13 Capítulo 3: Marco Conceptual Se presentan los antecedentes y las bases teóricas soportes de la investigación. 3.0 Misión Mercal (Mercado de Alimentos) La Real Academia Española define mercado como un conjunto de operaciones comerciales que afectan a un determinado sector de bienes. (Española, 2016) Mercado, en economía, es un conjunto de transacciones de procesos o intercambio de bienes o servicios entre individuos. El mercado no hace referencia directa al lucro o a las empresas, sino simplemente al acuerdo mutuo en el marco de las transacciones. Estas pueden tener como partícipes a individuos, empresas, cooperativas, ONG, entre otros. (Wikipedia, Wikipedia, 2016) El mercado también es el ambiente social (o virtual) que propicia las condiciones para el intercambio. En otras palabras, debe interpretarse como la institución u organización social a través de la cual los ofertantes (productores, vendedores) y demandantes (consumidores o compradores) de un determinado tipo de bien o de servicio, entran en estrecha relación comercial a fin de realizar abundantes transacciones comerciales. Una definición de mercado según la mercadotecnia: Organizaciones o individuos con necesidades o deseos que tienen capacidad y que tienen la voluntad para comprar bienes y servicios para satisfacer sus necesidades. La Misión Mercal C.A. (Mercado de Alimentos) es uno de los programas sociales incentivados por el gobierno venezolano. Creada oficialmente el 24 de abril de 2003, la Misión Mercal está destinada al sector alimentario, dependiente del Ministerio de la Alimentación. 14 El programa consiste en construir y dotar almacenes y supermercados con alimentos y otros productos de primera necesidad a bajos precios para que sean accesibles a la población más necesitada. Los alimentos están subvencionados y llegan a los estantes sin intermediarios, de manera que los precios ofrecidos suelen tener un descuento de entre el 30 y el 45 por ciento de los observados en las otras cadenas de distribución. El programa consiste en construir establecimientos de ventas, dotarlos y facilitar la distribución. Mercal se ha ampliado con los llamados "Mercalitos" (establecimientos de tamaños más reducidos, llamados coloquialmente en el país, Bodegas) que se encuentran en sectores más inaccesibles, camiones con víveres que venden directamente en la calle. También existe "Megamercal", que es un mercado improvisado en la vía pública, de enormes dimensiones en determinado día, ampliándose el número de alimentos y productos a la venta, en donde se ofrece simultáneamente otros servicios sociales tales como cedulación, odontología y revisión de la visión (óptica). 3.1 Software Libre Software libre es el software que respeta la libertad de los usuarios y la comunidad. A grandes rasgos, significa que los usuarios tienen la libertad de ejecutar, copiar, distribuir, estudiar, modificar y mejorar el software. Es decir, el software libre es una cuestión de libertad, no de precio. Para entender el concepto, piense en libre como en libre expresión. (gnu.org-2017) Promovemos estas libertades porque todos merecen tenerlas. Con estas libertades, los usuarios (tanto individualmente como en forma colectiva) controlan el programa y lo que este hace. Cuando los usuarios no controlan el programa, decimos que dicho programa no es libre, o que es «privativo». Un programa que no es libre controla a los usuarios, y el programador controla el programa, con lo cual el programa resulta ser un instrumento de poder injusto. 15 3.1.1 Las cuatro libertades esenciales Un programa es software libre si los usuarios tienen las cuatro libertades esenciales:  La libertad de ejecutar el programa como se desea, con cualquier propósito (libertad 0).  La libertad de estudiar cómo funciona el programa, y cambiarlo para que haga lo que usted quiera (libertad 1). El acceso al código fuente es una condición necesaria para ello.  La libertad de redistribuir copias para ayudar a su prójimo (libertad 2).  La libertad de distribuir copias de sus versiones modificadas a terceros (libertad 3). Esto le permite ofrecer a toda la comunidad la oportunidad de beneficiarse de las modificaciones. El acceso al código fuente es una condición necesaria para ello. Un programa es software libre si otorga a los usuarios todas estas libertades de manera adecuada. De lo contrario no es libre. Existen diversos esquemas de distribución que no son libres, y si bien podemos distinguirlos en base a cuánto les falta para llegar a ser libres, nosotros los consideramos contrarios a la ética a todos por igual. En cualquier circunstancia, estas libertades deben aplicarse a todo código que pensemos utilizar hacer que otros utilicen. Tomemos por ejemplo un programa A que automáticamente ejecuta un programa B para que realice alguna tarea. Si se tiene la intención de distribuir A tal cual, esto implica que los usuarios necesitarán B, de modo que es necesario considerar si tanto A como B son libres. No obstante, si se piensa modificar A para que no haga uso de B, solo A debe ser libre; B no es relevante en este caso. Software libre no significa que no es comercial. Un programa libre debe estar disponible para el uso comercial, la programación comercial y la distribución comercial. La programación comercial de software libre ya no es inusual; el software libre comercial es muy importante. Puede haber pagado dinero para 16 obtener copias de software libre, o puede haber obtenido copias sin costo. Pero sin tener en cuenta cómo obtuvo sus copias, siempre tiene la libertad de copiar y modificar el software, incluso de vender copias. 3.1.1.1 La libertad de ejecutar el programa como se desee La libertad de ejecutar el programa significa que cualquier tipo de persona u organización es libre de usarlo en cualquier tipo de sistema de computación, para cualquier tipo de trabajo y finalidad, sin que exista obligación alguna de comunicarlo al programador ni a ninguna otra entidad específica. En esta libertad, lo que importa es el propósito del usuario, no el del programador. Usted como usuario es libre de ejecutar el programa para alcanzar sus propósitos, y si lo distribuye a otra persona, también esa persona será libre de ejecutarlo para lo que necesite; usted no tiene el derecho de imponerle sus propios objetivos a la otra persona. La libertad de ejecutar el programa como se desea significa que al usuario no se le prohíbe o no se le impide hacerlo. No tiene nada que ver con el tipo de funcionalidades que el programa posee ni con el hecho de que el programa sea o no sea útil para lo que se quiere hacer. 3.1.1.2 La libertad de estudiar el código fuente y modificarlo Para que las libertades 1 y 3 (realizar cambios y publicar las versiones modificadas) tengan sentido, usted debe tener acceso al código fuente del programa. Por consiguiente, el acceso al código fuente es una condición necesaria para el software libre. El «código fuente» ofuscado no es código fuente real y no cuenta como código fuente. La libertad 1 incluye la libertad de usar su versión modificada en lugar de la original. Si el programa se entrega unido a un producto diseñado para ejecutar versiones modificadas por terceros, pero rechaza ejecutar las suyas —práctica conocida como «bloqueo», o (según la terminología perversa de quienes lo practican) «arranque seguro»—, la libertad 1 se convierte en una vana 17 simulación más que una realidad práctica. Estos binarios no son software libre, aun cuando se hayan compilado a partir de un código fuente libre. Una manera importante de modificar el programa es agregándole subrutinas y módulos libres ya disponibles. Si la licencia del programa específica que no se pueden añadir módulos que ya existen y que están bajo una licencia apropiada, por ejemplo, si requiere que usted sea el titular del copyright del código que desea añadir, entonces se trata de una licencia demasiado restrictiva como para considerarla libre. Si una modificación constituye o no una mejora, es un asunto subjetivo. Si su derecho a modificar un programa se limita, básicamente, a modificaciones que alguna otra persona considera una mejora, el programa no es libre. 3.1.1.3 La libertad de redistribuir copias si así lo desea: requisitos básicos La libertad para distribuir (libertades 2 y 3) significa que usted tiene la libertad para redistribuir copias con o sin modificaciones, ya sea gratuitamente o cobrando una tarifa por la distribución, a cualquiera en cualquier parte. Ser libre de hacer esto significa, entre otras cosas, que no tiene que pedir ni pagar ningún permiso para hacerlo. También debe tener la libertad de hacer modificaciones y usarlas en privado para su propio trabajo o pasatiempo, sin siquiera mencionar que existen. Si publica sus cambios, no debe estar obligado a notificarlo a nadie en particular, ni de ninguna manera en particular. La libertad 3 incluye la libertad de publicar sus versiones modificadas como software libre. Una licencia libre también puede autorizar otras formas de publicación; en otras palabras, no tiene que ser una licencia con copyleft. No obstante, una licencia que requiera que las versiones modificadas no sean libres, no se puede considerar libre. La libertad de redistribuir copias debe incluir las formas binarias o ejecutables del programa, así como el código fuente, tanto para las versiones modificadas 18 como para las que no lo estén. (Distribuir programas en forma de ejecutables es necesario para que los sistemas operativos libres se puedan instalar fácilmente). Resulta aceptable si no existe un modo de producir un formato binario o ejecutable para un programa específico, dado que algunos lenguajes no incorporan esa característica, pero debe tener la libertad de redistribuir dichos formatos si encontrara o programara una forma de hacerlo. 3.1.1.4 Copyleft Ciertos tipos de reglas sobre la manera de distribuir software libre son aceptables, cuando no entran en conflicto con las libertades principales. Por ejemplo, el copyleft, definido muy sucintamente, es la regla en base a la cual, cuando redistribuye el programa, no se puede agregar restricciones para denegar a los demás las libertades principales. Esta regla no entra en conflicto con las libertades principales, más bien las protege. En el proyecto GNU se usa el copyleft para proteger legalmente las cuatro libertades para todos. Existen razones importantes por las que es mejor usar el copyleft. De todos modos, el software libre sin copyleft también es ético. 3.1.2 Reglas acerca del empaquetamiento y la distribución Eventuales reglas sobre cómo empaquetar una versión modificada son aceptables si no limitan substancialmente su libertad para publicar versiones modificadas, o su libertad para hacer y usar versiones modificadas en privado. Así, es aceptable que una licencia le obligue a cambiar el nombre de la versión modificada, eliminar el logotipo o identificar sus modificaciones como suyas. Son aceptables siempre y cuando esas obligaciones no sean tan agobiantes que le dificulten la publicación de las modificaciones. Como ya está realizando otras modificaciones al programa, no le supondrá un problema hacer algunas más. Las reglas del tipo «si pone a disposición su versión de este modo, también debe hacerlo de este otro modo» también pueden ser, bajo la misma condición, admisibles. Un ejemplo de una regla admisible sería alguna que requiera que, si usted ha distribuido una versión modificada y uno de los programadores 19 anteriores le solicita una copia, usted deba enviársela (tenga en cuenta que tal regla le sigue permitiendo optar por distribuir o no distribuir su versión). Las reglas que obligan a suministrar el código fuente a los usuarios de las versiones publicadas también son admisibles. Un problema particular se presenta cuando la licencia requiere que a un programa se le cambie el nombre con el cual será invocado por otros programas. De hecho, este requisito dificulta la publicación de la versión modificada para reemplazar al original cuando sea invocado por esos otros programas. Este tipo de requisitos es aceptable únicamente cuando exista un instrumento adecuado para la asignación de alias que permita especificar el nombre del programa original como un alias de la versión modificada. 3.1.3 Normas de exportación En algunos casos las normas de control de exportación y las sanciones comerciales impuestas por el Gobierno pueden limitar la libertad de distribuir copias de los programas a nivel internacional. Los desarrolladores de software no tienen el poder de eliminar o pasar por alto estas restricciones, pero lo que sí pueden y deben hacer es rehusar imponerlas como condiciones para el uso del programa. De este modo, las restricciones no afectarán las actividades ni a las personas fuera de las jurisdicciones de tales Gobiernos. Por tanto, las licencias de software libre no deben requerir la obediencia a ninguna norma de exportación que no sea trivial como condición para ejercer cualquiera de las libertades esenciales. La mera mención de la existencia de normas de exportación, sin ponerlas como condición de la licencia misma, es aceptable ya que esto no restringe a los usuarios. Si una norma de exportación es de hecho trivial para el software libre, ponerla como condición no constituye un problema real; sin embargo, es un problema potencial ya que un futuro cambio en la ley de exportación podría hacer que el requisito dejara de ser trivial y que el software dejara de ser libre. 20 3.1.4 Consideraciones legales Para que estas libertades sean reales, deben ser permanentes e irrevocables siempre que usted no cometa ningún error; si el programador del software tiene el poder de revocar la licencia, o de añadir restricciones a las condiciones de uso en forma retroactiva, sin que haya habido ninguna acción de parte del usuario que lo justifique, el software no es libre. Una licencia libre no puede exigir la conformidad con la licencia de un programa que no es libre. Así, por ejemplo, si una licencia requiere que se cumpla con las licencias de «todos los programas que se usan», en el caso de un usuario que ejecuta programas que no son libres este requisito implicaría cumplir con las licencias de esos programas privativos, lo cual hace que la licencia no sea libre. Es aceptable que una licencia especifique la jurisdicción de competencia o la sede para la resolución de conflictos, o ambas cosas. 3.1.5 Licencias basadas en contrato La mayoría de las licencias de software libre están basadas en el copyright, y existen límites en los tipos de requisitos que se pueden imponer a través del copyright. Si una licencia basada en el copyright respeta la libertad en las formas antes mencionadas, es poco probable que surja otro tipo de problema que no hayamos anticipado (a pesar de que esto ocurre ocasionalmente). Sin embargo, algunas licencias de software libre están basadas en contratos, y los contratos pueden imponer un rango mucho más grande de restricciones. Esto significa que existen muchas maneras posibles de que tal licencia sea inaceptablemente restrictiva y que no sea libre. Si una licencia basada en un contrato restringe al usuario de un modo que no se puede hacer con las licencias basadas en el copyright, y que no está mencionado aquí como legítimo, tendremos que analizar el caso, y probablemente concluyamos que no es libre. 21 3.2 Inteligencia de Negocios (BI) Inteligencia de negocios o BI (del inglés Business Intelligence), es el conjunto de estrategias, aplicaciones, datos, productos, tecnologías y arquitectura técnica, los cuales están enfocados a la administración y creación de conocimiento sobre el medio, a través del análisis de los datos existentes en una organización o empresa. Es posible diferenciar datos, informaciones, y conocimientos, conceptos en los que se centra la inteligencia empresarial, ya que como sabemos, un dato es algo vago, por ejemplo "10 000", la información es algo más preciso, por ejemplo "Las ventas del mes de mayo fueron de 10 000", y el conocimiento se obtiene mediante el análisis de la información, por ejemplo "Las ventas del mes de mayo fueron 10 000. Mayo es el mes más bajo en ventas". Aquí es donde la BI entra en juego, ya que al obtener conocimiento del negocio una vez capturada la información de todas las áreas en la empresa, es posible establecer estrategias y determinar cuáles son las fortalezas y las debilidades. 3.2.1 Definición Una interesante definición para inteligencia de negocios o BI, por sus siglas en inglés, según el Data Warehouse Institute, lo define como la combinación de tecnología, herramientas y procesos que permiten transformar los datos almacenados en información, esta información en conocimiento y este conocimiento dirigido a un plan o una estrategia comercial. La inteligencia de negocios debe ser parte de la estrategia empresarial, esta le permite optimizar la utilización de recursos, monitorear el cumplimiento de los objetivos de la empresa y la capacidad de tomar buenas decisiones para así obtener mejores resultados. Las herramientas de inteligencia se basan en la utilización de un sistema de información de inteligencia que se forma con distintos datos extraídos de la 22 producción, con información relacionada con la empresa o sus ámbitos, y con datos económicos. Mediante las herramientas y técnicas ETL (Del inglés "Extract, transform & Load"), o ETC (equivalente en Castellano: "extracción, transformación y carga"), se extraen los datos de distintas fuentes, se depuran y preparan (homogeneización de los datos), para luego cargarlos en un almacén de datos. La vida o el periodo de éxito de un software de inteligencia de negocios dependerá únicamente del éxito de su uso en beneficio de la empresa; si esta empresa es capaz de incrementar su nivel financiero-administrativo y sus decisiones mejoran la actuación de la empresa, el software de inteligencia de negocios seguirá presente mucho tiempo, en caso contrario será sustituido por otro que aporte mejores y más precisos resultados. Finalmente, las herramientas de inteligencia analítica posibilitan el modelado de las representaciones basadas en consultas para crear un cuadro de mando integral que sirve de base para la presentación de informes. 3.2.2 Historia En un artículo de 1958, el investigador de IBM Hans Peter Luhn utiliza el término Inteligencia de Negocio. Se define la inteligencia como: "La capacidad de comprender las interrelaciones de los hechos presentados de tal forma que consiga orientar la acción hacia una meta deseada". La inteligencia de negocios, tal como se entiende hoy en día, se dice que ha evolucionado desde los sistemas de apoyo a las decisiones que se inició en la década de 1960 y desarrollado a lo largo de mediados de los años 80. DSS se originó en los modelos por computadora, creado para ayudar en la toma de decisiones y la planificación. Desde DSS, data warehouses, sistemas de información ejecutiva, OLAP e inteligencia de negocios entraron en principio centrándose a finales de los años 80. 23 En 1989, Howard Dresner (más tarde, un analista de Gartner Group) propuso la "inteligencia de negocios" como un término general para describir "los conceptos y métodos para mejorar la toma de decisiones empresariales mediante el uso de sistemas basados en hechos de apoyo", sin embargo, esta expresión no se popularizó hasta finales de la década de los 90. 3.2.3 Características Este conjunto de herramientas y metodologías tienen en común las siguientes características: Accesibilidad a la información. Los datos son la fuente principal de este concepto. Lo primero que debe garantizar este tipo de herramientas y técnicas será el acceso de los usuarios a los datos con independencia de la procedencia de estos. Apoyo en la toma de decisiones. Se busca ir más allá en la presentación de la información, de manera que los usuarios tengan acceso a herramientas de análisis que les permitan seleccionar y manipular sólo aquellos datos que les interesen. Orientación al usuario final. Se busca independencia entre los conocimientos técnicos de los usuarios y su capacidad para utilizar estas herramientas. 3.2.4 Niveles de Realización de BI De acuerdo a su nivel de complejidad se pueden clasificar las soluciones de Business Intelligence en: - Informes - Informes predefinidos - Informes a medida 24 - Consultas (Query) / Cubos OLAP (On-Line Analytic Processing). - Alertas - Análisis - Análisis estadístico - Pronósticos (Forecasting) - Modelado predictivo o Minería de datos (Data Mining) - Optimización - Minería de Procesos 3.2.5 Inteligencia de Empresas La Inteligencia de Empresas es el concepto más amplio del uso de la inteligencia en las organizaciones. Desde distintas perspectivas, la inteligencia de empresas ha ido emergiendo a partir de la contribución de muchas áreas del conocimiento: market intelligence (inteligencia de mercados), competitive intelligence (Inteligencia Competitiva), business intelligence (inteligencia empresarial). Este concepto ha sido muy utilizado en el mundo de la tecnología con distintos significados como inteligencia de negocios, strategic foresight (Inteligencia Estratégica), corporate intelligence (Inteligencia Corporativa), vigilancia tecnológica, prospectiva tecnológica, etc. 3.2.6 Inteligencia de Mercados internacionales. La estrategia debe ser vista como un proceso creativo, buscar nuevas formas de hacer las cosas, de generar valor en el mundo de continuo cambio, y ser efectivo en el corto plazo por lo cual se necesita: - Inteligencia para crear y compartir el conocimiento. - La habilidad para integrar y administrar este conocimiento. - La imaginación para visualizar acciones alternativas a las usuales y analizar sus consecuencias. 25 - La pericia para manejar los recursos y atender las necesidades actuales sin dejar de construir el futuro deseable. Así como en mercadotecnia se tienen las 4 p's también en inteligencia de mercados se tienen 4 P's PLAN: curso de acción conscientemente determinado. POSICIÓN: un medio para ubicar a la organización (nicho, rentas, dominio). PATRÓN: es un modelo que implica consistencia. PERSPECTIVA: una manera particular de percibir el mundo (concepto, cultura, ideología). Con la globalización, la competencia se convierte en hipercompetencia para lo cual hay que reaccionar con rapidez, sorpresa, anticipación, también hay que cambiar las reglas del juego y hacer productos innovadores integrales para demostrar superioridad ante la competencia. 3.2.7 Datawarehouse & Business Intelligence Ralf Kimball (1944) es considerado el inventor del Modelo Dimensional y pionero en Data Warehouse e Inteligencia de Negocios. Define un almacén de datos como: "una copia de las transacciones de datos específicamente estructurada para la consulta y el análisis". También fue Kimball quien determinó que un datawarehouse no era más que: "la unión de todos los Data Marts de una entidad". Defiende por tanto una metodología ascendente (buttom-up). Entre las bibliografías más destacadas y conocidas de Ralf Kimball se encuentran: - Kimball, Ralph; Margy Ross (2013). The Data Warehouse Toolkit: The Definitive Guide to Dimensional Modeling (3rd ed.). Wiley. ISBN978-1- 118-53080-1. 26 - Kimball, Ralph; Margy Ross (2010). The Kimball Group Reader. Wiley. ISBN 978-0-470-56310-6. - Kimball, Ralph; Margy Ross, Warren Thornthwaite, Joy Mundy, Bob Becker (2008). The Data Warehouse Lifecycle Toolkit (2nd ed.). Wiley. ISBN 978-0-470-14977-5. - Kimball, Ralph; Joe Caserta (2004). The Data Warehouse ETL Toolkit. Wiley. ISBN 0-7645-6757-8. - Kimball, Ralph; Margy Ross (2002). The Data Warehouse Toolkit: The Complete Guide to Dimensional Modeling (2nd ed.). Wiley. ISBN 0-471- 20024-7. - Kimball, Ralph; Richard Merz (2000). The Data Webhouse Toolkit: Building the Web-Enabled Data Warehouse. Wiley. ISBN 0-471-37680-9. - Kimball, Ralph; et al. (1998). The Data Warehouse Lifecycle Toolkit. Wiley. ISBN 0-471-25547-5. - Kimball, Ralph (1996). The Data Warehouse Toolkit. Wiley. ISBN 978-0- 471-15337-5. 3.2.8 La Metodologia de Kimball La metodología se basa en lo que Kimball denomina Ciclo de Vida Dimensional del Negocio (Business Dimensional Lifecycle). Este ciclo de vida del proyecto de Data Warehouse, está basado en cuatro principios básicos: Centrarse en el negocio: Hay que concentrarse en la identificación de los requerimientos del negocio y su valor asociado, y usar estos esfuerzos para desarrollar relaciones sólidas con el negocio, agudizando el análisis del mismo y la competencia consultiva de los implementadores. Construir una infraestructura de información adecuada: Diseñar una base de información única, integrada, fácil de usar, de alto rendimiento donde se reflejará la amplia gama de requerimientos de negocio identificados en la empresa. 27 Realizar entregas en incrementos significativos: Crear el almacén de datos (DW) en incrementos entregables en plazos de 6 a 12 meses. Hay que usar el valor de negocio de cada elemento identificado para determinar el orden de aplicación de los incrementos. En esto la metodología se parece a las metodologías ágiles de construcción de software. Ofrecer la solución completa: Proporcionar todos los elementos necesarios para entregar valor a los usuarios de negocios. Para comenzar, esto significa tener un almacén de datos sólido, bien diseñado, con calidad probada, y accesible. También se deberá entregar herramientas de consulta ad hoc, aplicaciones para informes y análisis avanzado, capacitación, soporte, sitio web y documentación. Figura 2 Tareas de la metodología (Ciclo de Vida) Como se puede apreciar en la figura, los Requerimientos del Negocio son el soporte inicial de las tareas subsiguientes. También tiene influencia en el plan de proyecto (puede notar la doble fecha entre la caja de definición de requerimientos y la de planificación). Podemos también ver tres rutas o caminos que se enfocan en tres diferentes áreas: 28 Tecnología (Camino Superior): Implica tareas relacionadas con software específico, por ejemplo, Microsoft SQL Analysis Services. Datos (Camino del medio): En la misma diseñaremos e implementaremos el modelo dimensional, y desarrollaremos el subsistema de Extracción, Transformación y Carga (Extract, Transformation, and Load - ETL) para cargar el DW. Aplicaciones de Inteligencia de Negocios (Camino Inferior): En esta ruta se encuentran tareas en las que diseñamos y desarrollamos las aplicaciones de negocios para los usuarios finales. Estas rutas se combinan cuando se instala finalmente el sistema. En la parte de debajo de la figura se muestra la actividad general de administración del proyecto. A continuación, describiremos cada una de las tareas: PLANIFICACIÓN: En este proceso se determina el propósito del proyecto de DW/BI, sus objetivos específicos y el alcance del mismo, los principales riesgos y una aproximación inicial a las necesidades de información. En la visión de programas y proyectos de Kimball, Proyecto, se refiere a una iteración simple del Ciclo de Vida de Kimball, desde el lanzamiento hasta el despliegue. Esta tarea incluye las siguientes acciones típicas de un plan de proyecto: - Definir el alcance (Entender los Requerimientos del Negocio) - Identificar las tareas - Programar las tareas - Planificar el uso de los recursos - Asignar la carga de trabajo a los recursos - Elaboración de un documento final que representa un plan del Proyecto ANÁLISIS DE REQUERIMIENTOS: La definición de los requerimientos es en gran medida un proceso de entrevistar al personal de negocio y técnico, pero siempre conviene tener un poco de preparación previa. Se debe aprender tanto 29 como se pueda sobre el negocio, los competidores, la industria y los clientes del mismo. Hay que leer todos los informes posibles de la organización; rastrear los documentos de estrategia interna; entrevistar a los empleados, analizar lo que se dice en la prensa acerca de la organización, la competencia y la industria. Se deben conocer los términos y la terminología del negocio. MODELADO DIMENSIONAL: El proceso de diseño comienza con un modelo dimensional de alto nivel obtenido a partir de los procesos priorizados de la matriz descrita en el punto anterior. El proceso iterativo consiste en cuatro pasos: - Elegir el Proceso de Negocio. - Establecer el Nivel de Granularidad. - Elegir las Dimensiones. - Identificar medidas y las tablas de hechos. DISEÑO FÍSICO: En esta parte, intentamos contestar las siguientes preguntas: - ¿Cómo puede determinar cuán grande será el sistema de DW/BI? - ¿Cuáles son los factores de uso que llevarán a una configuración más grande y más compleja? - ¿Cómo se debe configurar el sistema? - ¿Cuánta memoria y servidores se necesitan? ¿Qué tipo de almacenamiento y procesadores? - ¿Cómo instalar el software en los servidores de desarrollo, prueba y producción? - ¿Qué necesitan instalar los diferentes miembros del equipo de DW/BI en sus estaciones de trabajo? - ¿Cómo convertir el modelo de datos lógico en un modelo de datos físicos en la base de datos relacional? - ¿Cómo conseguir un plan de indexación inicial? - ¿Debe usarse la partición en las tablas relacionales? 30 DISEÑO DEL SISTEMA DE EXTRACCIÓN, TRANSFORMACIÓN Y CARGA (ETL): Es la base sobre la cual se alimenta el Datawarehouse. Si el sistema ETL se diseña adecuadamente, puede extraer los datos de los sistemas de origen de datos, aplicar diferentes reglas para aumentar la calidad y consistencia de los mismos, consolidar la información proveniente de distintos sistemas, y finalmente cargar (grabar) la información en el DW en un formato acorde para la utilización por parte de las herramientas de análisis. ESPECIFICACIÓN Y DESARROLLO DE APLICACIONES BI: Las aplicaciones de BI son la cara visible de la inteligencia de negocios: los informes y aplicaciones de análisis proporcionan información útil a los usuarios. Las aplicaciones de BI incluyen un amplio espectro de tipos de informes y herramientas de análisis, que van desde informes simples de formato fijo a sofisticadas aplicaciones analíticas que usan complejos algoritmos e información del dominio. Kimball divide a estas aplicaciones en dos categorías basadas en el nivel de sofisticación, y les llama informes estándar y aplicaciones analíticas. 3.3 Big Data 3.3.1 Introducción El concepto de Big Data se refiere al almacenamiento y procesamiento de enormes cantidades de datos, tan desproporcionadamente grandes que no es posible tratarlos con las herramientas de base de datos convencionales. Sin embargo, el término Big Data no hace referencia a alguna cantidad en específico, debido a que lo que para una empresa determinada puede ser Big Data, puede no serlo para otra (Figura 3 Big Data en las empresas Fuente: Herramientas para Big Data: Entono Hadoop (Sanchez, 2014)). 31 Actualmente para considerar una solución Big Data, se toma en cuenta que los datos presenten cuatro características principalmente, los cuales son: Variedad, Velocidad, Volumen y Veracidad (también se hablan de otras V’s como la visualización, sin embargo, las cuatro mencionadas anteriormente son las principales). La variedad indica que la data puede estar en diferentes formatos, puede ser estructurada o no estructurada y venir de distintas fuentes, la velocidad indica la rapidez con la que se generan los datos, el volumen indica la cantidad que se genera y la veracidad indica la certeza de los datos. (IBM, 2015) Para poder comprender mejor el uso de Big Data primero debemos comprender la naturaleza de los problemas que lo han hecho necesario. Actualmente vivimos en la era de la información, en la que el volumen total de datos almacenados de forma electrónica no se mide fácilmente. Sin embargo, en 2006 se estimó que el tamaño del universo digital ascendía a 0,18 zettabytes y en 2011 ya eran 1,8 zettabytes. Estos son unos ejemplos de la cantidad de datos que manejan algunos de los servicios más conocidos: (Magazine, 2013)  Facebook almacena aproximadamente 10 mil millones de fotografías  Ancestry.com almacena aproximadamente 2,5 petabytes de datos.  New York Stock Exchange genera alrededor de 1 terabyte al día.  El colisionador de hadrones de Suiza produce alrededor de 15 petabytes de datos al año. Figura 3 Big Data en las empresas Fuente: Herramientas para Big Data: Entono Hadoop (Sanchez, 2014) 32  Los usuarios de YouTube suben 100 horas de video en 1 minuto  Se producen 300.000 tweets en un minuto.  2.000.000 de consultas a Google en sólo un minuto. Y como éstos existen muchos casos similares de diversas empresas u organizaciones. Además del gran volumen de información que se genera diariamente, ésta existe en una gran variedad de datos que pueden ser representados de diversas maneras en todo el mundo, por ejemplo, de dispositivos móviles, audio, video, sistemas GPS, incontables sensores digitales en equipos industriales, automóviles, entre otros. Las aplicaciones que analizan éstos datos requieren que la velocidad de respuesta sea lo demasiado rápida para lograr obtener la información correcta en el momento preciso. Los principales tipos de datos se resumen en 5 (Figura 4 Tipos de datos en Big Data)  Web and Social Media: Contenido web e información que es obtenida de las redes sociales como Instagram, Facebook, Snapchat, blogs. Figura 4 Tipos de datos en Big Data Fuente: (Soares, 2012) 33  Machine-to-Machine: Sensores o medidores que capturan algún evento en particular (velocidad, temperatura, presión, variables meteorológicas o variables químicas) los cuales transmiten a través de redes alámbricas, inalámbricas o híbridas a otras aplicaciones que traducen estos eventos en información significativa.  Big Transaction Data: Registros de facturación, en telecomunicaciones registros detallados de las llamadas, etc. Estos datos transaccionales están disponibles en formatos tanto semiestructurados como no estructurados.  Biometrics: Información biométrica en la que se incluye huellas digitales, escaneo de retina, reconocimiento facial, genética, etc. En el área de seguridad e inteligencia, los datos biométricos han sido información importante para las agencias de investigación.  Human Generated: Las personas generamos diversas cantidades de datos como la información que guarda un call center al establecer una llamada telefónica, notas de voz, correos electrónicos, documentos electrónicos, estudios médicos, etc. 3.3.2 Historia de Big Data Desde que surgieron las primeras formas de escritura hasta los centros de datos modernos, la raza humana no ha dejado de recopilar información. El crecimiento del sector tecnológico ha provocado el aumento desmesurado del volumen de datos, por lo que son necesarios sistemas de almacenamiento de datos más sofisticados. Se expondrá brevemente los hitos más importantes en el surgimiento del Big Data: (winshuttle, 2016)  El término Big Data es un concepto que hace referencia al almacenamiento de grandes volúmenes de datos y a los procesos necesarios para encontrar patrones repetitivos dentro de esos datos.  Comienzo de sobrecarga de información (1880): El Censo de los Estados Unidos del año 1880 tardó ocho años en tabularse, y se calcula que el 34 censo de 1890 hubiera necesitado más de 10 años para procesarse con los métodos disponibles en la época. Si no se hubieran realizado avances en la metodología, la tabulación no habría finalizado antes de que tuviera que realizarse el censo de 1900.  La máquina tabuladora de Hollerith (1881): La influencia de los datos del censo derivó en la invención de la máquina tabuladora de Hollerith (tarjetas perforadas), que fue capaz de domar esta ingente cantidad de información y permitir realizar el trabajo aproximadamente en un año. Hizo que Hollerith se convirtiera en emprendedor, y su empresa pasó a formar parte de lo que hoy en día conocemos como IBM.  El boom del crecimiento demográfico (1932): La sobrecarga de información prosiguió con el aumento desmesurado de la población en los Estados Unidos, la emisión de los números de la seguridad social y el crecimiento general del conocimiento (y la investigación), aspectos que exigían un registro de la información más preciso y organizado. Lea el resto de la cronología en: http://www.winshuttle.es/big-data-historia- cronologica/ 3.3.2.1 Big Data Analítica Sabemos ya pues a que nos referimos con el término Big Data y de los aconteceres en el devenir de la historia, pero nos surge una nueva interrogante ¿Qué es Big Data Analítica? Big Data Analítica es el proceso de recopilación, organización y análisis de grandes conjuntos de datos (como hemos mencionado Big Data) para descubrir patrones y otra información útil. El análisis de grandes datos puede ayudar a las organizaciones a comprender mejor la información contenida en los datos y también ayudará a identificar los datos que representan gran importancia para el negocio y las decisiones futuras del mismo. 35 Los analistas y personas especializadas que trabajan con grandes volúmenes de datos, básicamente, quieren obtener el conocimiento que proviene de analizar los datos. Sin embargo, es de vital importancia recalcar que al momento de realizar análisis en Big Data se debe considerar una característica fundamental y primordial, a continuación, detallada: - Requiere de un Alto Rendimiento. Para analizar una gran cantidad de volumen de datos de este tipo se realiza típicamente utilizando herramientas de software y aplicaciones especializadas para el análisis predictivo, minería de datos, minería de texto, la previsión y optimización de datos. En conjunto, estos procesos son funciones separadas pero muy integradas de análisis de alto rendimiento. El uso de grandes herramientas de datos y software permite a una organización extremadamente procesar grandes volúmenes de datos que ha recogido una empresa para determinar qué datos son relevantes y pueden ser analizados para tomar mejores decisiones de negocio en el futuro. Para la mayoría de las organizaciones, el análisis de grandes volúmenes de datos es un reto. Teniendo en cuenta el gran volumen de datos y los diferentes formatos de los datos (tanto los datos estructurados y no estructurados) que se recogen a través de toda la organización y las diferentes maneras diferentes tipos de datos que se pueden combinar, contrastada y analizada para encontrar patrones y otros que representen para los negocios útil información. Además, el análisis de Big Data como toda tecnología representa una gran variedad de retos, el primero de ellos es la descomposición, limpieza y tratamiento de los datos, con el fin de tener acceso a un depósito puro y limpio de información correspondiente a las organizaciones en diferentes lugares y con frecuencia en diferentes sistemas. Un segundo desafío se presenta es en la creación de plataformas que se pueden introducir datos no estructurados con tanta facilidad como los datos estructurados. Este volumen masivo de datos 36 suele ser tan grande que es difícil de procesar mediante métodos de base de datos y software tradicionales. ¿Es usado en la actualidad el análisis en Big Data? A medida que la tecnología ayuda a una organización a romper las barreras y dificultades que puede vivir la misma con el fin de mejorar y superar sus límites, son cada vez más las que acuden al uso e incorporación de dichas tecnologías a su ámbito organizacional, así, el tratado de los datos y análisis correcto para la toma de decisiones certeras basados en Big Data no es una excepción, es de tal importancia que representa la tendencia en las organizaciones actuales y líderes en el mercado como se ha recalcado con anterioridad. De acuerdo con Datamation, los avances actuales en el análisis de grandes volúmenes de datos permiten a los investigadores a descifrar el ADN humano en cuestión de minutos, predicen que los terroristas planean atacar, determinar qué gen es sobre todo probable que sea responsable de ciertas enfermedades y, por supuesto, los anuncios que tienen más probabilidades de responder a en Facebook. Otro ejemplo es el de una de las mayores compañías de telefonía móvil en el mundo. Orange, de France puso en marcha su proyecto de Datos para el Desarrollo por la liberación de los datos de abonado para los clientes en la Costa de Marfil. Los 2,5 millones de registros, que se hicieron en el anonimato, incluidos detalles sobre las llamadas y mensajes de texto intercambiados entre los 5 millones de usuarios. Los investigadores acceder a los datos y se envían propuestas para Orange cómo los datos podrían servir de base para los proyectos de desarrollo para mejorar la salud y la seguridad públicas. Los proyectos propuestos incluyen una que mostraba cómo mejorar la seguridad pública mediante el seguimiento de los datos del teléfono celular para mapear donde la gente iba tras una emergencia; otra mostró cómo usar los datos celulares para la contención de la enfermedad. Las empresas buscan cada vez más para encontrar información procesable en sus datos. Muchos proyectos de grandes volúmenes de datos originados por la necesidad de responder a las preguntas específicas del negocio. Con las 37 plataformas adecuadas análisis de datos grandes en su lugar, una empresa puede aumentar las ventas, aumentar la eficiencia y mejorar las operaciones, servicio al cliente y la gestión de riesgos. Webopedia empresa matriz, QuinStreet, encuestó a 540 tomadores de decisiones de la empresa que participan en las compras de grandes datos para saber qué áreas de negocio compañías planean utilizar análisis de grandes datos para mejorar las operaciones. Aproximadamente la mitad de todos los encuestados dijeron que estaban aplicando análisis de grandes volúmenes de datos para mejorar la retención de clientes, ayudar con el desarrollo de productos y obtener una ventaja competitiva. En particular, la zona de negocios conseguir la mayor atención se relaciona con el aumento de la eficiencia y la optimización de las operaciones. En concreto, el 62 por ciento de los encuestados dijo que utilizan análisis de grandes volúmenes de datos para mejorar la velocidad y reducir la complejidad. Son infinitamente innumerables las aplicaciones del análisis de Big Data en las diversas categorías o puntos de vista donde se desee incorporar, por lo que podemos concluir que nuestro país Venezuela no debe ni deberá representar la excepción para la incorporación de esta poderosa y en incremento tecnología denominada Big Data Analítica. Para entender el funcionamiento de Big Data Analítica se expondrá cada una de las tecnologías involucradas: 3.3.3 Tecnologías involucradas La implementación de una solución Big Data Analítica se descompone en diferentes tecnologías que trabajan conjuntamente para lograr el objetivo final de almacenar y procesar grandes volúmenes de datos. Entre las tecnologías involucradas se encuentran el framework del proyecto Apache Hadoop (Apache, Hadoop, 2016), el uso de base de datos NoSQL (NoSQL, 2016) entre las que 38 podemos mencionar Cassandra, Hbase, MongoDB, Neo4j, entre otras. Sin embargo, también se hace necesario mencionar el Teorema de Brewer o Teorema de CAP (Brewer, 2000), en el cual se enfocan las bases de datos NoSQL. Además, también debemos mencionar el almacén de Datos diseñado para Hadoop denominado Apache Hive y como es de gran importancia el paradigma MapReduce basado en los Sistemas de archivos Distribuidos. 3.3.3.1 Apache Hadoop Apache Hadoop es una plataforma que permite el procesamiento de grandes volúmenes de datos a través de clúster, usando un modelo simple de programación. Proporciona un framework, escrito en Java, sobre el cual desarrollar aplicaciones distribuidas que requieren un uso intensivo de datos y de alta escalabilidad. Este proyecto es administrado por Apache Software Foundation. Se presenta como una solución para los programadores sin experiencia en desarrollo de aplicaciones para entornos distribuidos, dado que oculta la implementación de detalles propios de estos sistemas: paralelización de tareas, administración de procesos, balanceo de carga y tolerancia a fallos. (Sánchez, 2014) 3.3.3.1.1 Historia Antes de empezar a hablar sobre el funcionamiento de Hadoop, vamos a destacar algunos detalles sobre su historia. Hadoop no es un acrónimo, es un nombre inventado. Es el nombre que el hijo del creador, Doug Cutting, le dio a un peluche de un elefante amarillo. Corto, 39 relativamente fácil de deletrear y pronunciar, sin significado y sin uso externo. Esto ha provocado que las herramientas relacionadas con Hadoop tiendan a tener nombres relacionados con elefantes u otras temáticas animales (Pig, Mahout, Hive, etc.). Sus orígenes se remontan a Apache Nutch, que era un motor de búsqueda web open-source dentro el proyecto Lucene. La construcción de este motor era una meta ambiciosa y muy cara, estimaban sus creadores, pero el objetivo valía la pena. Fue lanzado en 2002, y se propagó de forma rápida y eficaz. Sin embargo, al poco tiempo, se dieron cuenta de que su arquitectura era incapaz de escalar a los billones de páginas que había en la Web por aquel entonces, y no sabían cómo solucionarlo. En 2003 Google publicó un artículo que describía la arquitectura del Google’s Distributed Filesystem, lo cual proporcionaba a Nutch la solución al problema que se había encontrado. Esto también ayudaba a minimizar los tiempos gastados en tareas administrativas como la gestión de los nodos de almacenamiento. (Vance, 2009) En 2004 se comenzó a escribir una implementación propia y open-source del sistema de ficheros utilizado por Google, el Nutch Distributed FileSystem (NDFS). En ese mismo año Google publicó otro artículo en el que se presentaba MapReduce al mundo. A principios de 2005 los desarrolladores de Nutch ya tenían su propia implementación de MapReduce funcionando para el NDFS (Figura 5 Procesamiento paralelo en MapReduce). En 2006 las implementaciones del NDFS y MapReduce de Nutch se quedan grandes para el proyecto al que pertenecen, por lo que se extraen para crear un subproyecto de Lucene con el nombre de Hadoop, ya que eran aplicables más allá de los ámbitos de la búsqueda en que se estaban utilizando. En ese mismo año Doug Cutting se unió a Yahoo!, lo que le proporcionó un equipo y recursos dedicados para desarrollar su proyecto. 40 En 2008, Yahoo! anunció que su índice de búsquedas era generado por un clúster Hadoop de 10.000 cores. Por lo que, ese mismo año, Hadoop dejó de ser un sub- proyecto y se convirtió en un proyecto de Apache de máxima importancia. Por esta época, Hadoop ya era utilizado por otras compañías tales como Last.fm, Facebook y el New York Times. En abril de 2008, Hadoop rompía un récord mundial convirtiéndose en el sistema más rápido en ordenar 1 Terabyte de datos. Ejecutándose en un clúster de 910 nodos 8 lo consiguió en 209 segundos. En noviembre de ese mismo año, Google anunció que su implementación de MapReduce, podía ordenar ese volumen de datos en tan solo 68 segundos. Y en mayo del año siguiente, un equipo de Yahoo! ya lo consiguió en 62 segundos. Desde entonces Hadoop y su ecosistema es un referente en muchos aspectos y sus roles como plataforma de análisis y almacenamiento para grandes datos han sido reconocidos por la industria del sector. Hay distribuciones Hadoop tanto en las grandes empresas incluyendo IBM, Microsoft y Oracle, así como compañías Figura 5 Procesamiento paralelo en MapReduce Fuente: (Dean & Ghemawat, 2003) 41 especialistas en Hadoop tales como Cloudera, Hortonworks y MapR. (White, Hadoop: The Definitive Guide, 2012) 3.3.3.1.2 Funcionamiento 3.3.3.1.2.1 HDFS: Hadoop Distributed File System Es un sistema de ficheros para almacenar grandes archivos de datos y permitir accesos continuos a éstos. Cuando hablamos de grandes archivos, nos estamos refiriendo a archivos de cientos de Megabytes, Gigabytes o Terabytes. Hoy en día hay clústers de Hadoop que almacenan Petabytes en su sistema. HDFS está construido alrededor de la idea de que el patrón de procesamiento de datos más eficientes es el write-once, read-many-times (escribir una vez, leer varias veces). El funcionamiento que se suele hacer de un sistema así es que un conjunto de información es generado o copiado desde un origen y luego, sobre esos datos, se ejecutan varios procesos de análisis. Entonces este sistema da más importancia a la lectura de grandes bloques de datos consecutivos antes que a la realización de la primera escritura. Ya que Hadoop no requiere grandes máquinas a nivel de componentes, está diseñado para ser tolerante a una alta probabilidad de fallos de la máquina. HDFS es capaz de continuar trabajando sin interrupciones perceptibles por el usuario en caso de que se produzca dicho fallo. (White, Hadoop: The definitive guide, 2012) Para entender un poco mejor cómo funciona, debemos conocer algunos conceptos básicos de HDFS: (Shvachko, Kuang, Radia, & Chansler, 2006) Bloque: Un Bloque es la mínima cantidad de datos que se puede leer o escribir. Por regla general los sistemas de ficheros tienen bloques de pocos kilobytes mientras que los bloques de disco son de 512 bytes, esto difiere del concepto que ha tomado HDFS en el cual toman 64MB de valor por defecto del bloque. Por lo tanto, la información que se almacena en el HDFS tiene como unidad mínima el bloque. 42 La razón de que esto sea así es para minimizar el coste de las búsquedas, ya que, manteniendo un tamaño de bloque grande, el tiempo de transferencia desde disco mejora significativamente. Esto no supone un problema sino un beneficio dado el enfoque del HDFS, almacenar ficheros muy grandes. Namenode y Datanode: El funcionamiento de HDFS sigue el patrón master- workers. En este caso el master es el Namenode y los workers son los datanodes. El namenode es el encargado de gestionar el namespace del sistema de ficheros. Mantiene el árbol del sistema de ficheros, así como los metadatos de cada uno de los ficheros y directorios del árbol. También es el encargado de conocer la localización de los bloques que pertenecen a cada fichero dentro de los datanodes. Hay que tener en cuenta que esta información no es persistente debido a que se reconstruye desde los datanodes cada vez que el sistema inicia. De la misma forma en caso de pérdida de un datanode, éste se encarga de mantener la replicación en otro datanode y por lo tanto modifica esta información. Por otro lado, tenemos los datanodes, éstos son los encargados de almacenar y obtener la información cuando se les solicita, ya sea por el cliente o por el namenode. Con tal de mantener un correcto funcionamiento del sistema, periódicamente se envía la información de los bloques que se están almacenando en ese datanode al namenode. (Figura 6 Arquitectura básica de Hadoop). 43 3.3.3.1.2.2 MapReduce MapReduce es un paradigma de programación que permite procesar y generar grandes cantidades de datos con un algoritmo paralelo y distribuido en un clúster. Fue desarrollado por Google para el indexado de páginas web. El objetivo era crear un framework adaptado a la computación paralela que permitiera procesar y generar grandes colecciones de datos sobre máquinas genéricas, sin la necesidad de utilizar supercomputadores o servidores dedicados, y que fuera fácilmente escalable. En esencia, el modelo que propone MapReduce es bastante sencillo. El programador debe encargarse de implementar dos funciones, map y reduce, que serán aplicadas a todos los datos de entrada. Tareas como hacer particiones de los datos de entrada, despliegue de maestro y trabajadores, esquema de asignación de trabajos a los trabajadores, comunicación y sincronización entre procesos, tratamiento de caídas de procesos, quedan a cargo del entorno de ejecución, liberando de esa manera al programador. (White, Hadoop: The definitive guide, 2012) Figura 6 Arquitectura básica de Hadoop Fuente: (Chauhan, 2012) 44 El funcionamiento de este paradigma está dividido en dos pasos (para abstracción del programador):  Map: donde se realiza la ingestión y la transformación de los datos de entrada, en la cual los registros de entrada pueden ser procesados en paralelo. El nodo master obtiene la entrada, la divide en sub-problemas más pequeños y la distribuye a otros workers. Dicho worker procesa ese problema más pequeño y produce una lista de pares {clave, valor} y pasa la respuesta a su nodo master. Después de esto el framework de MapReduce junta los pares con la misma clave y los agrupa creando un grupo por cada clave generada.  Reduce: fase de agregación o resumen, donde todos los registros asociados entre sí deben ser procesados juntos por una misma entidad. El nodo master coge cada uno de estos grupos y los envía a un nodo para ejecutar la tarea de Reduce, por lo que cada sub-tarea reduce trabaja sobre una clave. El reducer trata estos datos y devuelve un output resultado de ello. 3.3.3.1.2.3 YARN (Yet Another Resource Manager) Uno de los problemas fundamentales que presenta Hadoop 1.0 es que solo admite un paradigma de programación: MapReduce. A pesar de que este modelo Figura 7 Conteo de palabras con MapReduce Fuente: (Lowell, 2013) 45 de programación es apropiado para el análisis de grandes conjuntos de datos, en ocasiones es necesario realizar otro tipo de análisis o sería más propicio utilizar otro tipo de software para analizar datos, pero aprovechándonos de la ventaja que proporciona un clúster Hadoop. Para intentar solventar este inconveniente surge un nuevo componente fundamental dentro de Hadoop: YARN. Apache Hadoop YARN es un subproyecto de Hadoop en la Apache Software Foundation introducido en la versión Hadoop 2.0 que separa la gestión de recursos de los componentes de procesamiento. YARN surge para corregir el inconveniente principal de Hadoop y permitir una gama más amplia de modelos de programación para analizar los datos almacenados en el HDFS más allá de MapReduce. La arquitectura de Hadoop 2.0 basada en YARN provee una plataforma de procesamiento más general y no restringido a MapReduce. En Hadoop 2.0, YARN toma las capacidades de gestión de los recursos que residían en MapReduce y las empaqueta para que puedan ser utilizados por los nuevos motores de procesado. Esto también simplifica la tarea de MapReduce en únicamente hacer lo que mejor sabe hacer, tratar datos. Con YARN, se permite ejecutar varias aplicaciones en Hadoop, todos compartiendo una gestión común de los recursos. MapReduce se convierte ahora en una librería Hadoop es decir una aplicación que reside en Hadoop y deja la gestión de recursos del clúster para el componente YARN. (Figura 8 Hadoop 1.0 a Hadoop 2.0). La aparición de YARN provoca el desarrollo de nuevas herramientas que cubren múltiples necesidades que únicamente con MapReduce no se podían completar. (Figura 9 Nuevas aplicaciones YARN) 46 3.3.3.1.2.4 Arquitectura de Hadoop 2.0 La idea fundamental de YARN es la de separar las dos mayores responsabilidades del JobTracker: la gestión de los recursos y la planificación/monitorización de las tareas en dos servicios separados, para ello tendremos dos nuevos componentes en YARN: un ResourceManager global y un ApplicationMaster por aplicación (AM). Figura 8 Hadoop 1.0 a Hadoop 2.0 Fuente (Murthy, 2013) Figura 9 Nuevas aplicaciones YARN Fuente: (Gutierrez, 2014) 47 Surgen así el ResourceManager para el master (sustituyendo al JobTracker) y un NodeManager (sustituyendo al TaskTracker) por cada slave. Forman el nuevo, y genérico, sistema de gestión de aplicaciones de una manera distribuida. Además, surge un componente llamado container que representa los recursos disponibles en cada nodo del clúster. El ResourceManager es la última autoridad que arbitra los recursos entre todas las aplicaciones en el sistema. El ApplicationMaster por aplicación es una entidad específica que se encarga de negociar los recursos con el ResourceManager y trabajar con los NodeManager para ejecutar y supervisar las tareas que lo componen. El ResourceManager está conectado a un planificador (Scheduler) que es responsable de la asignación de recursos a las diversas aplicaciones que se ejecutan con limitaciones conocidas de capacidades, las colas, etc. El Scheduler es un planificador puro en el sentido de que no realiza ningún monitoreo o seguimiento del estado de la aplicación. El Scheduler realiza su función de planificación en base a las necesidades de recursos de las aplicaciones fundamentándose en la noción abstracta de un container de recursos que incorpora elementos de recursos como la memoria, CPU, disco, red, etc. El NodeManager se encuentra en cada uno de los equipos esclavos y es responsable del lanzamiento del container de las aplicaciones, el seguimiento del uso de recursos (CPU, memoria, disco de red) y de informar del mismo al ResourceManager. El ApplicationMaster tiene la responsabilidad de negociar los containers de recursos necesarios desde el Scheduler, de realizar un seguimiento de su estado y de los progresos de ejecución. Desde la perspectiva del sistema, el propio ApplicationMaster se ejecuta como un container normal. (Figura 10 Arquitectura Hadoop 2.0 YARN) 48 A pesar de que los componentes evolucionen, una de las virtudes de YARN, es que mantiene el esquema de gestión de recursos utilizado con MapReduce lo que genera que Hadoop 2.0 esté totalmente integrado en este paradigma de programación. Figura 10 Arquitectura Hadoop 2.0 YARN Fuente: (Apache, Hadoop Yarn, 2016) 3.3.3.1.2.5 Ecosistema Hadoop El proyecto Hadoop consta de una serie de subproyectos, que vienen a complementar su funcionalidad profundizando en aspectos como el tratamiento, flujo e importación de datos, la monitorización de trabajos, etc. Existen multitud de proyectos relacionados con Hadoop que completan distintas necesidades (Figura 11 Ecosistema Hadoop). La mayoría son dirigidos por Apache, aunque empresas privadas como Cloudera o Hortonworks trabajan desarrollando todo este tipo de plataformas. A continuación, vamos a presentar los proyectos relacionados con Hadoop más importantes:  Ambari: Una herramienta basada en web para el aprovisionamiento, administración y seguimiento de clústeres Apache Hadoop, que incluye 49 soporte para Hadoop HDFS, Hadoop MapReduce, Colmena, HCatalog, Hbase, ZooKeeper, Oozie, Pig y Sqoop. Ambari también proporciona un panel de control para la visualización del estado del clúster, así como la capacidad de ver aplicaciones como MapReduce, Pig y Colmena con el objetivo de evaluar su rendimiento de una manera sencilla.  Avro: se trata de un sistema de serialización de datos que provee numerosas estructuras de datos, un formato de datos binario compacto y rápido, un archivo contenedor para almacenar datos persistentes y una sencilla integración con lenguajes dinámicos.  Cassandra: Apache Cassandra es una base de datos distribuida de segunda generación altamente escalable, que reúnele diseño totalmente distribuido de Dynamo y el modelo de datos basado en ColumnFamily de Bigtable. Cassandra se usa en Facebook, Digg, Twitter, Mahalo, Ooyala, SimpleGeo, Rackspace, y otras empresas que necesitan de una base de datos con alta escalabilidad, disponibilidad y tolerancia a fallos.  Chukwa: es un sistema de recopilación de datos de código abierto para el seguimiento de grandes sistemas distribuidos. Chukwa hereda la escalabilidad y robustez de Hadoop. Además, incluye un conjunto de herramientas flexibles y potentes para la visualización, seguimiento y análisis de resultados para hacer el mejor uso de los datos recogidos.  HBase: Una base de datos escalable, distribuida que soporta el almacenamiento de datos estructurados en tablas. Permite la realización de tablas a partir de ficheros de datos.  Hive: facilita la consulta y gestión de grandes conjuntos de datos que residen en almacenamiento distribuidos. Hive proporciona un mecanismo para la ver la estructura de los datos utilizando un lenguaje similar a SQL llamado HiveQL.  Mahout: se trata de un software libre centrado en la implementación de algoritmos de machine learning distribuidos.  Pig: Apache Pig es una plataforma para el análisis de grandes conjuntos de datos que se caracteriza por un lenguaje de alto nivel para la creación de los programas de análisis de datos, junto con la infraestructura 50 necesaria para la evaluación de estos programas. La propiedad más importante de los programas Pig es que su estructura es susceptible de una paralelización sustancial, lo que a su vez permite manejar grandes conjuntos de datos.  Spark: proporciona un motor de cálculo rápido y general para datos Hadoop. Spark proporciona un modelo de programación sencillo y expresivo que soporta una amplia gama de aplicaciones, incluyendo ETL, machine learning, procesamiento de flujo, y computación gráfica.  ZooKeeper: es un servicio centralizado construido para mantener la información de configuración, proporcionar sincronización distribuida y la prestación de servicios de grupo. Todos estos tipos de servicios se utilizan de una forma u otra por las aplicaciones distribuidas. Figura 11 Ecosistema Hadoop Fuente: (Blog, 2016) 51 3.3.3.1.3 Almacén de Datos Hadoop 3.3.3.1.3.1 Apache Hive Apache Hive es una infraestructura de almacenamiento de datos construida sobre Hadoop para proporcionar agrupación, consulta, y análisis de datos. Inicialmente desarrollado por Facebook, Apache Hive es ahora utilizada y desarrollado por otras empresas como Netflix y la Financial Industry Regulatory Authority (FINRA).2 3 Amazon mantiene una derivación de software de Apache Hive incluida en Amazon Elastic MapReduce en sus servicios Amazon Web Services. 3.3.3.1.3.2 Características Apache Hive soporta el análisis de grandes conjuntos de datos almacenados bajo HDFS de Hadoop y en sistemas compatibles como el sistema de archivos Amazon S3. Ofrece un lenguaje de consultas basado en SQL llamado HiveQL con esquemas para leer y convertir consultas de forma transparente en MapReduce, Apache Tez y tareas Spark. Los tres motores de ejecución pueden correr bajo YARN. Para acelerar las consultas, Hive provee índices, que incluyen índices de bitmaps. Otras características de Hive incluyen:  Indexación para proporcionar aceleración, tipo de índice que incluye compactación e índices de bitmaps. Otros tipos de índices serán incluidos en futuras versiones.  Diferentes tipos de almacenamiento como texto, RCFile, HBase, ORC, y otros.  Almacenamiento de metadatos en bases de datos relacionales, lo que permite reducir el tiempo para realizar verificaciones semánticas durante la ejecución de consultas.  Operaciones sobre datos comprimidos almacenados en el ecosistema Hadoop usando algoritmos que incluyen DEFLATE, BWT, snappy, etc. https://es.wikipedia.org/wiki/Almac%C3%A9n_de_datos https://es.wikipedia.org/wiki/Hadoop https://es.wikipedia.org/wiki/Facebook https://es.wikipedia.org/wiki/Netflix https://es.wikipedia.org/wiki/Financial_Industry_Regulatory_Authority https://es.wikipedia.org/wiki/Financial_Industry_Regulatory_Authority https://es.wikipedia.org/wiki/Apache_Hive#cite_note-Use_case-2 https://es.wikipedia.org/wiki/Apache_Hive#cite_note-OSCON-3 https://es.wikipedia.org/wiki/Amazon_Web_Services https://es.wikipedia.org/wiki/Amazon_Web_Services https://es.wikipedia.org/wiki/Hadoop https://es.wikipedia.org/wiki/Amazon_S3 https://es.wikipedia.org/wiki/SQL https://es.wikipedia.org/wiki/MapReduce https://es.wikipedia.org/w/index.php?title=Apache_Spark&action=edit&redlink=1 https://es.wikipedia.org/wiki/Deflaci%C3%B3n_(algoritmo) https://es.wikipedia.org/wiki/Compresi%C3%B3n_de_Burrows-Wheeler 52  Funciones definidas por el usuario (UDF) para manipular fechas, textos, y otras herramientas de minería de datos. Hive soporta la extensión de las funciones definidas por el usuario de manera de tratar casos no contemplados.  Consultas estilo SQL (HiveQL), las cuales son convertidas automáticamente a MapReduce o Tez, o tareas Spark. Por defecto, Hive almacena sus metadatos en una base de datos apache Derby, pero puede ser configurado para usar MySQL. 3.3.3.2 NoSQL (Not Only SQL) 3.3.3.2.1 Introducción Los sistemas manejadores de bases de datos relacionales son la tecnología predominante para almacenar datos estructurados en la web y aplicaciones de software en los negocios. Desde la publicación científica de Codd “A relational model of data for large shared data banks” de 1970 estos almacenes de datos apoyándose en el cálculo relacional y proveyendo consultas comprensivas ad hoc facilitadas por SQL han sido ampliamente adoptados y son comúnmente la única alternativa para el almacenamiento de datos accesibles a múltiples clientes de manera consistente. Los RDMBS siguen una filosofía “una talla para todos” lo que quiere decir que el modelo relacional sirve para todos los casos, sin embargo, en los últimos años ésta filosofía ha sido cuestionada por la ciencia y compañías relacionadas con la web, lo que ha llevado al surgimiento de una gran variedad de alternativas en bases de datos. Las cuales no siguen el modelo relacional propuesto por Frank Codd ya que poseen un esquema flexible y cada una surge para solventar una clase de problema en específico. Este movimiento es lo que se conoce como NoSQL (Not only SQL). El término NoSQL fue usado por primera vez en 1998 para una base de datos relacional que omitía el uso de SQL (Strozzi, 2010). El término fue utilizado de nuevo en el año 2009 para una conferencia realizada en San Francisco acerca https://es.wikipedia.org/wiki/Apache_Derby https://es.wikipedia.org/wiki/MySQL 53 de bases de datos que no seguían el modelo relacional, tales como Last.fm desarrollada por Jon Oskarsson quien fue el encargado de organizar el evento. (Evans, 2009) 3.3.3.2.2 Críticas a las Bases de Datos NoSQL Desde la perspectiva de los adeptos a los RDBMS podemos mencionar las siguientes críticas a las bases de datos NoSQL: (Antiñanco, 2013)  No hay un líder: El mercado de NoSQL está muy fragmentado, lo cual es un problema para el open-source porque se requiere una gran cantidad de desarrolladores para tener éxito.  No hay estandarización: Cada base de datos posee su propia interfaz y tipo de consultas, lo que ocasiona que la adaptación a una base de datos NoSQL tenga una inversión significativa para poder ser utilizada.  Se requiere una reestructuración de los modelos de desarrollo de aplicaciones: Utilizar una base datos NoSQL típicamente implica usar un modelo de desarrollo de aplicaciones diferente a la tradicional arquitectura de 3 capas. Por lo tanto, una aplicación existente de 3 capas no puede ser simplemente convertida para bases de datos NoSQL, debe ser reescrita, sin mencionar que no es fácil reestructurar los sistemas para que no ejecuten consultas con join o no poder confiar en el modelo de consistencia read-after- write  Modelos de datos sin esquema podría ser una mala decisión de diseño: Los modelos de datos sin esquema son flexibles desde el punto de vista del diseñador, pero son difíciles para consultar sus datos. 54 3.3.3.2.3 Puntos a favor de las Bases de Datos NoSQL Desde la visión de los adeptos a las bases de datos NoSQL podemos mencionar las siguientes razones para desarrollar y utilizar ésta tecnología:  Evitar la complejidad innecesaria: Los RDBMS proveen un conjunto amplio de características y obligan el cumplimiento de las propiedades ACID, sin embargo, para algunas aplicaciones éste set podría ser excesivo y el cumplimiento estricto de las propiedades ACID innecesario.  Alto rendimiento: Las bases de datos NoSQL proveen un rendimiento mayor a las relacionales, incluso hasta varios órdenes de magnitud.  Información no estructurada y hardware más económico: La mayoría de las bases de datos NoSQL son diseñadas para poder escalar horizontalmente. También permiten el almacenamiento de datos no estructurados, provenientes de diversas fuentes como pueden ser las redes sociales.  La filosofía “One size fits all” estaba y sigue estando equivocada: Existen muchos escenarios que no pueden ser abarcados con un enfoque de base de datos tradicional. Esto debido al continuo crecimiento de volúmenes de datos a ser almacenados y a la necesidad de procesar grandes volúmenes de datos en corto tiempo. 3.3.3.2.4 Taxonomía NoSQL De acuerdo a la manera en que las bases de datos NoSQL almacenan sus datos es posible clasificarlas de la siguiente manera:  Almacenamiento clave-valor  Bases de datos orientadas a columnas  Base de datos documentales  Base de datos orientada a grafos 55 3.3.3.2.5 Propiedades ACID vs BASE En esta sección se comparan las propiedades de los sistemas relacionales (ACID) con la de los sistemas no relacionales (BASE). Además, de una breve explicación del Teorema de CAP o Brewer, el cual es la base para el desarrollo de este tipo de tecnologías. 3.3.3.2.5.1 Teorema de CAP Durante el simposio de “Principios de computación distribuida” de ACM en el año 2000, Eric Brewer, un profesor de la universidad Berkeley de California y cofundador de Inktomi, a través de una presentación titulada “Hacia sistemas distribuidos robustos”, estableció la conjetura que los servicios web no pueden asegurar en forma conjunta las siguientes propiedades: Consistencia (C), Disponibilidad (A) y Tolerancia a particiones (P), esto es lo que se conoce como el teorema de CAP. Posteriormente en el año 2002, Seth Gilbert y Nancy Lynch de MIT publicaron una demostración formal de la conjetura de Brewer, convirtiéndola en un teorema. El teorema de CAP establece que, en un sistema distribuido con datos compartidos, se debe optar por favorecer dos de las tres características: Consistencia, Disponibilidad y Tolerancia a particiones. Bajo estas restricciones, Brewer indica que se debe utilizar como criterio de selección, los requerimientos que se consideren más críticos para el negocio, optando entre propiedades ACID y BASE. (Brewer, 2000). En la siguiente tabla basada en la presentación de Brewer se muestran las alternativas, características y ejemplos (Tabla 1 Alternativas en Teorema de CAP) 56 3.3.3.2.5.2 Propiedades ACID en sistemas distribuidos Las propiedades ACID presentes en las transacciones de las bases de datos relacionales simplifican el trabajo de los desarrolladores de aplicaciones al ofrecer garantías en cuanto a:  Atomicidad: Todas las operaciones en la transacción se completarán o ninguna lo hará.  Consistencia: El estado de la base de datos se mantiene válido tanto al inicio como al final de la transacción. Cualquier operación puede ver los cambios en la base de datos.  Aislamiento: Las transacciones se ejecutan de manera que una no puede afectar a la otra.  Durabilidad: Los cambios realizados durante una operación serán persistentes y no se podrá deshacer, aunque falle el sistema. Tabla 1 Alternativas en Teorema de CAP Fuente: (Antiñanco, 2013) 57 Los proveedores de bases de datos se percataron de la necesidad de particionar, por lo que introdujeron el protocolo de commit a 2 fases para seguir manteniendo las propiedades ACID en las instancias de las bases de datos. El cual consiste en:  Primera fase: el coordinador de la transacción solicita a cada base de datos involucrada que indiquen si es posible que realicen commit. Si es posible se procede a continuar con la segunda fase.  Segunda fase: El coordinador de la transacción solicita a cada base de datos que realice el commit. Sin embargo, si alguna base de datos no puede realizar el commit, se le solicita a todas las involucradas que realicen un roll-back de ésa transacción. Sería análogo a que un avión no pudiese despegar hasta que todos los pasajeros estén en sus asientos, a pesar de que el vuelo haya sido programado para una hora en específico. En vista de esto podemos observar que se generan retrasos al realizar las transacciones, lo que afecta, de acuerdo al teorema de CAP, la disponibilidad del sistema. (Pritchett, 2008) 3.3.3.2.5.3 Propiedades BASE Si las propiedades ACID se enfocan en la consistencia, las propiedades BASE se enfocan en la disponibilidad. La palabra BASE se refiere a básicamente disponible (BA), estado flexible (S) y eventualmente consistente (E). Las propiedades BASE son diametralmente opuestas a las propiedades ACID. Donde ACID es pesimista y fuerza la consistencia al finalizar cada operación, BASE es optimista y acepta que la consistencia de la base de datos estará en un estado flexible. Aunque esto suene imposible de lidiar, en la realidad es manejable y puede llevar a niveles de escalabilidad que no se pueden obtener con ACID. 58 La disponibilidad en las propiedades BASE es alcanzada a través de mecanismos de soporte de fallas parciales, que permite mantenerse operativos y evitar una falla total del sistema. Así, por ejemplo, si la información de usuarios estuviera particionada a través de 5 servidores de bases de datos, un diseño utilizando BASE alentaría una estrategia tal que una falla en uno de los servidores impacte sólo en el 20% de los usuarios de ese host. Tabla 2 ACID vs BASE (Brewer, 2000) ACID BASE Consistencia fuerte Consistencia débil Aislamiento Disponibilidad primero Centrado en “commit” Mejor esfuerzo Transacciones anidadas Respuesta aproximada ¿Disponibilidad? Agresivo (optimista) Conservativo (pesimista) Más simple Difícil evolución (ejemplo: el esquema) Más rápido. Fácil Evolución 3.3.3.3 Motores de Búsqueda Los motores de búsqueda son programas que permiten hacer búsquedas por palabras claves y retornan una lista de documentos en donde se encontraron las palabras claves. Funcionan mediante la creación de índices con los cuales luego se realizan las búsquedas y a diferencia de las bases de datos, permiten integrar búsquedas de diversas fuentes de datos, permiten la escalabilidad, ranking por relevancia y hacer búsquedas por facetas. (Wikipedia, Motores de busqueda, 2015) Estas herramientas son necesarias para facilitar las búsquedas a los usuarios, ya que al hablar de grandes volúmenes de datos como es el caso de la web es muy probable que el usuario final encuentre información que no le sea de utilidad, 59 por lo cual estas herramientas se manejan con análisis de contenido como es el caso del algoritmo de Google, Page Rank, en el cual se hace un ranking de las páginas más importantes dependiendo de la búsqueda realizada. Los principales motores de búsqueda en Big Data son Apache Lucene y Apache SolR, los cuales se explicarán brevemente: 3.3.3.3.1 Lucene Apache Lucene es una API de código abierto para la recuperación de información, implementada originalmente en Java por Doug Cutting. Se trata de una tecnología adecuada para cualquier aplicación que requiera de búsquedas por texto completo, especialmente si son multiplataforma (Apache, Lucene, 2016) Puede indexar cualquier formato de texto, como MS Word, HTML, XML, PDF y OpenDocument, siempre y cuando la información textual pueda ser extraída, lo que quiere decir que no puede indexar imágenes. Algunas características:  Es escalable y tiene indexación de alto rendimiento, ya que puede procesar 150 GB / Hora en máquinas modernas y requiere solo 1 MB de memoria  Potente, preciso y eficiente algoritmo de búsqueda. Búsqueda por ranking, mejores resultados devueltos primero. Diversos tipos de consultas como son: consultas por frases, por comodín, por proximidad, rangos y más.  Multiplataforma. Esta implementado 100 por ciento en Java, aunque también está disponible en otros lenguajes de programación.  Puede ordenar cualquier campo en cualquier documento  Internamente todo se maneja como un documento y no necesariamente tiene que referirse a un archivo real en disco, también podría asemejarse a una fila de una base de datos relacional.  Un documento es visto como una lista de campos, donde un campo tiene un nombre y un valor. 60  La unidad de indexación en Lucene es un término, el cual puede ser a menudo una palabra. Los índices rastrean las frecuencias de los términos  Lucene utiliza índices inversos que permiten localizar rápidamente los documentos asociados a la condición entrante de búsqueda. Figura 12 Indexación con Lucene Fuente: (MacCandles, Hatcher, & Gospodnetic, 2010) Algunas ventajas:  Poderosa sintaxis de búsqueda  Rápido indexamiento  Búsqueda rápida  Ordenamiento por relevancia y por otros tipos de campos Algunas desventajas:  No hay contratos formales de apoyo 61  No hay disponibilidad asegurada de formación u otros servicios profesionales para satisfacer las necesidades específicas del software o ayudar con la construcción de una aplicación.  Ningún programa de pruebas de liberación formalizado, calendario de lanzamiento o garantía de compatibilidad de actualización. 3.3.3.3.2 SolR Solr es un motor de búsqueda de código abierto, basado en Lucene, que permite el resaltado de resultados y la búsqueda por facetas, además posee una interfaz para su administración. Se ejecuta sobre un contenedor de servlets Java como Apache Tomcat (Wikipedia, Apache Solr, 2016) . Solr es escalable, permitiendo búsquedas distribuidas y replicación de índices La principal característica de Solr es su API estilo REST, ya que en vez de usar drivers para comunicarnos con Solr, podemos hacer peticiones HTTP y obtener resultados en XML o JSON. Algunas de sus características son:  Se comunica a través de HTTP enviando y recibiendo contenido en XML  Esta optimizado para un alto volumen de tráfico web.  Soporte de indexación distribuida (SolrCloud), replicación y la carga de consultas equilibradas.  Búsqueda por facetas  Análisis de texto (tokenización, normalización)  Permite realizar búsquedas geoespaciales.  Lee y escribe directamente al HDFS de Hadoop, además de soportar la replicación en el HDFS.  Es posible construir índices escalables a través del paradigma Map Reduce En su arquitectura Solr se divide en dos partes: (ver Figura 13 Arquitectura Solr) 62  Índice. Sistema de ficheros que almacenan la información. Contiene la configuración de Solr y la definición de la estructura de datos.  Servidor. Proporciona el acceso a los índices y las características adicionales. Admite plugins para añadir funcionalidades. Figura 13 Arquitectura Solr Fuente: (Marco, 2013) 3.3.3.4 Herramientas de visualización y análisis Una vez que se tienen todos estos datos en la arquitectura, se utilizan herramientas de visualización y análisis que permiten sacarle el valor a los datos, con los cuales es posible, mediante la aplicación de funciones estadísticas y de visualización, encontrar patrones, tendencias, hacer modelos predictivos, e incluso sistemas de recomendación. La importancia de la visualización de los datos radica en que permite al analista de datos, bien sea un gerente o un científico de datos, facilitarles la comprensión de los datos, de modo que puedan realizar sus funciones con mayor facilidad y puedan a su vez transmitir la información que encontraron en los datos. 63 Algunas herramientas de visualización más importantes podemos mencionar Apache Hue (Hadoop User Experience) y Banano. Entre las herramientas de análisis más conocidas en el mundo del Big Data se encuentran:  RStudio que es más conocida en el ámbito científico por su manejo de lenguaje de análisis R, el cual es muy utilizado por científicos de datos.  Podemos mencionar Apache Mahout, el cual me permite utilizar algoritmos en su librería para hacer máquinas de aprendizaje  Apache Spark el cual se integra con Hadoop y posee un módulo llamado Spark MILIB en el cual también se utiliza para hacer machine learning, además de poseer módulos para hacer consultas en SQL y análisis de grafos, con la peculiaridad de que Spark utiliza la memoria RAM de la máquina, lo que lo hace una herramienta más rápida que utilizar solo Hadoop. 3.4 La Inteligencia de Negocio y su evolución con el tiempo a Big Data Analítica Cada instante disponemos de más y más datos, más y más desestructurados, con más y más fuentes, con más y más usuarios, con menos y menos tiempo y, menos y menos paciencia. Fruto de todo ello es el desarrollo inusitado de los sistemas de inteligencia de negocio. El concepto Business Intelligence, BI, es el uso de datos en una empresa para facilitar la toma de decisiones. Incluye la comprensión del funcionamiento actual de la empresa, con el objetivo de ofrecer conocimientos para respaldar las decisiones empresariales. Basándose en la utilización de un sistema que se forma con los datos extraídos, de los datos de la producción mediante las herramientas y técnicas ETL (extraer, transformar y cargar). Las herramientas de inteligencia analítica facilitan el modelado para crear consultas, informes o cuadros de mando. 64 El concepto de Business Analytics es una evolución del Business Intelligence. En 2009 Michael J. Beller, en su publicación Next Generation Business Analytics lo explica como los conocimientos, tecnologías y prácticas para la investigación y exploración continuamente interactiva del rendimiento del negocio para ganar visión y capacidad de dirección en la planificación del negocio. El BA se focaliza, por tanto, en una nueva visión y comprensión del rendimiento del negocio, basado en datos y métodos estadísticos que ayudan a medir el pasado y guiar el futuro, en contraste con el BI que, tradicionalmente se focaliza en el uso y seguimiento de una seria de métricas o KPI’s. El BA hace extensivo el análisis estadístico, incluyendo también modelos predictivos, con estos modelos predictivos se sabe que está influyendo en la realidad, cuáles son los drivers, y en qué medida influyen, permitiendo, de esta manera, modificar la realidad en la medida que estos drivers sean accionables, se convierten en palancas con las que influir en el futuro del negocio. Se podrá contestar a las preguntas tales como ¿Qué pasó?, ¿Con qué frecuencia?, ¿Cuánto?, ¿Dónde está el problema?, ¿Qué acciones se necesitan?, y además, ¿Por qué pasó?, ¿Cuál es la tendencia?, ¿Qué pasará si continúa esta tendencia?, ¿Qué va a pasar?, ¿Qué es lo mejor que puede pasar? y ¿Cómo? El BA se puede plantear de diferentes maneras, en primer lugar, soportando decisiones humanas con la visualización de modelos que reflejan razones, también como incremento de visión con los datos históricos vía el uso de informes, cuadros de mando, segmentación… En tercer lugar, con BA predictivo usando estadísticas y técnicas machine learning. Finalmente prescribiendo el futuro usando técnicas de optimización, simulación, etc. Para el desarrollo de estos modelos estadísticos es fundamental que los datos con los que se trabaja tengan un volumen y una calidad excelente. En la actualidad, gracias a que el comportamiento de las personas en muchas ocasiones se produce con la intermediación en algún punto de una máquina, los datos que tenemos acerca de sus movimientos son inconmensurables, sin 65 embargo, si hacemos uso de la ciencia de los datos, conseguiremos la capacidad de profundizar en ellos y obtener una ventaja competitiva. El Big Data, BD, tiene la capacidad de revolucionar la toma de decisiones y los modelos de negocio, y brinda a la dirección toda una suerte de posibilidades para hacer más científica la gestión. Basándose en el estudio de datos de series temporales, se pueden conocer todas las variables que influyen en el comportamiento de las personas, tanto de manera estructural como, aquellas otras que lo hacen de forma esporádica, así como aquello que subyace y que nos puede servir para hacer planteamientos de futuro novedosos. Tanto, empresas emergentes y pequeñas, como empresas veteranas y gigantes están haciendo uso ya de toda la información de que disponen, usando como metodología la ciencia de los datos. La aproximación del BA junto con el BD está ya dando resultados reveladores en diversos ámbitos: Behavioral Analytics, Contextual Data Modeling, Cyber Analytics, Enterprise Optimización, Fraud Analytics, Marketing Analytics, Pricing Analytics, Retail Sales Analytics, Risk & Credit Analytics, Suply Chain Analytics, Talent Analytics, etc. Las nuevas herramientas de analítica de datos poco tienen que ver con las que se empleaban hace apenas un par de años. Como ejemplo, valga el informe que justamente hace poco más dos años, en 2013, publicó el IBM Institute for Business Value, una instancia que desde 2009 evalúa el impacto de la analítica de datos y la inteligencia de negocios en el mundo corporativo. Este estudio, una encuesta realizada a 900 cargos ejecutivos y responsables TI de 70 países distintos, a lo largo de dos meses y medio, fue analizado y debatido por un grupo de ejecutivos y gerentes de alto nivel (los componentes del BAO, o Business Analytics and Optimization Advisory Board, órgano también perteneciente a IBM) que logró, entre otras conclusiones, establecer los parámetros guía que regían en esos momentos las principales necesidades analíticas en los negocios. Unas conclusiones que sirvieron para marcar nuevos objetivos en el ámbito del BI, y que han desembocado en el desarrollo de soluciones capaces de afrontar 66 los nuevos retos que afrontan las organizaciones en lo que a analítica de datos se refiere. Así ha surgido una nueva generación de herramientas Big Data Analytics que han dejado atrás los tradicionales (aunque aún jóvenes, al fin y al cabo) recursos analíticos del Business Intelligence. Herramientas actualizadas, mejoradas y enormemente evolucionadas que permiten sacar el máximo partido a los datos y las informaciones a las que tienen acceso hoy las organizaciones, aportando valor añadido a las actividades de negocio que cuentan con ellas. Unas herramientas que aportan un valor objetivo, identificado en torno a 3 ejes principales: - Capacitación: capacitan a las organizaciones para detectar, analizar y aprovechar los datos y las informaciones más relevantes para sus operaciones. - Impulso: son herramientas altamente sofisticadas que proyectan las posibilidades reales de la compañía hacia el futuro, visibilizando e identificando las oportunidades que se abren en todos los escenarios posibles de negocio. - Amplificación: amplifican los valores y las fortalezas (internas y externas) de la actividad empresarial, permitiendo sacar el máximo partido a las nuevas oportunidades de negocio. Las nuevas herramientas de analítica avanzada permiten, al fin y al cabo, definir una estrategia corporativa mucho más realista y, al mismo tiempo, exigente y de alto rendimiento, lograr un mejor establecimiento de objetivos y una consecución más rápida, eficiente y efectiva de los mismos, y aventajar a la competencia con una toma de decisiones apoyada en detallados análisis predictivos. 67 Pues, es de esta manera que podemos concluir en que mientras que Business Intelligence analiza datos consolidados, Big Data es capaz de reforzar la función de B I en un universo más amplio, con más dinamismo y multiplicidad. Por permitir un análisis de una cantidad más grande de datos no estructurados, de mucha importancia para la organización, el Big Data aumenta todavía más la relevancia y la utilidad del Business Intelligence para el negocio. Es así como, Big data, que es considerado una evolución de BI, y de Business Intelligence, pueden ser vistos como complementarios (IBM, 2016). 3.5 Ciencia de Datos La ciencia de datos es un campo interdisciplinario de la ciencia que involucra los procesos y sistemas para extraer conocimiento de grandes volúmenes de datos en sus diferentes formas (estructurados o no estructurados) y formatos (textos, imágenes, documentos, vídeos, entre otros), mediante el uso de la computación, la matemática y la estadística. A pesar de que se puede aplicar ciencia de datos, sin la necesidad de tener una arquitectura Big Data y viceversa, en los últimos años hemos visto cómo es posible combinar ambas de manera armoniosa para solventar problemas, mejorar servicios o incluso prever eventos mediante el uso de modelos predictivos. En la ciencia de datos se involucran 3 principales componentes de acuerdo a Drew Conway los cuales son (Figura 14 Diagrama de Venn Ciencia de datos):  Habilidades Hacking o computacionales  Conocimiento Estadístico y matemático  Conocimientos sustantivos (área en la que se investiga). 68 Los científicos de datos son uno de los mayores beneficiados por el uso del Big Data, ya que de esta manera disponen de mayor cantidad de datos a analizar y de las herramientas necesarias para sus respectivos análisis. Entre las funciones que realizan los científicos de datos, podemos resumirlas en 3 principalmente: (Sánchez, 2014)  Captura de los datos: Captura y almacenamiento de la información. Es el procedimiento manual de convertir “raw data” (información en bruto) en información con formato para que pueda ser analizada.  Análisis de los datos: Obtención de valor a partir de la información. Para lograrlo es necesario realizar procesos de minería de datos como limpieza o transformaciones, luego aplicar funciones estadísticas, experimentar con modelos predictivos y mediante el cálculo de errores observar cuál es el que se adapta mejor al problema a resolver. Figura 14 Diagrama de Venn Ciencia de datos Fuente: (Conway, 2010) 69  Visualización de los datos: Visualización de los resultados obtenidos anteriormente. En muchos casos el científico de datos debe dirigirse con un lenguaje más accesible para explicar los resultados obtenidos, por lo que requiere de habilidad para expresar sus investigaciones de manera clara y entendible apoyándose en el uso de gráficos que puedan ayudarle a comunicar los resultados obtenidos. Hoy en día es posible ver muchos casos de éxito en la aplicación de la ciencia de datos en conjunto con Big Data, bien sea para evitar epidemias, promocionar productos, evitar riesgos financieros, análisis y alertas de datos en tiempo real de pacientes, fraudes en tarjetas de crédito, entre otros. 3.6 Arquitectura de Software Todos los sistemas tienen una arquitectura, es decir, una estructura de alto nivel que define todo el sistema. En la arquitectura de software se observa como todas las piezas que componen el sistema encajan para aportar una solución al problema que se desea resolver el cliente. Podemos definir la Arquitectura de software como el proceso de definición de una solución estructurada que cumple con todos los requisitos técnicos y operativos, al tiempo que optimiza los atributos de calidad comunes, como el rendimiento, la seguridad y la capacidad de gestión. Implica una serie de decisiones basadas en una amplia gama de factores y cada una de estas decisiones puede tener un impacto considerable en la calidad, el rendimiento, la facilidad de mantenimiento y el éxito general de la aplicación. (Microsoft, 2017) 3.6.1 Importancia de la Arquitectura de Software Existen fundamentalmente 3 razones que dan importancia a las arquitecturas de software: 70  Comunicación entre los stakeholders: La arquitectura de software representa un medio de abstracción del sistema, en la cual la mayoría de los stakeholders pueden usar como una base para el entendimiento mutuo, negociar, comunicarse y llegar a consensos.  Primeras decisiones en diseño de sistemas: Las arquitecturas de software representan un primer ajuste en las decisiones de diseño (define restricciones en la implementación, dicta la estructura organizacional, ayuda en la evolución de prototipos, ayuda a una mayor estimación de los costos monetarios y en tiempo)  Abstracción de sistemas transferibles: Proporcionan un modelo de como un sistema está estructurado y de cómo sus elementos interactúan entre sí. Este modelo puede ser transferido entre sistemas. 3.6.2 Componentes en una arquitectura de software La arquitectura de software provee una abstracción de alto nivel del sistema a ser construido, por lo que debe cubrir los siguientes aspectos: (Hanmer, 2013)  Filosofía y objetivo del sistema: La arquitectura explica los objetivos y describe el propósito del sistema, quien lo usa y que problema resuelve.  Suposición arquitectural y dependencias: La arquitectura explica las suposiciones hechas del sistema con su medio ambiente. También explica cualquier dependencia con otro sistema.  Requerimientos significativos de la arquitectura  Instrucciones de los subsistemas y componentes: Explica como los componentes del sistema son desplegados en plataformas computacionales y como se deben combinar para su correcto funcionamiento.  Subsistemas críticos y capas: La arquitectura describe las diferentes vistas y partes del sistema y como se relacionan. También explica en detalle sus subsistemas más críticos.  Interfaces criticas del sistema 71  Escenarios claves que describen el comportamiento del sistema 3.6.3 Estructuras de la arquitectura y vistas Un neurólogo, un cardiólogo y un hematólogo tendrán diferentes vistas de las estructuras del cuerpo humano, cada uno de acuerdo a su respectiva especialidad, lo mismo ocurre en el mundo del software. Los sistemas modernos computacionales son tan complejos que es muy difícil entender todas sus estructuras como un todo, por lo que se diseñan y estudian modularmente. Las estructuras arquitecturales pueden dividirse en 3 grupos principales, dependiendo de la naturaleza de elementos que muestren:  Estructuras modulares: Aquí los elementos son módulos, que son unidades de representación, los cuales se le asignan responsabilidades funcionales. Nos permiten responder preguntas tales como: ¿Cuál es la función principal asignada a cada módulo? ¿Qué elementos de software puede usar cada módulo? ¿Qué módulos están relacionados con otros módulos por especialización o generalización?  Estructuras de componentes y conectores: Aquí los elementos son componentes de ejecución y sus conectores. Estas estructuras nos permiten responder preguntas tales como: ¿Cuáles son los mayores componentes de ejecución y cómo interactúan? ¿Qué partes del sistema están replicadas? ¿Cómo la data progresa a través del sistema? ¿Cuáles son los mayores almacenes de datos compartidos?  Estructuras de asignación: Las estructuras de asignación muestran cuales son las relaciones entre los elementos de software y los elementos en uno o más ambientes externos en los que el software es creado y ejecutado. Nos permite responder preguntas como ¿En qué procesador se ejecuta cada elemento de software? ¿En qué archivos se almacena cada elemento durante su desarrollo, prueba y creación del sistema? ¿Cuáles son los elementos de software asignados a cada equipo de desarrollo? 72 Estas tres estructuras corresponden a tres tipos de decisiones que envuelven el diseño de la arquitectura, como son:  ¿Cómo se estructura el sistema como un conjunto de unidades de código? (módulos)  ¿Cómo se estructura el sistema como un conjunto de elementos que tienen un comportamiento de ejecución (componentes) y sus interacciones (conectores)?  ¿Cómo el sistema se relaciona con estructuras que no son de software en su medio ambiente? (CPU, archivos, equipos de desarrollo, entre otros). 3.6.3.1 Estructuras de software Algunas de las estructuras de software más comunes se muestran en la siguiente figura (ver Figura 15 Estructuras de software) Figura 15 Estructuras de software Fuente: (Microsoft, 2017) 3.6.3.1.1 Módulos Las estructuras modulares incluyen lo siguiente:  Descomposición. Las unidades son módulos relacionados entre sí por la relación “es submodulo de”, mostrando como módulos grandes son 73 descompuestos en unos más pequeños recursivamente, hasta que son los suficientemente pequeños para ser fácilmente comprendidos.  Usos. Las unidades de estas estructuras también son módulos, o procedimientos o recursos en las interfaces de los mismos. Las unidades son relacionadas por una relación de “uso”. Una unidad utiliza otra si la primera necesita la presencia de la segunda. Estas estructuras son utilizadas para diseñar sistemas que puedan ampliarse fácilmente para añadir funcionalidades o de la que subconjuntos funcionales útiles se puedan extraer fácilmente.  Capas. Cuando las relaciones de usos en esta estructura son controladas de forma particular, un sistema de capas emerge, en el cual una capa es un conjunto de funcionalidades relacionadas.  Clases, o generalización. Las unidades de módulo de estas estructuras, son llamadas clases. La relación es “hereda de” o “es una instancia de”. Esta vista permite el razonamiento de colecciones con el mismo comportamiento o capacidad y parametrizar diferencias que son capturadas por subclases. 3.6.3.1.2 Componentes y conectores Estas estructuras incluyen lo siguiente:  Procesos. Las unidades aquí son procesos o hilos que están conectados entre sí por la comunicación, sincronización, y/o exclusión de operaciones. Las estructuras de procesos ayudan a diseñar en los sistemas el rendimiento de ejecución y disponibilidad.  Concurrencia. Estas estructuras ayudan a los arquitectos a determinar oportunidades de paralelismo y la locación donde la contención de recursos pueda ocurrir. Las unidades son componentes y los conectores son “hilos lógicos”  Datos compartidos o repositorios. Esta estructura incluye componentes y conectores que crean, almacenan y acceden a data persistente. Muestra como la data es producida y consumida por 74 elementos de software en tiempo de ejecución y puede ser usado para asegurar un buen rendimiento e integridad de la data.  Cliente-Servidor. Los componentes son clientes-servidores y lo conectores son los protocolos que comparten para el pase de mensajes. Si el sistema está construido como un grupo de clientes y servidores que cooperan entre sí, esta es una buena estructura de componentes y servidores a ser utilizada. 3.6.3.1.3 Asignación Las estructuras de asignación incluyen lo siguiente:  Despliegue. Las estructuras de despliegue muestran cómo se asigna el software a los elementos de hardware de procesamiento y de comunicación. Esta vista permite al ingeniero razonar acerca del rendimiento, integridad de la data, disponibilidad y seguridad. Es de un interés particular en sistemas distribuidos o paralelos.  Implementación. Muestran como las estructuras de software (usualmente módulos) son asignados a las estructuras de archivos en el desarrollo del sistema.  Asignación de trabajo. Estas estructuras asignan las responsabilidades a los equipos de trabajo apropiados para implementar e integrar los módulos. 3.6.3.2 Relaciones entre estructuras  Elementos de una estructura se relacionan con elementos de otras estructuras.  A menudo la estructura dominante es la descomposición de módulos, ya que genera la estructura del proyecto.  Las estructuras proporcionan una poderosa separación de problemas para la creación de la arquitectura.  Las estructuras son la base de la documentación de la arquitectura. 75 3.6.3.3 Escogencia de la estructura En 1995, Philippe Krutchen público una investigación, describiendo el concepto de arquitecturas que comprende estructuras separadas y aconsejando concentrarse en cuatro. Las cuales son:  Lógica. Asigna el sistema en clases y componentes, se enfoca en las partes del sistema que proveen una funcionalidad y que los usuarios verán cuando interactúen con el sistema.  Procesos. Explica como las partes del sistema trabajan en conjunto y como se mantienen en sincronización. También explica como el sistema asigna las unidades computacionales, como son los procesos e hilos.  Desarrollo. Explica cómo se gestionará el software durante el desarrollo  Física. Explica como el software que implementa el sistema es asignado en las plataformas computacionales. 3.6.4 Métodos y procesos de desarrollo de software El desarrollo de software puede ser hacerse de muchas maneras. Estas maneras distintas son llamadas métodos o procesos, las cuales pueden ser:  Método cascado: En los métodos cascada, las diferentes fases de las actividades de desarrollo del sistema se siguen de forma secuencial.  Proceso unificado: Es un proceso popular en el que varias actividades, tales como generación de requerimientos, desarrollo y pruebas se superponen. En lugar de estar asociados a determinados productos y las tareas que los crean, las fases en el proceso unificado siguen la vida del producto, desde el inicio hasta la elaboración de su construcción y, finalmente, la transición.  Métodos agiles: Los métodos agiles son consecuencia del manifiesto ágil (Manifesto, 2001) el cual declara (entre otras cosas) que hay más valor 76 en el software trabajado que en la documentación generada por métodos como la cascada o el proceso unificado. 3.7 Análisis de arquitecturas Big Data Existen diferentes propuestas de arquitecturas en el mercado, las cuales fueron diseñadas por empresas como IBM, DataStax, Cloudera, Hortonworks. A continuación, se mostrarán las arquitecturas propuestas por algunas de las compañías antes mencionadas. 3.7.1 Propuesta de IBM: La propuesta de IBM se compone principalmente de dos módulos, los cuales son el módulo de aplicaciones analíticas y la plataforma de Big Data. También señalan que toda solución Big Data se compone de 4 capas, la capa de fuente de datos, la capa de almacenamiento, la capa de análisis y la capa de consumo En el módulo de aplicaciones analíticas encontramos las aplicaciones que se encargan de hacer reportes de inteligencia de negocio, la visualización, análisis predictivo, aplicaciones funcionales de la empresa, entre otras. Luego al descomponer el módulo de la plataforma de Big Data, se puede encontrar varios sub módulos, entre los cuales se encuentran en una primera capa, los sistemas de visualización junto con el desarrollo de aplicaciones y gestionado de sistemas. En la siguiente capa encontramos aceleradores, para aumentar la velocidad de las consultas o queries. Luego encontramos en una capa inferior el ecosistema Hadoop, en el cual está el HDFS, en el mismo nivel se encuentran los sistemas de transmisión y los almacenes de datos, que puede ser uno o varios, bien sean de modelos relacionales o de alguna base de datos NoSQL. Luego encontramos un nivel de integración de información, en el cual se integran todas las fuentes de datos de la empresa. 77 Figura 16 Arquitectura Big Data IBM 3.7.2 Propuesta de Cloudera: La propuesta de Cloudera consiste en diseñar la arquitectura planeando primero el clúster, en el cual se decide el tipo de hardware que se utilizara. Luego se planifica la ingesta de los datos al sistema, bien sea si el consumo será por lotes o por eventos. Después viene el estacionamiento el cual es el Hadoop. Se procede a planificar el procesamiento de los datos, en el cual se hace la transformación y el análisis utilizando MapReduce o Apache Spark. Por último, se toma en consideración las herramientas de análisis BI o Inteligencia de Negocios y buscadores como Solr, orientado a todo tipo de usuarios del sistema. Por último, viene el pipeline de datos para integrar todos los componentes antes mencionados. 78 Figura 17 Arquitectura Big Data Cloudera 3.7.3 Propuesta de Hortonworks: En la propuesta de Hortonworks se puede observar que utilizan un patrón parecido a las otras propuestas, en la cual se encuentran 3 capas principales, las cuales se componen de varios módulos. Tenemos la capa de fuente de datos, en la cual se obtienen todos los datos estructurados o no estructurados de diversas fuentes y formatos. Luego se observa en la capa del sistema de datos, en la cual tenemos el módulo de datos en memoria, que se comunica con el almacén de datos de la empresa, que a su vez se comunica con el ecosistema Hadoop. Al mismo tiempo esta capa antes mencionada se comunica con la capa de análisis, en la cual tenemos los datamarts, el análisis del negocio con los reportes y la visualización o dashboards para tomar las decisiones del negocio. 79 Figura 18 Arquitectura Big Data Hortonworks 3.7.4 Propuesta de Datastax y Hortonworks: En la propuesta de Datastax y Hortonworks es parecida a la de Hortonworks, solo que se sustituye el módulo de almacén de datos por la plataforma de base de datos NoSQL de Datastax, el cual consiste en un clúster de Cassandra. Además de integrar los sistemas de Datastax DevCenter y OpsCenter. 80 Figura 19 Arquitectura Big Data Datastax-Hortonworks 3.8 Diseño de una Arquitectura de Software Los pasos para realizar un diseño de arquitectura de software, se describen a continuación:  Arquitectura en el ciclo de vida  Diseñando la arquitectura  Formando la estructura del equipo y su relación con la arquitectura  Creación de un esqueleto del sistema 3.8.1 Arquitectura en el ciclo de vida Cualquier organización que adopte una arquitectura como fundamento para sus procesos de desarrollo de software, necesita entender su lugar en el ciclo vital. Existen muchos modelos de ciclo vital, pero uno que pone la arquitectura como tema central es el modelo de Entrega Evolutiva o Evolutionary Delivery Life Cycle 81 (ver Figura 20 Ciclo de Vida de entrega evolutiva). El objetivo de este modelo es involucrar al usuario y clientes en el desarrollo, obteniendo retroalimentación de ellos e iterar entre varios lanzamientos hasta generar un lanzamiento final del producto. Figura 20 Ciclo de Vida de entrega evolutiva Fuente: (Microsoft, 2017) Para saber cuándo comenzar a diseñar la arquitectura es necesario conocer los requerimientos del sistema de antemano. Es necesario identificar los principales objetivos del negocio, luego volverlos en escenarios de calidad o casos de usos. Al hacer esto se escogen aquellos que tendrán un mayor impacto en la arquitectura. 3.8.2 Diseñando la arquitectura En esta sección se explicará un método de diseño de arquitectura para satisfacer los requerimientos funcionales y de calidad. Este método es conocido como Atribute-Driven Design (ADD). 82 ADD es una aproximación para definir una arquitectura de software que basa su proceso de descomposición en los atributos de calidad que el software debe satisfacer. Este proceso de descomposición es recursivo y es un método top- down, donde en cada etapa se escogen tácticas y patrones de arquitectura para satisfacer un conjunto de escenarios de calidad. La salida que genera ADD es la primera de varios niveles de vistas de descomposición de una arquitectura. No todos los detalles de las vistas provienen de aplicar el modelo ADD; el sistema es descrito como un conjunto de contenedores de funcionalidades e interacciones entre ellos. Esta es la primera articulación de la arquitectura durante el proceso de diseño y, por tanto, es necesariamente grano grueso. Sin embargo, es fundamental para el logro de las cualidades deseadas, y proporciona un marco para lograr la funcionalidad. La diferencia entre una arquitectura que resulta de aplicar ADD y una lista para su implementación está en las decisiones de diseño más detalladas que se deben hacer. 3.8.2.1 Entrada del ADD La entrada de ADD es un conjunto de requerimientos. ADD asume requisitos funcionales (típicamente expresados como casos de uso) y restricciones como entrada, así como otros métodos de diseño. Éste método necesita que los requerimientos de calidad sean expresados como un conjunto de escenarios de calidad de especificaciones del sistema, los cuales deben ser definidos con el detalle necesario para la aplicación. Como escenarios de calidad para una puerta de garaje podríamos incluir los siguientes: 83  Los dispositivos y controles para abrir y cerrar la puerta son diferentes por la variedad de productos en la línea de productos.  El procesador utilizado en diferentes productos diferirá. La arquitectura del producto para cada procesador en específico debe ser directamente derivado de la arquitectura de la línea de productos.  Si un obstáculo (persona u objeto) es detectado por la puerta del garaje durante su cierre, debe detenerse y reabrirse en 0.1 segundos.  La puerta del garaje debe ser accesible para recibir diagnósticos y administración de un sistema de información del hogar, usando un protocolo de diagnóstico para productos. 3.8.2.2 Comenzando el ADD ADD depende de la identificación de los drivers de la arquitectura, los cuales como se mencionó anteriormente son los escenarios de calidad que tengan mayor impacto en la arquitectura. Durante el diseño, el determinar que drivers de arquitectura son clave puede cambiar, ya sea como resultado de un mejor entendimiento de los requerimientos o por cambio en los mismos. Aun así, el proceso puede comenzar cuando los requerimientos de los drivers arquitecturales son conocidos con cierta garantía. 3.8.2.3 Pasos del ADD A continuación, se mencionan brevemente los pasos para diseñar una arquitectura utilizando el método ADD:  Escoger el modulo a descomponer: El modulo para empezar usualmente es el sistema completo. Todas las entradas requeridas para este módulo deben estar disponibles (restricciones, requerimientos funcionales, requerimientos de calidad).  Refinar el módulo de acuerdo a los siguientes pasos: 84 o Escoger los drivers de la arquitectura del conjunto de escenarios de calidad y requerimientos funcionales. Este paso determina que es lo más importante para la descomposición. o Escoger un patrón arquitectural que satisfaga los drivers de la arquitectura. Crea (o selecciona) el patrón basado en las tácticas que pueden ser usadas para lograr los drivers arquitecturales. Identificar los módulos secundarios necesarios para poner en práctica las tácticas. o Instanciar los módulos y asignar la funcionalidad de los casos de usos representándolos usando múltiples puntos de vista. o Definir las interfaces de los módulos secundarios. La descomposición proporciona módulos y restricciones en los tipos de interacción de los módulos. Documenta esta información en la interfaz de documentación para cada módulo. o Verificar y refinar los casos de uso y los escenarios de calidad y convertirlos en restricciones para los módulos secundarios. En este paso se verifica que nada importante fue olvidado y prepara los módulos secundarios para su futura descomposición o implementación.  Repetir los pasos anteriores para cada módulo que necesite ser descompuesto. 3.8.3 Formando la estructura del equipo y su relación con la arquitectura Una vez que los primeros niveles de la descomposición de los módulos de la arquitectura son bastante estables, estos módulos pueden ser asignados a los equipos de desarrollo. Una vez que se ha acordado con una arquitectura para el sistema en construcción, los equipos de trabajo son asignados a cada módulo principal. Cada equipo entonces crea sus propias prácticas de trabajo internas. Además, los procedimientos de calidad y de prueba se establecen para cada equipo y 85 cada equipo debe establecer sus enlaces de comunicación y coordinación con otros grupos. Reconocer los módulos como mini dominios inmediatamente sugiere que el modo más efectivo de utilizar al personal es el de asignar miembros a los equipos de acuerdo a su experticia. Por ejemplo, un experto en base de datos vera el problema desde su punto de vista (almacenamiento de la data, persistencia, replicación), en cambio un experto en telecomunicaciones vera el sistema en términos de telecomunicaciones. 3.8.4 Creación de un esqueleto del sistema Una vez que la arquitectura está suficientemente diseñada y los equipos de trabajo están posicionados para comenzar su construcción, el esqueleto del sistema puede ser construido. La idea en esta etapa es proporcionar una capacidad subyacente para implementar la funcionalidad de un sistema en un orden ventajoso para el proyecto. Las practicas clásicas de la ingeniería del software recomiendan el método stub (Wikipedia, Stub Method, 2016) para que así secciones del sistema puedan ser agregados de forma separada y ser probadas independientemente. Sin embargo, ¿qué secciones de código deben aplicársele el método stub? Mediante el uso de la arquitectura como guía, se hace clara una secuencia para su implementación. En primer lugar, implementar el software que se ocupa de la ejecución y la interacción de los componentes arquitectónicos. Luego se pueden escoger cuales componentes proveedores de funcionalidad deben ser agregados al sistema, ya sea basado en disminuir el riesgo direccionando las áreas más problemáticas primero o basado en los tipos de personal disponible, o puede estar basado en sacar algo útil al mercado lo más pronto posible. 86 Una vez que se han elegido los elementos que proporcionan el siguiente incremento de la funcionalidad, se puede emplear las estructuras de usos que digan cual software adicional debe estar funcionando correctamente en el sistema para apoyar esa funcionalidad. Este proceso continúa creciendo y creciendo en los incrementos de funcionalidades del sistema, hasta que todo está en su lugar. En ningún momento la integración y las pruebas se hacen abrumadoras ya que en cada incremento se hace fácil encontrar cualquier falla. 87 Capítulo 4: Marco Metodológico 4.1 Bases metodológicas de la investigación La metodología hace referencia al conjunto de procedimientos basados en principios lógicos, utilizados para alcanzar una gama de objetivos que rigen en una investigación científica. (Eyssautier de la Mora, 2006). 4.1.1 Tipo de investigación Tomando en cuenta el problema planteado, este Trabajo Especial de Grado se define del tipo Proyecto Factible, que según UPEL (2003): “Consiste en la elaboración y desarrollo de una propuesta de un modelo operativo viable para solucionar problemas, requerimientos o necesidades de organizaciones o grupos sociales; puede referirse a la formulación de políticas, programas, tecnologías, métodos o procesos. El proyecto debe tener apoyo en una investigación de tipo documental, de campo o un diseño que incluya ambas modalidades” (p. 16). El proyecto factible conforma un proceso de planificación en el cual la investigación es una etapa que le proporciona información para sustentar la propuesta. Tomando en cuenta las diversas concepciones, el proyecto factible se desarrolla a través de las siguientes etapas (Moya, 2002):  El diagnóstico de las necesidades, el cual puede basarse en una investigación de campo o en una investigación documental. Las necesidades del presente trabajo fueron obtenidas al interactuar y dialogar con los stakeholders de la Misión Mercal C.A., además de realizar investigación bibliográfica de casos de uso similares.  Planteamiento y fundamentación teórica de la propuesta. El cual se encuentra en el marco teórico del presente trabajo. 88  El procedimiento metodológico. En este trabajo se aplicó la metodología Atribute Driven Design (ADD).  Las actividades y recursos necesarios para su ejecución. En el presente trabajo, por cada objetivo específico se realizaron una serie de actividades como son: (a) La investigación bibliográfica y de casos de uso de las diferentes propuestas de arquitecturas analíticas para grandes volúmenes de datos, en las cuales se encontraron patrones para solucionar problemas comunes; (b) Para el diseño de la arquitectura se procedió a aplicar la metodología Atribute Driven Design (ADD); (c) Para la selección de las herramientas asociadas a cada componente de la arquitectura diseñada se utilizaron matrices de evaluación en base a la metodología Desmet (Kitchenham, 1996); Para implementar la arquitectura seleccionada, fue necesario el diseño de los mecanismos de Extracción, Transformación y Carga (ETL), además se desplego y probo la arquitectura en ambientes de desarrollo, calidad y producción en la sede de Misión Mercal C.A. de Altamira.  El análisis de viabilidad o factibilidad del proyecto. Se contó con el apoyo de los encargados de la oficina de tecnología de información de la Misión Mercal C.A. y de los asesores de Phd 2014 Consultores CA, quienes, a través del juicio de experto, confirmaron la viabilidad y factibilidad del proyecto.  En el caso de que se tenga que desarrollar el proyecto factible, es necesario indicar la ejecución de la propuesta y la evaluación tanto del proceso como de sus resultados. Para la ejecución del presente trabajo se contó con un ambiente de desarrollo y un ambiente de calidad, los cuales fueron desplegados en la oficina de la Misión Mercal C.A. ubicada en Altamira. En dichos ambientes se instalaron y configuraron todos los componentes de la arquitectura de acuerdo a las herramientas seleccionadas a través de las matrices de evaluación en base a la metodología Desmet. Se desarrollaron los mecanismos de Extracción, Transformación y Carga (ETL). Se diseñaron los diversos modelos de datos para cada uno de los módulos correspondientes a la arquitectura desarrollada y se realizaron las pruebas respectivas. 89 4.1.2 Población y Muestra La población de este Trabajo Especial de Grado está representada por todos los Registros asociados a Inventario, Ventas, Proveedores y Clientes de cada una de las tiendas en Venezuela de la Misión Mercal C.A., que están conformadas por un aproximado de 300 a nivel nacional. Tomando en cuenta que la investigación pretende desarrollar una Arquitectura Analítica de Big Data, la muestra establecida fueron datos entregados por el cliente para hacer las pruebas, esta data fue solicitada para aplicarla en ambiente de desarrollo y no tiene implicaciones de confidencialidad. 4.1.3 Técnicas e Instrumentos de Recolección de Datos Las técnicas de recolección de datos son las distintas formas o maneras de obtener la información (Arias, 1999). Las técnicas utilizadas para la recolección de datos fueron:  Revisión Bibliográfica: en ella se acude especialmente a revistas científicas, informes y monografías, medios de comunicación que reflejan con más dinamismo que los libros los adelantos que se producen. (Sabino, 1992). La técnica consiste en recopilar y revisar todos aquellos documentos que permiten confrontar el aspecto teórico con la situación real o práctica dentro del diseño e implementación de arquitecturas Analíticas de Big Data en el mercado moderno. La revisión de los estudios previos nos permitirá (Pedraz, 2004): o Ahondar en la explicación de las razones por las que hemos elegido dicho tema de investigación 90 o Conocer el estado actual del tema: qué se sabe, qué aspectos quedan por investigar o Identificar el marco de referencia, las definiciones conceptuales y operativas de las variables estudiadas o Descubrir los métodos para la recolección y análisis de los datos utilizados o Contar con elementos para la discusión, donde se compararán los resultados que obtengamos con los de los estudios previos  Fuentes Infográficas: consiste en recopilar información a través de fuentes en línea tales como webinars, foros, páginas web. 4.2 Metodología de Desarrollo Se utilizó para el desarrollo de la Arquitectura Analítica Big Data la metodología Atribute-Driven Design (ADD), explicada previamente en el Marco Teórico. 91 Capítulo 5: Marco Aplicativo A continuación, se describe el diseño y desarrollo de la Arquitectura Analítica Big Data para el cálculo de indicadores de la Misión Mercal C.A., así como también las pruebas realizadas. Se explicarán los entregables del Trabajo Especial de Grado que se obtuvieron al aplicar la Metodología de Atribute Driven Design (ADD). 5.1 Entrada del ADD En la entrada de este método para diseño de arquitecturas de software, se tienen 3 tipos principalmente, los cuales son:  Requerimientos funcionales del Sistema  Restricciones de diseño  Requerimientos de calidad Las entradas deben ser obtenidas de los expertos del sistema, por lo que se involucró a las personas que estarían afectadas con la implementación del sistema. En nuestro caso, los directores de tecnología de la Misión Mercal C.A., técnicos del sistema y usuarios. Se obtuvieron las siguientes entradas: 5.1.1 Requerimientos funcionales  El Sistema debe permitir al usuario almacenar las transacciones diarias que se llevan a cabo en las Fuentes Logístico, Saf y Siga ya destacados.  El Sistema debe integrarse con los diversos sistemas Logístico, Saf y Siga, debido a que representan las fuentes de datos para el mismo.  El Sistema debe ser capaz de facilitar la visualización de los datos para la generación de reportes. 92  El Sistema debe garantizar el almacenamiento histórico de todas las transacciones fuentes.  El Sistema debe permitir realizar búsquedas rápidas al usuario.  El Sistema debe permitir integrar los datos de todas las oficinas de la Misión Mercal a nivel nacional.  El Sistema debe mantener su rendimiento aun cuando se maneje un gran volumen de datos. 5.1.2 Restricciones de diseño  El sistema debe cargar datos de la Base de datos Oracle 10g a un Clúster Hadoop/Hive.  Se debe utilizar Apache Hadoop como estacionamiento para operaciones de MapReduce, debido a que manejará un gran volumen de datos.  El Sistema debe correr en un ambiente GNU/Linux.  Los datos deben ser replicados en cada nodo del Clúster Hadoop/Hive  Los datos deben ser indexados con el fin de optimizar las consultas atreves de del motor de búsqueda SolR.  Las consultas del sistema, deben poder ser accedidas mediante una interfaz web en nuestro particular Hue. 5.1.3 Requerimientos de calidad Entre los requerimientos principales de calidad, de acuerdo a la norma ISO 9126 en su primera parte, la Arquitectura Analítica de Big Data propuesta para la Misión Mercal C.A., debe cumplir con características como funcionalidad, fiabilidad, usabilidad, eficiencia, mantenibilidad, portabilidad, calidad en uso. De estas características, los stakeholders seleccionaron las siguientes: Tabla 3 Seguridad Estímulo Un usuario desea acceder a los datos del sistema 93 Fuente de Estímulo Algún usuario intentando acceder o alguna otra persona ajena a la empresa. Ambiente El usuario intenta acceder a la plataforma del sistema mediante una autenticación. Artefacto Clúster Hadoop/Hive. Respuesta Se solicita al usuario que ingrese una contraseña, la cual previamente fue creada siguiendo normas de seguridad Medida de la Respuesta La contraseña es alfanumérica y con más de 7 dígitos. Tabla 4 Escalabilidad Estimulo El sistema procesa y almacena cada vez más datos Fuente de Estimulo Datos provenientes de las transacciones diarias de las más de 300 oficinas a nivel nacional. Ambiente Los datos son enviados al clúster HDFS mediante procesos de ETL. Los cuales se cargan desde Oracle 10g a un Clúster en Hadoop/Hive. Artefacto Clúster HDFS Hadoop/Hive. Respuesta El sistema debe replicar los datos, en diferentes nodos, de modo que los servidores no lleguen a su máxima capacidad. La escritura debe ser rápida. 94 Se debe poder almacenar gran cantidad de datos sin perder significativamente el rendimiento del sistema. Medida de la Respuesta Las consultas de los datos, no deben tardar más de 2 minutos si revisa más de 50 millones de registros, dependiendo de la cantidad de datos. El sistema debe aceptar la escalabilidad horizontal. Tabla 5 Capacidad de pruebas Estimulo El Sistema debe ser probado antes de pasar a producción. Fuente de Estimulo Stakeholders deben cerciorar que el Sistema cumple sus requerimientos de negocio. Ambiente Ambiente de desarrollo, el cual fue aportado por la empresa Phd 2014 Consultores C.A. Artefacto Ambiente GNU/Linux Respuesta En el sistema se cargarán los datos de los diferentes modelos relacionales. Para la carga, solo se consideran los campos de relevancia en el cálculo de las métricas. Medida de la Respuesta Se debe verificar que la cantidad de registros que se almacenan en el Clúster de Hadoop/Hive, concuerda 95 con la cantidad de registros en las tablas del modelo relacional. Tabla 6 Tolerancia a fallos Estimulo En el sistema falla alguno de sus componentes o Nodos del Clúster. Fuente de Estimulo Ocurre una falla al hacer una consulta a un dato o indicador almacenado en algún Nodo. Reinicio del Sistema, Apagón eléctrico, catástrofe ambiental. Ambiente Ocurre una falla al hacer una consulta a un dato o indicador almacenado en algún Nodo. Reinicio del Sistema, Apagón eléctrico, catástrofe ambiental. Artefacto Clúster Hadoop/Hive u otro componente de la arquitectura. Respuesta El sistema debe ser capaz de responder a las solicitudes de consulta mediante algún respaldo de los datos e indicadores. Medida de la Respuesta El Sistema replica los datos e indicadores, de manera que, si algún nodo de almacenamiento falla, otro pueda responder en su lugar. Por lo que el sistema debe responder a la consulta de datos siempre. 96 Tabla 7 Visualización de datos Estimulo El usuario desea visualizar datos históricos de las transacciones e indicadores. Fuente de Estimulo Generación de reportes para tomar decisiones del negocio Ambiente Interfaz web Artefacto Componente de visualización o motor de búsqueda especializado en visualizar datos Hue/SolR. Respuesta Se debe permitir al usuario visualizar los datos por medio de dashboards y gráficos. Medida de la Respuesta Acceder a los datos y generar gráficos de los datos existentes en el almacén de datos. Tabla 8 Análisis de datos Estimulo Disponer de datos históricos para realizar consultas y generar reportes Fuente de Estimulo Los stakeholders desean generar reportes de sus transacciones. Ambiente Almacén de datos en HDFS Hive. Artefacto Estacionamiento BI, el cual será Apache Hadoop/Hive por su capacidad de hacer operaciones MapReduce. Respuesta Almacenamiento histórico de la data, mediante la creación de cubos. Medida de la Respuesta Los datos se almacenarán en HDFS y luego se crean los cubos utilizando 97 motores de consultas para analizar los datos y generar los reportes. 5.2 Primera iteración de la metodología ADD 5.2.1 Paso 1: Confirmar que haya suficiente información de los requerimientos Verificamos que los requerimientos proporcionados son suficientes. Los requerimientos mostrados en la sección anterior son efectivamente suficientes para comenzar el diseño de la Arquitectura. El levantamiento de requerimientos fue llevado a cabo con miembros de la empresa afectada por el diseño de la nueva arquitectura en reuniones planteadas. 5.2.2 Paso 2: Escoger un elemento del sistema a descomponer En este paso, se descompuso el sistema completo. 5.2.3 Paso 3: Identificar los drivers de la arquitectura En este paso se analizaron los escenarios y su importancia en la arquitectura. Tabla 9 Drivers de la Arquitectura Fuente: Elaboración Propia # Drivers de la arquitectura Importancia Dificultad 1 Velocidad Alta Mediana 2 Volumen Alta Mediana 3 Variedad Alta Mediana 4 Consulta de datos sobre el estacionamiento HDFS Alta Alta 5 Tolerancia a fallos Alta Mediana 6 Generación de reportes Alta Alta 98  El primer driver de la arquitectura es la Velocidad, lo cual quiere decir que las consultas realizadas a un conjunto de datos no deben tardar más de 60 segundos en responder. El sistema debe poder realizar consultas a gran cantidad de datos tomando menos de 60 segundos.  El segundo Driver a analizar es el Volumen, lo cual significa que el sistema debe ser escalable debido a la enorme cantidad de datos transaccionales provenientes de los diversos sistemas de las oficinas a nivel nacional.  El tercer driver es la Variedad, debido a que las fuentes contienen muchos datos en sus tablas en NULL, es necesario que el esquema sea flexible y acepte una amplia variedad de datos (Formatos).  El cuarto driver es Consulta de datos sobre el estacionamiento HDFS, debido a que, para la Misión Mercal C.A., es de gran importancia poder realizar nuevas consultas sobre los datos almacenados con el fin de poder generar reportes propios y personalizados a mediano y largo plazo.  El quinto driver es la tolerancia a Fallos. Usualmente en Big Data se utiliza commodity hardware el cual tiene un alto riesgo a fallar, por lo cual deben existir mecanismos para evitar perder datos.  El sexto driver es la generación de reportes, lo cual significa que en la empresa Misión Mercal C.A es de primordial importancia que sus plataformas permitan la generación de reportes y dashboards, mediante la utilización de herramientas de inteligencia de negocio para tomar acciones acertadas. 5.2.4 Paso 4: Escoger un patrón que satisfaga los drivers de la arquitectura Este es el primer paso de diseño del método. 5.2.4.1 Aspectos de diseño asociados a los drivers de la arquitectura En el sistema se distinguen los siguientes aspectos de diseño por driver:  Velocidad: Se generan muchos datos en un corto periodo de tiempo. Los datos se generan principalmente de fuentes de datos relacionales. Las consultas no deben consumir largos periodos de tiempo. 99  Volumen: La cantidad de datos crece exponencialmente con el tiempo. El procesamiento y almacenamiento de los datos debe hacerse a gran escala. El desempeño del sistema no debe decaer significativamente.  Variabilidad: Existen datos de diferentes formatos y pueden ser no estructurados.  Consulta de datos sobre el estacionamiento HDFS: Las posibles indicadores a mediano y largo plazo forman una parte importante del sistema, por lo que este, debe ser capaz de almacenar adecuadamente los datos de mayor relevancia que vienen desde las diversas bases de datos relacionales.  Tolerancia a fallos: Los datos pueden perderse debido a fallas en el hardware, por lo que el sistema debe ser capaz de replicar los datos a otros nodos.  Generación de reportes: Para la generación de reportes es necesario que se permita acceder directamente a los datos para realizar consultas y también el uso de herramientas de generación de dashboards o inteligencia de negocio que sean compatibles con la plataforma. 5.2.4.2 Lista de patrones por cada aspecto de diseño Los patrones mostrados a continuación son específicos de arquitecturas Analíticas Big Data, los cuales son aplicados dependiendo del caso de uso y la problemática a solventar. (Buhler, Erl, & Khattak, 2015) Se generan muchos datos en un corto periodo de tiempo:  Alto volumen de almacenamiento binario Los datos se generan principalmente de fuentes de datos relacionales.  Fuente Relacional La cantidad de datos crece exponencialmente con el tiempo: 100  Alto volumen de almacenamiento binario  Fuente Relacional  Procesamiento Batch a gran escala El procesamiento y almacenamiento de los datos debe hacerse a gran escala:  Procesamiento Batch a gran escala  Depuración del conjunto de datos El desempeño del sistema no debe decaer significativamente:  Fragmentación de datos automático  Replicación y reconstrucción de los datos automático  Gobierno de conjunto de datos centralizada Existen datos de diferentes formatos y pueden ser no estructurados:  Alto volumen de almacenamiento binario Los datos pueden perderse debido a fallas en el hardware:  Replicación y reconstrucción de los datos automático  Fragmentación de datos automático Acceder directamente a los datos y herramientas de inteligencia de negocios compatibles con la plataforma:  Acceso directo a los datos  Acceso indirecto a los datos  Procesamiento Batch a gran escala 101 5.2.4.3 Selección de los patrones Para hacer la tabla más sencilla se le asignó a cada driver un número: 1. Velocidad 2. Volumen 3. Consulta de datos sobre el estacionamiento HDFS 4. Tolerancia a fallos 5. Variabilidad 6. Generación de reportes Por cada driver de la arquitectura se tienen los siguientes patrones: Tabla 10 Patrones vs Drivers Fuente: Elaboración Propia Drivers Patrones 1 2 3 4 5 6 Alto volumen de almacenamiento binario Si Si Si No Si No Fuente Relacional Si Si No No No No Procesamiento Batch a gran escala Si Si No No No Si Fragmentación de datos automático Si Si No Si No No 102 Replicación y reconstrucción de los datos automático No Si No Si No No Gobierno de conjunto de datos centralizada No Si No No No No Acceso directo a los datos No No No No No Si Acceso indirecto a los datos No No No No No Si A continuación, se explicarán brevemente los patrones seleccionados los cuales no son mutuamente excluyente, por lo tanto, es posible aplicar varios patrones a la vez sin ningún inconveniente. En muchos casos los patrones hacen uso de los mismos elementos o mecanismos:  Alto volumen de almacenamiento binario: Este patrón fue escogido debido a la presencia de datos de tipo blob o binarios almacenados en Oracle.  Problema: El almacenamiento de grandes cantidades de datos no estructurados en tecnologías de bases de datos tradicionales, no solo incurre en penalización del rendimiento, también sufre de problemas de escalabilidad cuando la cantidad de datos aumenta.  Solución: Los datos no estructurados se almacenan en base a una simple técnica de almacenamiento basado en clúster que implementa el acceso a las bases de datos a través de llaves o claves primarias.  Aplicación: Se utiliza un almacén de datos HDFS que se encarga de almacenar los datos binarios con una clave de identificación única, de manera que cada unidad de datos pueda ser sustituida, eliminada, encontrada o recuperada de forma individual. 103  Mecanismos: Mecanismo de almacenamiento y motor de serialización.  Fuente Relacional: Patrón utilizado debido a que los datos de la Misión Mercal C.A. se encuentran en bases de datos relacionales de licenciamiento privado, como es Oracle. Este patrón viene a satisfacer un nuevo driver de la arquitectura no contemplado anteriormente, que es el de fuentes de datos relacionales.  Problema: Exportar grandes cantidades de datos de una fuente relacional para luego importarlas, no solo consume mucho tiempo, también es ineficiente.  Solución: Para importar datos relacionales se hace una conexión directa desde la plataforma Big Data hasta la base de datos relacional.  Aplicación: Se utiliza un motor de transferencia de datos (herramienta ETL) en el cual se emplean diferentes conectores para conectarse a diferentes bases de datos y luego ejecutar consultas SQL para obtener los datos que serán importados.  Mecanismos: Motor de transferencia de datos, motor de procesamiento, mecanismo de almacenamiento, motor de flujo de trabajo, portal de productividad, manejador de recursos, motor de coordinación.  Procesamiento Batch a gran escala: Este patrón ayuda para que se puedan hacer operaciones en los datos a gran escala sin problemas de rendimiento, de manera que el sistema pueda tener un estacionamiento para luego hacer los reportes.  Problema: Procesar grandes cantidades de datos puede llegar a tener bajo rendimiento, además de que las técnicas tradicionales de procesamiento de datos son ineficientes para grandes volúmenes de datos debido a la latencia de transferencia de datos.  Solución: Los datos son consolidados en forma de un gran conjunto de datos y luego se procesan utilizando una técnica de procesamiento distribuido. 104  Aplicación: Los datos se procesan utilizando un sistema de procesamiento por lotes distribuidos, de tal manera que todo el conjunto de datos se procesa como parte del mismo ciclo de procesamiento de una manera distribuida.  Mecanismos: motor de procesamiento, motor de transferencia de datos, mecanismo de almacenamiento, manejador de recursos, motor de coordinación.  Fragmentación de datos automático: Este patrón se utiliza para mejorar el rendimiento de acceso de los clientes al conjunto de datos, sin embargo, en caso de tener muchos datos fragmentados, el rendimiento puede decaer. Este patrón es usado en conjunto con el patrón de replicación y reconstrucción de datos automáticos para evitar perdida de datos.  Problema: Mientras la cantidad de datos y el número de clientes accediendo a los datos aumentan, la latencia de acceso de los datos va aumentando gradualmente, lo que afecta el tiempo en completar las consultas.  Solución: El conjunto de datos es dividido horizontalmente por lo que los subconjuntos de filas son almacenados en diferentes maquinas a través del clúster, de este modo se distribuye la carga garantizando un alto rendimiento.  Aplicación: Se utiliza un estacionamiento HDFS que implementa fragmentación automática que dirige a los clientes a diferentes fragmentos en función de su respectivo criterio de consulta.  Mecanismos: Mecanismo de almacenamiento que soporte la fragmentación automática.  Replicación y reconstrucción de datos automático: Patrón utilizado para cumplir con el driver de tolerancia a fallos.  Problema: Por lo general en Big Data se utiliza hardware básico, por lo que comparado con hardware de nivel empresarial tiene una mayor probabilidad de fallo y por consecuencia de perdida de datos. 105  Solución: Varias copias de los datos son guardados y cualquier perdida en los datos debido a fallas de hardware son reconstruidos automáticamente.  Aplicación: Se utiliza una tecnología de almacenamiento Big Data que implemente replicación de datos automático, de manera que un mismo conjunto de datos se encuentra en varias máquinas del clúster, además de proporcionar reconstrucción de los datos.  Mecanismos: Mecanismo de almacenamiento que soporte la replicación de los datos.  Gobierno de conjunto de datos centralizado: Este patrón es necesario para proporcionarle al usuario final una interfaz centralizada para manejo de inteligencia de negocio.  Problema: El análisis de los datos utilizando tecnologías de Big Data, garantizando una continua gobernabilidad de los datos, desde su adquisición hasta su almacenamiento, puede ser una tarea desalentadora debido a la gran variedad y escenarios de uso no previstos de los datos.  Solución: La gobernabilidad de los datos está centralizado y se introduce un sistema que automatice las tareas de control de datos, incluyendo la gestión de ciclo de vida de los datos, la auditoria de acceso de datos e identificación del linaje de datos.  Aplicación: Se introduce un componente dentro de la plataforma Big Data que proporciona una interfaz centralizada para las políticas de creación y el seguimiento de auditoria.  Mecanismos: Manejador de gobierno de datos, motor de flujo de trabajo, motor de seguridad.  Acceso directo a los datos: Este patrón es utilizado para realizar conexiones directas a los datos y hacer consultas complejas.  Problema: Analizar un gran volumen de datos utilizando herramientas avanzadas de análisis que se basan en primero exportar los datos y luego 106 importarlos en otro mecanismo de almacenamiento compatible con la herramienta, no solo es ineficiente, sino también consume mucho tiempo.  Solución: Una conexión directa es hecha entre la plataforma Big Data y la herramienta de análisis mediante algún conector o estandarización que permita el acceso de los datos.  Aplicación: Un conector de dos vías es introducido entre la herramienta de análisis y la plataforma Big Data, el cual se encargará de traducir las llamadas de la herramienta a la plataforma.  Mecanismos: Mecanismo de almacenamiento, motor de consulta, motor de procesamiento, manejador de recursos, motor de coordinación.  Acceso indirecto a los datos: Este patrón es utilizado para realizar reportes en herramientas BI en las cuales los datos deben ser exportados y transformados para su correcto procesamiento.  Problema: Los analistas de datos que utilizan herramientas tradicionales de Inteligencia de negocio quizás deban acceder a datos procesados en la plataforma Big Data. Sin embargo, el uso de tecnologías de almacenamiento no relacional hace de esta tarea algo difícil, debido a que las herramientas tradicionales de inteligencia de negocio soportan solo conexiones a almacenes de datos relacionales.  Solución: Los datos ya procesados son exportados al almacén de datos distribuido, desde donde puede ser accedido por las herramientas existentes de inteligencia de negocio sin la necesidad de hacer conexiones separadas.  Aplicación: Los datos procesados son convertidos al esquema requerido, para luego ser exportados al almacén de datos distribuido usando una conexión.  Mecanismos: Motor de transferencia de datos, mecanismo de almacenamiento, portal de productividad, motor de flujo de trabajo, motor de consultas. 107 5.2.4.4 Combinación de los patrones Algunos patrones es posible combinarlos, debido a que poseen los mismos mecanismos o existen componentes que satisfacen varios de estos patrones a la vez. Se encontró que los siguientes patrones pueden combinarse en un solo patrón, debido a la similitud de sus componentes o mecanismos y a que mediante las matrices de evaluación en base a la metodología Desmet se encontraron herramientas que cumplen con varios de estos patrones de diseño. Para el primer conjunto de patrones combinados se encuentran: Replicación y reconstrucción de datos automáticos, fragmentación de datos automáticos y alto volumen de almacenamiento binario. Estos patrones pueden combinarse en un solo patrón que satisfaga todos los requerimientos, el cual llamaremos “patrón combinado de almacenamiento”. Para el segundo grupo de patrones combinados solo encontramos: Fuente relacional. A este nuevo patrón le llamaremos “patrón combinado de transferencia relacional”. 5.2.5 Paso 5: Instanciar los elementos de la arquitectura y asignar responsabilidades. En la realización del paso anterior se observó cómo los patrones seleccionados fueron capaces de satisfacer uno o más drivers de la arquitectura, por lo cual en este paso se explicará qué solución y elementos se encargaron de satisfacer las necesidades de la arquitectura, de manera que se compararan los patrones que satisfacen los respectivos drivers y la solución a la que se llega. La instanciación de los elementos se realizó de la siguiente manera, al comparar el patrón más el driver que satisface, dando como resultado una solución o componente de la arquitectura. A continuación, se presenta la instanciación de los elementos de la arquitectura y sus respectivas responsabilidades: 108  Patrón combinado de almacenamiento  Drivers relacionados: Volumen, Velocidad y Variabilidad.  Componente de la arquitectura: Estacionamiento y Almacén de Datos HDFS.  Responsabilidad: Componente encargado de almacenar todos los datos transaccionales depurados y de mayor relevancia para el cálculo de las métricas de la empresa.  Patrón combinado de transferencia relacional  Drivers relacionados: Velocidad y Volumen.  Componente de la arquitectura: Mecanismos de Extracción, Transformación y Carga.  Responsabilidad: Componente encargado de realizar la transferencia de datos a otros componentes de la arquitectura que lo requieran.  Procesamiento Batch a gran escala  Drivers relacionados: Volumen, Velocidad y Generación de reportes (paso de pre-procesar los datos).  Componente de la arquitectura: Estacionamiento de grandes volúmenes de datos con sistema de archivos distribuidos con capacidad de realizar procesamiento en paralelo.  Responsabilidad: Componente encargado de almacenar grandes volúmenes de datos para realizar operaciones sobre ellos.  Acceso directo a los datos:  Drivers relacionados: Volumen y Generación de reportes.  Componente de la arquitectura: Herramienta de software con lenguaje parecido a SQL y capacidad de conectarse directamente al componente de Estacionamiento.  Responsabilidad: Componente encargado de realizar las operaciones sobre el conjunto de datos del estacionamiento para la creación de cubos y consultas complejas.  Acceso indirecto a los datos:  Drivers relacionados: Volumen y Generación de reportes. 109  Componente de la arquitectura: Herramienta de inteligencia de negocios tradicional o con capacidad para grandes volúmenes de datos.  Responsabilidad: Componente encargado de realizar los reportes y dashboards de los datos que previamente fueron procesados por las diferentes consultas del negocio.  Gobernabilidad centralizada de los datos:  Drivers relacionados: Generación de reportes y centralización del sistema.  Componente de la arquitectura: Portal de Inteligencia de negocio o Interfaz web especializada en el manejo de las diferentes herramientas Big Data.  Responsabilidad: Componente que se encarga de integrar y manejar la mayoría de las herramientas de la arquitectura mediante una interfaz usable y segura. 5.2.6 Paso 6: Definir las interfaces de los elementos instanciados. En el paso se obtuvieron una serie de componentes que conforman la arquitectura. Por lo que en este paso se procede a la selección de herramientas de software comerciales o libres que conforman los componentes, utilizando la metodología Desmet y matrices de evaluación de herramientas de software. 5.2.6.1 Evaluación de herramientas de software Para la comparación de herramientas se utilizó la metodología Desmet ya que es una metodología creada para evaluar herramientas de software. (Kitchenham, 1996) Las herramientas que son objeto de estudio pertenecen al ámbito de los sistemas de archivos distribuidos, almacén de datos distribuido e inteligencia de negocios para Big Data. 110 Acorde a la metodología Desmet; como una herramienta fiable para la evaluación de aplicaciones de software: Los criterios de evaluación son:  Requerimientos funcionales: Abarcan aspectos de rendimiento, características avanzadas, funcionalidades específicas, etc.  Requerimientos específicos: Abarcan aspectos generales de cada herramienta y condiciones necesarias que deben cumplir las herramientas para encajar en la cultura de la organización. Para la aplicación efectiva del método Desmet, se usó una matriz de evaluación que consta de los siguientes atributos:  Tipo: Tipo de requerimiento.  Descripción del criterio evaluado: El criterio a evaluar.  Condición: Condición que debe cumplir el criterio (O-Obligatorio, D- Deseable, S-Suplementario e I-Informativo).  Peso: El valor de importancia del criterio (0= sin valor o criterio no importante, 5= máximo valor o importancia).  Cumplimiento: Si la herramienta cumple o no con el criterio.  Observaciones: Información relevante sobre la herramienta en relación al criterio.  Estrategia: El valor que posee el criterio en la herramienta. Si el criterio tiene como condición que No se cumple, este valor será de cero (0). Si el 111 criterio se cumple, este valor puede ir de 1 a 6 según en qué porcentaje aproximado se cumple el criterio.  Calificación ponderada: Es la calificación obtenida por la herramienta para el criterio dado. Se obtiene de la siguiente fórmula: Peso multiplicado por la Estrategia dividido entre 6. A continuación, se presentarán los resultados de la evaluación de las herramientas. (Para ver las matrices de evaluación ir a ANEXO 1: Pentaho vs Palo; ANEXO 2: Hadoop vs Otros; ANEXO 3: Hive vs ; ANEXO 4: Solr vs Elasticsearch y ANEXO 5: HUE vs Banano). 5.2.6.1.1 Componente para mecanismos de Extracción, Transformación y Carga de datos La herramienta seleccionada es Pentaho Data Integration. Con respecto a la plataforma que soportará las actividades correspondientes a la inteligencia de negocios, Pentaho comprende un conjunto de componentes robustos respaldados por una comunidad activa que garantiza un desempeño aceptable de los procesos necesarios para llevar acabo la inteligencia de negocios. Entre las características más relevantes que permiten a Pentaho destacarse del resto está la capacidad de realizar actividades de forma offline, soportar lenguaje SQL y permitir análisis en tiempo real. Por último, mencionar que su versión más reciente, incorpora el concepto de integración con Big Data y posee conectores para tal fin. 5.2.6.1.2 Componente de Estacionamiento con sistema de archivos distribuido La herramienta seleccionada es Apache Hadoop. 112 Hadoop es un framework de código abierto auspiciado por Apache Foundation, el cual desde su lanzamiento se ha transformado en un estándar en la industria, las razones que soportan este rápido auge estriban en las bondades de Hadoop para procesar grandes conjuntos de datos mediante equipos con bajas capacidades de computo agrupados en clúster o sistemas distribuidos. Adicionalmente Hadoop se integra con un ecosistema de aplicaciones que le facilita cubrir de forma transversal todas las actividades inmersas en la consulta, análisis, obtención de datos, publicación de mecanismos de seguridad, estudio y predicción de comportamiento. 5.2.6.1.3 Almacén de Datos Distribuido Apache Hive es una infraestructura de almacenamiento de datos construida sobre Hadoop para proporcionar agrupación, consulta, y análisis de datos, se posiciona en el mercado como la herramienta líder en el campo de almacén de datos distribuidos y creación de DataMarts, además de encontrarse bien posicionada en el mercado open source, la mayoría de los usuarios sigue inclinándose por el proyecto Apache Hive producto de alta disponibilidad, tolerancia a fallos, su extensa comunidad y sus APIS para desarrollo. Herramienta seleccionada: Apache Hive. 5.2.6.1.4 Motor de búsqueda Big Data como herramienta de inteligencia de negocio Para el componente de herramienta de inteligencia de negocio se pudo haber seleccionado cualquier herramienta tradicional de este tipo. Sin embargo, los motores de búsqueda actuales disponibles comercialmente, permiten la generación de gran variedad de reportes para inteligencia de negocio, además de que soportan la generación de reportes por facetas, es decir, filtrando las búsquedas sobre datos que fueron cargados previamente a la herramienta, por https://es.wikipedia.org/wiki/Almac%C3%A9n_de_datos https://es.wikipedia.org/wiki/Hadoop 113 lo que todos los reportes son hechos directamente sobre la herramienta. También los motores de búsqueda están diseñados para ser escalables, lo que le da una mayor ventaja competitiva en comparación con las herramientas tradicionales de inteligencia de negocio. En base a los resultados obtenidos de las matrices de evaluación, la herramienta seleccionada es Apache Solr. Esta herramienta fue comparada con Elasticsearch. Apache Solr y Elasticsearch son los motores de búsqueda más populares en la actualidad. Ambas herramientas son muy similares y brindan funcionalidades parecidas. En el 95% de los casos será indiferente escoger uno o el otro, ambas herramientas poseen licencia de Apache y una comunidad amplia. Sin embargo, Apache Solr tiene una contribución de compañías importantes en el área de Big Data, tales como Hortonworks, MapR y Cloudera. 5.2.6.1.5 Componente de Acceso directo a los datos Como herramienta seleccionada para este componente se decidió por Apache Hive, debido a que esta herramienta es un requerimiento particular del cliente. Apache Hive permite realizar consultas parecidas a SQL y así crear cubos y tablas para obtener los indicadores del negocio. Además, automatiza la creación del algoritmo MapReduce por lo que el usuario no tiene que programar el algoritmo directamente. . 5.2.6.1.6 Componente de manejo centralizado de datos y herramienta de visualización En base a los resultados obtenidos de las matrices de evaluación, la herramienta seleccionada es Hadoop User Experience (HUE). 114 Apache Hue es una herramienta que permite la visualización de los datos que se encuentren en Hadoop, la cual se puede integrar con cualquiera de sus versiones. Posee diversas funcionalidades para generar reportes, analizar, procesar, consultar, entre otras. Además de que permite integrar varias herramientas Big Data como son: Hive, Pig, Solr, HDFS, Impala, entre otras. 5.2.6.1.7 Componente balanceador de Carga de Peticiones En base a los resultados obtenidos de la evaluación de la larga lista de sitios web conocidos que lo usan, la herramienta seleccionada es Nginx. Nginx es un servidor web/proxy inverso ligero de al to rendimiento y un proxy para protocolos de correo electrónico (IMAP/POP3), usado principalmente para el balanceo de peticiones sobre servidores dedicados. 5.2.6.2 Información transmitida entre las herramientas:  Pentaho Data Integration: Este componente consume los datos que provienen de las fuentes Oracle, los procesa y transforma para que puedan ser cargados en Hadoop, en una segunda oportunidad es usado para migrar los datos depurados de hadoop a el almacén de datos Hive para su posterior procesamiento, en una tercera etapa es usado para convertir los datos provenientes de los cubos dimensionales de Hive en documentos Json que serán almacenados en el motor de búsqueda SolR.  Apache Hadoop: Recibe los datos generados por los ETL que consultan y procesan sobre las fuentes Oracle. En este componente se almacenan la mayoría de los archivos que serán utilizados por el componente de consultas especializadas o también denominado almacén de datos para crear tablas o cubos en el HDFS. https://es.wikipedia.org/wiki/Internet_Message_Access_Protocol https://es.wikipedia.org/wiki/Post_Office_Protocol 115  Apache Hive: En esta etapa la información es consultada directamente al HDFS el cual realiza un trabajo de mapeo y reducción sobre los datos, traduciendo la consulta hecha en HQL a un trabajo Map Reduce que generará los cubos asociados a las métricas a calcular.  Apache Solr: Se hace una conexión a la base de datos por defecto utilizada por Hive, para luego enviar mediante una transformación en Pentaho, los datos al motor de búsqueda Solr con el fin de indexarlos y procesarlos en un formato ligero Json, para posteriormente realizar reportes, gráficos, dashboards, entre otros.  HUE: Este componente se conecta directamente con Solr, HDFS y Hive, por lo que sirve de interfaz para realizar las consultas en Hive y luego generar los dashboards o reportes en Solr.  Nginx: Este componente se conecta directamente con cada uno de los servidores HUE, con el fin de facilitar el balanceo de peticiones para el procesamiento y cálculo de los indicadores. 5.2.7 Paso 7: Verificar y refinar los requerimientos para hacer restricciones en los elementos instanciados Se procede a verificar que los elementos instanciados cumplan con los diferentes requerimientos funcionales, requerimientos de calidad y las restricciones de diseño. 5.2.7.1 Componentes versus los requerimientos de calidad, requerimientos funcionales y restricciones de diseño  Componente para mecanismos de extracción, transformación y carga  Requerimientos Funcionales: Cumple con los requerimientos de permitir al usuario almacenar las transacciones diarias, debido a que es un mecanismo para transferir datos al estacionamiento de datos HDFS. 116 También ayuda en la integración de las demás oficinas a nivel nacional, mediante el uso de transformaciones de mediación, que permitan transferir cada cierto tiempo los datos actualizados de las distintas oficinas del país.  Requerimientos de calidad: Capacidad de pruebas, Escalabilidad, Tolerancia a fallos. La herramienta fue probada en ambiente GNU/Linux, además de que se transfirieron millones de registros en una gran cantidad de tablas migradas y se utilizó un trabajo en la herramienta que indicaba las transformaciones fallidas.  Restricciones de diseño: Este componente es muy versátil, ya que permite realizar migraciones a bases de datos relacionales como al sistema HDFS. Pentaho Data Integration no tiene ningún problema de ser ejecutada en un ambiente GNU/Linux.  Componente de estacionamiento Hadoop  Requerimientos funcionales: Permite integrar todos los datos para su posterior análisis.  Requerimientos de calidad: La herramienta posee tolerancia a fallos, debido a que los datos pueden ser replicados a través de varios nodos para mantener un respaldo.  Restricciones de diseño: Es utilizada como estacionamiento debido a que posee la capacidad de realizar operaciones Map Reduce sobre un gran volumen de datos.  Componente de Acceso directo a los datos o Almacén de Datos Hive  Requerimientos funcionales: Este componente sirve de ayuda previa para la creación de reportes, debido a que permite realizar consultas complejas y con los resultados crear los reportes en el siguiente componente de la arquitectura.  Requerimientos de calidad: La herramienta fue instalada y probada en ambiente de desarrollo. La seguridad que posee, es que se accede a ella 117 por medio de otra herramienta de interfaz web como HUE la cual solicita usuario y contraseña.  Restricciones de diseño: La herramienta se ejecuta sin ningún inconveniente en el ambiente GNU/Linux, como lo exige la restricción de diseño.  Componente de integración de herramientas y visualización de datos  Requerimientos funcionales: Mediante esta herramienta se generan los reportes de la empresa, debido a que integra las principales herramientas de análisis de datos (Solr y Hive).  Requerimientos de calidad: La herramienta permite la integración de las demás herramientas. Si falla, solo es necesario volver a levantar el servidor sin ningún problema. Cuando el usuario desea acceder a las funcionalidades de la herramienta, le solicita una autenticación. Permite la creación de grupos de usuario con sus respectivos permisos (roles).  Restricciones de diseño: La herramienta puede ejecutarse bajo ambiente GNU/Linux como un servidor. Las consultas y generación de reportes son accedidas mediante esta interfaz web.  Componente de Motor de búsqueda  Requerimientos funcionales: Permite realizar consultas y generar reportes facetados, es decir, por filtros. Mantiene un rendimiento estable al haber muchos datos debido a que está diseñada para ser escalable. Contiene diversas funciones que permiten y facilitan la generación de reportes, gráficos y dashboards para la inteligencia del negocio.  Requerimientos de calidad: La herramienta fue probada en ambiente de desarrollo y es escalable horizontalmente.  Restricciones de diseño: Se ejecuta bajo ambiente GNU/Linux, permite integración con interfaz web para visualización y generación de reportes. 118  Componente Balanceador de Carga.  Requerimientos funcionales: Permite realizar el balanceo de peticiones para el procesamiento y cálculo de los indicadores sobre cada uno de los servidores HUE acorde a un algoritmo de peticiones.  Requerimientos de calidad: La herramienta fue probada en ambiente de desarrollo y es escalable horizontalmente.  Restricciones de diseño: Se ejecuta bajo ambiente GNU/Linux, permite integración con cualquier componente; en nuestro caso particular HUE. Con esto concluye la ejecución de la metodología ADD. A continuación, se procede a mostrar el diagrama de componentes y el diagrama de casos de uso para el nivel cero y primer nivel.  Diagrama de componentes: En el siguiente diagrama de componentes se puede observar la manera en la que se conectan cada uno de ellos. (Ver Figura 21 Diagrama de componentes) 119 Figura 21 Diagrama de componentes Fuente Elaboración Propia 120  Diagrama de casos de uso nivel 0: En el diagrama de casos de uso se puede observar el nivel cero, en el cual se muestra de forma general los actores y el rol que desempeñan en el sistema. (Ver Figura 22 Caso de uso Nivel 0 y Tabla 11 Actores - Casos de Uso Nivel 0) Figura 22 Caso de uso Nivel 0; Fuente: Elaboración Propia 121 Tabla 11 Actores - Casos de Uso Nivel 0 Fuente: Elaboración Propia Actor Descripción Mercal Usuario final del sistema, el cual se encarga de utilizar la interfaz web de HUE con todas sus funcionalidades, como generar reportes y hacer consultas para obtener información. HUE Interfaz web encargada de administrar las herramientas, generar reportes y gráficos (Integra Solr, Hive y HDFS) Solr Es un motor de búsqueda, utilizado como herramienta de inteligencia de negocio para generar reportes por facetados Hive Es el encargado de realizar consultas complejas, parecidas a SQL y crear cubos para los indicadores del negocio. Hadoop Funciona como el estacionamiento de la arquitectura, sobre el cual se realizan la mayoría de las operaciones de análisis, limpieza, procesamiento, entre otros. Pentaho Data Integration Encargado de crear y ejecutar transformaciones o Jobs que permiten la transferencia de datos a los componentes requeridos. 122 Nginx Encargado de balancear las peticiones sobre la interfaz HUE para el cálculo de indicadores. Tabla 12 Funcionalidades Casos de uso nivel 0 Fuente: Elaboración Propia Caso de uso Obtener y visualizar las métricas calculadas. Actor Mercal Flujo Básico El usuario de Mercal inicia sesión en la interfaz web de HUE. Luego puede realizar una serie de tareas como: a) Consultas utilizando Hive b) Generar reportes/dashboards con Solr c) Administrar HDFS d) Visualizar indicadores calculados Pre-Condición Principalmente debe estar funcionando Hadoop, Solr, Hive y el servidor de HUE. El usuario fue creado por el administrador o es administrador. Caso de uso Extraer Transformar y cargar datos Actor Pentaho Data Integration Flujo Básico Se procede a realizar la transferencia de datos entre los diversos componentes de la Arquitectura Pre-Condición Es necesario establecer la conexión con la fuente de datos a extraer transformar y con el componente a cargar los datos Caso de uso Estacionar datos de relevancia Actor Hadoop Flujo Básico Los datos son almacenados en el HDFS de Hadoop, para su posterior procesamiento utilizando Map Reduce. Pre-Condición Los datos fueron obtenidos de los sistemas fuente utilizando una transformación de Pentaho. 123 Caso de uso Crear cubos Actor Hive Flujo Básico Se crean tablas o cubos en la base de datos de Hive con los datos de Hadoop. Pre-Condición Las tablas son almacenadas en HDFS mediante una configuración (hive- site.xml) indicando que se utilizara como base de datos una ruta en el HDFS Caso de uso Consultar los datos Actor Hive Flujo Básico Los datos pueden ser consultados mediante un lenguaje de alto nivel parecido a SQL, el cual consulta directamente a los datos que se encuentran en el HDFS de Hadoop denominado HQL Pre-Condición Previamente los datos fueron cargados a Hadoop y Hive pre configurado para usar HDFS como lugar de almacenamiento físico. Caso de uso Indexar Documentos para la generación de reportes Actor Solr Flujo Básico Se procede a indexar los datos obtenidos mediante una transformación de Pentaho, a formato Json para en un posterior uso generar los reportes asociados a cada indicador. Se crean dashboards o graficas geo referenciales si el usuario lo solicita. Pre-Condición Los datos fueron previamente cargados a Solr en formato JSON mediante una transformación de Pentaho, los cuales fueron extraídos de las tablas creadas por Hive. Caso de uso Administrar y utilizar herramientas Actor HUE Flujo Básico El usuario inicia sesión Puede generar consultas con Hive Crear tablas en Hive 124 Administrar el HDFS Crear grupos de usuarios Generar reportes y gráficos en Solr Pre-Condición HUE fue configurado previamente para conectarse con las distintas herramientas a utilizar en la arquitectura. Caso de uso Balancear Peticiones Actor Nginx Flujo Básico El usuario que se conecte a algún servidor HUE será despachado por el balanceador de carga a alguno que se encuentre disponible o con menor flujo de trabajo. Pre-Condición Nginx fue configurado previamente para conectarse con los servidores HUE a utilizar en la arquitectura. Para ver el diagrama de casos de uso nivel 1 ver ANEXO 7: Casos de uso Nivel 1. A continuación, se procede a explicar las actividades realizadas para desarrollar la Arquitectura Analítica de Big Data, como son: Software necesario como Prerrequisito, diseño de ETL, conversión de datos a formato ligero, integración y comunicación de los diversos componentes. 5.3 Implementación de la Arquitectura Se mostrará a continuación, la evidencia de la instalación de las herramientas, para una información más detallada de como instalar y configurar cada una de ellas, vea el ANEXO 8: Instalación y configuración de Herramientas. En la figura 23 se muestra la versión utilizada de Java, la cual se instaló mediante repositorios apt-get install en todos los servidores de la arquitectura. 125 Figura 23 Instalación Java En la figura 24 se evidencia la ejecución e inicialización de los nodos del estacionamiento HDFS. Figura 24 Cluster Estacionamiento HDFS 3 nodos En la figura 25, puede observarse los procesos levantados por el Nodo Maestro, entre los cuales encontramos el NameNode, que es levantado solo por el Maestro, el SecondaryNamenode (puede levantarse en otra máquina para mayor seguridad) y su respectivo Datanode. 126 Figura 25 Procesos Hadoop Nodo Maestro En la figura 26 se evidencia la consola de cliente de hive, mediante la cual se hacen las consultas HQL. Figura 26 Evidencia ejecución Hive En la figura 27 se evidencia el despliegue del servidor de solr, mediante el comando. /solr start 127 Figura 27 Ejecución Solr Para implementar la Arquitectura fue necesario diseñar los modelos de datos asociados a los cubos Hive como a los documentos Json de SolR, para lograr esto se analizaron los diversos modelos de datos correspondientes a los sistemas fuentes de la Misión Mercal C.A. y con apoyo de los especialistas de área (bajo 3era forma normal en Oracle 10g) de manera que se pudiese hacer un correcto modelo de datos acorde a los requerimientos de Negocio. Luego fue necesario el diseño de los mecanismos de ETL y las pruebas para lograr una correcta extracción, limpieza, transformación e inserción de los datos en el clúster de estacionamiento HDFS y demás componentes de la arquitectura. (Ver, ANEXO 6: Diseño de los mecanismos ETL). Una vez que se finalizó la construcción de los modelos precisados y de las transformaciones necesarias para la migración, seguidamente fue necesario monitorear y observar el comportamiento de las herramientas en ejecución, principalmente de las transformaciones, las cuales manejaron gran volumen de datos y del clúster de estacionamiento HDFS, por lo que se observó el comportamiento del sistema realizando diversas pruebas. 128 Conclusiones y Recomendaciones Tener la información significa poder, por eso cuesta trabajo creer que muchas empresas e incluso gobiernos no sepan aprovechar el enorme potencial que tienen de datos de personas y procesos en sus propios computadores. Bug Data ofrece un abanico de tecnologías para el análisis inteligente de la enorme cantidad de datos que están expuestos en las nuevas tecnologías, y se está teniendo en cuenta cada vez más para vaticinar los problemas del mundo en los negocios, el comercio y en la vida cotidiana. La creación y diseños de arquitecturas de software que permitan almacenar y procesar grandes volúmenes de datos para sacarles el mayor provecho, representa una gran ventaja competitiva en la actualidad. En casi todas las áreas de negocio se está aplicando Big Data en conjunto con la ciencia de datos, para tomar decisiones más precisas o generar campañas de mercadeo más exitosas. En los próximos años esta tendencia será casi obligatoria para gran parte de las empresas, ya que con el aumento exponencial de datos en la web y con el auge del internet de las cosas, será aún más necesario procesar y almacenar toda esta cantidad de datos, utilizando técnicas y metodologías innovadoras y creativas. De acuerdo a los objetivos planteados se evidenció y ratificó los siguientes puntos: - Primer y Segundo objetivo específico (Diseñar la arquitectura), se puede concluir, que de manera obligatoria en cualquier diseño de una arquitectura es de prioridad alta, tomar en cuenta los requerimientos del cliente, en este caso la Misión Mercal C.A., debido a que permite identificar de manera precisa e inmediata los casos de uso y posibles escenarios que impactaran en la propuesta de arquitectura. Además, se pudo observar que los patrones de Big Data, mostrados en el marco aplicativo, soportan y proporcionan una valiosa columna vertebral para solucionar problemas específicos, los cuales se aplicaron exitosamente 129 junto a la metodología ADD. Cumpliendo con el principio de Ingeniería de Software que impulsa el uso de una Arquitectura inicial con la finalidad de reutilizar componentes y patrones en el desarrollo de software. Cabe destacar que dicho proceso permite generalizar la arquitectura planteada a otros sistemas, como un proceso de migración a Big Data Analítica para la empresa que lo requiera. - Tercer objetivo específico (Seleccionar las herramientas de la Arquitectura) se concluye, que el uso de la metodología Desmet permite satisfacer los requerimientos del sistema para la organización, debido a que con el uso de requerimientos de software y ponderaciones se toman las decisiones más acertada en la elección de las herramientas, dependiendo del caso de estudio y componente a implantar. - Para el último objetivo específico se concluye, que acorde a la implementación de la arquitectura no solo basta diseñarla e instalar cada uno de sus componentes, es necesario realizar pruebas de cada una de las herramientas que la conforman, de manera que estas pudiesen interoperar observando el funcionamiento óptimo de las mismas. En cada uno de los componentes de la arquitectura fue necesario realizar actividades de entonación para lograr satisfacer las necesidades del cliente. Además, debido a que el cliente no está familiarizado con el paradigma Big Data, fue necesario emprender actividades de gestión del cambio. Recomendaciones a nivel técnico:  Es recomendable que la transformación se configure por lotes, debido a que transferir datos muy pesados puede consumir rápidamente los recursos de la máquina virtual de Java, lo que ocasiona que el kernel del sistema finalice el proceso antes por consumir muchos recursos.  Se recomienda que los servidores en los que se despliegue la arquitectura o herramientas, sea de alta capacidad para empresas grandes como la 130 Misión Mercal C.A., mínimo 8 GB de RAM y 2 procesadores con sistema operativo GNU/Linux.  Se recomienda probar las herramientas en ambiente de calidad para adaptarse y solucionar cualquier inconveniente que pueda surgir durante la implementación de arquitecturas Analíticas Big Data.  Se recomienda que la red en la cual se maneja el tráfico de datos sea una red de alta velocidad y en una red virtual separada. Recomendaciones a nivel General:  No en todos los casos es viable la utilización de arquitecturas o plataformas Analíticas de Big Data, por lo que es recomendable hacer un estudio de planificación de capacidad en la organización antes de hacer cualquier propuesta de Big Data.  El implementar una arquitectura Analítica Big Data en una organización no significa que se deba descartar la utilización de sistemas relacionales, en algunos casos será necesario trabajar con ambas tecnologías como lo es, por ejemplo, para la Misión Mercal C.A.  En Venezuela existen muchas organizaciones que manejan gran cantidad de datos y que utilizan un modelo relacional que comienza a tener problemas de renovación de licenciamiento en moneda extranjera, lo cual implica que existe una gran posibilidad de reutilización de la arquitectura planteada a través del presente trabajo.  Además, es recomendable que el personal encargado del área de tecnologías de información se instruya en la utilización y desarrollo de la arquitectura propuesta en este trabajo para poder satisfacer las necesidades del mercado y que las empresas puedan brindar un mejor servicio a sus clientes. 131 Bibliografía Antiñanco, M. J. (2013). Bases de Datos NoSQL: Escalabilidad y alta disponibilidad a través de patrones de diseño. 10-20. Apache. (2016). Hadoop. Obtenido de Apache Hadoop Org: http://hadoop.apache.org/ Apache. (26 de Enero de 2016). Hadoop Yarn. Obtenido de Apache Org: http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html Apache. (2016). Lucene. Obtenido de http://lucene.apache.org/core/ Arias, F. G. (1999). El Proyecto de Investigacion : Guia para su elaboracion. Caracas: Episteme. Blog, T. B. (7 de Noviembre de 2016). The Big Data Blog. Obtenido de Hadoop Ecosystem Overview: http://thebigdatablog.weebly.com/blog/the-hadoop-ecosystem-overview Brewer, E. (2000). Toward Robust Distributed Systems. Obtenido de http://www.cs.berkeley.edu/~brewer/cs262b-2004/PODC-keynote.pdf Buhler, P., Erl, T., & Khattak, W. (2015). Big Data Fundamentals: Concepts, Drivers & Techniques. Chauhan, A. (2012). Blog Microsoft. Obtenido de https://blogs.msdn.microsoft.com/avkashchauhan/2012/02/27/primary-namenode- and-secondary-namenode-configuration-in-apache-hadoop/ Comercio, C. d. (2016). codigo de derecho. Obtenido de https://derechovenezolano.wordpress.com/2012/11/01/el-codigo-de-comercio-el- registro-mercantil-concepto-documentos-sujetos-a-registro-efectos/ Conway, D. (30 de Septiembre de 2010). Drew Conway Venn Diagram. Obtenido de Drew Conway: http://drewconway.com/zia/2013/3/26/the-data-science-venn-diagram Dean, J., & Ghemawat, S. (2003). Research Google Inc. Obtenido de http://research.google.com/archive/mapreduce-osdi04-slides/index-auto-0008.html Española, R. A. (Marzo de 2016). Diccionario de la lengua española. Obtenido de http://dle.rae.es/?id=Vj40asb Evans, E. (12 de 05 de 2009). NOSQL 2009. May 2009. – Blog post of 2009-05-12. Obtenido de http://blog.sym-link.com/2009/05/12/nosql_2009.html Eyssautier de la Mora, M. (2006). Metodología de la investigación: desarrollo de la inteligencia. Cengage Learning Editores. 132 Gutierrez, D. (Junio de 2014). Yarn is All the Rage at Hadoop Summit 2014. Obtenido de Kdnuggets: http://www.kdnuggets.com/2014/06/yarn-all-rage-hadoop-summit.html Hanmer, R. (2013). Pattern-Oriented Software Architecture for Dummies. En R. Hanmer. Wiley. IBM. (2015). IBM. Obtenido de Infografia: http://www.ibmbigdatahub.com/infographic/four- vs-big-data Kitchenham, B. (1996). DESMET: A method for evaluating software engineering methods and tools. Lowell, U. (2013). Computer Science. Obtenido de Umass Lowell: http://www.cs.uml.edu/~jlu1/doc/source/report/MapReduce.html MacCandles, M., Hatcher, E., & Gospodnetic, O. (2010). Lucene in Action. Manning. Magazine, L. V. (10 de noviembre de 2013). Big Data, el tesoro oculto del siglo XXI. Manifesto, A. (2001). Agil Manifesto. Obtenido de http://agilemanifesto.org/ Marco. (8 de Mayo de 2013). Slideshare. Obtenido de Seminario Apache Solr: http://es.slideshare.net/paradigmatecnologico/seminario-apache-solr MERCAL. (2017). MERCAL. Obtenido de http://www.mercal.gob.ve/?p=41 MERCAL. (2017). Organigrama. Obtenido de Estructura Organizativa Mercal: http://www.mercal.gob.ve/wp-content/uploads/2013/06/ORGANIGRAMA2.jpg Microsoft. (2017). Microsoft. Obtenido de https://msdn.microsoft.com/en- us/library/ee658098.aspx Morgan, L. (5 de Abril de 2015). Information Week. Obtenido de Information Week Big Data: http://www.informationweek.com/big-data/big-data-analytics/6-ways-to-master-the- data-driven-enterprise/d/d-id/1320234 Moya, D. d. (2002). El Proyecto Factible: una modalidad de investigación. Caracas: Sapiens. Murthy, A. (15 de Octubre de 2013). Apache Hadoop. Obtenido de Hortonworks: http://hortonworks.com/blog/apache-hadoop-2-is-ga/ NoSQL. (2016). NoSQL Database . Obtenido de NoSQL Database org: http://nosql- database.org/ Pedraz, A. (2004). La revisión bibliográfica. Pritchett, D. (2008). BASE: An Acid Alternative. Obtenido de http://queue.acm.org/detail.cfm?id=1394128 Sabino, C. (1992). EL PROCESO DE INVESTIGACIÓN. Caracas. 133 Sánchez, F. M. (2014). Herramientas para Big Data: Entorno Hadoop. 59. Shvachko, K., Kuang, H., Radia, S., & Chansler, R. (2006). The Hadoop Distributed File System. Soares, S. (2012). Dataversity. Obtenido de http://www.dataversity.net/not-your-type-big- data-matchmaker-on-five-data-types-you-need-to-explore-today/ Strozzi, C. (2010). NoSQL – A relational database management system. Obtenido de http://www.strozzi.it/cgi-bin/CSA/tw7/I/en_US/nosql/Home%20Page The Register. (2006). Obtenido de http://www.theregister.co.uk/2006/08/15/beer_diapers/ Vance, A. (2009). Hadoop, a Free Software Program, Finds Uses Beyond Search. New York: New York Times. Wall, M. (6 de Marzo de 2014). BBC. Obtenido de BBC Mundo: http://www.bbc.com/mundo/noticias/2014/03/140304_big_data_grandes_datos_rg White, T. (2012). Hadoop: The definitive guide. 41-47. White, T. (2012). Hadoop: The Definitive Guide (3rd ed.). O’Reilly Media. Wikipedia. (2015). Motores de busqueda. Obtenido de https://es.wikipedia.org/wiki/Motor_de_b%C3%BAsqueda Wikipedia. (2016). Apache Solr. Obtenido de https://es.wikipedia.org/wiki/Apache_Solr Wikipedia. (2016). Stub Method. Obtenido de https://en.wikipedia.org/wiki/Method_stub Wikipedia. (2016). Wikipedia. Obtenido de https://es.wikipedia.org/wiki/Registro_mercantil winshuttle. (2016). winshuttle.com. Obtenido de http://www.winshuttle.es/big-data-historia- cronologica/ 134 ANEXO 1: Pentaho vs Palo 135 ANEXO 2: Hadoop vs Otros 136 ANEXO 3: Hive vs Pig 137 ANEXO 4: Solr vs Elasticsearch Leyenda Solr Elasticsearch Tip o Descripció n Condició n Pes o Cumplimient o Observacione s Estrategi a Calificació n Ponderada Replicación O-Obligatorio 5 S-Si Cumple 6 5,00 S-Si Cumple 6 5,00 Facetado O-Obligatorio 5 S-Si Cumple 6 5,00 S-Si Cumple 6 5,00 Escalabilidad O-Obligatorio 5 S-Si Cumple 6 5,00 S-Si Cumple 6 5,00 Búsqueda orientada a texto 5 S-Si Cumple 6 5,00 S-Si Cumple ES provee queries complejos de análisis 5 4,17 Búsqueda geo- espacial D-Deseable 4 S-Si Cumple 6 4,00 S-Si Cumple 6 4,00 Visualización S- Suplementari o 4 S-Si Cumple 6 4,00 S-Si Cumple 6 4,00 Debe poseer licencia de código libre o abierto O-Obligatorio 5 S-Si Cumple Licencia de Apache 6 5,00 S-Si Cumple Licencia de Apache 6 5,00 Documentació n O-Obligatorio 5 S-Si Cumple Solr se encuentra muy bien documentado. 6 5,00 S-Si Cumple 5 5,00 Comunidad D-Deseable 4 S-Si Cumple La comunidad es muy amplia y puede realizar cambios. Aporte de varias compañías 6 4,00 S-Si Cumple La comunidad tiene acceso al código, pero solo la compañía puede hacer cambios 5 3,33 Sistemas operativos soportados O-Obligatorio 5 S-Si Cumple GNU/Linux, Unix, Windows 6 5,00 S-Si Cumple GNU/Linux, Unix, Windows 6 5,00 Total 4,7 4,55 R e q . F u n c io n a le s R e q . F u n c io n a le s R e q . E s p e c íf ic o s R e q . E s p e c íf ic o s 138 ANEXO 5: HUE vs Banano Leyenda Hue Banano Tip o Descripció n Condició n Pes o Cumplimient o Observacione s Estrategi a Calificació n Ponderada Integración con Ecosistema Hadoop O- Obligatorio 5 S-Si Cumple 6 5,00 S-Si Cumple Se integra con Solr y Solr al HDFS 4 3,3 Acceso al HDFS O- Obligatorio 5 S-Si Cumple 6 5,00 S-Si Cumple Mediante Solr 4 3,3 Generación de Dashboards y gráficos O- Obligatorio 5 S-Si Cumple 6 5,00 S-Si Cumple 6 5,00 Exportación de reportes y gráficos en diferentes formatos O- Obligatorio 5 S-Si Cumple 6 5,00 S-Si Cumple 6 5,00 Plataforma fácil de desplegar D-Deseable 4 S-Si Cumple Basado en Web 6 4,00 S-Si Cumple Debe integrarse con Solr 4 2,7 Comunidad O- Obligatorio 5 S-Si Cumple 6 5,00 S-Si Cumple 6 5,00 Gratuito y de licencia libre O- Obligatorio 5 S-Si Cumple 6 5,00 S-Si Cumple 6 5,00 Documentación O- Obligatorio 5 S-Si Cumple 6 5,00 S-Si Cumple 5 5,00 Seguridad O- Obligatorio 5 S-Si Cumple 6 5,00 N-No Cumple No posee autenticación 0 0 Total 4,9 3,8 R e q . F u n c io n a le s R e q . F u n c io n a le s R e q . E s p e c íf ic o s R e q . E s p e c íf ic o s 139 ANEXO 6: Diseño de los mecanismos ETL Procesos de Extracción, Transformación y Carga El mecanismo de ETL (Extracción, Transformación y Carga) facilita el movimiento de los datos entre diferentes sistemas aplicando métodos de Extracción, Limpieza, Organización, Transformación y Validación de información, de forma automatizada desde un Origen hacia otra fuente destino. La herramienta utilizada para construir los ETL es Pentaho Data Integration (PDI). Esta herramienta cuenta con una interfaz gráfica que permite construir procesos de Extracción, Limpieza, Organización, Transformación y Validación de la Data. Para este proyecto se diseñaron un conjunto significativo de mecanismos ETL para la carga inicial de datos provenientes de aproximadamente 300 oficinas de la Misión Mercal C.A. 1 Diseño de Mecanismos de ETL para la Carga Inicial Se presentan a continuación las características más relevantes de los mecanismos ETL, diseñados para la ejecución del proceso de Carga Inicial. Representación del proceso de obtención de datos, desde las oficinas de Mercal hacia el estacionamiento distribuido, mediante el uso de los mecanismos ETL. Figura 28 Diseño ETL Misión Mercal 2 Aspectos técnicos de los mecanismos de ETL de Carga Inicial. Para el desarrollo de los ETL de carga inicial, es necesario tomar como punto de partida una tabla de alguno de los esquemas de datos de los sistemas fuentes Logístico, Saf o Siga sobre el cual se procederá a calcular el indicador; procediendo de la siguiente forma: • Se analizan las tablas revisando sus relaciones primarias y secundarias. 140 • Generar una consulta SQL capaz de extraer el contenido de la tabla principal junto con las tablas relacionadas y los campos de importancia. • Se ejecuta un proceso de validación de datos y estructura. • Posteriormente se inserta en el estacionamiento HDFS. Observe el patrón de Jobs usado por los mecanismos ETL, en los próximos párrafos comprenderá el comportamiento y detalle de cada uno de los pasos. Cada job contiene Cinco (5) pasos, los cuales se detallan a continuación: 1. Start: Es el paso que únicamente da inicio el job. 2. Transf_pdi_hadoop: Es el paso encargado de la llamada a la transformación que realizara la carga de datos de las fuentes al estacionamiento HDFS. 2.1 Transformación (Transf_pdi_hadoop): Es la encargada de consultar los datos de relevancia para el cálculo de los indicadores y almacenarlos en el HDFS de Hadoop. Figura 29 Job Carga Inicial 141 Figura 30 Transf_pdi_hadoop 2.1.1 Table Input: Paso en el cual se establece la conexión compartida a los diferentes sistemas fuentes y consulta SQL. ◦ La Conexión a la Base de Datos: para ello se utilizó una conexión compartida para todas las transformaciones. El nombre de la conexión es Oracle_37, donde se indica el host (Dirección IP del Servidor), el nombre de la Base de Datos Oracle, el puerto a utilizar y las credenciales de acceso (Usuario y Contraseña) correspondiente al esquema específico para cada uno. 142 ◦ La Consulta SQL: Es una consulta SQL de tipo SELECT para obtener los datos de la base de datos Oracle que se quieren cargar en el estacionamiento HDFS. Figura 32 Paso de consulta SQL sobre Oracle Figura 31 Paso de conexión Base de datos Oracle 143 2.1.2 Calculator: Paso mediante el cual se realiza una conversión de los campos tipo date obtenidos de Oracle a un formato aceptado por el motor de búsqueda SolR. Figura 33 Calculator 2.1.3 Hadoop file Output: Es el encargado de la conexión con el clúster Hadoop y la carga de un archivo .txt con los datos y campos especificados que se obtuvieron de los pasos previos en el estacionamiento HDFS. Figura 34 Hadoop file Output 144 Figura 35 Campos a Cargar HDFS 3. Load_Hive: Llamada a la ejecución de un job encargado del procesamiento sobre el almacén distribuido Hive. 3.1 Start: Es el paso que únicamente da inicio el job. 3.2 Sql: En este paso se procede a crear una tabla en el almacén de datos Hive que corresponda a la estructura de los datos previamente obtenidos Figura 36 load_hive 145 de los sistemas fuentes en Oracle y almacenados en Hadoop. ◦ La Conexión a la Base de Datos: para ello se utilizó una conexión compartida para todas las transformaciones. El nombre de la conexión es Hive, donde se indica el host (Dirección IP del Servidor), el nombre de la Base de Datos Oracle, el puerto a utilizar y las credenciales de acceso (Usuario y Contraseña) correspondiente al esquema específico para cada uno. Figura 37 Conexión a Hive ◦ La Consulta HQL: Es una consulta muy parecida a SQL de tipo CREATE para crear una estructura acorde a los datos cargados en el estacionamiento HDFS. 146 Figura 38 Consulta HQL 3.3 Hadoop Copy Files: En este paso se realiza la carga de los datos que se encuentran en texto plano en el HDFS a la nueva estructura creada en Hive sobre el mismo estacionamiento especificando origen y destino. Figura 39 Carga de los datos en estructura Hive 4. Load_hive_map: Llamada a la ejecución de un job encargado del procesamiento para la generación de cubos sobre el almacén distribuido Hive. 147 4.1 Start: Es el paso que únicamente da inicio el job. 4.2 Sql: En este paso se procede a crear el cubo para los indicadores asociados sobre el almacén de datos Hive que corresponda a la estructura de los datos previamente obtenidos de la estructura Hive creada. ◦ La Conexión a la Base de Datos: para ello se utilizó una conexión compartida para todas las transformaciones. El nombre de la conexión es Hive, donde se indica el host (Dirección IP del Servidor), el nombre de la Base de Datos Oracle, el puerto a utilizar y las credenciales de acceso (Usuario y Contraseña) correspondiente al esquema específico para cada uno. Figura 40 load_hive_map 148 Figura 41 Conexión a Hive ◦ La Consulta HQL: Es una consulta muy parecida a SQL de tipo CREATE AS para crear una estructura de cubo proporcionado por Hive. Figura 42 Creación de Cubo Hive 149 5. HiveToSolr: En este paso se realiza la llamada a la transformación que se encarga de indexar los cubos Hive en nuevos documentos Json sobre el motor de búsqueda SolR. 5.1 Table Imput: En este paso se establece la conexión a Hive para obtener los datos almacenados en los cubos y la conversión a formato ligero tipo Json de cada registro del mismo. Figura 43 HiveToSolr 150 ◦ La Conexión a la Base de Datos: para ello se utilizó una conexión compartida para todas las transformaciones. El nombre de la conexión es Hive, donde se indica el host (Dirección IP del Servidor), el nombre de la Base de Datos Oracle, el puerto a utilizar y las credenciales de acceso (Usuario y Contraseña) correspondiente al esquema específico para cada uno. Figura 44 Conexión a Hive ◦ La Consulta HQL: Es una consulta muy parecida a SQL de tipo SELECT con el particular uso de una función creada en Hive (UDF), para la conversión de cada registro en formato Json. 151 Figura 45 Consulta HQL y Conversión a Json 5.2 Select Values: En este paso se procede a seleccionar el valor denominado data que representa a cada registro Json generado en el paso anterior. Figura 46 Selección de Valores 5.3 Crear campos faltantes: En este paso se crean los campos necesarios para llevar a cabo la conexión con el servidor solR. 152 Figura 47 Campos Conexión SolR 5.4 REST Client: Paso encargado de indexar cada uno de los registros obtenidos de los pasos previos en el servidor SolR a través del protocolo REST. Figura 48 REST Client 3 Aspectos técnicos de los mecanismos de ETL de Mediación. Para el desarrollo de los ETL de Mediación por cada uno de los indicadores previamente calculados, se hace de igual manera con la única variante de consideración de la fecha de última carga de datos, como fecha base, para el pase de diferencial de registros. 153 ANEXO 7: Casos de uso Nivel 1 Figura 49 Casos de uso nivel 1 Fuente: propia Tabla 13 Funcionalidades casos de uso nivel 1 Fuente: propia Caso de uso Conectarse a Repositorio Actor PDI Flujo Básico Para ejecutar una transformación de carga de datos, es necesario que 154 antes se haya conectado al repositorio de las transformaciones, las cuales están almacenadas en alguna base de datos, como puede ser postgreSQL Pre-Condición Las transformaciones fueron almacenadas en una base de datos relacional. Caso de uso Ejecución MapReduce Actor Hadoop Flujo Básico Una vez que se solicita la ejecución de un trabajo MapReduce, el jobtracker se encarga de asignar cada trabajo de mapeo a los tasktracker más cercanos a los datanodes, luego de que los tasktrackers finalicen su trabajo le notifican al jobtracker, el cual se encargara de asignar la tarea de reducción a un tasktracker en específico y guardar el resultado en la ruta esperada. Pre-Condición Se solicita la ejecución de un trabajo MapReduce y deben estar activos los procesos principales como el namenode, el datanode, el resource manager, el jobtracker, tasktracker. Caso de uso Almacenamiento en HDFS Actor Hadoop Flujo Básico Un proceso o usuario solicita almacenar en HDFS bien sea mediante un comando o mediante una configuración previa. Pre-Condición Debe estar ejecutándose los procesos principales de Hadoop (namenode, datanode, resource manager) Caso de uso Conectarse a HDFS Actor Hive Flujo Básico Para realizar las consultas en Hive, primero debe realizarse una conexión al HDFS la cual fue configurada previamente en el archivo hive- site.xml. Pre-Condición Pre configuración de hive-site.xml para almacenar en HDFS. 155 Caso de uso Conectarse a las herramientas Actor HUE Flujo Básico Mediante HUE se hace una integración con las herramientas principales de generación como son Hadoop, Hive y Solr, las cuales fueron configuradas previamente. Pre-Condición Se configuro el archivo hue.ini para indicar las herramientas a conectarse y los puertos mediante el cual se harán las conexiones Caso de uso Generación de reportes con Solr Actor Mercal Flujo Básico El usuario de Mercal al ingresar a la interfaz web, puede escoger la opción de generar reportes y dashboards en Solr para obtener información de los Registros Almacenados Pre-Condición El usuario ingreso al sistema mediante autenticación y las herramientas están funcionando correctamente. Caso de uso Consultas con Hive Actor Mercal Flujo Básico Una vez que el usuario ingreso a la interfaz de HUE y escoge la opción de hacer consultas con Hive, puede realizar cualquier consulta en HQL mediante un panel de texto de consultas. Pre-Condición El usuario ingreso previamente al sistema y el Hiveserver2 está corriendo. Caso de uso Administrar HDFS Actor Mercal Flujo Básico El usuario puede observar y administrar el HDFS mediante una interfaz que permite hacer búsquedas de archivos. Pre-Condición Los procesos de Hadoop deben estar levantados. Caso de uso Realizar Petición 156 Actor Nginx Flujo Básico El usuario realiza una petición al momento de ingresar a la interfaz Web que será redirigida a alguno de los servidores HUE configurados. Pre-Condición Todos los componentes y procesos de la arquitectura deben de estar iniciados y en correcto funcionamiento. 157 ANEXO 8: Instalación y configuración de Herramientas Instalación de Java ambiente de Desarrollo 1. Ingresar al servidor con el usuario phd2014. $> ssh phd2014@<ip-servidor> 2. Añada y descargue de los repositorios e instale java 8, luego se establece por defecto en el sistema, ejecute los siguientes comandos. $> su - $> echo "deb http://ppa.launchpad.net/webupd8team/java/ubuntu xenial main" | tee /etc/apt/sources.list.d/webupd8team-java.list $> echo "deb-src http://ppa.launchpad.net/webupd8team/java/ubuntu xenial main" | tee -a /etc/apt/sources.list.d/webupd8team-java.list $> apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys EEA14886 $> apt-get update $> apt-get install oracle-java8-installer $> exit 3. Verificar con el siguiente comando que el java se instaló correctamente: $> java –version Figura 50 Evidencia de la instalación de java en ambiente de desarrollo 4. Para termina, establecer java por defecto ejecute: sudo apt-get install oracle-java8-set-default 158 2 Instalación de Hadoop Una vez iniciada la sesión en el servidor, descargue e instale hadoop-2.7.1, descomprima y mueva la carpeta descomprimida al directorio /phd2014/hdfs/. $>wget http://apache.mesi.com.ar/hadoop/common/stable2/hadoop- 2.7.1.tar.gz $> cd /phd2014/hdfs/ $> tar xzvf hadoop-2.7.1.tar.gz $> rm -f hadoop-2.7.1.tar.gz $> mv hadoop-2.7.1/* . $> rm -rf hadoop-2.7.1 Nota: Para esta instalación ip-nodo-1 corresponde al servidor Maestro, ip-nodo- 2 y ip-nodo-3 los nodos esclavos. Configuración de Hadoop A continuación, se especifica la configuración del clúster de Apache Hadoop para la Misión Mercal C.A., el cual cuenta con tres nodos, designando el servidor de <IP-nodo1> como nodo maestro-esclavo y los servidores de <IP-nodo2> y <IP- nodo3> como nodos esclavos. Iniciar sesión en el servidor. Editar el archivo /etc/host en todos los nodos del clúster. Todos los nodos deben tener la misma configuración en el archivo /etc/hosts ya que esto permitirá conocer los demás nodos pertenecientes al clúster <ip-nodo1> master <ip-nodo2> slave1 <ip-nodo3> slave2 Nota: Este archivo tiene la misma configuración para los tres nodos del clúster. Edite los siguientes archivos en el directorio /phd2014/hdfs/etc/hadoop. 159 hadoop-env.sh: Esta configuración debe aplicarse a todos los nodos Hadoop del clúster. En este archivo debemos especificar el directorio de instalación de Java, para ello modifique la propiedad JAVA_HOME. export JAVA_HOME=/usr/lib/jvm/jdk1.8.0_45 core-site.xml: El nodo maestro y los nodos esclavos deben utilizar el mismo valor para la propiedad fs.defaultFS. Todos los nodos esclavos deben apuntar al nodo maestro. Todos los servicios de Hadoop y clientes usan este archivo para localizar el NameNode, ya que este archivo contiene el nombre del sistema de archivos por defecto. El nodo NameNode será el servidor master con <ip- nodo1>. Además, se especifica la propiedad hadoop.tmp.dir, la cual se utiliza como la base para los directorios temporales a nivel local, y también en el HDFS. Agregue las siguientes propiedades dentro de la etiqueta <configuration>al archivo. Este archivo queda igual para todos los nodos. <property> <name>fs.defaultFS</name> <value>hdfs:master:9000</value> </property> <property> <name>hadoop.tmp.dir</name> <value>file:/var/hadoop/data/hdfs/tmp</value> </property> Nota: Este archivo tiene la misma configuración para los tres nodos del clúster. Antes de configurar el archivo hdfs-site.xml debemos crear los directorios para los datos del datanode y el namenode. $> sudo mkdir -p /var/hadoop/data/hdfs/namenode $> sudo mkdir -p /var/hadoop/data/hdfs/datanode $> sudo mkdir -p /var/hadoop/data/hdfs/tmp 160 $> sudo chown -R phd2014:phd2014 /var/hadoop/* Hdfs-site.xml: Esta configuración debe aplicarse al nodo maestro y a los nodos esclavos. En este archivo especificamos el directorio en el sistema de archivos local donde el NameNode almacenará su archivo de metadatos y DataNode almacenará los bloques de datos. Además, se indica el factor de replicación que tendrá la data del HDFS, esto significa que, por cada archivo almacenado en el HDFS, habrá una repetición redundante de ese archivo en algún otro nodo del clúster. Este archivo queda igual para todos los nodos. Agregar al archivo las siguientes propiedades: <property> <name>dfs.replication</name> <value>3</value> </property> <property> <name>dfs.name.dir</name> <value>file:/var/hadoop/data/hdfs/namenode</value> </property> <property> <name>dfs.data.dir</name> <value>file:/var/hadoop/data/hdfs/datanode</value> </property> Nota: Este archivo tiene la misma configuración para los tres nodos del clúster. yarn-site.xml: Esta configuración debe aplicarse al nodo maestro y los nodos esclavos. Este archivo es necesario para que un nodo trabaje como un nodo hilado, porque posee la configuración adecuada que permite dividir las dos principales funcionalidades del JobTracker, la gestión de recursos (ResourceManager) y trabajo de planificación/monitoreo (NodeManager), en demonios separados (hilos). Los Nodos maestro y esclavo deben utilizar el mismo valor para las siguientes propiedades, y deben estar apuntando a nodo maestro solamente. 161 <property> <name>yarn.nodemanager.aux-services</name> <value>mapreduce_shuffle</value> </property> <property> <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name> <value>org.apache.hadoop.mapred.ShuffleHandler</value> </property> <property> <name>yarn.resourcemanager.address</name> <value>master:8050</value> </property> <property> <name>yarn.resourcemanager.resource-tracker.address</name> <value>master:8025</value> </property> <property> <name>yarn.resourcemanager.scheduler.address</name> <value>master:8035</value> </property> Nota: Este archivo tiene la misma configuración para los tres nodos del clúster. mapred-site.xml: Esta configuración debe aplicarse solo al nodo maestro. En este archivo se especifica el host y el puerto donde se ejecuta el JobTracker y el framework para la ejecución de trabajos MapReduce. Este archivo no existe y se debe crear copiando el archivo mapred-site0.xml.template con el nombre de mapred-site.xml. 162 <property> <name>mapreduce.jobtracker.address</name> <value>master:5431</value> </property> <property> <name>mapreduce.framework.name</name> <value>yarn</value> </property> Actualiza el archivo slaves que se encuentra en el directorio /phd2014/hdfs/etc/hadoop solamente para el nodo maestro. Coloque solo el nombre de los servidores que actúan como esclavos en el clúster. Este archivo es utilizado por los scripts de Hadoop para iniciar los servicios apropiados en los nodos maestros y esclavos. master slave1 slave2 Cree el archivo masters en el directorio /phd2014/hdfs/etc/hadoop solamente para el nodo maestro. Coloque solo nombre del servidor que actúa como maestro en el clúster. Este archivo es utilizado por los scripts de Hadoop para iniciar los servicios apropiados en los nodos maestros y esclavos. Master 4. Configuración SSH Hadoop requiere acceso SSH para administrar sus nodos (máquinas remotas) y su máquina local. El próximo paso es generar una clave ssh sin contraseña de inicio de sesión entre el nodo maestro y los nodos esclavos. Ejecute los siguientes comandos sólo en el nodo maestro. 163 $> su – phd2014 $> ssh-keygen -t rsa -P "" $>cat /home/phd2014/.ssh/id_rsa.pub >> /home/phd2014/.ssh/authorized_keys $> chmod 600 authorized_keys $> ssh-copy-id -i ~/.ssh/id_rsa.pub slave1 $> ssh-copy-id -i ~/.ssh/id_rsa.pub slave2 $> ssh master $> ssh slave1 $> ssh slave2 5. Copiar las siguientes líneas en el archivo .bashrc ubicado en el directorio / home/phd2014 Realizar este paso en el nodo maestro y cada nodo esclavo. export HADOOP_HOME=/phd2014/hdfs export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin Nota: Luego de modificar el archivo .bashrc, éste debe ser cargado nuevamente para que tome los cambios realizados, ejecute source ~/.bashrc para volver a cargar el archivo. 6. Formatear el NameNode Antes de iniciar el clúster, se debe formatear el NameNode solo en el nodo maestro. Utilice el siguiente comando para formatear el nodo. Tenga en cuenta que cada vez que formatea el namenode se elimina la data almacenada en el HDFS. $> cd /phd2014/hdfs/bin $> ./hdfs namenode –format 7. Inicie el sistema de archivo desde el nodo maestro. 164 $> cd /phd2014/hdfs/sbin $> ./start-dfs.sh Para validar que el DFS se inició con éxito, ejecute el siguiente comando en el nodo maestro y el nodo esclavo: $> jps La salida de este comando debe enumerar NameNode y SecondaryNameNode iniciado en el nodo maestro y DataNode en todos los nodos esclavos. 8. Inicie el Yarn MapReduce desde el nodo maestro. $> cd /phd2014/hdfs/sbin $> ./start-yarn.sh Para validar que el Yarn Mapreduce se haya iniciado correctamente, ejecute el comando jps nuevamente en los nodos maestro y esclavos. La salida de este comando debe enumerar ResourceManager en el nodo maestro y NodeManager, en todos los nodos esclavos. 9. Valide el inicio exitoso del clúster a través de las Consolas Web: Para ResourceManager: http://<ip-nodo-1>:8088 Para NameNode: http://<ip-nodo-1>:50070 Nota: Para esta instalación ip-nodo-1 corresponde al servidor master. Figura 51 Evidencia de salida del comando. Enumera NameNode y SecondaryNameNode Figura 52 Evidencia de salida del comando. Enumera NameNode y SecondaryNameNode 165 3 Instalación de Apache Hive, Solr y HUE A continuación, se muestra la instalación de las herramientas Hive, Solr y HUE, las cuales permiten la ejecución de consultas sobre la data almacenada en el clúster de Hadoop de una manera eficiente. Dichas instalaciones solo deben realizarse en el nodo Hadoop Maestro para Hive y SolR y HUE en sus respectivos servidores. Preparación del Entorno: Ingresar al servidor y descargar el paquete apache-hive-1.2.1-bin.tar.gz mediante el comando: wget http://www-us.apache.org/dist/hive/hive- 1.2.1/apache-hive-1.2.1-bin.tar.gz, Luego, se crean los directorios /phd2014/hive/ en el servidor master y /phd2014/hue/ en el servidor HUE. $> mkdir /phd2014/hive $> mkdir /phd2014/hue Instalación de Apache Hive Descomprima el archivo apache-hive-1.2.1-bin.tar.gz, ubicado en el directorio de descarga, dentro del directorio /phd2014/hive/.: Edite el archivo /home/phd2014/.bashrc y agregue las siguientes líneas: export HIVE_HOME=/phd2014/hive export PATH=$PATH:$HIVE_HOME/bin Nota: Luego de modificar el archivo .bashrc, éste debe ser cargado nuevamente para que tome los cambios realizados, ejecute source ~/.bashrc para volver a cargar el archivo. 3. Hive ha actualizado la librería Jline2, pero existe una versión anterior de esta librería en el directorio HADOOP_HOME/share/hadoop/yarn/lib la cual debemos eliminar antes de iniciar Hive, para ello ejecute el siguiente comando: 166 rm -r /phd2014/hdfs/share/hadoop/yarn/lib/jline-0.9.94.jar 4. Ejecute hive con el siguiente comando y verifique la instalación: $> /phd2014/bin/./hive Logging initialized using configuration in jar:file:/phd2014/hive/lib/hive-common- 1.2.0.jar!/hive-log4j.properties hive> 5. Para salir de hive ejecute: hive> exit; Instalación de Solr 1. Una vez dentro del servidor Solr descomprimir el archivo solr-6.1.0.zip,, ubicado en la ruta: /phd2014/instaladores/, dentro de /phd2014/solr/. La estructura del directorio debe quedar como se muestra continuación: 2. Edite el archivo /home/phd2014/.bashrc y agregue las siguientes líneas: Figura 53 Evidencia Ejecución Hive Figura 54 Evidencia Ejecución Hive 167 export SOLR_HOME=/phd2014/solr export PATH=$PATH:$SOLR_HOME/bin Nota: Luego de modificar el archivo .bashrc, éste debe ser cargado nuevamente para que tome los cambios realizados, ejecute source ~/.bashrc para volver a cargar el archivo. 3. Ejecute solr con el siguiente comando. $> cd /phd2014/solr/bin $> ./solr start 4. Verifique la instalación: http://<ip-nodo>:8983 Instalación de Hadoop User Experience (HUE) Instalando requisitos previos Ejecutar los siguientes comandos para instalar los paquetes de desarrollo Nota: La instalación debe realizarse en el mismo usuario donde esté instalado Hadoop. Este debe poseer permisos de administrador para poder realizar la instalación. Nota: La instalación debe realizarse en el mismo usuario donde esté instalado Hadoop. Este debe poseer permisos de administrador para poder realizar la instalación. Si el usuario no posee los permisos de administrador, estos pueden ser otorgados desde el usuario root modificando el archivo sudoers con el comando sudo nano etc/sudoers, luego agregamos la línea “<usuario> ALL=(ALL:ALL) ALL” justo debajo de los privilegios de root y guardamos el archivo. sudo apt-get install python2.7-dev make libkrb5-dev libxml2- dev libxslt-dev libsqlite3-dev libssl-dev libldap2-dev python- pip sudo apt-get install python2.7-dev make libkrb5-dev libxml2- dev libxslt-dev libsqlite3-dev libssl-dev libldap2-dev python- pip sudo apt-get install ant gcc g++ libmysqlclient-dev libssl-dev libsasl2-dev libsasl2-modules-gssapi-mit libtidy-0.99-0 make libldap2-dev maven python-dev python-setuptools libgmp3-dev 168 Instalación de Maven Utilizamos este comando para descargarnos la última versión de Maven Tarball Figura 55 Descarga Maven Tarball Descomprimimos el archivo Figura 56 Descomprimir Maven wget http://www-eu.apache.org/dist/maven/maven- 3/3.3.9/binaries/apache-maven-3.3.9-bin.tar.gz wget http://www-eu.apache.org/dist/maven/maven- 3/3.3.9/binaries/apache-maven-3.3.9-bin.tar.gz tar xzvf apache-maven-3.3.9-bin.tar.gz tar xzvf apache-maven-3.3.9-bin.tar.gz 169 Wget https://dl.dropboxusercontent.com/u/730827/hue/releases/3.8.1/hue- 3.8.1.tgz Verificamos el valor de nuestra variable $JAVA_HOME (este valor variará según la versión de Java instalada) Figura 57 Verificando versión Java Agregamos a PATH y verificamos instalación Figura 58 Verificar Instalación Maven Instalando y configurando Hue. Para descargar Hue utilizamos el siguiente comando export PATH=/rutahastaelarchivomaven/apache-maven-3.3.9/bin:$PATH export PATH=/rutahastaelarchivomaven/apache-maven-3.3.9/bin:$PATH mvn -v 170 make apps tar xzvf hue-3.8.1.tgz mv hue-3.8.1 hue Cd Hue Figura 59 Descargando HUE Descomprimimos el archivo y renombramos la carpeta Figura 60 Construyendo HUE Para instalar Hue nos posicionamos en la carpeta Hue e instalamos 171 ./build/env/bin/hue runserver http://127.0.0.1:8000/ Figura 61 Construyendo HUE Así debería finalizar la instalación Figura 62 Finalizando instalación HUE Después de terminar la instalación podemos iniciar Hue posicionándonos otra vez en la carpeta de Hue y ejecutando Figura 63 Iniciando HUE Y luego accedemos desde el navegador web a Hue solicitará que se cree un usuario para poder continuar. Es recomendado que este coincida con el nombre de usuario de GNU/linux donde está instalado Hue y Hadoop. 172 <property> <name>dfs.webhdfs.enabled</name> <value>true</value> </property> cd $HADOOP_HOME/etc/hadoop nano hdfs-site.xml Figura 64 Interfaz Web HUE Enlazar Hue y Hadoop Abrimos y modificamos el archivo hdfs-site.xml de hadoop y agregamos las siguientes líneas 173 <property> <name>hadoop.proxyuser.hue.hosts</name> <value>*</value> </property> <property> <name>hadoop.proxyuser.hue.groups</name> <value>*</value> </property> cd $HADOOP_HOME/etc/hadoop nano core-site.xml Figura 65 Enlazando HUE - Hadoop Abrimos y modificamos el archivo core.site.xml de hadoop y agregamos las siguientes líneas 174 Figura 66 Enlazando HUE – Hadoop 2 Para verificar que Hue y Hadoop fueron enlazados exitosamente iniciar los demonios de hadoop, iniciar hue con el comando ./build/env/bin/hue runserver, acceder a http://127.0.0.1:8000/, y seleccionar la opción Gestionar HFDS que se encuentra en la parte superior derecha de la página principal de Hue. Ahí deberían visualizarse los archivos HDFS. Figura 67 Gestión HDFS con HUE http://127.0.0.1:8000/ 175 Instalando y configuración Nginx. Para lograr la configuración requerida y el correcto balance de peticiones entrantes es necesario la utilización de un balanceador de carga. En esta arquitectura se utilizará un servidor de NGINX. Instalando requisitos previos: Instalar las dependencias necesarias para la compilación: $> sudo apt-get install build-essential libssl-dev libpcre3-dev Descargar la versión del NGINX: $> wget http://nginx.org/download/nginx-1.4.4.tar.gz Descomprima el archivo nginx-1.4.4.tar.gz que acaba de descargar mueva la carpeta generada al directorio /home/<su_usuario>/mercal/ $> tar xzvf nginx-1.4.4.tar.gz $> mv nginx-1.4.4 nginx $> mv nginx /home/<su_usuario>/mercal/ Queda compilarlo e instalarlo, de momento necesitaremos los módulos siguientes para la ejecución de la configuración: --with-http_gzip_static_module –sbin-path=/usr/local/sbin -- with-http_ssl_module --without-mail_pop3_module -- without-mail_imap_module --without-mail_smtp_module -- with-http_stub_status_module --with-http_realip_module Compilar e instalar: 176 $> cd /home/<su_usuario>/mercal/nginx $> ./configure --with-http_gzip_static_module --sbin- path=/usr/local/sbin \ --with-http_ssl_module --without-mail_pop3_module –without- mail_imap_module\ --without-mail_smtp_module --with- http_stub_status_module --with-http_realip_module $> make -j 4 && make install El resultado debe ser similar a: Figura 68 Compilar Nginx Despliegue del Nginx Luego, es necesario descargar el script que permite iniciar, detener, reiniciar y recargar NGINX mediante el comando service, podemos descargarlo utilizand los siguientes comandos: 177 $> wget https://raw.github.com/JasonGiedymin/nginx-init- ubuntu/master/nginx $> sudo mv nginx /etc/init.d/nginx $> sudo chmod +x /etc/init.d/nginx $> sudo chown root:root /etc/init.d/nginx $> update-rc.d nginx defaults Por último inicie el servicio $> service nginx start Verificación del despliegue Para verificar que el NGINX se ejecuta de manera exitosa escriba por consola el siguiente comando: $> service nginx status La salida debe ser similar a esto Figura 69 Status Nginx También puede navegar a través de algún navegador con la dirección ip de su servidor por el puerto 80 y se encontrara la siguiente interfaz: 178 Figura 70 Interfaz Nginx Archivos de configuración utilizados Para configurar el nginx ubíquese a la ruta /usr/local/nginx/sites-available/ y modifique el archivo default upstream lb { # ip_hash; server ip-server1:8243; server ip-server2:8243; server ip-serverN:8243; } server { listen 443; # ssl on; # ssl_certificate /etc/nginx/keys/current/esb/cacert.pem; # ssl_certificate_key /etc/nginx/keys/current/esb/newcakey.pem; # ssl_session_timeout 5m; # ssl_protocols SSLv3 TLSv1; # ssl_ciphers ALL:!ADH:!EXPORT56:RC4+RSA:+HIGH:+MEDIUM:+LOW:+SSLv3: +EXP; # ssl_prefer_server_ciphers on; location / { proxy_pass https://lb; 179 proxy_read_timeout 240s; } } El archivo permite que toda solicitud que se haga utilizando el nombre provisto lb agregado previamente en el archivo /etc/hosts por el puerto seguro https 443, sea redirigida a alguno de los servidores a balancear la carga. Configuración de los aspectos de seguridad Si desea habilitar la seguridad puede descomentar las líneas del archivo que se muestran a continuación: upstream lb { # ip_hash; server ip-server1:8243; server ip-server2:8243; server ip-serverN:8243; } server { listen 443; ssl on; ssl_certificate /usr/local/nginx/keys/current/esb/cacert.pem; ssl_certificate_key /usr/local/nginx/keys/current/esb/newcakey.pem; ssl_session_timeout 5m; ssl_protocols SSLv3 TLSv1; ssl_ciphers ALL:!ADH:!EXPORT56:RC4+RSA:+HIGH:+MEDIUM:+LOW:+SSLv3:+E XP; ssl_prefer_server_ciphers on; location / { proxy_pass https://lb; proxy_read_timeout 240s; 180 } } En ellas debe indicar la ubicación con los archivos con las llaves públicas y privadas. Verificación y ubicación de archivos Log Para verificar el registro de su nginx ubique la ruta /usr/local/nginx/logs Revise los archivos access.log y error.log Comandos utilizados frecuentemente El siguiente comando inicia el NGINX en el sistema. $> service nginx start Para detener el proceso basta con ejecutar el siguiente comando. $> service nginx stop Por último podemos reiniciar el NGINX en caso de existir algún comportamiento no deseado. $> service nginx restart Recomendaciones y mejores prácticas • Utilizar nombres simbólicos en los archivos de configuración y agregar estos a su archivo hosts ubicado en la ruta /etc/ esto evitara volver a modificar los archivos en caso de necesitar cambiar las direcciones de los servidores a los que se le realiza el balanceo de carga. • Antes de comenzar con la instalación ejecute el comando $> netstat -putan 181 • Verifique que el puerto 80 no esté ocupado. Ese puerto es el por defecto del NGINX. • Apache2 utiliza ese puerto por defecto también por lo tanto si este está activo ejecute $> service apache stop • Si su usuario no posee privilegios de administrador, al momento de ejecutar el comando make hágalo como un supe usuario. • Si utiliza la seguridad a través de llaves, guarde esos archivos en una carpeta dentro de la misma ruta /usr/local/nginx. 182 ANEXO 9: Arquitectura Propuesta de Componentes 183 ANEXO 10: Arquitectura Propuesta con Herramientas Seleccionadas 184 ANEXO 11: Indicadores Calculados Ilustración 1 Cheques Emitidos Ilustración 2 Ingresos 185 ANEXO 12: Juicio de Experto Caracas, Enero de 2017 Estimado (a) señor (a): Motiva la presente el solicitar su valiosa colaboración en la revisión del instrumento anexo, el cual tiene como objeto obtener la validación del cuestionario que se aplicará para la fundamentación y desarrollo de la tesis de grado titulada “Implementación de una Arquitectura Analítica BIG DATA basada en herramientas de software libre”. Acudo a usted debido a sus conocimientos y experiencias en la materia, los cuales aportarían una útil y completa información para la culminación exitosa de este trabajo especial de grado. Gracias por su valioso aporte y participación. Atentamente, José Luis Prato Zuñiga. Hever Alfonso Salas Garcia. INSTRUCCIONES A) Lea detenidamente las preguntas antes de responder. B) Este instrumento de validación consta de una primera parte de identificación del experto, seguidamente otra en donde se identifica el título de la investigación, los objetivos, indicadores y alternativas de respuesta del cuestionario objeto de esta validación. Luego se encuentra una sección en la que se pide el juicio de experto con respecto al cuestionario, la cual está formada por siete preguntas, cuyas respuestas son: suficiente, medianamente suficiente e insuficiente, las dos 186 primeras interrogantes, y si o no las restantes, seleccione la opción de su preferencia marcando una equis (x) en el espacio indicado para tal fin. C) Seguido del juicio del experto se solicita una opinión sobre el instrumento diseñado. D) Por último, se pide al experto que analizó el cuestionario una constancia de que realizó dicha tarea. 1. Identificación del Experto: Nombre y Apellido: ______________________________________________ Instituto donde Trabaja: ___________________________________________ Título de Pregrado: ______________________________________________ Título de Postgrado: ____________________________________Institución donde lo obtuvo: ________________________________________________ Año: _________________________________________Trabajos Publicados: _______________________________________________________________ ___________________________________________________________ 2. Título de la Investigación: Implementación de una Arquitectura Analítica BIG DATA basada en herramientas de software libre. 2.1. Objetivos del Estudio: 2.2. Objetivo General: Implementación de una Arquitectura Analítica BIG DATA basada en herramientas de software libre, específicamente para la Misión Mercal C.A. (Mercado de Alimentos).2.3. Objetivos Específicos:  Diseñar la arquitectura Analítica Big Data.  Seleccionar las herramientas asociadas a cada componente de la arquitectura diseñada.  Proponer e implementar la arquitectura con las herramientas seleccionadas. 187 3. Variable que se pretende medir: Desarrollo de una Arquitectura Analítica BIG DATA basada en herramientas de software libre para la Misión Mercal C.A. (Mercado de Alimentos). 3.1. Indicadores: 3.1.1. Seguridad 3.1.2. Capacidad de pruebas 3.1.3. Escalabilidad 3.1.4. Tolerancia a fallos 3.1.5. Análisis de datos 3.1.6. Visualización de datos 4. Alternativas de respuestas: Sí No 5. JUICIOS DEL EXPERTO: 5.1. En líneas generales, considera que los indicadores de la variable están inmersos en su contexto teórico de forma: _____ Suficiente _____ Medianamente suficiente _____ Insuficiente 5.2. Considera que los reactivos del cuestionario miden los indicadores seleccionados para la variable de manera: _____ Suficiente _____ Medianamente Suficiente _____ Insuficiente 5.3. Considera que existe pertinencia entre los objetivos de la investigación. _____ Si _____ No 188 Observaciones: ________________________________________________ _____________________________________________________________ 5.4. Considera que existe pertinencia entre los indicadores y la variable de estudio. _____ Si _____ No Observaciones: ________________________________________________ _____________________________________________________________ 5.5. Considera que existe pertinencia entre los indicadores y los objetivos de la investigación. _____ Si _____ No Observaciones: ________________________________________________ _____________________________________________________________ 5.6. Considera que existe pertinencia entre los indicadores y las dimensiones de la investigación. _____ Si _____ No Observaciones: ________________________________________________ _____________________________________________________________ 5.7. Considera que los reactivos del cuestionario están redactados de manera adecuada. _____ Si _____ No Observaciones: ________________________________________________ _____________________________________________________________ 6. El instrumento diseñado es: 189 _______________________________________________________________ _______________________________________________________________ _______________________________________________________________ _______________________________________________________________ _______________________________________________________________ _______________________________________________________________ _______________________________________________________________ ___________________________________ 7. Constancia de Juicio de experto: Yo, _______________________________, titular de la cédula de identidad No. ____________________ certifico que realicé el juicio del experto al instrumento diseñado por los bachilleres José Luis Prato Zuñiga y Hever Alfonso Salas Garcia en el Trabajo Especial de Grado: “Implementación de una Arquitectura Analítica BIG DATA basada en herramientas de software libre.” 190 Gráficos de los resultados 0 1 2 3 4 5 6 7 8 SUFICIENTE MEDIANAMENTE SUFICIENTE INSUFICIENTE Considera que los indicadores de la variable estan inmersos en su contexto teorico de forma Considera que los indicadores de la variable estan inmersos en su contexto teorico de forma 0 1 2 3 4 5 6 7 8 9 SUFICIENTE MEDIANAMENTE SUFICIENTE INSUFICIENTE Considera que los reactivos del cuestionario miden los indicadores seleccionados para la variable de manera Considera que los reactivos del cuestionario miden los indicadores seleccionados para la variable de manera 191 Considera que existe pertinencia entre los objetivos de la investigación SI NO Considera que existe pertinencia entre los indicadores y la variable de estudio SI NO 192 Considera que existe pertinencia entre los indicadores y los objetivos de la investigación SI NO Considera que existe pertinencia entre los indicadores y las dimensiones de la investigación SI NO 193 Considera que los reactivos del cuestionario están redactados de manera adecuada SI NOUniversidad Central de Venezuela Facultad de Ciencias Escuela de Computación Centro de Investigación en Sistemas de Información Implementación de una Arquitectura Analítica BIG DATA basada en herramientas de software libre Trabajo Especial de Grado presentado ante la Ilustre Universidad Central de Venezuela por los Bachiller José Luis Prato Zuñiga Hever Alfonso Salas Garcia para optar al título de Licenciado en Computación Tutor: Dr. Pedro N. Bonillo R. Septiembre, 2017 i ii AGRADECIMIENTOS A Dios por darme la oportunidad de vivir y permitirme lograr cada una de las metas propuesta con su apoyo y presencia espiritual. A mis familiares y seres queridos, por su existencia, por su constancia, dedicación y sacrificio, por apoyarme en todo momento y por inculcarme que se puede alcanzar cualquier meta si uno se lo propone. A mi tutor, Prof. Dr. Pedro Bonillo, por haberme dado su apoyo en las consultas y presencia en el desarrollo del presente trabajo. A cada uno de los profesores de la Escuela de Computación quienes impartieron y compartieron sus conocimientos y experiencias en cada una de las etapas vividas durante la carrera. A todas aquellas personas no nombradas, pero que de una u otra manera prestaron su excelente colaboración. A todos estos, solo puedo decir, muchas gracias. iii RESUMEN Cualquier acción que realizamos en internet (la compra de un artículo de ropa, consulta de vuelos, navegar por algunas webs, etc.) deja una huella digital muy valiosa. El auge de las nuevas tecnologías ha provocado que generemos grandes cantidades de estos datos. Estos grandes volúmenes de datos son imposibles de analizar o procesar mediante métodos convencionales. Debido a la limitación en el tratamiento de estas cantidades de datos, nace el concepto Big Data. Debido al gran volumen de datos que comienzan a manejar muchas organizaciones en Venezuela, en muchos casos se requieren de soluciones tecnológicas distintas al modelo relacional, el cual no está diseñado para manejar tantos datos de manera óptima; y mucho menos en el procesamiento y cálculo de indicadores que ayuden a la toma de decisiones sobre dicho volumen de datos. Entre estas organizaciones encontramos el Sistema de Mercado de Alimentos Misión Mercal C.A., el cual maneja una cantidad considerable de datos transaccionales diariamente en sus tiendas encargadas de los registros de inventarios, ventas, compras, cuentas, etc, a nivel nacional. Este Trabajo Especial de Grado tiene como objetivo desarrollar e implementar una arquitectura analítica de Big Data para almacenar, procesar, calcular, visualizar y consultar indicadores originados por grandes volúmenes de datos, en el que actualmente las operaciones de consulta sobre el modelo relacional en más de cien millones de registros, no son capaces de ejecutarse en menos de cinco segundos sobre las tablas del sistema. En el presente TEG se hace uso de la metodología Attribute Driven Design para diseñar la arquitectura, luego se implementa cada uno de sus componentes con herramientas de software libre, haciendo fuerte mención en Apache Hadoop como estacionamiento, Apache Hive como Almacén de Datos sobre Hadoop y motor de consultas y HUE (Hadoop User Experience, por sus siglas en inglés) como herramienta de administración y visualización de reportes y datos, además de fuentes de datos diversas. Palabras claves: Mercado de Alimentos, Big Data, Arquitectura Analítica. iv ABSTRACT Any action we take on the internet (buying an article of clothing, browsing flights, browsing some websites, etc.) leaves a very valuable fingerprint. The rise of new technologies has caused us to generate large amounts of this data. These large volumes of information are impossible to analyze or process using conventional methods. Due to the limitation in the treatment of these amounts of data, the Big Data concept is born. Using Big Data techniques allows you to analyze tastes to offer personalized product offerings, analyze crime trends to know in advance the likelihood of a crime occurring, analyze the success or failure of political or commercial campaigns, etc. Due to the large volume of data that many organizations in Venezuela are beginning to handle, different technological solutions are required to the relational model, which is not designed to handle so much data in an optimal way; Much less in the processing and calculation of indicators that help the decision making on this volume of data. Among these organizations, we find the Misión Mercal (Food Market System), which handles a considerable amount of transactional data daily in its stores in charge of records of inventories, sales, purchases, accounts, etc., at the national level. Aims this work is development and implementation an Analytical Architecture of Big Data. In this work, Attribute Driven Design methodology is used to design the architecture, then it was implemented each of its components with free software tools, making strong mention in Apache Hadoop as stage, Apache Hive as Data Warehouse on Hadoop and query engine, Hadoop User Experience (HUE) as a tool for managing and viewing reports and data. Keywords: Food Market, Big Data, analytics architecture. v INDICE AGRADECIMIENTOS .................................................................................................................... ii RESUMEN ..................................................................................................................................... iii ABSTRACT ................................................................................................................................... iv INDICE .......................................................................................................................................... v INDICE DE TABLAS .................................................................................................................... vii INDICE DE FIGURAS .................................................................................................................. vii INTRODUCCIÓN........................................................................................................................... 1 Capítulo 1: Identificación de la Empresa ....................................................................................... 4 1.1 Reseña Histórica ................................................................................................................. 4 1.2 Misión .................................................................................................................................. 6 1.3 Visión .................................................................................................................................. 6 1.4 Objetivos de la empresa ..................................................................................................... 7 1.5 Organigrama ....................................................................................................................... 7 Capítulo 2: Problema de Investigación ......................................................................................... 8 2.1 Contexto .............................................................................................................................. 8 2.2 Planteamiento del Problema ............................................................................................... 9 2.4 Objetivos ........................................................................................................................... 10 2.4.1 Objetivo General ........................................................................................................ 10 2.4.2 Objetivos Específicos ................................................................................................ 10 2.5 Justificación ....................................................................................................................... 11 2.6 Alcance ............................................................................................................................. 12 2.7 Limitaciones ...................................................................................................................... 12 Capítulo 3: Marco Conceptual ..................................................................................................... 13 3.0 Misión Mercal (Mercado de Alimentos) ..................................................................... 13 3.1 Software Libre ............................................................................................................ 14 3.2 Inteligencia de Negocios (BI) ..................................................................................... 21 3.2.1 Definición ................................................................................................................... 21 3.2.2 Historia ....................................................................................................................... 22 3.2.3 Características ........................................................................................................... 23 3.2.4 Niveles de Realización de BI ..................................................................................... 23 3.2.5 Inteligencia de Empresas ........................................................................................... 24 3.2.6 Inteligencia de Mercados internacionales. ................................................................. 24 3.2.7 Datawarehouse & Business Intelligence ................................................................... 25 3.2.8 La Metodologia de Kimball ......................................................................................... 26 vi 3.3 Big Data ............................................................................................................................ 30 3.3.1 Introducción ............................................................................................................... 30 3.3.2 Historia de Big Data ................................................................................................... 33 3.3.2.1 Big Data Analítica ................................................................................................... 34 3.3.3 Tecnologías involucradas .......................................................................................... 37 3.4 La Inteligencia de Negocio y su evolución con el tiempo a Big Data Analítica ..................... 63 3.5 Ciencia de Datos ............................................................................................................... 67 3.6 Arquitectura de Software .................................................................................................. 69 3.7 Análisis de arquitecturas Big Data .................................................................................... 76 3.8 Diseño de una Arquitectura de Software .......................................................................... 80 Capítulo 4: Marco Metodológico .................................................................................................. 87 4.1 Bases metodológicas de la investigación ......................................................................... 87 4.1.1 Tipo de investigación ................................................................................................. 87 4.1.2 Población y Muestra .................................................................................................. 89 4.1.3 Técnicas e Instrumentos de Recolección de Datos .................................................. 89 4.2 Metodología de Desarrollo ................................................................................................ 90 Capítulo 5: Marco Aplicativo ........................................................................................................ 91 5.1 Entrada del ADD ............................................................................................................... 91 5.1.1 Requerimientos funcionales ...................................................................................... 91 5.1.2 Restricciones de diseño ............................................................................................ 92 5.1.3 Requerimientos de calidad ........................................................................................ 92 5.2 Primera iteración de la metodología ADD ........................................................................ 97 5.2.1 Paso 1: Confirmar que haya suficiente información de los requerimientos .............. 97 5.2.2 Paso 2: Escoger un elemento del sistema a descomponer ...................................... 97 5.2.3 Paso 3: Identificar los drivers de la arquitectura ....................................................... 97 5.2.4 Paso 4: Escoger un patrón que satisfaga los drivers de la arquitectura ................... 98 5.2.5 Paso 5: Instanciar los elementos de la arquitectura y asignar responsabilidades. . 107 5.2.6 Paso 6: Definir las interfaces de los elementos instanciados. ................................ 109 5.2.7 Paso 7: Verificar y refinar los requerimientos para hacer restricciones en los elementos instanciados ...................................................................................................................... 115 5.3 Implementación de la Arquitectura ................................................................................. 124 Conclusiones y Recomendaciones ........................................................................................... 128 Bibliografía ................................................................................................................................. 131 ANEXO 1: Pentaho vs Palo ....................................................................................................... 134 ANEXO 2: Hadoop vs Otros ...................................................................................................... 135 ANEXO 3: Hive vs Pig ............................................................................................................... 136 ANEXO 4: Solr vs Elasticsearch ............................................................................................... 137 ANEXO 5: HUE vs Banano ....................................................................................................... 138 ANEXO 6: Diseño de los mecanismos ETL .............................................................................. 139 vii ANEXO 7: Casos de uso Nivel 1 ............................................................................................... 153 ANEXO 8: Instalación y configuración de Herramientas .......................................................... 157 ANEXO 9: Arquitectura Propuesta de Componentes ............................................................... 182 ANEXO 10: Arquitectura Propuesta con Herramientas Seleccionadas .................................... 183 ANEXO 11: Indicadores Calculados ......................................................................................... 184 ANEXO 12: Juicio de Experto ................................................................................................... 185 INDICE DE TABLAS TABLA 1 ALTERNATIVAS EN TEOREMA DE CAP ................................................................... 56 TABLA 2 ACID VS BASE (BREWER, 2000) ............................................................................... 58 TABLA 3 SEGURIDAD ................................................................................................................ 92 TABLA 4 ESCALABILIDAD ......................................................................................................... 93 TABLA 5 CAPACIDAD DE PRUEBAS ........................................................................................ 94 TABLA 6 TOLERANCIA A FALLOS ............................................................................................ 95 TABLA 7 VISUALIZACIÓN DE DATOS ...................................................................................... 96 TABLA 8 ANÁLISIS DE DATOS ................................................................................................. 96 TABLA 9 DRIVERS DE LA ARQUITECTURA ............................................................................ 97 TABLA 10 PATRONES VS DRIVERS ...................................................................................... 101 TABLA 11 ACTORES - CASOS DE USO NIVEL 0 .................................................................. 121 TABLA 12 FUNCIONALIDADES CASOS DE USO NIVEL 0 .................................................... 122 TABLA 13 FUNCIONALIDADES CASOS DE USO NIVEL 1 .................................................... 153 INDICE DE FIGURAS FIGURA 1 ORGANIGRAMA MERCAL ......................................................................................... 7 FIGURA 2 TAREAS DE LA METODOLOGÍA (CICLO DE VIDA) ............................................... 27 FIGURA 3 BIG DATA EN LAS EMPRESAS FUENTE: HERRAMIENTAS PARA BIG DATA: ENTONO HADOOP (SANCHEZ,2014) .............................................................................. 31 FIGURA 4 TIPOS DE DATOS EN BIG DATA ............................................................................. 32 FIGURA 5 PROCESAMIENTO PARALELO EN MAPREDUCE ................................................. 40 FIGURA 6 ARQUITECTURA BÁSICA DE HADOOP ................................................................. 43 FIGURA 7 CONTEO DE PALABRAS CON MAPREDUCE ........................................................ 44 file:///C:/Users/phd2014/Desktop/Tesis/Tesis_Jose_Prato_Hever_Salas.docx%23_Toc484008555 viii FIGURA 8 HADOOP 1.0 A HADOOP 2.0 ................................................................................... 46 FIGURA 9 NUEVAS APLICACIONES YARN ............................................................................. 46 FIGURA 10 ARQUITECTURA HADOOP 2.0 YARN................................................................... 48 FIGURA 11 ECOSISTEMA HADOOP......................................................................................... 50 FIGURA 12 INDEXACIÓN CON LUCENE .................................................................................. 60 FIGURA 13 ARQUITECTURA SOLR ......................................................................................... 62 FIGURA 14 DIAGRAMA DE VENN CIENCIA DE DATOS ......................................................... 68 FIGURA 15 ESTRUCTURAS DE SOFTWARE .......................................................................... 72 FIGURA 16 ARQUITECTURA BIG DATA IBM ........................................................................... 77 FIGURA 17 ARQUITECTURA BIG DATA CLOUDERA ............................................................. 78 FIGURA 18 ARQUITECTURA BIG DATA HORTONWORKS .................................................... 79 FIGURA 19 ARQUITECTURA BIG DATA DATASTAX-HORTONWORKS ................................ 80 FIGURA 20 CICLO DE VIDA DE ENTREGA EVOLUTIVA ........................................................ 81 FIGURA 21 DIAGRAMA DE COMPONENTES ........................................................................ 119 FIGURA 22 CASO DE USO NIVEL 0; FUENTE: ELABORACIÓN PROPIA ............................ 120 FIGURA 23 INSTALACIÓN JAVA ............................................................................................. 125 FIGURA 24 CLUSTER ESTACIONAMIENTO HDFS 3 NODOS .............................................. 125 FIGURA 25 PROCESOS HADOOP NODO MAESTRO ........................................................... 126 FIGURA 26 EVIDENCIA EJECUCIÓN HIVE ............................................................................ 126 FIGURA 27 EJECUCIÓN SOLR ............................................................................................... 127 FIGURA 28 DISEÑO ETL MISIÓN MERCAL ........................................................................... 139 FIGURA 29 JOB CARGA INICIAL ............................................................................................ 140 FIGURA 30 TRANSF_PDI_HADOOP ....................................................................................... 141 FIGURA 31 PASO DE CONEXIÓN BASE DE DATOS ORACLE ............................................ 142 FIGURA 32 PASO DE CONSULTA SQL SOBRE ORACLE .................................................... 142 FIGURA 33 CALCULATOR ....................................................................................................... 143 FIGURA 34 HADOOP FILE OUTPUT ....................................................................................... 143 FIGURA 35 CAMPOS A CARGAR HDFS ................................................................................ 144 FIGURA 36 LOAD_HIVE ........................................................................................................... 144 FIGURA 37 CONEXIÓN A HIVE ............................................................................................... 145 FIGURA 38 CONSULTA HQL ................................................................................................... 146 FIGURA 39 CARGA DE LOS DATOS EN ESTRUCTURA HIVE ............................................. 146 FIGURA 40 LOAD_HIVE_MAP ................................................................................................. 147 FIGURA 41 CONEXIÓN A HIVE ............................................................................................... 148 FIGURA 42 CREACIÓN DE CUBO HIVE ................................................................................. 148 FIGURA 43 HIVETOSOLR ........................................................................................................ 149 ix FIGURA 44 CONEXIÓN A HIVE ............................................................................................... 150 FIGURA 45 CONSULTA HQL Y CONVERSIÓN A JSON ........................................................ 151 FIGURA 46 SELECCION DE VALORES .................................................................................. 151 FIGURA 47 CAMPOS CONEXIÓN SOLR ................................................................................ 152 FIGURA 48 REST CLIENT ....................................................................................................... 152 FIGURA 49 CASOS DE USO NIVEL 1 ..................................................................................... 153 FIGURA 50 EVIDENCIA DE LA INSTALACIÓN DE JAVA EN AMBIENTE DE DESARROLLO .......................................................................................................................................... 157 FIGURA 51 EVIDENCIA DE SALIDA DEL COMANDO. ENUMERA NAMENODE Y SECONDARYNAMENODE .............................................................................................. 164 FIGURA 52 EVIDENCIA DE SALIDA DEL COMANDO. ENUMERA NAMENODE Y SECONDARYNAMENODE .............................................................................................. 164 FIGURA 53 EVIDENCIA EJECUCIÓN HIVE ............................................................................ 166 FIGURA 54 EVIDENCIA EJECUCIÓN HIVE ............................................................................ 166 FIGURA 55 DESCARGA MAVEN TARBALL ............................................................................ 168 FIGURA 56 DESCOMPRIMIR MAVEN .................................................................................... 168 FIGURA 57 VERIFICANDO VERSIÓN JAVA ........................................................................... 169 FIGURA 58 VERIFICAR INSTALACIÓN MAVEN ..................................................................... 169 FIGURA 59 DESCARGANDO HUE .......................................................................................... 170 FIGURA 60 CONSTRUYENDO HUE........................................................................................ 170 FIGURA 61 CONSTRUYENDO HUE........................................................................................ 171 FIGURA 62 FINALIZANDO INSTALACIÓN HUE ..................................................................... 171 FIGURA 63 INICIANDO HUE .................................................................................................... 171 FIGURA 64 INTERFAZ WEB HUE ........................................................................................... 172 FIGURA 65 ENLAZANDO HUE - HADOOP ............................................................................. 173 FIGURA 66 ENLAZANDO HUE – HADOOP 2 ......................................................................... 174 FIGURA 67 GESTIÓN HDFS CON HUE .................................................................................. 174 FIGURA 68 COMPILAR NGINX ................................................................................................ 176 FIGURA 69 STATUS NGINX .................................................................................................... 177 FIGURA 70 INTERFAZ NGINX ................................................................................................. 178 1 INTRODUCCIÓN El primer cuestionamiento que posiblemente llegue a su mente en este momento es ¿Qué es Big Data y porqué se ha vuelto tan importante? pues bien, en términos generales podríamos referirnos a este concepto, como a la tendencia en el avance de la tecnología que ha abierto las puertas hacia un nuevo enfoque de entendimiento y toma de decisiones, la cual es utilizada para describir enormes cantidades de datos (estructurados, no estructurados y semi estructurados) que tomaría demasiado tiempo y sería muy costoso cargarlos a un base de datos relacional para su análisis. De tal manera que, el concepto de Big Data aplica para toda aquella información que no puede ser procesada o analizada utilizando procesos o herramientas tradicionales. Sin embargo, Big Data no se refiere a alguna cantidad en específico, ya que es usualmente utilizado cuando se habla en términos de petabytes y exabytes de datos. Entonces ¿Cuánto es demasiada información de manera que sea elegible para ser procesada y analizada utilizando Big Data? Analicemos primeramente en términos de bytes: Gigabyte = 109 = 1,000,000,000 Terabyte = 1012 = 1,000,000,000,000 Petabyte = 1015 = 1,000,000,000,000,000 Exabyte = 1018 = 1,000,000,000,000,000,000 Además del gran volumen de información, esta existe en una gran variedad de datos que pueden ser representados de diversas maneras en todo el mundo, por ejemplo de dispositivos móviles, audio, video, sistemas GPS, incontables sensores digitales en equipos industriales, automóviles, medidores eléctricos, veletas, anemómetros, etc., los cuales pueden medir y comunicar el posicionamiento, movimiento, vibración, temperatura, humedad y hasta los cambios químicos que sufre el aire, de tal forma que las aplicaciones que analizan estos datos requieren que la velocidad de respuesta sea lo demasiado rápida para lograr obtener la información correcta en el momento preciso. Estas son las características principales de una oportunidad para Big Data. Es importante entender que las bases de datos convencionales son una parte importante y relevante para una solución analítica. De hecho, se vuelve mucho 2 más vital cuando se usa en conjunto con la plataforma de Big Data. Pensemos en nuestras manos izquierda y derecha, cada una ofrece fortalezas individuales para cada tarea en específico. Por ejemplo, un beisbolista sabe que una de sus manos es mejor para lanzar la pelota y la otra para atraparla; puede ser que cada mano intente hacer la actividad de la otra, mas sin embargo, el resultado no será el más óptimo. El problema surge en situaciones en las que se manejan grandes volúmenes de datos. El modelo relacional propuesto por Edgar Frank Codd es difícil de escalar, por lo que no es aplicable en este tipo de situaciones. La finalidad de este Trabajo Especial de Grado es desarrollar una arquitectura analítica para el manejo de grandes volúmenes de datos y cálculo de indicadores, basada en componentes de software libre. Para cumplir con el objetivo del proyecto, se procedió a analizar cada una de las herramientas utilizadas en una arquitectura Big Data mediante el uso de patrones de diseño Big Data y se encontraron que aquellas que siendo de Software Libre lograron cumplir las expectativas y los objetivos de la organización, utilizando como alcance la base de datos relacional desarrollada bajo Oracle en los Sistemas Logístico, Siga y SAFS, además de archivos xls externos de la Misión Mercal de la República Bolivariana de Venezuela. La estructura del presente TEG se muestra a continuación: Capítulo 1: Identificación y descripción de la organización caso de estudio de este TEG. Destacando la reseña histórica además de la misión, visión y objetivos de la empresa. Capítulo 2: Presentación del problema de investigación, manifestaciones y evidencias necesarias para establecer el objetivo general, los objetivos específicos, la justificación, el alcance y las limitaciones. 3 Capítulo 3: Conceptualización de las bases teóricas que soportan la investigación. Descripción de las tecnologías involucradas en Big Data, breve historia, definición de arquitectura de Software, Software Libre, entre otras, además del diseño de una arquitectura de software utilizando alguna metodología. Capítulo 4: Presentación del marco metodológico, planteamiento de la Metodología Atribute-Driven Design, tipo de investigación, la población y muestra, y de las técnicas e instrumentos de recolección de datos. Capítulo 5: Correspondiente al marco aplicativo, presentando los resultados obtenidos de aplicar la metodología seleccionada, mediante la cual se desarrolló y probó la arquitectura. Por último, las conclusiones y recomendaciones, seguido por la Bibliografía y Anexos. 4 Capítulo 1: Identificación de la Empresa En este capítulo se describirá la empresa Mercado de Alimentos (Misión Mercal C.A.), ya que es el caso de estudio en este trabajo. 1.1 Reseña Histórica La Misión MERCAL tiene su génesis en acontecimientos ocurridos entre diciembre del año 2002 y enero de 2003. En esos meses nuestro país vivió las horas más angustiosas a consecuencia del nefasto sabotaje cometido contra el pueblo venezolano: la paralización de nuestra principal industria, PDVSA. Esa paralización generó un grave problema con la alimentación del pueblo, las principales industrias de producción y comercialización de alimentos se sumaron al vil sabotaje y por esta razón el Estado asume la responsabilidad de garantizar la seguridad alimentaria con la creación del Plan Especial de Seguridad Alimentaria (PESA), donde se conjugó el esfuerzo de empresas como CASA y PROAL, ambas, apoyadas en el hombro inquebrantable de nuestra gloriosa Fuerza Armada Nacional. Aquel esfuerzo mancomunado pronto se vio organizado con la iniciativa del Comandante Hugo Chávez al proponer la creación de un sistema logístico, basado en la planificación de jornadas de ventas de alimentos realizadas al aire libre en las comunidades más desasistidas, a objeto de ofrecer alimentos bajo un esquema de precios accesibles; de este modo, se prevenía cualquier otro intento de vulnerar el derecho de los venezolanos y venezolanas de alimentarse y es por ello que nace Mercados de Alimentos, CA. (MERCAL). El 22 de abril de 2003, MERCAL inicia sus actividades con la inauguración de un Mercal Tipo I realizada en el Sector Ruiz Pineda de la Parroquia Caricuao. Este fue el primer establecimiento en aperturarse y fue comandado por el ciudadano Presidente de la República Bolivariana de Venezuela, Hugo Chávez Frías, quien con orgullo y su acostumbrado furor indicó que se daba inicio a la primera etapa de MERCAL. “Triunfar, triunfar y triunfar, ese es el destino de 5 nuestro pueblo” así lo hizo saber el Comandante Presidente, dando paso a MERCAL que se constituyó en uno de los programas sociales que impulsó el Gobierno Bolivariano para garantizar la cesta alimentaría a los más desposeídos. En sus inicios MERCAL beneficiaba a 55.632 personas y contaba con cinco (5) establecimientos: tres (3) Mercales Tipo I y dos (2) Centros de Acopio; sin embargo, la revolución activó sus mecanismos para la ampliación de los puntos de venta y al cierre del 2003, la red contaba con 1.625 establecimientos, pero la tarea no concluyó allí, se han realizado importantes esfuerzos para que el pueblo pueda acceder a los alimentos y es por ello que al cierre del 2009, MERCAL ha experimentado un gigantesco incremento de beneficiarios elevando la cifra a más de 10 millones de personas. En el 2010, cuenta con más de dieciséis mil ochocientos puntos de venta distribuidos en: 210 Mercales Tipo I, 991 Mercales Tipo II, 36 Supermercales de víveres, 114 centros de acopio, 4 centros frigoríficos, 3 Supermercales de hortalizas, frutas y verduras, 346 Mercalitos móviles, 1.695 Mercalitos comunales y 13.417 Mercalitos. A través del tiempo se ha logrado una conexión entre el pueblo y el Estado, pues esta Misión trabaja de la mano de los Comités de Alimentación de los Consejos Comunales (anteriormente gabinetes de alimentación) para fortalecer la Soberanía y Seguridad Alimentaria de todo el país, siempre basados en la premisa de que un pueblo libre y organizado, debe velar junto al Estado para que la población disfrute, goce, y ejerza su derecho a recibir una alimentación sana, de calidad y a precios justos. En el transcurrir de los años, MERCAL ha reacondicionado sus infraestructuras para elevar la cantidad de toneladas de alimentos según las necesidades del pueblo. También ha celebrado convenios estratégicos con productores locales para arrimar sus productos hasta los puntos de venta más convenientes para los habitantes de las zonas beneficiadas, todo esto, ha incidido significativamente en la creación de empleos directos e indirectos, lo cual se traduce en una mejora del sector productivo regional y en consecuencia, la reactivación de la economía nacional. 6 La implementación de las jornadas de alimentación tales como operativas, mercados a cielo abierto, hallacazos socialistas, operativos especiales de azúcar, entre otros, han logrado canalizar y atender la necesidad de los venezolanos y venezolanas que durante muchos años se vieron olvidados por los gobiernos capitalistas. Otra idea que tiene raíces puramente socialistas, es la creación de la fórmula llamada Mercalitos Comunales cuya estructura busca fortalecer la organización de las comunidades para garantizar la transferencia de poder al pueblo. Hasta el 2010 MERCAL, ha expendido más de 8.4 millones de toneladas y ha garantizado el acceso a una cesta alimentaría balanceada para los sectores de menores recursos, evolucionando con numerosos beneficiarios desde la creación de esta misión, garantizando no sólo el soberano derecho a la alimentación al pueblo venezolano, sino que también se ha transformado en una importante empresa donde laboran de forma directa más de 8 mil trabajadores y de forma indirecta más de 40 mil personas a nivel nacional. Todo esto ha sido el resultado de un exhaustivo trabajo cargado de compromiso y lealtad por parte de su talento humano. (MERCAL, MERCAL, 2017). 1.2 Misión Consolidar en toda la geografía nacional la distribución planificada de alimentos en las zonas de pobreza extrema, bajo los preceptos que impulsa la Revolución y el Gobierno Nacional para erradicar el hambre en nuestro país. 1.3 Visión En Mercal garantizamos la distribución planificada de alimentos en todas las parroquias en las que hacen vida familias en estado de vulnerabilidad, esto como parte del Plan de Ofensiva y Erradicación de la Pobreza Extrema que lleva a cabo el Sistema Nacional de Misiones y Grandes Misiones Socialistas “Hugo Chávez”. 7 1.4 Objetivos de la empresa El objetivo fundamental de la Misión Mercal es contribuir en forma sustancial a mejorar la situación nutricional, la salud y calidad de vida de la población venezolana de manera permanente y sustentable. 1.5 Organigrama La Figura 1 muestra el organigrama de la empresa MERCAL, en ella se destaca La Oficina de Tecnología de la Información, ya que representa el área en la cual se desarrolló este Trabajo Especial de Grado. Figura 1 Organigrama MERCAL Fuente: (MERCAL, Organigrama, 2017) 8 Capítulo 2: Problema de Investigación A continuación, se describe la situación de estudio, ubicándola en el contexto para entender su origen y de esta manera proceder a plantear, justificar y limitar el problema; también se define los objetivos del Trabajo Especial de Grado (TEG). 2.1 Contexto En el mundo, la generación de datos ha incrementado exponencialmente en los últimos años con el auge del internet y la globalización. Estos datos provienen de diferentes fuentes y en diferentes formatos, y pueden estar estructurados o no estructurados, lo cual implica un reto en cuanto a su almacenamiento y procesamiento. El presente trabajo está enfocado en implementar una arquitectura Analítica Big Data para el cálculo de indicadores que ayuden a la toma de decisiones por lo cual se explicará el contexto de la institución Mercado de Alimentos Misión Mercal C.A qué hará uso de la misma. La Misión Mercal C.A. maneja una cantidad de registros numerosa en los diversos orígenes de datos, datos que son actualmente almacenados en diversos formatos y en diversos sistemas, archivos Excel o en sus Sistemas de Bases de Datos Relacional SAF (Sistema Automatizado de Facturación), LOGISTICO y SIGA (Sistema de Inventario y Gestión Administrativa). Dicha organización es la encargada de gestionar el almacenamiento y procesamiento del conjunto de transacciones asociado a las operaciones de inventario, ventas, proveedores y clientes, de cada uno de las tiendas denominadas Centros de Acopio, Mercales, Megamercales, entre otros, y de los cuales se hace de mayor relevancia el cálculo de indicadores de gestión para la toma de decisiones en la empresa. 9 2.2 Planteamiento del Problema La Misión Mercal C.A. maneja una gran cantidad de registros, proveniente en distintos formatos. El 100% de sus datos proviene de transacciones, las cuales se almacenan dependiendo del tipo de transacción en uno de sus diferentes Sistemas de Bases de Datos SAF, LOGISTICO y SIGA, o en su defecto en archivos externos. Los servidores destinados a dichas operaciones no habitan en un ambiente centralizado, además, están alcanzando el máximo de su capacidad, por otra parte las consultas que se llevan a cabo en cada uno de estos sistemas varían en un tiempo considerable de respuesta, llegando a exceder hasta los 10 minutos dependiendo de la complejidad de lo que se desee calcular, lo cual indica que el modelo relacional operativo no se encuentra en la capacidad de soportar por si solo los requerimientos del negocio. Además, es necesario destacar que no disponen de un clúster centralizado actualizado, por lo que la actualización de sus datos se realiza con frecuencia variable y no necesariamente en todas las tiendas. Cabe destacar que los contratos de licenciamiento de Oracle de sus diversos Sistemas de Bases de Datos exceden en inversiones monetarias costosas difíciles de costear. El punto fuerte del problema se acrecienta debido a que los Sistemas Actuales no permiten obtener la información de manera oportuna y necesaria para administrar de forma centralizada los datos y menos aún en el cálculo de las métricas de relevancia para el soporte a la toma de decisiones de la empresa, esto se traduce en pérdidas de dinero y de credibilidad en la institución. Pues así, se plantean las siguientes interrogantes:  ¿Existen arquitecturas alternativas, que permitirían manejar adecuadamente la cantidad de registros que actualmente almacena la Misión Mercal C.A. acorde a sus necesidades de negocio?  ¿Cuáles son estas arquitecturas alternativas?  ¿Cuáles son los componentes de estas arquitecturas alternativas? 10  ¿Cuáles de estos componentes pueden estar en Software Libre?  ¿Cuáles de estos componentes pueden incluirse en la propuesta solución de una nueva arquitectura Analítica Big Data?  ¿Existe alguna empresa en Venezuela que apoye en el mantenimiento, crecimiento, evolución y cambios de la arquitectura solución a fin de garantizar la continuidad? 2.4 Objetivos Se plantean a continuación, el objetivo general y los objetivos específicos para respaldar y solventar la situación planteada: 2.4.1 Objetivo General El objetivo general del presente trabajo es la Implementación de una Arquitectura Analítica BIG DATA basada en herramientas de software libre, específicamente para la Misión Mercal C.A. (Mercado de Alimentos). 2.4.2 Objetivos Específicos Los objetivos específicos del presente trabajo son listados a continuación:  Estudiar las arquitecturas Analítica Big Data disponibles en el mercado  Proponer y diseñar la arquitectura Analítica Big Data.  Seleccionar las herramientas asociadas a cada componente de la arquitectura diseñada.  Implementar la arquitectura con las herramientas seleccionadas. 11 2.5 Justificación Se listan a continuación los hechos que satisfacen el desarrollo del TEG:  Los servidores de la Misión Mercal C.A. alcanzaron su capacidad máxima de almacenamiento y procesamiento acorde a sus requerimientos de negocio por lo que surge la necesidad de comprar más servidores con mayor capacidad o implementar una nueva arquitectura de almacenamiento de la información que se adapte a las necesidades de la empresa.  La Misión Mercal C.A. no dispone de un Sistema centralizada que permita hacer el cálculo de indicadores que garanticen la generación oportuna de reportes asociados a las medidas de interés para la organización.  La licencia de Oracle que utiliza la Misión Mercal C.A alcanza grandes inversiones monetarias, por lo que se necesitó pensar en una solución de Software Libre, además de que el presupuesto aportado a la misma no abarca el pago a empresas que cobren en dólares.  Las consultas sobre las métricas a calcular realizadas en sus diversos sistemas de bases de datos consumen bastante tiempo en responder, además, de presentar una gran complejidad y en la mayoría de los casos imposibilidad de cálculo debido a la no interoperabilidad.  Al disponer de tantos registros en sus bases de datos el sistema relacional comienza a tener bajas de rendimiento, como el caso de las consultas. Esto debido a que los sistemas de bases de datos relacionales no fueron diseñados para almacenar y procesar enormes cantidades de registros y el uso de joins consume muchos recursos del computador.  La depuración y limpieza de los datos ayuda a disminuir los tiempos de lectura y escritura, por lo que una solución utilizando un Sistema de Archivos Distribuido junto con un paradigma Map Reduce es lo más indicado en este caso.  El desarrollo de una arquitectura Analítica de Big Data en la Misión Mercal C.A. implica una solución innovadora a nivel nacional. 12 2.6 Alcance Se planteó fijar el alcance en el desarrollo de la nueva arquitectura, incluyendo las pruebas de carga y volumen, pero sin las pruebas de estrés. El desarrollo e implementación de la arquitectura se realizó a partir de los datos almacenados en los diferentes Sistemas LOGISTICO, SAF y SIGA de Oracle actualmente usados por la organización, que abarca alrededor de unas 300 tiendas a nivel nacional encargadas de los registros transaccionales. La propuesta de la arquitectura se desarrolló en la sede de Altamira de la Misión Mercal C.A., Municipio Chacao en Caracas, Venezuela. 2.7 Limitaciones Listadas a continuación:  La poca información disponible para el diseño e implementación de una arquitectura Analítica Big Data.  La mayor parte de la información disponible se encuentra en el idioma inglés y el investigador tiene como lengua origen el castellano.  La complejidad para el entendimiento de los esquemas de datos debido a que para cada sistema es distinto. Adicionalmente la conexión remota a las oficinas de Mercal se realiza por un enlace intermitente y de muy baja velocidad.  Curva de aprendizaje inclinada en el entendimiento del funcionamiento de los componentes de la arquitectura Analítica Big Data solución. 13 Capítulo 3: Marco Conceptual Se presentan los antecedentes y las bases teóricas soportes de la investigación. 3.0 Misión Mercal (Mercado de Alimentos) La Real Academia Española define mercado como un conjunto de operaciones comerciales que afectan a un determinado sector de bienes. (Española, 2016) Mercado, en economía, es un conjunto de transacciones de procesos o intercambio de bienes o servicios entre individuos. El mercado no hace referencia directa al lucro o a las empresas, sino simplemente al acuerdo mutuo en el marco de las transacciones. Estas pueden tener como partícipes a individuos, empresas, cooperativas, ONG, entre otros. (Wikipedia, Wikipedia, 2016) El mercado también es el ambiente social (o virtual) que propicia las condiciones para el intercambio. En otras palabras, debe interpretarse como la institución u organización social a través de la cual los ofertantes (productores, vendedores) y demandantes (consumidores o compradores) de un determinado tipo de bien o de servicio, entran en estrecha relación comercial a fin de realizar abundantes transacciones comerciales. Una definición de mercado según la mercadotecnia: Organizaciones o individuos con necesidades o deseos que tienen capacidad y que tienen la voluntad para comprar bienes y servicios para satisfacer sus necesidades. La Misión Mercal C.A. (Mercado de Alimentos) es uno de los programas sociales incentivados por el gobierno venezolano. Creada oficialmente el 24 de abril de 2003, la Misión Mercal está destinada al sector alimentario, dependiente del Ministerio de la Alimentación. 14 El programa consiste en construir y dotar almacenes y supermercados con alimentos y otros productos de primera necesidad a bajos precios para que sean accesibles a la población más necesitada. Los alimentos están subvencionados y llegan a los estantes sin intermediarios, de manera que los precios ofrecidos suelen tener un descuento de entre el 30 y el 45 por ciento de los observados en las otras cadenas de distribución. El programa consiste en construir establecimientos de ventas, dotarlos y facilitar la distribución. Mercal se ha ampliado con los llamados "Mercalitos" (establecimientos de tamaños más reducidos, llamados coloquialmente en el país, Bodegas) que se encuentran en sectores más inaccesibles, camiones con víveres que venden directamente en la calle. También existe "Megamercal", que es un mercado improvisado en la vía pública, de enormes dimensiones en determinado día, ampliándose el número de alimentos y productos a la venta, en donde se ofrece simultáneamente otros servicios sociales tales como cedulación, odontología y revisión de la visión (óptica). 3.1 Software Libre Software libre es el software que respeta la libertad de los usuarios y la comunidad. A grandes rasgos, significa que los usuarios tienen la libertad de ejecutar, copiar, distribuir, estudiar, modificar y mejorar el software. Es decir, el software libre es una cuestión de libertad, no de precio. Para entender el concepto, piense en libre como en libre expresión. (gnu.org-2017) Promovemos estas libertades porque todos merecen tenerlas. Con estas libertades, los usuarios (tanto individualmente como en forma colectiva) controlan el programa y lo que este hace. Cuando los usuarios no controlan el programa, decimos que dicho programa no es libre, o que es «privativo». Un programa que no es libre controla a los usuarios, y el programador controla el programa, con lo cual el programa resulta ser un instrumento de poder injusto. 15 3.1.1 Las cuatro libertades esenciales Un programa es software libre si los usuarios tienen las cuatro libertades esenciales:  La libertad de ejecutar el programa como se desea, con cualquier propósito (libertad 0).  La libertad de estudiar cómo funciona el programa, y cambiarlo para que haga lo que usted quiera (libertad 1). El acceso al código fuente es una condición necesaria para ello.  La libertad de redistribuir copias para ayudar a su prójimo (libertad 2).  La libertad de distribuir copias de sus versiones modificadas a terceros (libertad 3). Esto le permite ofrecer a toda la comunidad la oportunidad de beneficiarse de las modificaciones. El acceso al código fuente es una condición necesaria para ello. Un programa es software libre si otorga a los usuarios todas estas libertades de manera adecuada. De lo contrario no es libre. Existen diversos esquemas de distribución que no son libres, y si bien podemos distinguirlos en base a cuánto les falta para llegar a ser libres, nosotros los consideramos contrarios a la ética a todos por igual. En cualquier circunstancia, estas libertades deben aplicarse a todo código que pensemos utilizar hacer que otros utilicen. Tomemos por ejemplo un programa A que automáticamente ejecuta un programa B para que realice alguna tarea. Si se tiene la intención de distribuir A tal cual, esto implica que los usuarios necesitarán B, de modo que es necesario considerar si tanto A como B son libres. No obstante, si se piensa modificar A para que no haga uso de B, solo A debe ser libre; B no es relevante en este caso. Software libre no significa que no es comercial. Un programa libre debe estar disponible para el uso comercial, la programación comercial y la distribución comercial. La programación comercial de software libre ya no es inusual; el software libre comercial es muy importante. Puede haber pagado dinero para 16 obtener copias de software libre, o puede haber obtenido copias sin costo. Pero sin tener en cuenta cómo obtuvo sus copias, siempre tiene la libertad de copiar y modificar el software, incluso de vender copias. 3.1.1.1 La libertad de ejecutar el programa como se desee La libertad de ejecutar el programa significa que cualquier tipo de persona u organización es libre de usarlo en cualquier tipo de sistema de computación, para cualquier tipo de trabajo y finalidad, sin que exista obligación alguna de comunicarlo al programador ni a ninguna otra entidad específica. En esta libertad, lo que importa es el propósito del usuario, no el del programador. Usted como usuario es libre de ejecutar el programa para alcanzar sus propósitos, y si lo distribuye a otra persona, también esa persona será libre de ejecutarlo para lo que necesite; usted no tiene el derecho de imponerle sus propios objetivos a la otra persona. La libertad de ejecutar el programa como se desea significa que al usuario no se le prohíbe o no se le impide hacerlo. No tiene nada que ver con el tipo de funcionalidades que el programa posee ni con el hecho de que el programa sea o no sea útil para lo que se quiere hacer. 3.1.1.2 La libertad de estudiar el código fuente y modificarlo Para que las libertades 1 y 3 (realizar cambios y publicar las versiones modificadas) tengan sentido, usted debe tener acceso al código fuente del programa. Por consiguiente, el acceso al código fuente es una condición necesaria para el software libre. El «código fuente» ofuscado no es código fuente real y no cuenta como código fuente. La libertad 1 incluye la libertad de usar su versión modificada en lugar de la original. Si el programa se entrega unido a un producto diseñado para ejecutar versiones modificadas por terceros, pero rechaza ejecutar las suyas —práctica conocida como «bloqueo», o (según la terminología perversa de quienes lo practican) «arranque seguro»—, la libertad 1 se convierte en una vana 17 simulación más que una realidad práctica. Estos binarios no son software libre, aun cuando se hayan compilado a partir de un código fuente libre. Una manera importante de modificar el programa es agregándole subrutinas y módulos libres ya disponibles. Si la licencia del programa específica que no se pueden añadir módulos que ya existen y que están bajo una licencia apropiada, por ejemplo, si requiere que usted sea el titular del copyright del código que desea añadir, entonces se trata de una licencia demasiado restrictiva como para considerarla libre. Si una modificación constituye o no una mejora, es un asunto subjetivo. Si su derecho a modificar un programa se limita, básicamente, a modificaciones que alguna otra persona considera una mejora, el programa no es libre. 3.1.1.3 La libertad de redistribuir copias si así lo desea: requisitos básicos La libertad para distribuir (libertades 2 y 3) significa que usted tiene la libertad para redistribuir copias con o sin modificaciones, ya sea gratuitamente o cobrando una tarifa por la distribución, a cualquiera en cualquier parte. Ser libre de hacer esto significa, entre otras cosas, que no tiene que pedir ni pagar ningún permiso para hacerlo. También debe tener la libertad de hacer modificaciones y usarlas en privado para su propio trabajo o pasatiempo, sin siquiera mencionar que existen. Si publica sus cambios, no debe estar obligado a notificarlo a nadie en particular, ni de ninguna manera en particular. La libertad 3 incluye la libertad de publicar sus versiones modificadas como software libre. Una licencia libre también puede autorizar otras formas de publicación; en otras palabras, no tiene que ser una licencia con copyleft. No obstante, una licencia que requiera que las versiones modificadas no sean libres, no se puede considerar libre. La libertad de redistribuir copias debe incluir las formas binarias o ejecutables del programa, así como el código fuente, tanto para las versiones modificadas 18 como para las que no lo estén. (Distribuir programas en forma de ejecutables es necesario para que los sistemas operativos libres se puedan instalar fácilmente). Resulta aceptable si no existe un modo de producir un formato binario o ejecutable para un programa específico, dado que algunos lenguajes no incorporan esa característica, pero debe tener la libertad de redistribuir dichos formatos si encontrara o programara una forma de hacerlo. 3.1.1.4 Copyleft Ciertos tipos de reglas sobre la manera de distribuir software libre son aceptables, cuando no entran en conflicto con las libertades principales. Por ejemplo, el copyleft, definido muy sucintamente, es la regla en base a la cual, cuando redistribuye el programa, no se puede agregar restricciones para denegar a los demás las libertades principales. Esta regla no entra en conflicto con las libertades principales, más bien las protege. En el proyecto GNU se usa el copyleft para proteger legalmente las cuatro libertades para todos. Existen razones importantes por las que es mejor usar el copyleft. De todos modos, el software libre sin copyleft también es ético. 3.1.2 Reglas acerca del empaquetamiento y la distribución Eventuales reglas sobre cómo empaquetar una versión modificada son aceptables si no limitan substancialmente su libertad para publicar versiones modificadas, o su libertad para hacer y usar versiones modificadas en privado. Así, es aceptable que una licencia le obligue a cambiar el nombre de la versión modificada, eliminar el logotipo o identificar sus modificaciones como suyas. Son aceptables siempre y cuando esas obligaciones no sean tan agobiantes que le dificulten la publicación de las modificaciones. Como ya está realizando otras modificaciones al programa, no le supondrá un problema hacer algunas más. Las reglas del tipo «si pone a disposición su versión de este modo, también debe hacerlo de este otro modo» también pueden ser, bajo la misma condición, admisibles. Un ejemplo de una regla admisible sería alguna que requiera que, si usted ha distribuido una versión modificada y uno de los programadores 19 anteriores le solicita una copia, usted deba enviársela (tenga en cuenta que tal regla le sigue permitiendo optar por distribuir o no distribuir su versión). Las reglas que obligan a suministrar el código fuente a los usuarios de las versiones publicadas también son admisibles. Un problema particular se presenta cuando la licencia requiere que a un programa se le cambie el nombre con el cual será invocado por otros programas. De hecho, este requisito dificulta la publicación de la versión modificada para reemplazar al original cuando sea invocado por esos otros programas. Este tipo de requisitos es aceptable únicamente cuando exista un instrumento adecuado para la asignación de alias que permita especificar el nombre del programa original como un alias de la versión modificada. 3.1.3 Normas de exportación En algunos casos las normas de control de exportación y las sanciones comerciales impuestas por el Gobierno pueden limitar la libertad de distribuir copias de los programas a nivel internacional. Los desarrolladores de software no tienen el poder de eliminar o pasar por alto estas restricciones, pero lo que sí pueden y deben hacer es rehusar imponerlas como condiciones para el uso del programa. De este modo, las restricciones no afectarán las actividades ni a las personas fuera de las jurisdicciones de tales Gobiernos. Por tanto, las licencias de software libre no deben requerir la obediencia a ninguna norma de exportación que no sea trivial como condición para ejercer cualquiera de las libertades esenciales. La mera mención de la existencia de normas de exportación, sin ponerlas como condición de la licencia misma, es aceptable ya que esto no restringe a los usuarios. Si una norma de exportación es de hecho trivial para el software libre, ponerla como condición no constituye un problema real; sin embargo, es un problema potencial ya que un futuro cambio en la ley de exportación podría hacer que el requisito dejara de ser trivial y que el software dejara de ser libre. 20 3.1.4 Consideraciones legales Para que estas libertades sean reales, deben ser permanentes e irrevocables siempre que usted no cometa ningún error; si el programador del software tiene el poder de revocar la licencia, o de añadir restricciones a las condiciones de uso en forma retroactiva, sin que haya habido ninguna acción de parte del usuario que lo justifique, el software no es libre. Una licencia libre no puede exigir la conformidad con la licencia de un programa que no es libre. Así, por ejemplo, si una licencia requiere que se cumpla con las licencias de «todos los programas que se usan», en el caso de un usuario que ejecuta programas que no son libres este requisito implicaría cumplir con las licencias de esos programas privativos, lo cual hace que la licencia no sea libre. Es aceptable que una licencia especifique la jurisdicción de competencia o la sede para la resolución de conflictos, o ambas cosas. 3.1.5 Licencias basadas en contrato La mayoría de las licencias de software libre están basadas en el copyright, y existen límites en los tipos de requisitos que se pueden imponer a través del copyright. Si una licencia basada en el copyright respeta la libertad en las formas antes mencionadas, es poco probable que surja otro tipo de problema que no hayamos anticipado (a pesar de que esto ocurre ocasionalmente). Sin embargo, algunas licencias de software libre están basadas en contratos, y los contratos pueden imponer un rango mucho más grande de restricciones. Esto significa que existen muchas maneras posibles de que tal licencia sea inaceptablemente restrictiva y que no sea libre. Si una licencia basada en un contrato restringe al usuario de un modo que no se puede hacer con las licencias basadas en el copyright, y que no está mencionado aquí como legítimo, tendremos que analizar el caso, y probablemente concluyamos que no es libre. 21 3.2 Inteligencia de Negocios (BI) Inteligencia de negocios o BI (del inglés Business Intelligence), es el conjunto de estrategias, aplicaciones, datos, productos, tecnologías y arquitectura técnica, los cuales están enfocados a la administración y creación de conocimiento sobre el medio, a través del análisis de los datos existentes en una organización o empresa. Es posible diferenciar datos, informaciones, y conocimientos, conceptos en los que se centra la inteligencia empresarial, ya que como sabemos, un dato es algo vago, por ejemplo "10 000", la información es algo más preciso, por ejemplo "Las ventas del mes de mayo fueron de 10 000", y el conocimiento se obtiene mediante el análisis de la información, por ejemplo "Las ventas del mes de mayo fueron 10 000. Mayo es el mes más bajo en ventas". Aquí es donde la BI entra en juego, ya que al obtener conocimiento del negocio una vez capturada la información de todas las áreas en la empresa, es posible establecer estrategias y determinar cuáles son las fortalezas y las debilidades. 3.2.1 Definición Una interesante definición para inteligencia de negocios o BI, por sus siglas en inglés, según el Data Warehouse Institute, lo define como la combinación de tecnología, herramientas y procesos que permiten transformar los datos almacenados en información, esta información en conocimiento y este conocimiento dirigido a un plan o una estrategia comercial. La inteligencia de negocios debe ser parte de la estrategia empresarial, esta le permite optimizar la utilización de recursos, monitorear el cumplimiento de los objetivos de la empresa y la capacidad de tomar buenas decisiones para así obtener mejores resultados. Las herramientas de inteligencia se basan en la utilización de un sistema de información de inteligencia que se forma con distintos datos extraídos de la 22 producción, con información relacionada con la empresa o sus ámbitos, y con datos económicos. Mediante las herramientas y técnicas ETL (Del inglés "Extract, transform & Load"), o ETC (equivalente en Castellano: "extracción, transformación y carga"), se extraen los datos de distintas fuentes, se depuran y preparan (homogeneización de los datos), para luego cargarlos en un almacén de datos. La vida o el periodo de éxito de un software de inteligencia de negocios dependerá únicamente del éxito de su uso en beneficio de la empresa; si esta empresa es capaz de incrementar su nivel financiero-administrativo y sus decisiones mejoran la actuación de la empresa, el software de inteligencia de negocios seguirá presente mucho tiempo, en caso contrario será sustituido por otro que aporte mejores y más precisos resultados. Finalmente, las herramientas de inteligencia analítica posibilitan el modelado de las representaciones basadas en consultas para crear un cuadro de mando integral que sirve de base para la presentación de informes. 3.2.2 Historia En un artículo de 1958, el investigador de IBM Hans Peter Luhn utiliza el término Inteligencia de Negocio. Se define la inteligencia como: "La capacidad de comprender las interrelaciones de los hechos presentados de tal forma que consiga orientar la acción hacia una meta deseada". La inteligencia de negocios, tal como se entiende hoy en día, se dice que ha evolucionado desde los sistemas de apoyo a las decisiones que se inició en la década de 1960 y desarrollado a lo largo de mediados de los años 80. DSS se originó en los modelos por computadora, creado para ayudar en la toma de decisiones y la planificación. Desde DSS, data warehouses, sistemas de información ejecutiva, OLAP e inteligencia de negocios entraron en principio centrándose a finales de los años 80. 23 En 1989, Howard Dresner (más tarde, un analista de Gartner Group) propuso la "inteligencia de negocios" como un término general para describir "los conceptos y métodos para mejorar la toma de decisiones empresariales mediante el uso de sistemas basados en hechos de apoyo", sin embargo, esta expresión no se popularizó hasta finales de la década de los 90. 3.2.3 Características Este conjunto de herramientas y metodologías tienen en común las siguientes características: Accesibilidad a la información. Los datos son la fuente principal de este concepto. Lo primero que debe garantizar este tipo de herramientas y técnicas será el acceso de los usuarios a los datos con independencia de la procedencia de estos. Apoyo en la toma de decisiones. Se busca ir más allá en la presentación de la información, de manera que los usuarios tengan acceso a herramientas de análisis que les permitan seleccionar y manipular sólo aquellos datos que les interesen. Orientación al usuario final. Se busca independencia entre los conocimientos técnicos de los usuarios y su capacidad para utilizar estas herramientas. 3.2.4 Niveles de Realización de BI De acuerdo a su nivel de complejidad se pueden clasificar las soluciones de Business Intelligence en: - Informes - Informes predefinidos - Informes a medida 24 - Consultas (Query) / Cubos OLAP (On-Line Analytic Processing). - Alertas - Análisis - Análisis estadístico - Pronósticos (Forecasting) - Modelado predictivo o Minería de datos (Data Mining) - Optimización - Minería de Procesos 3.2.5 Inteligencia de Empresas La Inteligencia de Empresas es el concepto más amplio del uso de la inteligencia en las organizaciones. Desde distintas perspectivas, la inteligencia de empresas ha ido emergiendo a partir de la contribución de muchas áreas del conocimiento: market intelligence (inteligencia de mercados), competitive intelligence (Inteligencia Competitiva), business intelligence (inteligencia empresarial). Este concepto ha sido muy utilizado en el mundo de la tecnología con distintos significados como inteligencia de negocios, strategic foresight (Inteligencia Estratégica), corporate intelligence (Inteligencia Corporativa), vigilancia tecnológica, prospectiva tecnológica, etc. 3.2.6 Inteligencia de Mercados internacionales. La estrategia debe ser vista como un proceso creativo, buscar nuevas formas de hacer las cosas, de generar valor en el mundo de continuo cambio, y ser efectivo en el corto plazo por lo cual se necesita: - Inteligencia para crear y compartir el conocimiento. - La habilidad para integrar y administrar este conocimiento. - La imaginación para visualizar acciones alternativas a las usuales y analizar sus consecuencias. 25 - La pericia para manejar los recursos y atender las necesidades actuales sin dejar de construir el futuro deseable. Así como en mercadotecnia se tienen las 4 p's también en inteligencia de mercados se tienen 4 P's PLAN: curso de acción conscientemente determinado. POSICIÓN: un medio para ubicar a la organización (nicho, rentas, dominio). PATRÓN: es un modelo que implica consistencia. PERSPECTIVA: una manera particular de percibir el mundo (concepto, cultura, ideología). Con la globalización, la competencia se convierte en hipercompetencia para lo cual hay que reaccionar con rapidez, sorpresa, anticipación, también hay que cambiar las reglas del juego y hacer productos innovadores integrales para demostrar superioridad ante la competencia. 3.2.7 Datawarehouse & Business Intelligence Ralf Kimball (1944) es considerado el inventor del Modelo Dimensional y pionero en Data Warehouse e Inteligencia de Negocios. Define un almacén de datos como: "una copia de las transacciones de datos específicamente estructurada para la consulta y el análisis". También fue Kimball quien determinó que un datawarehouse no era más que: "la unión de todos los Data Marts de una entidad". Defiende por tanto una metodología ascendente (buttom-up). Entre las bibliografías más destacadas y conocidas de Ralf Kimball se encuentran: - Kimball, Ralph; Margy Ross (2013). The Data Warehouse Toolkit: The Definitive Guide to Dimensional Modeling (3rd ed.). Wiley. ISBN978-1- 118-53080-1. 26 - Kimball, Ralph; Margy Ross (2010). The Kimball Group Reader. Wiley. ISBN 978-0-470-56310-6. - Kimball, Ralph; Margy Ross, Warren Thornthwaite, Joy Mundy, Bob Becker (2008). The Data Warehouse Lifecycle Toolkit (2nd ed.). Wiley. ISBN 978-0-470-14977-5. - Kimball, Ralph; Joe Caserta (2004). The Data Warehouse ETL Toolkit. Wiley. ISBN 0-7645-6757-8. - Kimball, Ralph; Margy Ross (2002). The Data Warehouse Toolkit: The Complete Guide to Dimensional Modeling (2nd ed.). Wiley. ISBN 0-471- 20024-7. - Kimball, Ralph; Richard Merz (2000). The Data Webhouse Toolkit: Building the Web-Enabled Data Warehouse. Wiley. ISBN 0-471-37680-9. - Kimball, Ralph; et al. (1998). The Data Warehouse Lifecycle Toolkit. Wiley. ISBN 0-471-25547-5. - Kimball, Ralph (1996). The Data Warehouse Toolkit. Wiley. ISBN 978-0- 471-15337-5. 3.2.8 La Metodologia de Kimball La metodología se basa en lo que Kimball denomina Ciclo de Vida Dimensional del Negocio (Business Dimensional Lifecycle). Este ciclo de vida del proyecto de Data Warehouse, está basado en cuatro principios básicos: Centrarse en el negocio: Hay que concentrarse en la identificación de los requerimientos del negocio y su valor asociado, y usar estos esfuerzos para desarrollar relaciones sólidas con el negocio, agudizando el análisis del mismo y la competencia consultiva de los implementadores. Construir una infraestructura de información adecuada: Diseñar una base de información única, integrada, fácil de usar, de alto rendimiento donde se reflejará la amplia gama de requerimientos de negocio identificados en la empresa. 27 Realizar entregas en incrementos significativos: Crear el almacén de datos (DW) en incrementos entregables en plazos de 6 a 12 meses. Hay que usar el valor de negocio de cada elemento identificado para determinar el orden de aplicación de los incrementos. En esto la metodología se parece a las metodologías ágiles de construcción de software. Ofrecer la solución completa: Proporcionar todos los elementos necesarios para entregar valor a los usuarios de negocios. Para comenzar, esto significa tener un almacén de datos sólido, bien diseñado, con calidad probada, y accesible. También se deberá entregar herramientas de consulta ad hoc, aplicaciones para informes y análisis avanzado, capacitación, soporte, sitio web y documentación. Figura 2 Tareas de la metodología (Ciclo de Vida) Como se puede apreciar en la figura, los Requerimientos del Negocio son el soporte inicial de las tareas subsiguientes. También tiene influencia en el plan de proyecto (puede notar la doble fecha entre la caja de definición de requerimientos y la de planificación). Podemos también ver tres rutas o caminos que se enfocan en tres diferentes áreas: 28 Tecnología (Camino Superior): Implica tareas relacionadas con software específico, por ejemplo, Microsoft SQL Analysis Services. Datos (Camino del medio): En la misma diseñaremos e implementaremos el modelo dimensional, y desarrollaremos el subsistema de Extracción, Transformación y Carga (Extract, Transformation, and Load - ETL) para cargar el DW. Aplicaciones de Inteligencia de Negocios (Camino Inferior): En esta ruta se encuentran tareas en las que diseñamos y desarrollamos las aplicaciones de negocios para los usuarios finales. Estas rutas se combinan cuando se instala finalmente el sistema. En la parte de debajo de la figura se muestra la actividad general de administración del proyecto. A continuación, describiremos cada una de las tareas: PLANIFICACIÓN: En este proceso se determina el propósito del proyecto de DW/BI, sus objetivos específicos y el alcance del mismo, los principales riesgos y una aproximación inicial a las necesidades de información. En la visión de programas y proyectos de Kimball, Proyecto, se refiere a una iteración simple del Ciclo de Vida de Kimball, desde el lanzamiento hasta el despliegue. Esta tarea incluye las siguientes acciones típicas de un plan de proyecto: - Definir el alcance (Entender los Requerimientos del Negocio) - Identificar las tareas - Programar las tareas - Planificar el uso de los recursos - Asignar la carga de trabajo a los recursos - Elaboración de un documento final que representa un plan del Proyecto ANÁLISIS DE REQUERIMIENTOS: La definición de los requerimientos es en gran medida un proceso de entrevistar al personal de negocio y técnico, pero siempre conviene tener un poco de preparación previa. Se debe aprender tanto 29 como se pueda sobre el negocio, los competidores, la industria y los clientes del mismo. Hay que leer todos los informes posibles de la organización; rastrear los documentos de estrategia interna; entrevistar a los empleados, analizar lo que se dice en la prensa acerca de la organización, la competencia y la industria. Se deben conocer los términos y la terminología del negocio. MODELADO DIMENSIONAL: El proceso de diseño comienza con un modelo dimensional de alto nivel obtenido a partir de los procesos priorizados de la matriz descrita en el punto anterior. El proceso iterativo consiste en cuatro pasos: - Elegir el Proceso de Negocio. - Establecer el Nivel de Granularidad. - Elegir las Dimensiones. - Identificar medidas y las tablas de hechos. DISEÑO FÍSICO: En esta parte, intentamos contestar las siguientes preguntas: - ¿Cómo puede determinar cuán grande será el sistema de DW/BI? - ¿Cuáles son los factores de uso que llevarán a una configuración más grande y más compleja? - ¿Cómo se debe configurar el sistema? - ¿Cuánta memoria y servidores se necesitan? ¿Qué tipo de almacenamiento y procesadores? - ¿Cómo instalar el software en los servidores de desarrollo, prueba y producción? - ¿Qué necesitan instalar los diferentes miembros del equipo de DW/BI en sus estaciones de trabajo? - ¿Cómo convertir el modelo de datos lógico en un modelo de datos físicos en la base de datos relacional? - ¿Cómo conseguir un plan de indexación inicial? - ¿Debe usarse la partición en las tablas relacionales? 30 DISEÑO DEL SISTEMA DE EXTRACCIÓN, TRANSFORMACIÓN Y CARGA (ETL): Es la base sobre la cual se alimenta el Datawarehouse. Si el sistema ETL se diseña adecuadamente, puede extraer los datos de los sistemas de origen de datos, aplicar diferentes reglas para aumentar la calidad y consistencia de los mismos, consolidar la información proveniente de distintos sistemas, y finalmente cargar (grabar) la información en el DW en un formato acorde para la utilización por parte de las herramientas de análisis. ESPECIFICACIÓN Y DESARROLLO DE APLICACIONES BI: Las aplicaciones de BI son la cara visible de la inteligencia de negocios: los informes y aplicaciones de análisis proporcionan información útil a los usuarios. Las aplicaciones de BI incluyen un amplio espectro de tipos de informes y herramientas de análisis, que van desde informes simples de formato fijo a sofisticadas aplicaciones analíticas que usan complejos algoritmos e información del dominio. Kimball divide a estas aplicaciones en dos categorías basadas en el nivel de sofisticación, y les llama informes estándar y aplicaciones analíticas. 3.3 Big Data 3.3.1 Introducción El concepto de Big Data se refiere al almacenamiento y procesamiento de enormes cantidades de datos, tan desproporcionadamente grandes que no es posible tratarlos con las herramientas de base de datos convencionales. Sin embargo, el término Big Data no hace referencia a alguna cantidad en específico, debido a que lo que para una empresa determinada puede ser Big Data, puede no serlo para otra (Figura 3 Big Data en las empresas Fuente: Herramientas para Big Data: Entono Hadoop (Sanchez, 2014)). 31 Actualmente para considerar una solución Big Data, se toma en cuenta que los datos presenten cuatro características principalmente, los cuales son: Variedad, Velocidad, Volumen y Veracidad (también se hablan de otras V’s como la visualización, sin embargo, las cuatro mencionadas anteriormente son las principales). La variedad indica que la data puede estar en diferentes formatos, puede ser estructurada o no estructurada y venir de distintas fuentes, la velocidad indica la rapidez con la que se generan los datos, el volumen indica la cantidad que se genera y la veracidad indica la certeza de los datos. (IBM, 2015) Para poder comprender mejor el uso de Big Data primero debemos comprender la naturaleza de los problemas que lo han hecho necesario. Actualmente vivimos en la era de la información, en la que el volumen total de datos almacenados de forma electrónica no se mide fácilmente. Sin embargo, en 2006 se estimó que el tamaño del universo digital ascendía a 0,18 zettabytes y en 2011 ya eran 1,8 zettabytes. Estos son unos ejemplos de la cantidad de datos que manejan algunos de los servicios más conocidos: (Magazine, 2013)  Facebook almacena aproximadamente 10 mil millones de fotografías  Ancestry.com almacena aproximadamente 2,5 petabytes de datos.  New York Stock Exchange genera alrededor de 1 terabyte al día.  El colisionador de hadrones de Suiza produce alrededor de 15 petabytes de datos al año. Figura 3 Big Data en las empresas Fuente: Herramientas para Big Data: Entono Hadoop (Sanchez, 2014) 32  Los usuarios de YouTube suben 100 horas de video en 1 minuto  Se producen 300.000 tweets en un minuto.  2.000.000 de consultas a Google en sólo un minuto. Y como éstos existen muchos casos similares de diversas empresas u organizaciones. Además del gran volumen de información que se genera diariamente, ésta existe en una gran variedad de datos que pueden ser representados de diversas maneras en todo el mundo, por ejemplo, de dispositivos móviles, audio, video, sistemas GPS, incontables sensores digitales en equipos industriales, automóviles, entre otros. Las aplicaciones que analizan éstos datos requieren que la velocidad de respuesta sea lo demasiado rápida para lograr obtener la información correcta en el momento preciso. Los principales tipos de datos se resumen en 5 (Figura 4 Tipos de datos en Big Data)  Web and Social Media: Contenido web e información que es obtenida de las redes sociales como Instagram, Facebook, Snapchat, blogs. Figura 4 Tipos de datos en Big Data Fuente: (Soares, 2012) 33  Machine-to-Machine: Sensores o medidores que capturan algún evento en particular (velocidad, temperatura, presión, variables meteorológicas o variables químicas) los cuales transmiten a través de redes alámbricas, inalámbricas o híbridas a otras aplicaciones que traducen estos eventos en información significativa.  Big Transaction Data: Registros de facturación, en telecomunicaciones registros detallados de las llamadas, etc. Estos datos transaccionales están disponibles en formatos tanto semiestructurados como no estructurados.  Biometrics: Información biométrica en la que se incluye huellas digitales, escaneo de retina, reconocimiento facial, genética, etc. En el área de seguridad e inteligencia, los datos biométricos han sido información importante para las agencias de investigación.  Human Generated: Las personas generamos diversas cantidades de datos como la información que guarda un call center al establecer una llamada telefónica, notas de voz, correos electrónicos, documentos electrónicos, estudios médicos, etc. 3.3.2 Historia de Big Data Desde que surgieron las primeras formas de escritura hasta los centros de datos modernos, la raza humana no ha dejado de recopilar información. El crecimiento del sector tecnológico ha provocado el aumento desmesurado del volumen de datos, por lo que son necesarios sistemas de almacenamiento de datos más sofisticados. Se expondrá brevemente los hitos más importantes en el surgimiento del Big Data: (winshuttle, 2016)  El término Big Data es un concepto que hace referencia al almacenamiento de grandes volúmenes de datos y a los procesos necesarios para encontrar patrones repetitivos dentro de esos datos.  Comienzo de sobrecarga de información (1880): El Censo de los Estados Unidos del año 1880 tardó ocho años en tabularse, y se calcula que el 34 censo de 1890 hubiera necesitado más de 10 años para procesarse con los métodos disponibles en la época. Si no se hubieran realizado avances en la metodología, la tabulación no habría finalizado antes de que tuviera que realizarse el censo de 1900.  La máquina tabuladora de Hollerith (1881): La influencia de los datos del censo derivó en la invención de la máquina tabuladora de Hollerith (tarjetas perforadas), que fue capaz de domar esta ingente cantidad de información y permitir realizar el trabajo aproximadamente en un año. Hizo que Hollerith se convirtiera en emprendedor, y su empresa pasó a formar parte de lo que hoy en día conocemos como IBM.  El boom del crecimiento demográfico (1932): La sobrecarga de información prosiguió con el aumento desmesurado de la población en los Estados Unidos, la emisión de los números de la seguridad social y el crecimiento general del conocimiento (y la investigación), aspectos que exigían un registro de la información más preciso y organizado. Lea el resto de la cronología en: http://www.winshuttle.es/big-data-historia- cronologica/ 3.3.2.1 Big Data Analítica Sabemos ya pues a que nos referimos con el término Big Data y de los aconteceres en el devenir de la historia, pero nos surge una nueva interrogante ¿Qué es Big Data Analítica? Big Data Analítica es el proceso de recopilación, organización y análisis de grandes conjuntos de datos (como hemos mencionado Big Data) para descubrir patrones y otra información útil. El análisis de grandes datos puede ayudar a las organizaciones a comprender mejor la información contenida en los datos y también ayudará a identificar los datos que representan gran importancia para el negocio y las decisiones futuras del mismo. 35 Los analistas y personas especializadas que trabajan con grandes volúmenes de datos, básicamente, quieren obtener el conocimiento que proviene de analizar los datos. Sin embargo, es de vital importancia recalcar que al momento de realizar análisis en Big Data se debe considerar una característica fundamental y primordial, a continuación, detallada: - Requiere de un Alto Rendimiento. Para analizar una gran cantidad de volumen de datos de este tipo se realiza típicamente utilizando herramientas de software y aplicaciones especializadas para el análisis predictivo, minería de datos, minería de texto, la previsión y optimización de datos. En conjunto, estos procesos son funciones separadas pero muy integradas de análisis de alto rendimiento. El uso de grandes herramientas de datos y software permite a una organización extremadamente procesar grandes volúmenes de datos que ha recogido una empresa para determinar qué datos son relevantes y pueden ser analizados para tomar mejores decisiones de negocio en el futuro. Para la mayoría de las organizaciones, el análisis de grandes volúmenes de datos es un reto. Teniendo en cuenta el gran volumen de datos y los diferentes formatos de los datos (tanto los datos estructurados y no estructurados) que se recogen a través de toda la organización y las diferentes maneras diferentes tipos de datos que se pueden combinar, contrastada y analizada para encontrar patrones y otros que representen para los negocios útil información. Además, el análisis de Big Data como toda tecnología representa una gran variedad de retos, el primero de ellos es la descomposición, limpieza y tratamiento de los datos, con el fin de tener acceso a un depósito puro y limpio de información correspondiente a las organizaciones en diferentes lugares y con frecuencia en diferentes sistemas. Un segundo desafío se presenta es en la creación de plataformas que se pueden introducir datos no estructurados con tanta facilidad como los datos estructurados. Este volumen masivo de datos 36 suele ser tan grande que es difícil de procesar mediante métodos de base de datos y software tradicionales. ¿Es usado en la actualidad el análisis en Big Data? A medida que la tecnología ayuda a una organización a romper las barreras y dificultades que puede vivir la misma con el fin de mejorar y superar sus límites, son cada vez más las que acuden al uso e incorporación de dichas tecnologías a su ámbito organizacional, así, el tratado de los datos y análisis correcto para la toma de decisiones certeras basados en Big Data no es una excepción, es de tal importancia que representa la tendencia en las organizaciones actuales y líderes en el mercado como se ha recalcado con anterioridad. De acuerdo con Datamation, los avances actuales en el análisis de grandes volúmenes de datos permiten a los investigadores a descifrar el ADN humano en cuestión de minutos, predicen que los terroristas planean atacar, determinar qué gen es sobre todo probable que sea responsable de ciertas enfermedades y, por supuesto, los anuncios que tienen más probabilidades de responder a en Facebook. Otro ejemplo es el de una de las mayores compañías de telefonía móvil en el mundo. Orange, de France puso en marcha su proyecto de Datos para el Desarrollo por la liberación de los datos de abonado para los clientes en la Costa de Marfil. Los 2,5 millones de registros, que se hicieron en el anonimato, incluidos detalles sobre las llamadas y mensajes de texto intercambiados entre los 5 millones de usuarios. Los investigadores acceder a los datos y se envían propuestas para Orange cómo los datos podrían servir de base para los proyectos de desarrollo para mejorar la salud y la seguridad públicas. Los proyectos propuestos incluyen una que mostraba cómo mejorar la seguridad pública mediante el seguimiento de los datos del teléfono celular para mapear donde la gente iba tras una emergencia; otra mostró cómo usar los datos celulares para la contención de la enfermedad. Las empresas buscan cada vez más para encontrar información procesable en sus datos. Muchos proyectos de grandes volúmenes de datos originados por la necesidad de responder a las preguntas específicas del negocio. Con las 37 plataformas adecuadas análisis de datos grandes en su lugar, una empresa puede aumentar las ventas, aumentar la eficiencia y mejorar las operaciones, servicio al cliente y la gestión de riesgos. Webopedia empresa matriz, QuinStreet, encuestó a 540 tomadores de decisiones de la empresa que participan en las compras de grandes datos para saber qué áreas de negocio compañías planean utilizar análisis de grandes datos para mejorar las operaciones. Aproximadamente la mitad de todos los encuestados dijeron que estaban aplicando análisis de grandes volúmenes de datos para mejorar la retención de clientes, ayudar con el desarrollo de productos y obtener una ventaja competitiva. En particular, la zona de negocios conseguir la mayor atención se relaciona con el aumento de la eficiencia y la optimización de las operaciones. En concreto, el 62 por ciento de los encuestados dijo que utilizan análisis de grandes volúmenes de datos para mejorar la velocidad y reducir la complejidad. Son infinitamente innumerables las aplicaciones del análisis de Big Data en las diversas categorías o puntos de vista donde se desee incorporar, por lo que podemos concluir que nuestro país Venezuela no debe ni deberá representar la excepción para la incorporación de esta poderosa y en incremento tecnología denominada Big Data Analítica. Para entender el funcionamiento de Big Data Analítica se expondrá cada una de las tecnologías involucradas: 3.3.3 Tecnologías involucradas La implementación de una solución Big Data Analítica se descompone en diferentes tecnologías que trabajan conjuntamente para lograr el objetivo final de almacenar y procesar grandes volúmenes de datos. Entre las tecnologías involucradas se encuentran el framework del proyecto Apache Hadoop (Apache, Hadoop, 2016), el uso de base de datos NoSQL (NoSQL, 2016) entre las que 38 podemos mencionar Cassandra, Hbase, MongoDB, Neo4j, entre otras. Sin embargo, también se hace necesario mencionar el Teorema de Brewer o Teorema de CAP (Brewer, 2000), en el cual se enfocan las bases de datos NoSQL. Además, también debemos mencionar el almacén de Datos diseñado para Hadoop denominado Apache Hive y como es de gran importancia el paradigma MapReduce basado en los Sistemas de archivos Distribuidos. 3.3.3.1 Apache Hadoop Apache Hadoop es una plataforma que permite el procesamiento de grandes volúmenes de datos a través de clúster, usando un modelo simple de programación. Proporciona un framework, escrito en Java, sobre el cual desarrollar aplicaciones distribuidas que requieren un uso intensivo de datos y de alta escalabilidad. Este proyecto es administrado por Apache Software Foundation. Se presenta como una solución para los programadores sin experiencia en desarrollo de aplicaciones para entornos distribuidos, dado que oculta la implementación de detalles propios de estos sistemas: paralelización de tareas, administración de procesos, balanceo de carga y tolerancia a fallos. (Sánchez, 2014) 3.3.3.1.1 Historia Antes de empezar a hablar sobre el funcionamiento de Hadoop, vamos a destacar algunos detalles sobre su historia. Hadoop no es un acrónimo, es un nombre inventado. Es el nombre que el hijo del creador, Doug Cutting, le dio a un peluche de un elefante amarillo. Corto, 39 relativamente fácil de deletrear y pronunciar, sin significado y sin uso externo. Esto ha provocado que las herramientas relacionadas con Hadoop tiendan a tener nombres relacionados con elefantes u otras temáticas animales (Pig, Mahout, Hive, etc.). Sus orígenes se remontan a Apache Nutch, que era un motor de búsqueda web open-source dentro el proyecto Lucene. La construcción de este motor era una meta ambiciosa y muy cara, estimaban sus creadores, pero el objetivo valía la pena. Fue lanzado en 2002, y se propagó de forma rápida y eficaz. Sin embargo, al poco tiempo, se dieron cuenta de que su arquitectura era incapaz de escalar a los billones de páginas que había en la Web por aquel entonces, y no sabían cómo solucionarlo. En 2003 Google publicó un artículo que describía la arquitectura del Google’s Distributed Filesystem, lo cual proporcionaba a Nutch la solución al problema que se había encontrado. Esto también ayudaba a minimizar los tiempos gastados en tareas administrativas como la gestión de los nodos de almacenamiento. (Vance, 2009) En 2004 se comenzó a escribir una implementación propia y open-source del sistema de ficheros utilizado por Google, el Nutch Distributed FileSystem (NDFS). En ese mismo año Google publicó otro artículo en el que se presentaba MapReduce al mundo. A principios de 2005 los desarrolladores de Nutch ya tenían su propia implementación de MapReduce funcionando para el NDFS (Figura 5 Procesamiento paralelo en MapReduce). En 2006 las implementaciones del NDFS y MapReduce de Nutch se quedan grandes para el proyecto al que pertenecen, por lo que se extraen para crear un subproyecto de Lucene con el nombre de Hadoop, ya que eran aplicables más allá de los ámbitos de la búsqueda en que se estaban utilizando. En ese mismo año Doug Cutting se unió a Yahoo!, lo que le proporcionó un equipo y recursos dedicados para desarrollar su proyecto. 40 En 2008, Yahoo! anunció que su índice de búsquedas era generado por un clúster Hadoop de 10.000 cores. Por lo que, ese mismo año, Hadoop dejó de ser un sub- proyecto y se convirtió en un proyecto de Apache de máxima importancia. Por esta época, Hadoop ya era utilizado por otras compañías tales como Last.fm, Facebook y el New York Times. En abril de 2008, Hadoop rompía un récord mundial convirtiéndose en el sistema más rápido en ordenar 1 Terabyte de datos. Ejecutándose en un clúster de 910 nodos 8 lo consiguió en 209 segundos. En noviembre de ese mismo año, Google anunció que su implementación de MapReduce, podía ordenar ese volumen de datos en tan solo 68 segundos. Y en mayo del año siguiente, un equipo de Yahoo! ya lo consiguió en 62 segundos. Desde entonces Hadoop y su ecosistema es un referente en muchos aspectos y sus roles como plataforma de análisis y almacenamiento para grandes datos han sido reconocidos por la industria del sector. Hay distribuciones Hadoop tanto en las grandes empresas incluyendo IBM, Microsoft y Oracle, así como compañías Figura 5 Procesamiento paralelo en MapReduce Fuente: (Dean & Ghemawat, 2003) 41 especialistas en Hadoop tales como Cloudera, Hortonworks y MapR. (White, Hadoop: The Definitive Guide, 2012) 3.3.3.1.2 Funcionamiento 3.3.3.1.2.1 HDFS: Hadoop Distributed File System Es un sistema de ficheros para almacenar grandes archivos de datos y permitir accesos continuos a éstos. Cuando hablamos de grandes archivos, nos estamos refiriendo a archivos de cientos de Megabytes, Gigabytes o Terabytes. Hoy en día hay clústers de Hadoop que almacenan Petabytes en su sistema. HDFS está construido alrededor de la idea de que el patrón de procesamiento de datos más eficientes es el write-once, read-many-times (escribir una vez, leer varias veces). El funcionamiento que se suele hacer de un sistema así es que un conjunto de información es generado o copiado desde un origen y luego, sobre esos datos, se ejecutan varios procesos de análisis. Entonces este sistema da más importancia a la lectura de grandes bloques de datos consecutivos antes que a la realización de la primera escritura. Ya que Hadoop no requiere grandes máquinas a nivel de componentes, está diseñado para ser tolerante a una alta probabilidad de fallos de la máquina. HDFS es capaz de continuar trabajando sin interrupciones perceptibles por el usuario en caso de que se produzca dicho fallo. (White, Hadoop: The definitive guide, 2012) Para entender un poco mejor cómo funciona, debemos conocer algunos conceptos básicos de HDFS: (Shvachko, Kuang, Radia, & Chansler, 2006) Bloque: Un Bloque es la mínima cantidad de datos que se puede leer o escribir. Por regla general los sistemas de ficheros tienen bloques de pocos kilobytes mientras que los bloques de disco son de 512 bytes, esto difiere del concepto que ha tomado HDFS en el cual toman 64MB de valor por defecto del bloque. Por lo tanto, la información que se almacena en el HDFS tiene como unidad mínima el bloque. 42 La razón de que esto sea así es para minimizar el coste de las búsquedas, ya que, manteniendo un tamaño de bloque grande, el tiempo de transferencia desde disco mejora significativamente. Esto no supone un problema sino un beneficio dado el enfoque del HDFS, almacenar ficheros muy grandes. Namenode y Datanode: El funcionamiento de HDFS sigue el patrón master- workers. En este caso el master es el Namenode y los workers son los datanodes. El namenode es el encargado de gestionar el namespace del sistema de ficheros. Mantiene el árbol del sistema de ficheros, así como los metadatos de cada uno de los ficheros y directorios del árbol. También es el encargado de conocer la localización de los bloques que pertenecen a cada fichero dentro de los datanodes. Hay que tener en cuenta que esta información no es persistente debido a que se reconstruye desde los datanodes cada vez que el sistema inicia. De la misma forma en caso de pérdida de un datanode, éste se encarga de mantener la replicación en otro datanode y por lo tanto modifica esta información. Por otro lado, tenemos los datanodes, éstos son los encargados de almacenar y obtener la información cuando se les solicita, ya sea por el cliente o por el namenode. Con tal de mantener un correcto funcionamiento del sistema, periódicamente se envía la información de los bloques que se están almacenando en ese datanode al namenode. (Figura 6 Arquitectura básica de Hadoop). 43 3.3.3.1.2.2 MapReduce MapReduce es un paradigma de programación que permite procesar y generar grandes cantidades de datos con un algoritmo paralelo y distribuido en un clúster. Fue desarrollado por Google para el indexado de páginas web. El objetivo era crear un framework adaptado a la computación paralela que permitiera procesar y generar grandes colecciones de datos sobre máquinas genéricas, sin la necesidad de utilizar supercomputadores o servidores dedicados, y que fuera fácilmente escalable. En esencia, el modelo que propone MapReduce es bastante sencillo. El programador debe encargarse de implementar dos funciones, map y reduce, que serán aplicadas a todos los datos de entrada. Tareas como hacer particiones de los datos de entrada, despliegue de maestro y trabajadores, esquema de asignación de trabajos a los trabajadores, comunicación y sincronización entre procesos, tratamiento de caídas de procesos, quedan a cargo del entorno de ejecución, liberando de esa manera al programador. (White, Hadoop: The definitive guide, 2012) Figura 6 Arquitectura básica de Hadoop Fuente: (Chauhan, 2012) 44 El funcionamiento de este paradigma está dividido en dos pasos (para abstracción del programador):  Map: donde se realiza la ingestión y la transformación de los datos de entrada, en la cual los registros de entrada pueden ser procesados en paralelo. El nodo master obtiene la entrada, la divide en sub-problemas más pequeños y la distribuye a otros workers. Dicho worker procesa ese problema más pequeño y produce una lista de pares {clave, valor} y pasa la respuesta a su nodo master. Después de esto el framework de MapReduce junta los pares con la misma clave y los agrupa creando un grupo por cada clave generada.  Reduce: fase de agregación o resumen, donde todos los registros asociados entre sí deben ser procesados juntos por una misma entidad. El nodo master coge cada uno de estos grupos y los envía a un nodo para ejecutar la tarea de Reduce, por lo que cada sub-tarea reduce trabaja sobre una clave. El reducer trata estos datos y devuelve un output resultado de ello. 3.3.3.1.2.3 YARN (Yet Another Resource Manager) Uno de los problemas fundamentales que presenta Hadoop 1.0 es que solo admite un paradigma de programación: MapReduce. A pesar de que este modelo Figura 7 Conteo de palabras con MapReduce Fuente: (Lowell, 2013) 45 de programación es apropiado para el análisis de grandes conjuntos de datos, en ocasiones es necesario realizar otro tipo de análisis o sería más propicio utilizar otro tipo de software para analizar datos, pero aprovechándonos de la ventaja que proporciona un clúster Hadoop. Para intentar solventar este inconveniente surge un nuevo componente fundamental dentro de Hadoop: YARN. Apache Hadoop YARN es un subproyecto de Hadoop en la Apache Software Foundation introducido en la versión Hadoop 2.0 que separa la gestión de recursos de los componentes de procesamiento. YARN surge para corregir el inconveniente principal de Hadoop y permitir una gama más amplia de modelos de programación para analizar los datos almacenados en el HDFS más allá de MapReduce. La arquitectura de Hadoop 2.0 basada en YARN provee una plataforma de procesamiento más general y no restringido a MapReduce. En Hadoop 2.0, YARN toma las capacidades de gestión de los recursos que residían en MapReduce y las empaqueta para que puedan ser utilizados por los nuevos motores de procesado. Esto también simplifica la tarea de MapReduce en únicamente hacer lo que mejor sabe hacer, tratar datos. Con YARN, se permite ejecutar varias aplicaciones en Hadoop, todos compartiendo una gestión común de los recursos. MapReduce se convierte ahora en una librería Hadoop es decir una aplicación que reside en Hadoop y deja la gestión de recursos del clúster para el componente YARN. (Figura 8 Hadoop 1.0 a Hadoop 2.0). La aparición de YARN provoca el desarrollo de nuevas herramientas que cubren múltiples necesidades que únicamente con MapReduce no se podían completar. (Figura 9 Nuevas aplicaciones YARN) 46 3.3.3.1.2.4 Arquitectura de Hadoop 2.0 La idea fundamental de YARN es la de separar las dos mayores responsabilidades del JobTracker: la gestión de los recursos y la planificación/monitorización de las tareas en dos servicios separados, para ello tendremos dos nuevos componentes en YARN: un ResourceManager global y un ApplicationMaster por aplicación (AM). Figura 8 Hadoop 1.0 a Hadoop 2.0 Fuente (Murthy, 2013) Figura 9 Nuevas aplicaciones YARN Fuente: (Gutierrez, 2014) 47 Surgen así el ResourceManager para el master (sustituyendo al JobTracker) y un NodeManager (sustituyendo al TaskTracker) por cada slave. Forman el nuevo, y genérico, sistema de gestión de aplicaciones de una manera distribuida. Además, surge un componente llamado container que representa los recursos disponibles en cada nodo del clúster. El ResourceManager es la última autoridad que arbitra los recursos entre todas las aplicaciones en el sistema. El ApplicationMaster por aplicación es una entidad específica que se encarga de negociar los recursos con el ResourceManager y trabajar con los NodeManager para ejecutar y supervisar las tareas que lo componen. El ResourceManager está conectado a un planificador (Scheduler) que es responsable de la asignación de recursos a las diversas aplicaciones que se ejecutan con limitaciones conocidas de capacidades, las colas, etc. El Scheduler es un planificador puro en el sentido de que no realiza ningún monitoreo o seguimiento del estado de la aplicación. El Scheduler realiza su función de planificación en base a las necesidades de recursos de las aplicaciones fundamentándose en la noción abstracta de un container de recursos que incorpora elementos de recursos como la memoria, CPU, disco, red, etc. El NodeManager se encuentra en cada uno de los equipos esclavos y es responsable del lanzamiento del container de las aplicaciones, el seguimiento del uso de recursos (CPU, memoria, disco de red) y de informar del mismo al ResourceManager. El ApplicationMaster tiene la responsabilidad de negociar los containers de recursos necesarios desde el Scheduler, de realizar un seguimiento de su estado y de los progresos de ejecución. Desde la perspectiva del sistema, el propio ApplicationMaster se ejecuta como un container normal. (Figura 10 Arquitectura Hadoop 2.0 YARN) 48 A pesar de que los componentes evolucionen, una de las virtudes de YARN, es que mantiene el esquema de gestión de recursos utilizado con MapReduce lo que genera que Hadoop 2.0 esté totalmente integrado en este paradigma de programación. Figura 10 Arquitectura Hadoop 2.0 YARN Fuente: (Apache, Hadoop Yarn, 2016) 3.3.3.1.2.5 Ecosistema Hadoop El proyecto Hadoop consta de una serie de subproyectos, que vienen a complementar su funcionalidad profundizando en aspectos como el tratamiento, flujo e importación de datos, la monitorización de trabajos, etc. Existen multitud de proyectos relacionados con Hadoop que completan distintas necesidades (Figura 11 Ecosistema Hadoop). La mayoría son dirigidos por Apache, aunque empresas privadas como Cloudera o Hortonworks trabajan desarrollando todo este tipo de plataformas. A continuación, vamos a presentar los proyectos relacionados con Hadoop más importantes:  Ambari: Una herramienta basada en web para el aprovisionamiento, administración y seguimiento de clústeres Apache Hadoop, que incluye 49 soporte para Hadoop HDFS, Hadoop MapReduce, Colmena, HCatalog, Hbase, ZooKeeper, Oozie, Pig y Sqoop. Ambari también proporciona un panel de control para la visualización del estado del clúster, así como la capacidad de ver aplicaciones como MapReduce, Pig y Colmena con el objetivo de evaluar su rendimiento de una manera sencilla.  Avro: se trata de un sistema de serialización de datos que provee numerosas estructuras de datos, un formato de datos binario compacto y rápido, un archivo contenedor para almacenar datos persistentes y una sencilla integración con lenguajes dinámicos.  Cassandra: Apache Cassandra es una base de datos distribuida de segunda generación altamente escalable, que reúnele diseño totalmente distribuido de Dynamo y el modelo de datos basado en ColumnFamily de Bigtable. Cassandra se usa en Facebook, Digg, Twitter, Mahalo, Ooyala, SimpleGeo, Rackspace, y otras empresas que necesitan de una base de datos con alta escalabilidad, disponibilidad y tolerancia a fallos.  Chukwa: es un sistema de recopilación de datos de código abierto para el seguimiento de grandes sistemas distribuidos. Chukwa hereda la escalabilidad y robustez de Hadoop. Además, incluye un conjunto de herramientas flexibles y potentes para la visualización, seguimiento y análisis de resultados para hacer el mejor uso de los datos recogidos.  HBase: Una base de datos escalable, distribuida que soporta el almacenamiento de datos estructurados en tablas. Permite la realización de tablas a partir de ficheros de datos.  Hive: facilita la consulta y gestión de grandes conjuntos de datos que residen en almacenamiento distribuidos. Hive proporciona un mecanismo para la ver la estructura de los datos utilizando un lenguaje similar a SQL llamado HiveQL.  Mahout: se trata de un software libre centrado en la implementación de algoritmos de machine learning distribuidos.  Pig: Apache Pig es una plataforma para el análisis de grandes conjuntos de datos que se caracteriza por un lenguaje de alto nivel para la creación de los programas de análisis de datos, junto con la infraestructura 50 necesaria para la evaluación de estos programas. La propiedad más importante de los programas Pig es que su estructura es susceptible de una paralelización sustancial, lo que a su vez permite manejar grandes conjuntos de datos.  Spark: proporciona un motor de cálculo rápido y general para datos Hadoop. Spark proporciona un modelo de programación sencillo y expresivo que soporta una amplia gama de aplicaciones, incluyendo ETL, machine learning, procesamiento de flujo, y computación gráfica.  ZooKeeper: es un servicio centralizado construido para mantener la información de configuración, proporcionar sincronización distribuida y la prestación de servicios de grupo. Todos estos tipos de servicios se utilizan de una forma u otra por las aplicaciones distribuidas. Figura 11 Ecosistema Hadoop Fuente: (Blog, 2016) 51 3.3.3.1.3 Almacén de Datos Hadoop 3.3.3.1.3.1 Apache Hive Apache Hive es una infraestructura de almacenamiento de datos construida sobre Hadoop para proporcionar agrupación, consulta, y análisis de datos. Inicialmente desarrollado por Facebook, Apache Hive es ahora utilizada y desarrollado por otras empresas como Netflix y la Financial Industry Regulatory Authority (FINRA).2 3 Amazon mantiene una derivación de software de Apache Hive incluida en Amazon Elastic MapReduce en sus servicios Amazon Web Services. 3.3.3.1.3.2 Características Apache Hive soporta el análisis de grandes conjuntos de datos almacenados bajo HDFS de Hadoop y en sistemas compatibles como el sistema de archivos Amazon S3. Ofrece un lenguaje de consultas basado en SQL llamado HiveQL con esquemas para leer y convertir consultas de forma transparente en MapReduce, Apache Tez y tareas Spark. Los tres motores de ejecución pueden correr bajo YARN. Para acelerar las consultas, Hive provee índices, que incluyen índices de bitmaps. Otras características de Hive incluyen:  Indexación para proporcionar aceleración, tipo de índice que incluye compactación e índices de bitmaps. Otros tipos de índices serán incluidos en futuras versiones.  Diferentes tipos de almacenamiento como texto, RCFile, HBase, ORC, y otros.  Almacenamiento de metadatos en bases de datos relacionales, lo que permite reducir el tiempo para realizar verificaciones semánticas durante la ejecución de consultas.  Operaciones sobre datos comprimidos almacenados en el ecosistema Hadoop usando algoritmos que incluyen DEFLATE, BWT, snappy, etc. https://es.wikipedia.org/wiki/Almac%C3%A9n_de_datos https://es.wikipedia.org/wiki/Hadoop https://es.wikipedia.org/wiki/Facebook https://es.wikipedia.org/wiki/Netflix https://es.wikipedia.org/wiki/Financial_Industry_Regulatory_Authority https://es.wikipedia.org/wiki/Financial_Industry_Regulatory_Authority https://es.wikipedia.org/wiki/Apache_Hive#cite_note-Use_case-2 https://es.wikipedia.org/wiki/Apache_Hive#cite_note-OSCON-3 https://es.wikipedia.org/wiki/Amazon_Web_Services https://es.wikipedia.org/wiki/Amazon_Web_Services https://es.wikipedia.org/wiki/Hadoop https://es.wikipedia.org/wiki/Amazon_S3 https://es.wikipedia.org/wiki/SQL https://es.wikipedia.org/wiki/MapReduce https://es.wikipedia.org/w/index.php?title=Apache_Spark&action=edit&redlink=1 https://es.wikipedia.org/wiki/Deflaci%C3%B3n_(algoritmo) https://es.wikipedia.org/wiki/Compresi%C3%B3n_de_Burrows-Wheeler 52  Funciones definidas por el usuario (UDF) para manipular fechas, textos, y otras herramientas de minería de datos. Hive soporta la extensión de las funciones definidas por el usuario de manera de tratar casos no contemplados.  Consultas estilo SQL (HiveQL), las cuales son convertidas automáticamente a MapReduce o Tez, o tareas Spark. Por defecto, Hive almacena sus metadatos en una base de datos apache Derby, pero puede ser configurado para usar MySQL. 3.3.3.2 NoSQL (Not Only SQL) 3.3.3.2.1 Introducción Los sistemas manejadores de bases de datos relacionales son la tecnología predominante para almacenar datos estructurados en la web y aplicaciones de software en los negocios. Desde la publicación científica de Codd “A relational model of data for large shared data banks” de 1970 estos almacenes de datos apoyándose en el cálculo relacional y proveyendo consultas comprensivas ad hoc facilitadas por SQL han sido ampliamente adoptados y son comúnmente la única alternativa para el almacenamiento de datos accesibles a múltiples clientes de manera consistente. Los RDMBS siguen una filosofía “una talla para todos” lo que quiere decir que el modelo relacional sirve para todos los casos, sin embargo, en los últimos años ésta filosofía ha sido cuestionada por la ciencia y compañías relacionadas con la web, lo que ha llevado al surgimiento de una gran variedad de alternativas en bases de datos. Las cuales no siguen el modelo relacional propuesto por Frank Codd ya que poseen un esquema flexible y cada una surge para solventar una clase de problema en específico. Este movimiento es lo que se conoce como NoSQL (Not only SQL). El término NoSQL fue usado por primera vez en 1998 para una base de datos relacional que omitía el uso de SQL (Strozzi, 2010). El término fue utilizado de nuevo en el año 2009 para una conferencia realizada en San Francisco acerca https://es.wikipedia.org/wiki/Apache_Derby https://es.wikipedia.org/wiki/MySQL 53 de bases de datos que no seguían el modelo relacional, tales como Last.fm desarrollada por Jon Oskarsson quien fue el encargado de organizar el evento. (Evans, 2009) 3.3.3.2.2 Críticas a las Bases de Datos NoSQL Desde la perspectiva de los adeptos a los RDBMS podemos mencionar las siguientes críticas a las bases de datos NoSQL: (Antiñanco, 2013)  No hay un líder: El mercado de NoSQL está muy fragmentado, lo cual es un problema para el open-source porque se requiere una gran cantidad de desarrolladores para tener éxito.  No hay estandarización: Cada base de datos posee su propia interfaz y tipo de consultas, lo que ocasiona que la adaptación a una base de datos NoSQL tenga una inversión significativa para poder ser utilizada.  Se requiere una reestructuración de los modelos de desarrollo de aplicaciones: Utilizar una base datos NoSQL típicamente implica usar un modelo de desarrollo de aplicaciones diferente a la tradicional arquitectura de 3 capas. Por lo tanto, una aplicación existente de 3 capas no puede ser simplemente convertida para bases de datos NoSQL, debe ser reescrita, sin mencionar que no es fácil reestructurar los sistemas para que no ejecuten consultas con join o no poder confiar en el modelo de consistencia read-after- write  Modelos de datos sin esquema podría ser una mala decisión de diseño: Los modelos de datos sin esquema son flexibles desde el punto de vista del diseñador, pero son difíciles para consultar sus datos. 54 3.3.3.2.3 Puntos a favor de las Bases de Datos NoSQL Desde la visión de los adeptos a las bases de datos NoSQL podemos mencionar las siguientes razones para desarrollar y utilizar ésta tecnología:  Evitar la complejidad innecesaria: Los RDBMS proveen un conjunto amplio de características y obligan el cumplimiento de las propiedades ACID, sin embargo, para algunas aplicaciones éste set podría ser excesivo y el cumplimiento estricto de las propiedades ACID innecesario.  Alto rendimiento: Las bases de datos NoSQL proveen un rendimiento mayor a las relacionales, incluso hasta varios órdenes de magnitud.  Información no estructurada y hardware más económico: La mayoría de las bases de datos NoSQL son diseñadas para poder escalar horizontalmente. También permiten el almacenamiento de datos no estructurados, provenientes de diversas fuentes como pueden ser las redes sociales.  La filosofía “One size fits all” estaba y sigue estando equivocada: Existen muchos escenarios que no pueden ser abarcados con un enfoque de base de datos tradicional. Esto debido al continuo crecimiento de volúmenes de datos a ser almacenados y a la necesidad de procesar grandes volúmenes de datos en corto tiempo. 3.3.3.2.4 Taxonomía NoSQL De acuerdo a la manera en que las bases de datos NoSQL almacenan sus datos es posible clasificarlas de la siguiente manera:  Almacenamiento clave-valor  Bases de datos orientadas a columnas  Base de datos documentales  Base de datos orientada a grafos 55 3.3.3.2.5 Propiedades ACID vs BASE En esta sección se comparan las propiedades de los sistemas relacionales (ACID) con la de los sistemas no relacionales (BASE). Además, de una breve explicación del Teorema de CAP o Brewer, el cual es la base para el desarrollo de este tipo de tecnologías. 3.3.3.2.5.1 Teorema de CAP Durante el simposio de “Principios de computación distribuida” de ACM en el año 2000, Eric Brewer, un profesor de la universidad Berkeley de California y cofundador de Inktomi, a través de una presentación titulada “Hacia sistemas distribuidos robustos”, estableció la conjetura que los servicios web no pueden asegurar en forma conjunta las siguientes propiedades: Consistencia (C), Disponibilidad (A) y Tolerancia a particiones (P), esto es lo que se conoce como el teorema de CAP. Posteriormente en el año 2002, Seth Gilbert y Nancy Lynch de MIT publicaron una demostración formal de la conjetura de Brewer, convirtiéndola en un teorema. El teorema de CAP establece que, en un sistema distribuido con datos compartidos, se debe optar por favorecer dos de las tres características: Consistencia, Disponibilidad y Tolerancia a particiones. Bajo estas restricciones, Brewer indica que se debe utilizar como criterio de selección, los requerimientos que se consideren más críticos para el negocio, optando entre propiedades ACID y BASE. (Brewer, 2000). En la siguiente tabla basada en la presentación de Brewer se muestran las alternativas, características y ejemplos (Tabla 1 Alternativas en Teorema de CAP) 56 3.3.3.2.5.2 Propiedades ACID en sistemas distribuidos Las propiedades ACID presentes en las transacciones de las bases de datos relacionales simplifican el trabajo de los desarrolladores de aplicaciones al ofrecer garantías en cuanto a:  Atomicidad: Todas las operaciones en la transacción se completarán o ninguna lo hará.  Consistencia: El estado de la base de datos se mantiene válido tanto al inicio como al final de la transacción. Cualquier operación puede ver los cambios en la base de datos.  Aislamiento: Las transacciones se ejecutan de manera que una no puede afectar a la otra.  Durabilidad: Los cambios realizados durante una operación serán persistentes y no se podrá deshacer, aunque falle el sistema. Tabla 1 Alternativas en Teorema de CAP Fuente: (Antiñanco, 2013) 57 Los proveedores de bases de datos se percataron de la necesidad de particionar, por lo que introdujeron el protocolo de commit a 2 fases para seguir manteniendo las propiedades ACID en las instancias de las bases de datos. El cual consiste en:  Primera fase: el coordinador de la transacción solicita a cada base de datos involucrada que indiquen si es posible que realicen commit. Si es posible se procede a continuar con la segunda fase.  Segunda fase: El coordinador de la transacción solicita a cada base de datos que realice el commit. Sin embargo, si alguna base de datos no puede realizar el commit, se le solicita a todas las involucradas que realicen un roll-back de ésa transacción. Sería análogo a que un avión no pudiese despegar hasta que todos los pasajeros estén en sus asientos, a pesar de que el vuelo haya sido programado para una hora en específico. En vista de esto podemos observar que se generan retrasos al realizar las transacciones, lo que afecta, de acuerdo al teorema de CAP, la disponibilidad del sistema. (Pritchett, 2008) 3.3.3.2.5.3 Propiedades BASE Si las propiedades ACID se enfocan en la consistencia, las propiedades BASE se enfocan en la disponibilidad. La palabra BASE se refiere a básicamente disponible (BA), estado flexible (S) y eventualmente consistente (E). Las propiedades BASE son diametralmente opuestas a las propiedades ACID. Donde ACID es pesimista y fuerza la consistencia al finalizar cada operación, BASE es optimista y acepta que la consistencia de la base de datos estará en un estado flexible. Aunque esto suene imposible de lidiar, en la realidad es manejable y puede llevar a niveles de escalabilidad que no se pueden obtener con ACID. 58 La disponibilidad en las propiedades BASE es alcanzada a través de mecanismos de soporte de fallas parciales, que permite mantenerse operativos y evitar una falla total del sistema. Así, por ejemplo, si la información de usuarios estuviera particionada a través de 5 servidores de bases de datos, un diseño utilizando BASE alentaría una estrategia tal que una falla en uno de los servidores impacte sólo en el 20% de los usuarios de ese host. Tabla 2 ACID vs BASE (Brewer, 2000) ACID BASE Consistencia fuerte Consistencia débil Aislamiento Disponibilidad primero Centrado en “commit” Mejor esfuerzo Transacciones anidadas Respuesta aproximada ¿Disponibilidad? Agresivo (optimista) Conservativo (pesimista) Más simple Difícil evolución (ejemplo: el esquema) Más rápido. Fácil Evolución 3.3.3.3 Motores de Búsqueda Los motores de búsqueda son programas que permiten hacer búsquedas por palabras claves y retornan una lista de documentos en donde se encontraron las palabras claves. Funcionan mediante la creación de índices con los cuales luego se realizan las búsquedas y a diferencia de las bases de datos, permiten integrar búsquedas de diversas fuentes de datos, permiten la escalabilidad, ranking por relevancia y hacer búsquedas por facetas. (Wikipedia, Motores de busqueda, 2015) Estas herramientas son necesarias para facilitar las búsquedas a los usuarios, ya que al hablar de grandes volúmenes de datos como es el caso de la web es muy probable que el usuario final encuentre información que no le sea de utilidad, 59 por lo cual estas herramientas se manejan con análisis de contenido como es el caso del algoritmo de Google, Page Rank, en el cual se hace un ranking de las páginas más importantes dependiendo de la búsqueda realizada. Los principales motores de búsqueda en Big Data son Apache Lucene y Apache SolR, los cuales se explicarán brevemente: 3.3.3.3.1 Lucene Apache Lucene es una API de código abierto para la recuperación de información, implementada originalmente en Java por Doug Cutting. Se trata de una tecnología adecuada para cualquier aplicación que requiera de búsquedas por texto completo, especialmente si son multiplataforma (Apache, Lucene, 2016) Puede indexar cualquier formato de texto, como MS Word, HTML, XML, PDF y OpenDocument, siempre y cuando la información textual pueda ser extraída, lo que quiere decir que no puede indexar imágenes. Algunas características:  Es escalable y tiene indexación de alto rendimiento, ya que puede procesar 150 GB / Hora en máquinas modernas y requiere solo 1 MB de memoria  Potente, preciso y eficiente algoritmo de búsqueda. Búsqueda por ranking, mejores resultados devueltos primero. Diversos tipos de consultas como son: consultas por frases, por comodín, por proximidad, rangos y más.  Multiplataforma. Esta implementado 100 por ciento en Java, aunque también está disponible en otros lenguajes de programación.  Puede ordenar cualquier campo en cualquier documento  Internamente todo se maneja como un documento y no necesariamente tiene que referirse a un archivo real en disco, también podría asemejarse a una fila de una base de datos relacional.  Un documento es visto como una lista de campos, donde un campo tiene un nombre y un valor. 60  La unidad de indexación en Lucene es un término, el cual puede ser a menudo una palabra. Los índices rastrean las frecuencias de los términos  Lucene utiliza índices inversos que permiten localizar rápidamente los documentos asociados a la condición entrante de búsqueda. Figura 12 Indexación con Lucene Fuente: (MacCandles, Hatcher, & Gospodnetic, 2010) Algunas ventajas:  Poderosa sintaxis de búsqueda  Rápido indexamiento  Búsqueda rápida  Ordenamiento por relevancia y por otros tipos de campos Algunas desventajas:  No hay contratos formales de apoyo 61  No hay disponibilidad asegurada de formación u otros servicios profesionales para satisfacer las necesidades específicas del software o ayudar con la construcción de una aplicación.  Ningún programa de pruebas de liberación formalizado, calendario de lanzamiento o garantía de compatibilidad de actualización. 3.3.3.3.2 SolR Solr es un motor de búsqueda de código abierto, basado en Lucene, que permite el resaltado de resultados y la búsqueda por facetas, además posee una interfaz para su administración. Se ejecuta sobre un contenedor de servlets Java como Apache Tomcat (Wikipedia, Apache Solr, 2016) . Solr es escalable, permitiendo búsquedas distribuidas y replicación de índices La principal característica de Solr es su API estilo REST, ya que en vez de usar drivers para comunicarnos con Solr, podemos hacer peticiones HTTP y obtener resultados en XML o JSON. Algunas de sus características son:  Se comunica a través de HTTP enviando y recibiendo contenido en XML  Esta optimizado para un alto volumen de tráfico web.  Soporte de indexación distribuida (SolrCloud), replicación y la carga de consultas equilibradas.  Búsqueda por facetas  Análisis de texto (tokenización, normalización)  Permite realizar búsquedas geoespaciales.  Lee y escribe directamente al HDFS de Hadoop, además de soportar la replicación en el HDFS.  Es posible construir índices escalables a través del paradigma Map Reduce En su arquitectura Solr se divide en dos partes: (ver Figura 13 Arquitectura Solr) 62  Índice. Sistema de ficheros que almacenan la información. Contiene la configuración de Solr y la definición de la estructura de datos.  Servidor. Proporciona el acceso a los índices y las características adicionales. Admite plugins para añadir funcionalidades. Figura 13 Arquitectura Solr Fuente: (Marco, 2013) 3.3.3.4 Herramientas de visualización y análisis Una vez que se tienen todos estos datos en la arquitectura, se utilizan herramientas de visualización y análisis que permiten sacarle el valor a los datos, con los cuales es posible, mediante la aplicación de funciones estadísticas y de visualización, encontrar patrones, tendencias, hacer modelos predictivos, e incluso sistemas de recomendación. La importancia de la visualización de los datos radica en que permite al analista de datos, bien sea un gerente o un científico de datos, facilitarles la comprensión de los datos, de modo que puedan realizar sus funciones con mayor facilidad y puedan a su vez transmitir la información que encontraron en los datos. 63 Algunas herramientas de visualización más importantes podemos mencionar Apache Hue (Hadoop User Experience) y Banano. Entre las herramientas de análisis más conocidas en el mundo del Big Data se encuentran:  RStudio que es más conocida en el ámbito científico por su manejo de lenguaje de análisis R, el cual es muy utilizado por científicos de datos.  Podemos mencionar Apache Mahout, el cual me permite utilizar algoritmos en su librería para hacer máquinas de aprendizaje  Apache Spark el cual se integra con Hadoop y posee un módulo llamado Spark MILIB en el cual también se utiliza para hacer machine learning, además de poseer módulos para hacer consultas en SQL y análisis de grafos, con la peculiaridad de que Spark utiliza la memoria RAM de la máquina, lo que lo hace una herramienta más rápida que utilizar solo Hadoop. 3.4 La Inteligencia de Negocio y su evolución con el tiempo a Big Data Analítica Cada instante disponemos de más y más datos, más y más desestructurados, con más y más fuentes, con más y más usuarios, con menos y menos tiempo y, menos y menos paciencia. Fruto de todo ello es el desarrollo inusitado de los sistemas de inteligencia de negocio. El concepto Business Intelligence, BI, es el uso de datos en una empresa para facilitar la toma de decisiones. Incluye la comprensión del funcionamiento actual de la empresa, con el objetivo de ofrecer conocimientos para respaldar las decisiones empresariales. Basándose en la utilización de un sistema que se forma con los datos extraídos, de los datos de la producción mediante las herramientas y técnicas ETL (extraer, transformar y cargar). Las herramientas de inteligencia analítica facilitan el modelado para crear consultas, informes o cuadros de mando. 64 El concepto de Business Analytics es una evolución del Business Intelligence. En 2009 Michael J. Beller, en su publicación Next Generation Business Analytics lo explica como los conocimientos, tecnologías y prácticas para la investigación y exploración continuamente interactiva del rendimiento del negocio para ganar visión y capacidad de dirección en la planificación del negocio. El BA se focaliza, por tanto, en una nueva visión y comprensión del rendimiento del negocio, basado en datos y métodos estadísticos que ayudan a medir el pasado y guiar el futuro, en contraste con el BI que, tradicionalmente se focaliza en el uso y seguimiento de una seria de métricas o KPI’s. El BA hace extensivo el análisis estadístico, incluyendo también modelos predictivos, con estos modelos predictivos se sabe que está influyendo en la realidad, cuáles son los drivers, y en qué medida influyen, permitiendo, de esta manera, modificar la realidad en la medida que estos drivers sean accionables, se convierten en palancas con las que influir en el futuro del negocio. Se podrá contestar a las preguntas tales como ¿Qué pasó?, ¿Con qué frecuencia?, ¿Cuánto?, ¿Dónde está el problema?, ¿Qué acciones se necesitan?, y además, ¿Por qué pasó?, ¿Cuál es la tendencia?, ¿Qué pasará si continúa esta tendencia?, ¿Qué va a pasar?, ¿Qué es lo mejor que puede pasar? y ¿Cómo? El BA se puede plantear de diferentes maneras, en primer lugar, soportando decisiones humanas con la visualización de modelos que reflejan razones, también como incremento de visión con los datos históricos vía el uso de informes, cuadros de mando, segmentación… En tercer lugar, con BA predictivo usando estadísticas y técnicas machine learning. Finalmente prescribiendo el futuro usando técnicas de optimización, simulación, etc. Para el desarrollo de estos modelos estadísticos es fundamental que los datos con los que se trabaja tengan un volumen y una calidad excelente. En la actualidad, gracias a que el comportamiento de las personas en muchas ocasiones se produce con la intermediación en algún punto de una máquina, los datos que tenemos acerca de sus movimientos son inconmensurables, sin 65 embargo, si hacemos uso de la ciencia de los datos, conseguiremos la capacidad de profundizar en ellos y obtener una ventaja competitiva. El Big Data, BD, tiene la capacidad de revolucionar la toma de decisiones y los modelos de negocio, y brinda a la dirección toda una suerte de posibilidades para hacer más científica la gestión. Basándose en el estudio de datos de series temporales, se pueden conocer todas las variables que influyen en el comportamiento de las personas, tanto de manera estructural como, aquellas otras que lo hacen de forma esporádica, así como aquello que subyace y que nos puede servir para hacer planteamientos de futuro novedosos. Tanto, empresas emergentes y pequeñas, como empresas veteranas y gigantes están haciendo uso ya de toda la información de que disponen, usando como metodología la ciencia de los datos. La aproximación del BA junto con el BD está ya dando resultados reveladores en diversos ámbitos: Behavioral Analytics, Contextual Data Modeling, Cyber Analytics, Enterprise Optimización, Fraud Analytics, Marketing Analytics, Pricing Analytics, Retail Sales Analytics, Risk & Credit Analytics, Suply Chain Analytics, Talent Analytics, etc. Las nuevas herramientas de analítica de datos poco tienen que ver con las que se empleaban hace apenas un par de años. Como ejemplo, valga el informe que justamente hace poco más dos años, en 2013, publicó el IBM Institute for Business Value, una instancia que desde 2009 evalúa el impacto de la analítica de datos y la inteligencia de negocios en el mundo corporativo. Este estudio, una encuesta realizada a 900 cargos ejecutivos y responsables TI de 70 países distintos, a lo largo de dos meses y medio, fue analizado y debatido por un grupo de ejecutivos y gerentes de alto nivel (los componentes del BAO, o Business Analytics and Optimization Advisory Board, órgano también perteneciente a IBM) que logró, entre otras conclusiones, establecer los parámetros guía que regían en esos momentos las principales necesidades analíticas en los negocios. Unas conclusiones que sirvieron para marcar nuevos objetivos en el ámbito del BI, y que han desembocado en el desarrollo de soluciones capaces de afrontar 66 los nuevos retos que afrontan las organizaciones en lo que a analítica de datos se refiere. Así ha surgido una nueva generación de herramientas Big Data Analytics que han dejado atrás los tradicionales (aunque aún jóvenes, al fin y al cabo) recursos analíticos del Business Intelligence. Herramientas actualizadas, mejoradas y enormemente evolucionadas que permiten sacar el máximo partido a los datos y las informaciones a las que tienen acceso hoy las organizaciones, aportando valor añadido a las actividades de negocio que cuentan con ellas. Unas herramientas que aportan un valor objetivo, identificado en torno a 3 ejes principales: - Capacitación: capacitan a las organizaciones para detectar, analizar y aprovechar los datos y las informaciones más relevantes para sus operaciones. - Impulso: son herramientas altamente sofisticadas que proyectan las posibilidades reales de la compañía hacia el futuro, visibilizando e identificando las oportunidades que se abren en todos los escenarios posibles de negocio. - Amplificación: amplifican los valores y las fortalezas (internas y externas) de la actividad empresarial, permitiendo sacar el máximo partido a las nuevas oportunidades de negocio. Las nuevas herramientas de analítica avanzada permiten, al fin y al cabo, definir una estrategia corporativa mucho más realista y, al mismo tiempo, exigente y de alto rendimiento, lograr un mejor establecimiento de objetivos y una consecución más rápida, eficiente y efectiva de los mismos, y aventajar a la competencia con una toma de decisiones apoyada en detallados análisis predictivos. 67 Pues, es de esta manera que podemos concluir en que mientras que Business Intelligence analiza datos consolidados, Big Data es capaz de reforzar la función de B I en un universo más amplio, con más dinamismo y multiplicidad. Por permitir un análisis de una cantidad más grande de datos no estructurados, de mucha importancia para la organización, el Big Data aumenta todavía más la relevancia y la utilidad del Business Intelligence para el negocio. Es así como, Big data, que es considerado una evolución de BI, y de Business Intelligence, pueden ser vistos como complementarios (IBM, 2016). 3.5 Ciencia de Datos La ciencia de datos es un campo interdisciplinario de la ciencia que involucra los procesos y sistemas para extraer conocimiento de grandes volúmenes de datos en sus diferentes formas (estructurados o no estructurados) y formatos (textos, imágenes, documentos, vídeos, entre otros), mediante el uso de la computación, la matemática y la estadística. A pesar de que se puede aplicar ciencia de datos, sin la necesidad de tener una arquitectura Big Data y viceversa, en los últimos años hemos visto cómo es posible combinar ambas de manera armoniosa para solventar problemas, mejorar servicios o incluso prever eventos mediante el uso de modelos predictivos. En la ciencia de datos se involucran 3 principales componentes de acuerdo a Drew Conway los cuales son (Figura 14 Diagrama de Venn Ciencia de datos):  Habilidades Hacking o computacionales  Conocimiento Estadístico y matemático  Conocimientos sustantivos (área en la que se investiga). 68 Los científicos de datos son uno de los mayores beneficiados por el uso del Big Data, ya que de esta manera disponen de mayor cantidad de datos a analizar y de las herramientas necesarias para sus respectivos análisis. Entre las funciones que realizan los científicos de datos, podemos resumirlas en 3 principalmente: (Sánchez, 2014)  Captura de los datos: Captura y almacenamiento de la información. Es el procedimiento manual de convertir “raw data” (información en bruto) en información con formato para que pueda ser analizada.  Análisis de los datos: Obtención de valor a partir de la información. Para lograrlo es necesario realizar procesos de minería de datos como limpieza o transformaciones, luego aplicar funciones estadísticas, experimentar con modelos predictivos y mediante el cálculo de errores observar cuál es el que se adapta mejor al problema a resolver. Figura 14 Diagrama de Venn Ciencia de datos Fuente: (Conway, 2010) 69  Visualización de los datos: Visualización de los resultados obtenidos anteriormente. En muchos casos el científico de datos debe dirigirse con un lenguaje más accesible para explicar los resultados obtenidos, por lo que requiere de habilidad para expresar sus investigaciones de manera clara y entendible apoyándose en el uso de gráficos que puedan ayudarle a comunicar los resultados obtenidos. Hoy en día es posible ver muchos casos de éxito en la aplicación de la ciencia de datos en conjunto con Big Data, bien sea para evitar epidemias, promocionar productos, evitar riesgos financieros, análisis y alertas de datos en tiempo real de pacientes, fraudes en tarjetas de crédito, entre otros. 3.6 Arquitectura de Software Todos los sistemas tienen una arquitectura, es decir, una estructura de alto nivel que define todo el sistema. En la arquitectura de software se observa como todas las piezas que componen el sistema encajan para aportar una solución al problema que se desea resolver el cliente. Podemos definir la Arquitectura de software como el proceso de definición de una solución estructurada que cumple con todos los requisitos técnicos y operativos, al tiempo que optimiza los atributos de calidad comunes, como el rendimiento, la seguridad y la capacidad de gestión. Implica una serie de decisiones basadas en una amplia gama de factores y cada una de estas decisiones puede tener un impacto considerable en la calidad, el rendimiento, la facilidad de mantenimiento y el éxito general de la aplicación. (Microsoft, 2017) 3.6.1 Importancia de la Arquitectura de Software Existen fundamentalmente 3 razones que dan importancia a las arquitecturas de software: 70  Comunicación entre los stakeholders: La arquitectura de software representa un medio de abstracción del sistema, en la cual la mayoría de los stakeholders pueden usar como una base para el entendimiento mutuo, negociar, comunicarse y llegar a consensos.  Primeras decisiones en diseño de sistemas: Las arquitecturas de software representan un primer ajuste en las decisiones de diseño (define restricciones en la implementación, dicta la estructura organizacional, ayuda en la evolución de prototipos, ayuda a una mayor estimación de los costos monetarios y en tiempo)  Abstracción de sistemas transferibles: Proporcionan un modelo de como un sistema está estructurado y de cómo sus elementos interactúan entre sí. Este modelo puede ser transferido entre sistemas. 3.6.2 Componentes en una arquitectura de software La arquitectura de software provee una abstracción de alto nivel del sistema a ser construido, por lo que debe cubrir los siguientes aspectos: (Hanmer, 2013)  Filosofía y objetivo del sistema: La arquitectura explica los objetivos y describe el propósito del sistema, quien lo usa y que problema resuelve.  Suposición arquitectural y dependencias: La arquitectura explica las suposiciones hechas del sistema con su medio ambiente. También explica cualquier dependencia con otro sistema.  Requerimientos significativos de la arquitectura  Instrucciones de los subsistemas y componentes: Explica como los componentes del sistema son desplegados en plataformas computacionales y como se deben combinar para su correcto funcionamiento.  Subsistemas críticos y capas: La arquitectura describe las diferentes vistas y partes del sistema y como se relacionan. También explica en detalle sus subsistemas más críticos.  Interfaces criticas del sistema 71  Escenarios claves que describen el comportamiento del sistema 3.6.3 Estructuras de la arquitectura y vistas Un neurólogo, un cardiólogo y un hematólogo tendrán diferentes vistas de las estructuras del cuerpo humano, cada uno de acuerdo a su respectiva especialidad, lo mismo ocurre en el mundo del software. Los sistemas modernos computacionales son tan complejos que es muy difícil entender todas sus estructuras como un todo, por lo que se diseñan y estudian modularmente. Las estructuras arquitecturales pueden dividirse en 3 grupos principales, dependiendo de la naturaleza de elementos que muestren:  Estructuras modulares: Aquí los elementos son módulos, que son unidades de representación, los cuales se le asignan responsabilidades funcionales. Nos permiten responder preguntas tales como: ¿Cuál es la función principal asignada a cada módulo? ¿Qué elementos de software puede usar cada módulo? ¿Qué módulos están relacionados con otros módulos por especialización o generalización?  Estructuras de componentes y conectores: Aquí los elementos son componentes de ejecución y sus conectores. Estas estructuras nos permiten responder preguntas tales como: ¿Cuáles son los mayores componentes de ejecución y cómo interactúan? ¿Qué partes del sistema están replicadas? ¿Cómo la data progresa a través del sistema? ¿Cuáles son los mayores almacenes de datos compartidos?  Estructuras de asignación: Las estructuras de asignación muestran cuales son las relaciones entre los elementos de software y los elementos en uno o más ambientes externos en los que el software es creado y ejecutado. Nos permite responder preguntas como ¿En qué procesador se ejecuta cada elemento de software? ¿En qué archivos se almacena cada elemento durante su desarrollo, prueba y creación del sistema? ¿Cuáles son los elementos de software asignados a cada equipo de desarrollo? 72 Estas tres estructuras corresponden a tres tipos de decisiones que envuelven el diseño de la arquitectura, como son:  ¿Cómo se estructura el sistema como un conjunto de unidades de código? (módulos)  ¿Cómo se estructura el sistema como un conjunto de elementos que tienen un comportamiento de ejecución (componentes) y sus interacciones (conectores)?  ¿Cómo el sistema se relaciona con estructuras que no son de software en su medio ambiente? (CPU, archivos, equipos de desarrollo, entre otros). 3.6.3.1 Estructuras de software Algunas de las estructuras de software más comunes se muestran en la siguiente figura (ver Figura 15 Estructuras de software) Figura 15 Estructuras de software Fuente: (Microsoft, 2017) 3.6.3.1.1 Módulos Las estructuras modulares incluyen lo siguiente:  Descomposición. Las unidades son módulos relacionados entre sí por la relación “es submodulo de”, mostrando como módulos grandes son 73 descompuestos en unos más pequeños recursivamente, hasta que son los suficientemente pequeños para ser fácilmente comprendidos.  Usos. Las unidades de estas estructuras también son módulos, o procedimientos o recursos en las interfaces de los mismos. Las unidades son relacionadas por una relación de “uso”. Una unidad utiliza otra si la primera necesita la presencia de la segunda. Estas estructuras son utilizadas para diseñar sistemas que puedan ampliarse fácilmente para añadir funcionalidades o de la que subconjuntos funcionales útiles se puedan extraer fácilmente.  Capas. Cuando las relaciones de usos en esta estructura son controladas de forma particular, un sistema de capas emerge, en el cual una capa es un conjunto de funcionalidades relacionadas.  Clases, o generalización. Las unidades de módulo de estas estructuras, son llamadas clases. La relación es “hereda de” o “es una instancia de”. Esta vista permite el razonamiento de colecciones con el mismo comportamiento o capacidad y parametrizar diferencias que son capturadas por subclases. 3.6.3.1.2 Componentes y conectores Estas estructuras incluyen lo siguiente:  Procesos. Las unidades aquí son procesos o hilos que están conectados entre sí por la comunicación, sincronización, y/o exclusión de operaciones. Las estructuras de procesos ayudan a diseñar en los sistemas el rendimiento de ejecución y disponibilidad.  Concurrencia. Estas estructuras ayudan a los arquitectos a determinar oportunidades de paralelismo y la locación donde la contención de recursos pueda ocurrir. Las unidades son componentes y los conectores son “hilos lógicos”  Datos compartidos o repositorios. Esta estructura incluye componentes y conectores que crean, almacenan y acceden a data persistente. Muestra como la data es producida y consumida por 74 elementos de software en tiempo de ejecución y puede ser usado para asegurar un buen rendimiento e integridad de la data.  Cliente-Servidor. Los componentes son clientes-servidores y lo conectores son los protocolos que comparten para el pase de mensajes. Si el sistema está construido como un grupo de clientes y servidores que cooperan entre sí, esta es una buena estructura de componentes y servidores a ser utilizada. 3.6.3.1.3 Asignación Las estructuras de asignación incluyen lo siguiente:  Despliegue. Las estructuras de despliegue muestran cómo se asigna el software a los elementos de hardware de procesamiento y de comunicación. Esta vista permite al ingeniero razonar acerca del rendimiento, integridad de la data, disponibilidad y seguridad. Es de un interés particular en sistemas distribuidos o paralelos.  Implementación. Muestran como las estructuras de software (usualmente módulos) son asignados a las estructuras de archivos en el desarrollo del sistema.  Asignación de trabajo. Estas estructuras asignan las responsabilidades a los equipos de trabajo apropiados para implementar e integrar los módulos. 3.6.3.2 Relaciones entre estructuras  Elementos de una estructura se relacionan con elementos de otras estructuras.  A menudo la estructura dominante es la descomposición de módulos, ya que genera la estructura del proyecto.  Las estructuras proporcionan una poderosa separación de problemas para la creación de la arquitectura.  Las estructuras son la base de la documentación de la arquitectura. 75 3.6.3.3 Escogencia de la estructura En 1995, Philippe Krutchen público una investigación, describiendo el concepto de arquitecturas que comprende estructuras separadas y aconsejando concentrarse en cuatro. Las cuales son:  Lógica. Asigna el sistema en clases y componentes, se enfoca en las partes del sistema que proveen una funcionalidad y que los usuarios verán cuando interactúen con el sistema.  Procesos. Explica como las partes del sistema trabajan en conjunto y como se mantienen en sincronización. También explica como el sistema asigna las unidades computacionales, como son los procesos e hilos.  Desarrollo. Explica cómo se gestionará el software durante el desarrollo  Física. Explica como el software que implementa el sistema es asignado en las plataformas computacionales. 3.6.4 Métodos y procesos de desarrollo de software El desarrollo de software puede ser hacerse de muchas maneras. Estas maneras distintas son llamadas métodos o procesos, las cuales pueden ser:  Método cascado: En los métodos cascada, las diferentes fases de las actividades de desarrollo del sistema se siguen de forma secuencial.  Proceso unificado: Es un proceso popular en el que varias actividades, tales como generación de requerimientos, desarrollo y pruebas se superponen. En lugar de estar asociados a determinados productos y las tareas que los crean, las fases en el proceso unificado siguen la vida del producto, desde el inicio hasta la elaboración de su construcción y, finalmente, la transición.  Métodos agiles: Los métodos agiles son consecuencia del manifiesto ágil (Manifesto, 2001) el cual declara (entre otras cosas) que hay más valor 76 en el software trabajado que en la documentación generada por métodos como la cascada o el proceso unificado. 3.7 Análisis de arquitecturas Big Data Existen diferentes propuestas de arquitecturas en el mercado, las cuales fueron diseñadas por empresas como IBM, DataStax, Cloudera, Hortonworks. A continuación, se mostrarán las arquitecturas propuestas por algunas de las compañías antes mencionadas. 3.7.1 Propuesta de IBM: La propuesta de IBM se compone principalmente de dos módulos, los cuales son el módulo de aplicaciones analíticas y la plataforma de Big Data. También señalan que toda solución Big Data se compone de 4 capas, la capa de fuente de datos, la capa de almacenamiento, la capa de análisis y la capa de consumo En el módulo de aplicaciones analíticas encontramos las aplicaciones que se encargan de hacer reportes de inteligencia de negocio, la visualización, análisis predictivo, aplicaciones funcionales de la empresa, entre otras. Luego al descomponer el módulo de la plataforma de Big Data, se puede encontrar varios sub módulos, entre los cuales se encuentran en una primera capa, los sistemas de visualización junto con el desarrollo de aplicaciones y gestionado de sistemas. En la siguiente capa encontramos aceleradores, para aumentar la velocidad de las consultas o queries. Luego encontramos en una capa inferior el ecosistema Hadoop, en el cual está el HDFS, en el mismo nivel se encuentran los sistemas de transmisión y los almacenes de datos, que puede ser uno o varios, bien sean de modelos relacionales o de alguna base de datos NoSQL. Luego encontramos un nivel de integración de información, en el cual se integran todas las fuentes de datos de la empresa. 77 Figura 16 Arquitectura Big Data IBM 3.7.2 Propuesta de Cloudera: La propuesta de Cloudera consiste en diseñar la arquitectura planeando primero el clúster, en el cual se decide el tipo de hardware que se utilizara. Luego se planifica la ingesta de los datos al sistema, bien sea si el consumo será por lotes o por eventos. Después viene el estacionamiento el cual es el Hadoop. Se procede a planificar el procesamiento de los datos, en el cual se hace la transformación y el análisis utilizando MapReduce o Apache Spark. Por último, se toma en consideración las herramientas de análisis BI o Inteligencia de Negocios y buscadores como Solr, orientado a todo tipo de usuarios del sistema. Por último, viene el pipeline de datos para integrar todos los componentes antes mencionados. 78 Figura 17 Arquitectura Big Data Cloudera 3.7.3 Propuesta de Hortonworks: En la propuesta de Hortonworks se puede observar que utilizan un patrón parecido a las otras propuestas, en la cual se encuentran 3 capas principales, las cuales se componen de varios módulos. Tenemos la capa de fuente de datos, en la cual se obtienen todos los datos estructurados o no estructurados de diversas fuentes y formatos. Luego se observa en la capa del sistema de datos, en la cual tenemos el módulo de datos en memoria, que se comunica con el almacén de datos de la empresa, que a su vez se comunica con el ecosistema Hadoop. Al mismo tiempo esta capa antes mencionada se comunica con la capa de análisis, en la cual tenemos los datamarts, el análisis del negocio con los reportes y la visualización o dashboards para tomar las decisiones del negocio. 79 Figura 18 Arquitectura Big Data Hortonworks 3.7.4 Propuesta de Datastax y Hortonworks: En la propuesta de Datastax y Hortonworks es parecida a la de Hortonworks, solo que se sustituye el módulo de almacén de datos por la plataforma de base de datos NoSQL de Datastax, el cual consiste en un clúster de Cassandra. Además de integrar los sistemas de Datastax DevCenter y OpsCenter. 80 Figura 19 Arquitectura Big Data Datastax-Hortonworks 3.8 Diseño de una Arquitectura de Software Los pasos para realizar un diseño de arquitectura de software, se describen a continuación:  Arquitectura en el ciclo de vida  Diseñando la arquitectura  Formando la estructura del equipo y su relación con la arquitectura  Creación de un esqueleto del sistema 3.8.1 Arquitectura en el ciclo de vida Cualquier organización que adopte una arquitectura como fundamento para sus procesos de desarrollo de software, necesita entender su lugar en el ciclo vital. Existen muchos modelos de ciclo vital, pero uno que pone la arquitectura como tema central es el modelo de Entrega Evolutiva o Evolutionary Delivery Life Cycle 81 (ver Figura 20 Ciclo de Vida de entrega evolutiva). El objetivo de este modelo es involucrar al usuario y clientes en el desarrollo, obteniendo retroalimentación de ellos e iterar entre varios lanzamientos hasta generar un lanzamiento final del producto. Figura 20 Ciclo de Vida de entrega evolutiva Fuente: (Microsoft, 2017) Para saber cuándo comenzar a diseñar la arquitectura es necesario conocer los requerimientos del sistema de antemano. Es necesario identificar los principales objetivos del negocio, luego volverlos en escenarios de calidad o casos de usos. Al hacer esto se escogen aquellos que tendrán un mayor impacto en la arquitectura. 3.8.2 Diseñando la arquitectura En esta sección se explicará un método de diseño de arquitectura para satisfacer los requerimientos funcionales y de calidad. Este método es conocido como Atribute-Driven Design (ADD). 82 ADD es una aproximación para definir una arquitectura de software que basa su proceso de descomposición en los atributos de calidad que el software debe satisfacer. Este proceso de descomposición es recursivo y es un método top- down, donde en cada etapa se escogen tácticas y patrones de arquitectura para satisfacer un conjunto de escenarios de calidad. La salida que genera ADD es la primera de varios niveles de vistas de descomposición de una arquitectura. No todos los detalles de las vistas provienen de aplicar el modelo ADD; el sistema es descrito como un conjunto de contenedores de funcionalidades e interacciones entre ellos. Esta es la primera articulación de la arquitectura durante el proceso de diseño y, por tanto, es necesariamente grano grueso. Sin embargo, es fundamental para el logro de las cualidades deseadas, y proporciona un marco para lograr la funcionalidad. La diferencia entre una arquitectura que resulta de aplicar ADD y una lista para su implementación está en las decisiones de diseño más detalladas que se deben hacer. 3.8.2.1 Entrada del ADD La entrada de ADD es un conjunto de requerimientos. ADD asume requisitos funcionales (típicamente expresados como casos de uso) y restricciones como entrada, así como otros métodos de diseño. Éste método necesita que los requerimientos de calidad sean expresados como un conjunto de escenarios de calidad de especificaciones del sistema, los cuales deben ser definidos con el detalle necesario para la aplicación. Como escenarios de calidad para una puerta de garaje podríamos incluir los siguientes: 83  Los dispositivos y controles para abrir y cerrar la puerta son diferentes por la variedad de productos en la línea de productos.  El procesador utilizado en diferentes productos diferirá. La arquitectura del producto para cada procesador en específico debe ser directamente derivado de la arquitectura de la línea de productos.  Si un obstáculo (persona u objeto) es detectado por la puerta del garaje durante su cierre, debe detenerse y reabrirse en 0.1 segundos.  La puerta del garaje debe ser accesible para recibir diagnósticos y administración de un sistema de información del hogar, usando un protocolo de diagnóstico para productos. 3.8.2.2 Comenzando el ADD ADD depende de la identificación de los drivers de la arquitectura, los cuales como se mencionó anteriormente son los escenarios de calidad que tengan mayor impacto en la arquitectura. Durante el diseño, el determinar que drivers de arquitectura son clave puede cambiar, ya sea como resultado de un mejor entendimiento de los requerimientos o por cambio en los mismos. Aun así, el proceso puede comenzar cuando los requerimientos de los drivers arquitecturales son conocidos con cierta garantía. 3.8.2.3 Pasos del ADD A continuación, se mencionan brevemente los pasos para diseñar una arquitectura utilizando el método ADD:  Escoger el modulo a descomponer: El modulo para empezar usualmente es el sistema completo. Todas las entradas requeridas para este módulo deben estar disponibles (restricciones, requerimientos funcionales, requerimientos de calidad).  Refinar el módulo de acuerdo a los siguientes pasos: 84 o Escoger los drivers de la arquitectura del conjunto de escenarios de calidad y requerimientos funcionales. Este paso determina que es lo más importante para la descomposición. o Escoger un patrón arquitectural que satisfaga los drivers de la arquitectura. Crea (o selecciona) el patrón basado en las tácticas que pueden ser usadas para lograr los drivers arquitecturales. Identificar los módulos secundarios necesarios para poner en práctica las tácticas. o Instanciar los módulos y asignar la funcionalidad de los casos de usos representándolos usando múltiples puntos de vista. o Definir las interfaces de los módulos secundarios. La descomposición proporciona módulos y restricciones en los tipos de interacción de los módulos. Documenta esta información en la interfaz de documentación para cada módulo. o Verificar y refinar los casos de uso y los escenarios de calidad y convertirlos en restricciones para los módulos secundarios. En este paso se verifica que nada importante fue olvidado y prepara los módulos secundarios para su futura descomposición o implementación.  Repetir los pasos anteriores para cada módulo que necesite ser descompuesto. 3.8.3 Formando la estructura del equipo y su relación con la arquitectura Una vez que los primeros niveles de la descomposición de los módulos de la arquitectura son bastante estables, estos módulos pueden ser asignados a los equipos de desarrollo. Una vez que se ha acordado con una arquitectura para el sistema en construcción, los equipos de trabajo son asignados a cada módulo principal. Cada equipo entonces crea sus propias prácticas de trabajo internas. Además, los procedimientos de calidad y de prueba se establecen para cada equipo y 85 cada equipo debe establecer sus enlaces de comunicación y coordinación con otros grupos. Reconocer los módulos como mini dominios inmediatamente sugiere que el modo más efectivo de utilizar al personal es el de asignar miembros a los equipos de acuerdo a su experticia. Por ejemplo, un experto en base de datos vera el problema desde su punto de vista (almacenamiento de la data, persistencia, replicación), en cambio un experto en telecomunicaciones vera el sistema en términos de telecomunicaciones. 3.8.4 Creación de un esqueleto del sistema Una vez que la arquitectura está suficientemente diseñada y los equipos de trabajo están posicionados para comenzar su construcción, el esqueleto del sistema puede ser construido. La idea en esta etapa es proporcionar una capacidad subyacente para implementar la funcionalidad de un sistema en un orden ventajoso para el proyecto. Las practicas clásicas de la ingeniería del software recomiendan el método stub (Wikipedia, Stub Method, 2016) para que así secciones del sistema puedan ser agregados de forma separada y ser probadas independientemente. Sin embargo, ¿qué secciones de código deben aplicársele el método stub? Mediante el uso de la arquitectura como guía, se hace clara una secuencia para su implementación. En primer lugar, implementar el software que se ocupa de la ejecución y la interacción de los componentes arquitectónicos. Luego se pueden escoger cuales componentes proveedores de funcionalidad deben ser agregados al sistema, ya sea basado en disminuir el riesgo direccionando las áreas más problemáticas primero o basado en los tipos de personal disponible, o puede estar basado en sacar algo útil al mercado lo más pronto posible. 86 Una vez que se han elegido los elementos que proporcionan el siguiente incremento de la funcionalidad, se puede emplear las estructuras de usos que digan cual software adicional debe estar funcionando correctamente en el sistema para apoyar esa funcionalidad. Este proceso continúa creciendo y creciendo en los incrementos de funcionalidades del sistema, hasta que todo está en su lugar. En ningún momento la integración y las pruebas se hacen abrumadoras ya que en cada incremento se hace fácil encontrar cualquier falla. 87 Capítulo 4: Marco Metodológico 4.1 Bases metodológicas de la investigación La metodología hace referencia al conjunto de procedimientos basados en principios lógicos, utilizados para alcanzar una gama de objetivos que rigen en una investigación científica. (Eyssautier de la Mora, 2006). 4.1.1 Tipo de investigación Tomando en cuenta el problema planteado, este Trabajo Especial de Grado se define del tipo Proyecto Factible, que según UPEL (2003): “Consiste en la elaboración y desarrollo de una propuesta de un modelo operativo viable para solucionar problemas, requerimientos o necesidades de organizaciones o grupos sociales; puede referirse a la formulación de políticas, programas, tecnologías, métodos o procesos. El proyecto debe tener apoyo en una investigación de tipo documental, de campo o un diseño que incluya ambas modalidades” (p. 16). El proyecto factible conforma un proceso de planificación en el cual la investigación es una etapa que le proporciona información para sustentar la propuesta. Tomando en cuenta las diversas concepciones, el proyecto factible se desarrolla a través de las siguientes etapas (Moya, 2002):  El diagnóstico de las necesidades, el cual puede basarse en una investigación de campo o en una investigación documental. Las necesidades del presente trabajo fueron obtenidas al interactuar y dialogar con los stakeholders de la Misión Mercal C.A., además de realizar investigación bibliográfica de casos de uso similares.  Planteamiento y fundamentación teórica de la propuesta. El cual se encuentra en el marco teórico del presente trabajo. 88  El procedimiento metodológico. En este trabajo se aplicó la metodología Atribute Driven Design (ADD).  Las actividades y recursos necesarios para su ejecución. En el presente trabajo, por cada objetivo específico se realizaron una serie de actividades como son: (a) La investigación bibliográfica y de casos de uso de las diferentes propuestas de arquitecturas analíticas para grandes volúmenes de datos, en las cuales se encontraron patrones para solucionar problemas comunes; (b) Para el diseño de la arquitectura se procedió a aplicar la metodología Atribute Driven Design (ADD); (c) Para la selección de las herramientas asociadas a cada componente de la arquitectura diseñada se utilizaron matrices de evaluación en base a la metodología Desmet (Kitchenham, 1996); Para implementar la arquitectura seleccionada, fue necesario el diseño de los mecanismos de Extracción, Transformación y Carga (ETL), además se desplego y probo la arquitectura en ambientes de desarrollo, calidad y producción en la sede de Misión Mercal C.A. de Altamira.  El análisis de viabilidad o factibilidad del proyecto. Se contó con el apoyo de los encargados de la oficina de tecnología de información de la Misión Mercal C.A. y de los asesores de Phd 2014 Consultores CA, quienes, a través del juicio de experto, confirmaron la viabilidad y factibilidad del proyecto.  En el caso de que se tenga que desarrollar el proyecto factible, es necesario indicar la ejecución de la propuesta y la evaluación tanto del proceso como de sus resultados. Para la ejecución del presente trabajo se contó con un ambiente de desarrollo y un ambiente de calidad, los cuales fueron desplegados en la oficina de la Misión Mercal C.A. ubicada en Altamira. En dichos ambientes se instalaron y configuraron todos los componentes de la arquitectura de acuerdo a las herramientas seleccionadas a través de las matrices de evaluación en base a la metodología Desmet. Se desarrollaron los mecanismos de Extracción, Transformación y Carga (ETL). Se diseñaron los diversos modelos de datos para cada uno de los módulos correspondientes a la arquitectura desarrollada y se realizaron las pruebas respectivas. 89 4.1.2 Población y Muestra La población de este Trabajo Especial de Grado está representada por todos los Registros asociados a Inventario, Ventas, Proveedores y Clientes de cada una de las tiendas en Venezuela de la Misión Mercal C.A., que están conformadas por un aproximado de 300 a nivel nacional. Tomando en cuenta que la investigación pretende desarrollar una Arquitectura Analítica de Big Data, la muestra establecida fueron datos entregados por el cliente para hacer las pruebas, esta data fue solicitada para aplicarla en ambiente de desarrollo y no tiene implicaciones de confidencialidad. 4.1.3 Técnicas e Instrumentos de Recolección de Datos Las técnicas de recolección de datos son las distintas formas o maneras de obtener la información (Arias, 1999). Las técnicas utilizadas para la recolección de datos fueron:  Revisión Bibliográfica: en ella se acude especialmente a revistas científicas, informes y monografías, medios de comunicación que reflejan con más dinamismo que los libros los adelantos que se producen. (Sabino, 1992). La técnica consiste en recopilar y revisar todos aquellos documentos que permiten confrontar el aspecto teórico con la situación real o práctica dentro del diseño e implementación de arquitecturas Analíticas de Big Data en el mercado moderno. La revisión de los estudios previos nos permitirá (Pedraz, 2004): o Ahondar en la explicación de las razones por las que hemos elegido dicho tema de investigación 90 o Conocer el estado actual del tema: qué se sabe, qué aspectos quedan por investigar o Identificar el marco de referencia, las definiciones conceptuales y operativas de las variables estudiadas o Descubrir los métodos para la recolección y análisis de los datos utilizados o Contar con elementos para la discusión, donde se compararán los resultados que obtengamos con los de los estudios previos  Fuentes Infográficas: consiste en recopilar información a través de fuentes en línea tales como webinars, foros, páginas web. 4.2 Metodología de Desarrollo Se utilizó para el desarrollo de la Arquitectura Analítica Big Data la metodología Atribute-Driven Design (ADD), explicada previamente en el Marco Teórico. 91 Capítulo 5: Marco Aplicativo A continuación, se describe el diseño y desarrollo de la Arquitectura Analítica Big Data para el cálculo de indicadores de la Misión Mercal C.A., así como también las pruebas realizadas. Se explicarán los entregables del Trabajo Especial de Grado que se obtuvieron al aplicar la Metodología de Atribute Driven Design (ADD). 5.1 Entrada del ADD En la entrada de este método para diseño de arquitecturas de software, se tienen 3 tipos principalmente, los cuales son:  Requerimientos funcionales del Sistema  Restricciones de diseño  Requerimientos de calidad Las entradas deben ser obtenidas de los expertos del sistema, por lo que se involucró a las personas que estarían afectadas con la implementación del sistema. En nuestro caso, los directores de tecnología de la Misión Mercal C.A., técnicos del sistema y usuarios. Se obtuvieron las siguientes entradas: 5.1.1 Requerimientos funcionales  El Sistema debe permitir al usuario almacenar las transacciones diarias que se llevan a cabo en las Fuentes Logístico, Saf y Siga ya destacados.  El Sistema debe integrarse con los diversos sistemas Logístico, Saf y Siga, debido a que representan las fuentes de datos para el mismo.  El Sistema debe ser capaz de facilitar la visualización de los datos para la generación de reportes. 92  El Sistema debe garantizar el almacenamiento histórico de todas las transacciones fuentes.  El Sistema debe permitir realizar búsquedas rápidas al usuario.  El Sistema debe permitir integrar los datos de todas las oficinas de la Misión Mercal a nivel nacional.  El Sistema debe mantener su rendimiento aun cuando se maneje un gran volumen de datos. 5.1.2 Restricciones de diseño  El sistema debe cargar datos de la Base de datos Oracle 10g a un Clúster Hadoop/Hive.  Se debe utilizar Apache Hadoop como estacionamiento para operaciones de MapReduce, debido a que manejará un gran volumen de datos.  El Sistema debe correr en un ambiente GNU/Linux.  Los datos deben ser replicados en cada nodo del Clúster Hadoop/Hive  Los datos deben ser indexados con el fin de optimizar las consultas atreves de del motor de búsqueda SolR.  Las consultas del sistema, deben poder ser accedidas mediante una interfaz web en nuestro particular Hue. 5.1.3 Requerimientos de calidad Entre los requerimientos principales de calidad, de acuerdo a la norma ISO 9126 en su primera parte, la Arquitectura Analítica de Big Data propuesta para la Misión Mercal C.A., debe cumplir con características como funcionalidad, fiabilidad, usabilidad, eficiencia, mantenibilidad, portabilidad, calidad en uso. De estas características, los stakeholders seleccionaron las siguientes: Tabla 3 Seguridad Estímulo Un usuario desea acceder a los datos del sistema 93 Fuente de Estímulo Algún usuario intentando acceder o alguna otra persona ajena a la empresa. Ambiente El usuario intenta acceder a la plataforma del sistema mediante una autenticación. Artefacto Clúster Hadoop/Hive. Respuesta Se solicita al usuario que ingrese una contraseña, la cual previamente fue creada siguiendo normas de seguridad Medida de la Respuesta La contraseña es alfanumérica y con más de 7 dígitos. Tabla 4 Escalabilidad Estimulo El sistema procesa y almacena cada vez más datos Fuente de Estimulo Datos provenientes de las transacciones diarias de las más de 300 oficinas a nivel nacional. Ambiente Los datos son enviados al clúster HDFS mediante procesos de ETL. Los cuales se cargan desde Oracle 10g a un Clúster en Hadoop/Hive. Artefacto Clúster HDFS Hadoop/Hive. Respuesta El sistema debe replicar los datos, en diferentes nodos, de modo que los servidores no lleguen a su máxima capacidad. La escritura debe ser rápida. 94 Se debe poder almacenar gran cantidad de datos sin perder significativamente el rendimiento del sistema. Medida de la Respuesta Las consultas de los datos, no deben tardar más de 2 minutos si revisa más de 50 millones de registros, dependiendo de la cantidad de datos. El sistema debe aceptar la escalabilidad horizontal. Tabla 5 Capacidad de pruebas Estimulo El Sistema debe ser probado antes de pasar a producción. Fuente de Estimulo Stakeholders deben cerciorar que el Sistema cumple sus requerimientos de negocio. Ambiente Ambiente de desarrollo, el cual fue aportado por la empresa Phd 2014 Consultores C.A. Artefacto Ambiente GNU/Linux Respuesta En el sistema se cargarán los datos de los diferentes modelos relacionales. Para la carga, solo se consideran los campos de relevancia en el cálculo de las métricas. Medida de la Respuesta Se debe verificar que la cantidad de registros que se almacenan en el Clúster de Hadoop/Hive, concuerda 95 con la cantidad de registros en las tablas del modelo relacional. Tabla 6 Tolerancia a fallos Estimulo En el sistema falla alguno de sus componentes o Nodos del Clúster. Fuente de Estimulo Ocurre una falla al hacer una consulta a un dato o indicador almacenado en algún Nodo. Reinicio del Sistema, Apagón eléctrico, catástrofe ambiental. Ambiente Ocurre una falla al hacer una consulta a un dato o indicador almacenado en algún Nodo. Reinicio del Sistema, Apagón eléctrico, catástrofe ambiental. Artefacto Clúster Hadoop/Hive u otro componente de la arquitectura. Respuesta El sistema debe ser capaz de responder a las solicitudes de consulta mediante algún respaldo de los datos e indicadores. Medida de la Respuesta El Sistema replica los datos e indicadores, de manera que, si algún nodo de almacenamiento falla, otro pueda responder en su lugar. Por lo que el sistema debe responder a la consulta de datos siempre. 96 Tabla 7 Visualización de datos Estimulo El usuario desea visualizar datos históricos de las transacciones e indicadores. Fuente de Estimulo Generación de reportes para tomar decisiones del negocio Ambiente Interfaz web Artefacto Componente de visualización o motor de búsqueda especializado en visualizar datos Hue/SolR. Respuesta Se debe permitir al usuario visualizar los datos por medio de dashboards y gráficos. Medida de la Respuesta Acceder a los datos y generar gráficos de los datos existentes en el almacén de datos. Tabla 8 Análisis de datos Estimulo Disponer de datos históricos para realizar consultas y generar reportes Fuente de Estimulo Los stakeholders desean generar reportes de sus transacciones. Ambiente Almacén de datos en HDFS Hive. Artefacto Estacionamiento BI, el cual será Apache Hadoop/Hive por su capacidad de hacer operaciones MapReduce. Respuesta Almacenamiento histórico de la data, mediante la creación de cubos. Medida de la Respuesta Los datos se almacenarán en HDFS y luego se crean los cubos utilizando 97 motores de consultas para analizar los datos y generar los reportes. 5.2 Primera iteración de la metodología ADD 5.2.1 Paso 1: Confirmar que haya suficiente información de los requerimientos Verificamos que los requerimientos proporcionados son suficientes. Los requerimientos mostrados en la sección anterior son efectivamente suficientes para comenzar el diseño de la Arquitectura. El levantamiento de requerimientos fue llevado a cabo con miembros de la empresa afectada por el diseño de la nueva arquitectura en reuniones planteadas. 5.2.2 Paso 2: Escoger un elemento del sistema a descomponer En este paso, se descompuso el sistema completo. 5.2.3 Paso 3: Identificar los drivers de la arquitectura En este paso se analizaron los escenarios y su importancia en la arquitectura. Tabla 9 Drivers de la Arquitectura Fuente: Elaboración Propia # Drivers de la arquitectura Importancia Dificultad 1 Velocidad Alta Mediana 2 Volumen Alta Mediana 3 Variedad Alta Mediana 4 Consulta de datos sobre el estacionamiento HDFS Alta Alta 5 Tolerancia a fallos Alta Mediana 6 Generación de reportes Alta Alta 98  El primer driver de la arquitectura es la Velocidad, lo cual quiere decir que las consultas realizadas a un conjunto de datos no deben tardar más de 60 segundos en responder. El sistema debe poder realizar consultas a gran cantidad de datos tomando menos de 60 segundos.  El segundo Driver a analizar es el Volumen, lo cual significa que el sistema debe ser escalable debido a la enorme cantidad de datos transaccionales provenientes de los diversos sistemas de las oficinas a nivel nacional.  El tercer driver es la Variedad, debido a que las fuentes contienen muchos datos en sus tablas en NULL, es necesario que el esquema sea flexible y acepte una amplia variedad de datos (Formatos).  El cuarto driver es Consulta de datos sobre el estacionamiento HDFS, debido a que, para la Misión Mercal C.A., es de gran importancia poder realizar nuevas consultas sobre los datos almacenados con el fin de poder generar reportes propios y personalizados a mediano y largo plazo.  El quinto driver es la tolerancia a Fallos. Usualmente en Big Data se utiliza commodity hardware el cual tiene un alto riesgo a fallar, por lo cual deben existir mecanismos para evitar perder datos.  El sexto driver es la generación de reportes, lo cual significa que en la empresa Misión Mercal C.A es de primordial importancia que sus plataformas permitan la generación de reportes y dashboards, mediante la utilización de herramientas de inteligencia de negocio para tomar acciones acertadas. 5.2.4 Paso 4: Escoger un patrón que satisfaga los drivers de la arquitectura Este es el primer paso de diseño del método. 5.2.4.1 Aspectos de diseño asociados a los drivers de la arquitectura En el sistema se distinguen los siguientes aspectos de diseño por driver:  Velocidad: Se generan muchos datos en un corto periodo de tiempo. Los datos se generan principalmente de fuentes de datos relacionales. Las consultas no deben consumir largos periodos de tiempo. 99  Volumen: La cantidad de datos crece exponencialmente con el tiempo. El procesamiento y almacenamiento de los datos debe hacerse a gran escala. El desempeño del sistema no debe decaer significativamente.  Variabilidad: Existen datos de diferentes formatos y pueden ser no estructurados.  Consulta de datos sobre el estacionamiento HDFS: Las posibles indicadores a mediano y largo plazo forman una parte importante del sistema, por lo que este, debe ser capaz de almacenar adecuadamente los datos de mayor relevancia que vienen desde las diversas bases de datos relacionales.  Tolerancia a fallos: Los datos pueden perderse debido a fallas en el hardware, por lo que el sistema debe ser capaz de replicar los datos a otros nodos.  Generación de reportes: Para la generación de reportes es necesario que se permita acceder directamente a los datos para realizar consultas y también el uso de herramientas de generación de dashboards o inteligencia de negocio que sean compatibles con la plataforma. 5.2.4.2 Lista de patrones por cada aspecto de diseño Los patrones mostrados a continuación son específicos de arquitecturas Analíticas Big Data, los cuales son aplicados dependiendo del caso de uso y la problemática a solventar. (Buhler, Erl, & Khattak, 2015) Se generan muchos datos en un corto periodo de tiempo:  Alto volumen de almacenamiento binario Los datos se generan principalmente de fuentes de datos relacionales.  Fuente Relacional La cantidad de datos crece exponencialmente con el tiempo: 100  Alto volumen de almacenamiento binario  Fuente Relacional  Procesamiento Batch a gran escala El procesamiento y almacenamiento de los datos debe hacerse a gran escala:  Procesamiento Batch a gran escala  Depuración del conjunto de datos El desempeño del sistema no debe decaer significativamente:  Fragmentación de datos automático  Replicación y reconstrucción de los datos automático  Gobierno de conjunto de datos centralizada Existen datos de diferentes formatos y pueden ser no estructurados:  Alto volumen de almacenamiento binario Los datos pueden perderse debido a fallas en el hardware:  Replicación y reconstrucción de los datos automático  Fragmentación de datos automático Acceder directamente a los datos y herramientas de inteligencia de negocios compatibles con la plataforma:  Acceso directo a los datos  Acceso indirecto a los datos  Procesamiento Batch a gran escala 101 5.2.4.3 Selección de los patrones Para hacer la tabla más sencilla se le asignó a cada driver un número: 1. Velocidad 2. Volumen 3. Consulta de datos sobre el estacionamiento HDFS 4. Tolerancia a fallos 5. Variabilidad 6. Generación de reportes Por cada driver de la arquitectura se tienen los siguientes patrones: Tabla 10 Patrones vs Drivers Fuente: Elaboración Propia Drivers Patrones 1 2 3 4 5 6 Alto volumen de almacenamiento binario Si Si Si No Si No Fuente Relacional Si Si No No No No Procesamiento Batch a gran escala Si Si No No No Si Fragmentación de datos automático Si Si No Si No No 102 Replicación y reconstrucción de los datos automático No Si No Si No No Gobierno de conjunto de datos centralizada No Si No No No No Acceso directo a los datos No No No No No Si Acceso indirecto a los datos No No No No No Si A continuación, se explicarán brevemente los patrones seleccionados los cuales no son mutuamente excluyente, por lo tanto, es posible aplicar varios patrones a la vez sin ningún inconveniente. En muchos casos los patrones hacen uso de los mismos elementos o mecanismos:  Alto volumen de almacenamiento binario: Este patrón fue escogido debido a la presencia de datos de tipo blob o binarios almacenados en Oracle.  Problema: El almacenamiento de grandes cantidades de datos no estructurados en tecnologías de bases de datos tradicionales, no solo incurre en penalización del rendimiento, también sufre de problemas de escalabilidad cuando la cantidad de datos aumenta.  Solución: Los datos no estructurados se almacenan en base a una simple técnica de almacenamiento basado en clúster que implementa el acceso a las bases de datos a través de llaves o claves primarias.  Aplicación: Se utiliza un almacén de datos HDFS que se encarga de almacenar los datos binarios con una clave de identificación única, de manera que cada unidad de datos pueda ser sustituida, eliminada, encontrada o recuperada de forma individual. 103  Mecanismos: Mecanismo de almacenamiento y motor de serialización.  Fuente Relacional: Patrón utilizado debido a que los datos de la Misión Mercal C.A. se encuentran en bases de datos relacionales de licenciamiento privado, como es Oracle. Este patrón viene a satisfacer un nuevo driver de la arquitectura no contemplado anteriormente, que es el de fuentes de datos relacionales.  Problema: Exportar grandes cantidades de datos de una fuente relacional para luego importarlas, no solo consume mucho tiempo, también es ineficiente.  Solución: Para importar datos relacionales se hace una conexión directa desde la plataforma Big Data hasta la base de datos relacional.  Aplicación: Se utiliza un motor de transferencia de datos (herramienta ETL) en el cual se emplean diferentes conectores para conectarse a diferentes bases de datos y luego ejecutar consultas SQL para obtener los datos que serán importados.  Mecanismos: Motor de transferencia de datos, motor de procesamiento, mecanismo de almacenamiento, motor de flujo de trabajo, portal de productividad, manejador de recursos, motor de coordinación.  Procesamiento Batch a gran escala: Este patrón ayuda para que se puedan hacer operaciones en los datos a gran escala sin problemas de rendimiento, de manera que el sistema pueda tener un estacionamiento para luego hacer los reportes.  Problema: Procesar grandes cantidades de datos puede llegar a tener bajo rendimiento, además de que las técnicas tradicionales de procesamiento de datos son ineficientes para grandes volúmenes de datos debido a la latencia de transferencia de datos.  Solución: Los datos son consolidados en forma de un gran conjunto de datos y luego se procesan utilizando una técnica de procesamiento distribuido. 104  Aplicación: Los datos se procesan utilizando un sistema de procesamiento por lotes distribuidos, de tal manera que todo el conjunto de datos se procesa como parte del mismo ciclo de procesamiento de una manera distribuida.  Mecanismos: motor de procesamiento, motor de transferencia de datos, mecanismo de almacenamiento, manejador de recursos, motor de coordinación.  Fragmentación de datos automático: Este patrón se utiliza para mejorar el rendimiento de acceso de los clientes al conjunto de datos, sin embargo, en caso de tener muchos datos fragmentados, el rendimiento puede decaer. Este patrón es usado en conjunto con el patrón de replicación y reconstrucción de datos automáticos para evitar perdida de datos.  Problema: Mientras la cantidad de datos y el número de clientes accediendo a los datos aumentan, la latencia de acceso de los datos va aumentando gradualmente, lo que afecta el tiempo en completar las consultas.  Solución: El conjunto de datos es dividido horizontalmente por lo que los subconjuntos de filas son almacenados en diferentes maquinas a través del clúster, de este modo se distribuye la carga garantizando un alto rendimiento.  Aplicación: Se utiliza un estacionamiento HDFS que implementa fragmentación automática que dirige a los clientes a diferentes fragmentos en función de su respectivo criterio de consulta.  Mecanismos: Mecanismo de almacenamiento que soporte la fragmentación automática.  Replicación y reconstrucción de datos automático: Patrón utilizado para cumplir con el driver de tolerancia a fallos.  Problema: Por lo general en Big Data se utiliza hardware básico, por lo que comparado con hardware de nivel empresarial tiene una mayor probabilidad de fallo y por consecuencia de perdida de datos. 105  Solución: Varias copias de los datos son guardados y cualquier perdida en los datos debido a fallas de hardware son reconstruidos automáticamente.  Aplicación: Se utiliza una tecnología de almacenamiento Big Data que implemente replicación de datos automático, de manera que un mismo conjunto de datos se encuentra en varias máquinas del clúster, además de proporcionar reconstrucción de los datos.  Mecanismos: Mecanismo de almacenamiento que soporte la replicación de los datos.  Gobierno de conjunto de datos centralizado: Este patrón es necesario para proporcionarle al usuario final una interfaz centralizada para manejo de inteligencia de negocio.  Problema: El análisis de los datos utilizando tecnologías de Big Data, garantizando una continua gobernabilidad de los datos, desde su adquisición hasta su almacenamiento, puede ser una tarea desalentadora debido a la gran variedad y escenarios de uso no previstos de los datos.  Solución: La gobernabilidad de los datos está centralizado y se introduce un sistema que automatice las tareas de control de datos, incluyendo la gestión de ciclo de vida de los datos, la auditoria de acceso de datos e identificación del linaje de datos.  Aplicación: Se introduce un componente dentro de la plataforma Big Data que proporciona una interfaz centralizada para las políticas de creación y el seguimiento de auditoria.  Mecanismos: Manejador de gobierno de datos, motor de flujo de trabajo, motor de seguridad.  Acceso directo a los datos: Este patrón es utilizado para realizar conexiones directas a los datos y hacer consultas complejas.  Problema: Analizar un gran volumen de datos utilizando herramientas avanzadas de análisis que se basan en primero exportar los datos y luego 106 importarlos en otro mecanismo de almacenamiento compatible con la herramienta, no solo es ineficiente, sino también consume mucho tiempo.  Solución: Una conexión directa es hecha entre la plataforma Big Data y la herramienta de análisis mediante algún conector o estandarización que permita el acceso de los datos.  Aplicación: Un conector de dos vías es introducido entre la herramienta de análisis y la plataforma Big Data, el cual se encargará de traducir las llamadas de la herramienta a la plataforma.  Mecanismos: Mecanismo de almacenamiento, motor de consulta, motor de procesamiento, manejador de recursos, motor de coordinación.  Acceso indirecto a los datos: Este patrón es utilizado para realizar reportes en herramientas BI en las cuales los datos deben ser exportados y transformados para su correcto procesamiento.  Problema: Los analistas de datos que utilizan herramientas tradicionales de Inteligencia de negocio quizás deban acceder a datos procesados en la plataforma Big Data. Sin embargo, el uso de tecnologías de almacenamiento no relacional hace de esta tarea algo difícil, debido a que las herramientas tradicionales de inteligencia de negocio soportan solo conexiones a almacenes de datos relacionales.  Solución: Los datos ya procesados son exportados al almacén de datos distribuido, desde donde puede ser accedido por las herramientas existentes de inteligencia de negocio sin la necesidad de hacer conexiones separadas.  Aplicación: Los datos procesados son convertidos al esquema requerido, para luego ser exportados al almacén de datos distribuido usando una conexión.  Mecanismos: Motor de transferencia de datos, mecanismo de almacenamiento, portal de productividad, motor de flujo de trabajo, motor de consultas. 107 5.2.4.4 Combinación de los patrones Algunos patrones es posible combinarlos, debido a que poseen los mismos mecanismos o existen componentes que satisfacen varios de estos patrones a la vez. Se encontró que los siguientes patrones pueden combinarse en un solo patrón, debido a la similitud de sus componentes o mecanismos y a que mediante las matrices de evaluación en base a la metodología Desmet se encontraron herramientas que cumplen con varios de estos patrones de diseño. Para el primer conjunto de patrones combinados se encuentran: Replicación y reconstrucción de datos automáticos, fragmentación de datos automáticos y alto volumen de almacenamiento binario. Estos patrones pueden combinarse en un solo patrón que satisfaga todos los requerimientos, el cual llamaremos “patrón combinado de almacenamiento”. Para el segundo grupo de patrones combinados solo encontramos: Fuente relacional. A este nuevo patrón le llamaremos “patrón combinado de transferencia relacional”. 5.2.5 Paso 5: Instanciar los elementos de la arquitectura y asignar responsabilidades. En la realización del paso anterior se observó cómo los patrones seleccionados fueron capaces de satisfacer uno o más drivers de la arquitectura, por lo cual en este paso se explicará qué solución y elementos se encargaron de satisfacer las necesidades de la arquitectura, de manera que se compararan los patrones que satisfacen los respectivos drivers y la solución a la que se llega. La instanciación de los elementos se realizó de la siguiente manera, al comparar el patrón más el driver que satisface, dando como resultado una solución o componente de la arquitectura. A continuación, se presenta la instanciación de los elementos de la arquitectura y sus respectivas responsabilidades: 108  Patrón combinado de almacenamiento  Drivers relacionados: Volumen, Velocidad y Variabilidad.  Componente de la arquitectura: Estacionamiento y Almacén de Datos HDFS.  Responsabilidad: Componente encargado de almacenar todos los datos transaccionales depurados y de mayor relevancia para el cálculo de las métricas de la empresa.  Patrón combinado de transferencia relacional  Drivers relacionados: Velocidad y Volumen.  Componente de la arquitectura: Mecanismos de Extracción, Transformación y Carga.  Responsabilidad: Componente encargado de realizar la transferencia de datos a otros componentes de la arquitectura que lo requieran.  Procesamiento Batch a gran escala  Drivers relacionados: Volumen, Velocidad y Generación de reportes (paso de pre-procesar los datos).  Componente de la arquitectura: Estacionamiento de grandes volúmenes de datos con sistema de archivos distribuidos con capacidad de realizar procesamiento en paralelo.  Responsabilidad: Componente encargado de almacenar grandes volúmenes de datos para realizar operaciones sobre ellos.  Acceso directo a los datos:  Drivers relacionados: Volumen y Generación de reportes.  Componente de la arquitectura: Herramienta de software con lenguaje parecido a SQL y capacidad de conectarse directamente al componente de Estacionamiento.  Responsabilidad: Componente encargado de realizar las operaciones sobre el conjunto de datos del estacionamiento para la creación de cubos y consultas complejas.  Acceso indirecto a los datos:  Drivers relacionados: Volumen y Generación de reportes. 109  Componente de la arquitectura: Herramienta de inteligencia de negocios tradicional o con capacidad para grandes volúmenes de datos.  Responsabilidad: Componente encargado de realizar los reportes y dashboards de los datos que previamente fueron procesados por las diferentes consultas del negocio.  Gobernabilidad centralizada de los datos:  Drivers relacionados: Generación de reportes y centralización del sistema.  Componente de la arquitectura: Portal de Inteligencia de negocio o Interfaz web especializada en el manejo de las diferentes herramientas Big Data.  Responsabilidad: Componente que se encarga de integrar y manejar la mayoría de las herramientas de la arquitectura mediante una interfaz usable y segura. 5.2.6 Paso 6: Definir las interfaces de los elementos instanciados. En el paso se obtuvieron una serie de componentes que conforman la arquitectura. Por lo que en este paso se procede a la selección de herramientas de software comerciales o libres que conforman los componentes, utilizando la metodología Desmet y matrices de evaluación de herramientas de software. 5.2.6.1 Evaluación de herramientas de software Para la comparación de herramientas se utilizó la metodología Desmet ya que es una metodología creada para evaluar herramientas de software. (Kitchenham, 1996) Las herramientas que son objeto de estudio pertenecen al ámbito de los sistemas de archivos distribuidos, almacén de datos distribuido e inteligencia de negocios para Big Data. 110 Acorde a la metodología Desmet; como una herramienta fiable para la evaluación de aplicaciones de software: Los criterios de evaluación son:  Requerimientos funcionales: Abarcan aspectos de rendimiento, características avanzadas, funcionalidades específicas, etc.  Requerimientos específicos: Abarcan aspectos generales de cada herramienta y condiciones necesarias que deben cumplir las herramientas para encajar en la cultura de la organización. Para la aplicación efectiva del método Desmet, se usó una matriz de evaluación que consta de los siguientes atributos:  Tipo: Tipo de requerimiento.  Descripción del criterio evaluado: El criterio a evaluar.  Condición: Condición que debe cumplir el criterio (O-Obligatorio, D- Deseable, S-Suplementario e I-Informativo).  Peso: El valor de importancia del criterio (0= sin valor o criterio no importante, 5= máximo valor o importancia).  Cumplimiento: Si la herramienta cumple o no con el criterio.  Observaciones: Información relevante sobre la herramienta en relación al criterio.  Estrategia: El valor que posee el criterio en la herramienta. Si el criterio tiene como condición que No se cumple, este valor será de cero (0). Si el 111 criterio se cumple, este valor puede ir de 1 a 6 según en qué porcentaje aproximado se cumple el criterio.  Calificación ponderada: Es la calificación obtenida por la herramienta para el criterio dado. Se obtiene de la siguiente fórmula: Peso multiplicado por la Estrategia dividido entre 6. A continuación, se presentarán los resultados de la evaluación de las herramientas. (Para ver las matrices de evaluación ir a ANEXO 1: Pentaho vs Palo; ANEXO 2: Hadoop vs Otros; ANEXO 3: Hive vs ; ANEXO 4: Solr vs Elasticsearch y ANEXO 5: HUE vs Banano). 5.2.6.1.1 Componente para mecanismos de Extracción, Transformación y Carga de datos La herramienta seleccionada es Pentaho Data Integration. Con respecto a la plataforma que soportará las actividades correspondientes a la inteligencia de negocios, Pentaho comprende un conjunto de componentes robustos respaldados por una comunidad activa que garantiza un desempeño aceptable de los procesos necesarios para llevar acabo la inteligencia de negocios. Entre las características más relevantes que permiten a Pentaho destacarse del resto está la capacidad de realizar actividades de forma offline, soportar lenguaje SQL y permitir análisis en tiempo real. Por último, mencionar que su versión más reciente, incorpora el concepto de integración con Big Data y posee conectores para tal fin. 5.2.6.1.2 Componente de Estacionamiento con sistema de archivos distribuido La herramienta seleccionada es Apache Hadoop. 112 Hadoop es un framework de código abierto auspiciado por Apache Foundation, el cual desde su lanzamiento se ha transformado en un estándar en la industria, las razones que soportan este rápido auge estriban en las bondades de Hadoop para procesar grandes conjuntos de datos mediante equipos con bajas capacidades de computo agrupados en clúster o sistemas distribuidos. Adicionalmente Hadoop se integra con un ecosistema de aplicaciones que le facilita cubrir de forma transversal todas las actividades inmersas en la consulta, análisis, obtención de datos, publicación de mecanismos de seguridad, estudio y predicción de comportamiento. 5.2.6.1.3 Almacén de Datos Distribuido Apache Hive es una infraestructura de almacenamiento de datos construida sobre Hadoop para proporcionar agrupación, consulta, y análisis de datos, se posiciona en el mercado como la herramienta líder en el campo de almacén de datos distribuidos y creación de DataMarts, además de encontrarse bien posicionada en el mercado open source, la mayoría de los usuarios sigue inclinándose por el proyecto Apache Hive producto de alta disponibilidad, tolerancia a fallos, su extensa comunidad y sus APIS para desarrollo. Herramienta seleccionada: Apache Hive. 5.2.6.1.4 Motor de búsqueda Big Data como herramienta de inteligencia de negocio Para el componente de herramienta de inteligencia de negocio se pudo haber seleccionado cualquier herramienta tradicional de este tipo. Sin embargo, los motores de búsqueda actuales disponibles comercialmente, permiten la generación de gran variedad de reportes para inteligencia de negocio, además de que soportan la generación de reportes por facetas, es decir, filtrando las búsquedas sobre datos que fueron cargados previamente a la herramienta, por https://es.wikipedia.org/wiki/Almac%C3%A9n_de_datos https://es.wikipedia.org/wiki/Hadoop 113 lo que todos los reportes son hechos directamente sobre la herramienta. También los motores de búsqueda están diseñados para ser escalables, lo que le da una mayor ventaja competitiva en comparación con las herramientas tradicionales de inteligencia de negocio. En base a los resultados obtenidos de las matrices de evaluación, la herramienta seleccionada es Apache Solr. Esta herramienta fue comparada con Elasticsearch. Apache Solr y Elasticsearch son los motores de búsqueda más populares en la actualidad. Ambas herramientas son muy similares y brindan funcionalidades parecidas. En el 95% de los casos será indiferente escoger uno o el otro, ambas herramientas poseen licencia de Apache y una comunidad amplia. Sin embargo, Apache Solr tiene una contribución de compañías importantes en el área de Big Data, tales como Hortonworks, MapR y Cloudera. 5.2.6.1.5 Componente de Acceso directo a los datos Como herramienta seleccionada para este componente se decidió por Apache Hive, debido a que esta herramienta es un requerimiento particular del cliente. Apache Hive permite realizar consultas parecidas a SQL y así crear cubos y tablas para obtener los indicadores del negocio. Además, automatiza la creación del algoritmo MapReduce por lo que el usuario no tiene que programar el algoritmo directamente. . 5.2.6.1.6 Componente de manejo centralizado de datos y herramienta de visualización En base a los resultados obtenidos de las matrices de evaluación, la herramienta seleccionada es Hadoop User Experience (HUE). 114 Apache Hue es una herramienta que permite la visualización de los datos que se encuentren en Hadoop, la cual se puede integrar con cualquiera de sus versiones. Posee diversas funcionalidades para generar reportes, analizar, procesar, consultar, entre otras. Además de que permite integrar varias herramientas Big Data como son: Hive, Pig, Solr, HDFS, Impala, entre otras. 5.2.6.1.7 Componente balanceador de Carga de Peticiones En base a los resultados obtenidos de la evaluación de la larga lista de sitios web conocidos que lo usan, la herramienta seleccionada es Nginx. Nginx es un servidor web/proxy inverso ligero de al to rendimiento y un proxy para protocolos de correo electrónico (IMAP/POP3), usado principalmente para el balanceo de peticiones sobre servidores dedicados. 5.2.6.2 Información transmitida entre las herramientas:  Pentaho Data Integration: Este componente consume los datos que provienen de las fuentes Oracle, los procesa y transforma para que puedan ser cargados en Hadoop, en una segunda oportunidad es usado para migrar los datos depurados de hadoop a el almacén de datos Hive para su posterior procesamiento, en una tercera etapa es usado para convertir los datos provenientes de los cubos dimensionales de Hive en documentos Json que serán almacenados en el motor de búsqueda SolR.  Apache Hadoop: Recibe los datos generados por los ETL que consultan y procesan sobre las fuentes Oracle. En este componente se almacenan la mayoría de los archivos que serán utilizados por el componente de consultas especializadas o también denominado almacén de datos para crear tablas o cubos en el HDFS. https://es.wikipedia.org/wiki/Internet_Message_Access_Protocol https://es.wikipedia.org/wiki/Post_Office_Protocol 115  Apache Hive: En esta etapa la información es consultada directamente al HDFS el cual realiza un trabajo de mapeo y reducción sobre los datos, traduciendo la consulta hecha en HQL a un trabajo Map Reduce que generará los cubos asociados a las métricas a calcular.  Apache Solr: Se hace una conexión a la base de datos por defecto utilizada por Hive, para luego enviar mediante una transformación en Pentaho, los datos al motor de búsqueda Solr con el fin de indexarlos y procesarlos en un formato ligero Json, para posteriormente realizar reportes, gráficos, dashboards, entre otros.  HUE: Este componente se conecta directamente con Solr, HDFS y Hive, por lo que sirve de interfaz para realizar las consultas en Hive y luego generar los dashboards o reportes en Solr.  Nginx: Este componente se conecta directamente con cada uno de los servidores HUE, con el fin de facilitar el balanceo de peticiones para el procesamiento y cálculo de los indicadores. 5.2.7 Paso 7: Verificar y refinar los requerimientos para hacer restricciones en los elementos instanciados Se procede a verificar que los elementos instanciados cumplan con los diferentes requerimientos funcionales, requerimientos de calidad y las restricciones de diseño. 5.2.7.1 Componentes versus los requerimientos de calidad, requerimientos funcionales y restricciones de diseño  Componente para mecanismos de extracción, transformación y carga  Requerimientos Funcionales: Cumple con los requerimientos de permitir al usuario almacenar las transacciones diarias, debido a que es un mecanismo para transferir datos al estacionamiento de datos HDFS. 116 También ayuda en la integración de las demás oficinas a nivel nacional, mediante el uso de transformaciones de mediación, que permitan transferir cada cierto tiempo los datos actualizados de las distintas oficinas del país.  Requerimientos de calidad: Capacidad de pruebas, Escalabilidad, Tolerancia a fallos. La herramienta fue probada en ambiente GNU/Linux, además de que se transfirieron millones de registros en una gran cantidad de tablas migradas y se utilizó un trabajo en la herramienta que indicaba las transformaciones fallidas.  Restricciones de diseño: Este componente es muy versátil, ya que permite realizar migraciones a bases de datos relacionales como al sistema HDFS. Pentaho Data Integration no tiene ningún problema de ser ejecutada en un ambiente GNU/Linux.  Componente de estacionamiento Hadoop  Requerimientos funcionales: Permite integrar todos los datos para su posterior análisis.  Requerimientos de calidad: La herramienta posee tolerancia a fallos, debido a que los datos pueden ser replicados a través de varios nodos para mantener un respaldo.  Restricciones de diseño: Es utilizada como estacionamiento debido a que posee la capacidad de realizar operaciones Map Reduce sobre un gran volumen de datos.  Componente de Acceso directo a los datos o Almacén de Datos Hive  Requerimientos funcionales: Este componente sirve de ayuda previa para la creación de reportes, debido a que permite realizar consultas complejas y con los resultados crear los reportes en el siguiente componente de la arquitectura.  Requerimientos de calidad: La herramienta fue instalada y probada en ambiente de desarrollo. La seguridad que posee, es que se accede a ella 117 por medio de otra herramienta de interfaz web como HUE la cual solicita usuario y contraseña.  Restricciones de diseño: La herramienta se ejecuta sin ningún inconveniente en el ambiente GNU/Linux, como lo exige la restricción de diseño.  Componente de integración de herramientas y visualización de datos  Requerimientos funcionales: Mediante esta herramienta se generan los reportes de la empresa, debido a que integra las principales herramientas de análisis de datos (Solr y Hive).  Requerimientos de calidad: La herramienta permite la integración de las demás herramientas. Si falla, solo es necesario volver a levantar el servidor sin ningún problema. Cuando el usuario desea acceder a las funcionalidades de la herramienta, le solicita una autenticación. Permite la creación de grupos de usuario con sus respectivos permisos (roles).  Restricciones de diseño: La herramienta puede ejecutarse bajo ambiente GNU/Linux como un servidor. Las consultas y generación de reportes son accedidas mediante esta interfaz web.  Componente de Motor de búsqueda  Requerimientos funcionales: Permite realizar consultas y generar reportes facetados, es decir, por filtros. Mantiene un rendimiento estable al haber muchos datos debido a que está diseñada para ser escalable. Contiene diversas funciones que permiten y facilitan la generación de reportes, gráficos y dashboards para la inteligencia del negocio.  Requerimientos de calidad: La herramienta fue probada en ambiente de desarrollo y es escalable horizontalmente.  Restricciones de diseño: Se ejecuta bajo ambiente GNU/Linux, permite integración con interfaz web para visualización y generación de reportes. 118  Componente Balanceador de Carga.  Requerimientos funcionales: Permite realizar el balanceo de peticiones para el procesamiento y cálculo de los indicadores sobre cada uno de los servidores HUE acorde a un algoritmo de peticiones.  Requerimientos de calidad: La herramienta fue probada en ambiente de desarrollo y es escalable horizontalmente.  Restricciones de diseño: Se ejecuta bajo ambiente GNU/Linux, permite integración con cualquier componente; en nuestro caso particular HUE. Con esto concluye la ejecución de la metodología ADD. A continuación, se procede a mostrar el diagrama de componentes y el diagrama de casos de uso para el nivel cero y primer nivel.  Diagrama de componentes: En el siguiente diagrama de componentes se puede observar la manera en la que se conectan cada uno de ellos. (Ver Figura 21 Diagrama de componentes) 119 Figura 21 Diagrama de componentes Fuente Elaboración Propia 120  Diagrama de casos de uso nivel 0: En el diagrama de casos de uso se puede observar el nivel cero, en el cual se muestra de forma general los actores y el rol que desempeñan en el sistema. (Ver Figura 22 Caso de uso Nivel 0 y Tabla 11 Actores - Casos de Uso Nivel 0) Figura 22 Caso de uso Nivel 0; Fuente: Elaboración Propia 121 Tabla 11 Actores - Casos de Uso Nivel 0 Fuente: Elaboración Propia Actor Descripción Mercal Usuario final del sistema, el cual se encarga de utilizar la interfaz web de HUE con todas sus funcionalidades, como generar reportes y hacer consultas para obtener información. HUE Interfaz web encargada de administrar las herramientas, generar reportes y gráficos (Integra Solr, Hive y HDFS) Solr Es un motor de búsqueda, utilizado como herramienta de inteligencia de negocio para generar reportes por facetados Hive Es el encargado de realizar consultas complejas, parecidas a SQL y crear cubos para los indicadores del negocio. Hadoop Funciona como el estacionamiento de la arquitectura, sobre el cual se realizan la mayoría de las operaciones de análisis, limpieza, procesamiento, entre otros. Pentaho Data Integration Encargado de crear y ejecutar transformaciones o Jobs que permiten la transferencia de datos a los componentes requeridos. 122 Nginx Encargado de balancear las peticiones sobre la interfaz HUE para el cálculo de indicadores. Tabla 12 Funcionalidades Casos de uso nivel 0 Fuente: Elaboración Propia Caso de uso Obtener y visualizar las métricas calculadas. Actor Mercal Flujo Básico El usuario de Mercal inicia sesión en la interfaz web de HUE. Luego puede realizar una serie de tareas como: a) Consultas utilizando Hive b) Generar reportes/dashboards con Solr c) Administrar HDFS d) Visualizar indicadores calculados Pre-Condición Principalmente debe estar funcionando Hadoop, Solr, Hive y el servidor de HUE. El usuario fue creado por el administrador o es administrador. Caso de uso Extraer Transformar y cargar datos Actor Pentaho Data Integration Flujo Básico Se procede a realizar la transferencia de datos entre los diversos componentes de la Arquitectura Pre-Condición Es necesario establecer la conexión con la fuente de datos a extraer transformar y con el componente a cargar los datos Caso de uso Estacionar datos de relevancia Actor Hadoop Flujo Básico Los datos son almacenados en el HDFS de Hadoop, para su posterior procesamiento utilizando Map Reduce. Pre-Condición Los datos fueron obtenidos de los sistemas fuente utilizando una transformación de Pentaho. 123 Caso de uso Crear cubos Actor Hive Flujo Básico Se crean tablas o cubos en la base de datos de Hive con los datos de Hadoop. Pre-Condición Las tablas son almacenadas en HDFS mediante una configuración (hive- site.xml) indicando que se utilizara como base de datos una ruta en el HDFS Caso de uso Consultar los datos Actor Hive Flujo Básico Los datos pueden ser consultados mediante un lenguaje de alto nivel parecido a SQL, el cual consulta directamente a los datos que se encuentran en el HDFS de Hadoop denominado HQL Pre-Condición Previamente los datos fueron cargados a Hadoop y Hive pre configurado para usar HDFS como lugar de almacenamiento físico. Caso de uso Indexar Documentos para la generación de reportes Actor Solr Flujo Básico Se procede a indexar los datos obtenidos mediante una transformación de Pentaho, a formato Json para en un posterior uso generar los reportes asociados a cada indicador. Se crean dashboards o graficas geo referenciales si el usuario lo solicita. Pre-Condición Los datos fueron previamente cargados a Solr en formato JSON mediante una transformación de Pentaho, los cuales fueron extraídos de las tablas creadas por Hive. Caso de uso Administrar y utilizar herramientas Actor HUE Flujo Básico El usuario inicia sesión Puede generar consultas con Hive Crear tablas en Hive 124 Administrar el HDFS Crear grupos de usuarios Generar reportes y gráficos en Solr Pre-Condición HUE fue configurado previamente para conectarse con las distintas herramientas a utilizar en la arquitectura. Caso de uso Balancear Peticiones Actor Nginx Flujo Básico El usuario que se conecte a algún servidor HUE será despachado por el balanceador de carga a alguno que se encuentre disponible o con menor flujo de trabajo. Pre-Condición Nginx fue configurado previamente para conectarse con los servidores HUE a utilizar en la arquitectura. Para ver el diagrama de casos de uso nivel 1 ver ANEXO 7: Casos de uso Nivel 1. A continuación, se procede a explicar las actividades realizadas para desarrollar la Arquitectura Analítica de Big Data, como son: Software necesario como Prerrequisito, diseño de ETL, conversión de datos a formato ligero, integración y comunicación de los diversos componentes. 5.3 Implementación de la Arquitectura Se mostrará a continuación, la evidencia de la instalación de las herramientas, para una información más detallada de como instalar y configurar cada una de ellas, vea el ANEXO 8: Instalación y configuración de Herramientas. En la figura 23 se muestra la versión utilizada de Java, la cual se instaló mediante repositorios apt-get install en todos los servidores de la arquitectura. 125 Figura 23 Instalación Java En la figura 24 se evidencia la ejecución e inicialización de los nodos del estacionamiento HDFS. Figura 24 Cluster Estacionamiento HDFS 3 nodos En la figura 25, puede observarse los procesos levantados por el Nodo Maestro, entre los cuales encontramos el NameNode, que es levantado solo por el Maestro, el SecondaryNamenode (puede levantarse en otra máquina para mayor seguridad) y su respectivo Datanode. 126 Figura 25 Procesos Hadoop Nodo Maestro En la figura 26 se evidencia la consola de cliente de hive, mediante la cual se hacen las consultas HQL. Figura 26 Evidencia ejecución Hive En la figura 27 se evidencia el despliegue del servidor de solr, mediante el comando. /solr start 127 Figura 27 Ejecución Solr Para implementar la Arquitectura fue necesario diseñar los modelos de datos asociados a los cubos Hive como a los documentos Json de SolR, para lograr esto se analizaron los diversos modelos de datos correspondientes a los sistemas fuentes de la Misión Mercal C.A. y con apoyo de los especialistas de área (bajo 3era forma normal en Oracle 10g) de manera que se pudiese hacer un correcto modelo de datos acorde a los requerimientos de Negocio. Luego fue necesario el diseño de los mecanismos de ETL y las pruebas para lograr una correcta extracción, limpieza, transformación e inserción de los datos en el clúster de estacionamiento HDFS y demás componentes de la arquitectura. (Ver, ANEXO 6: Diseño de los mecanismos ETL). Una vez que se finalizó la construcción de los modelos precisados y de las transformaciones necesarias para la migración, seguidamente fue necesario monitorear y observar el comportamiento de las herramientas en ejecución, principalmente de las transformaciones, las cuales manejaron gran volumen de datos y del clúster de estacionamiento HDFS, por lo que se observó el comportamiento del sistema realizando diversas pruebas. 128 Conclusiones y Recomendaciones Tener la información significa poder, por eso cuesta trabajo creer que muchas empresas e incluso gobiernos no sepan aprovechar el enorme potencial que tienen de datos de personas y procesos en sus propios computadores. Bug Data ofrece un abanico de tecnologías para el análisis inteligente de la enorme cantidad de datos que están expuestos en las nuevas tecnologías, y se está teniendo en cuenta cada vez más para vaticinar los problemas del mundo en los negocios, el comercio y en la vida cotidiana. La creación y diseños de arquitecturas de software que permitan almacenar y procesar grandes volúmenes de datos para sacarles el mayor provecho, representa una gran ventaja competitiva en la actualidad. En casi todas las áreas de negocio se está aplicando Big Data en conjunto con la ciencia de datos, para tomar decisiones más precisas o generar campañas de mercadeo más exitosas. En los próximos años esta tendencia será casi obligatoria para gran parte de las empresas, ya que con el aumento exponencial de datos en la web y con el auge del internet de las cosas, será aún más necesario procesar y almacenar toda esta cantidad de datos, utilizando técnicas y metodologías innovadoras y creativas. De acuerdo a los objetivos planteados se evidenció y ratificó los siguientes puntos: - Primer y Segundo objetivo específico (Diseñar la arquitectura), se puede concluir, que de manera obligatoria en cualquier diseño de una arquitectura es de prioridad alta, tomar en cuenta los requerimientos del cliente, en este caso la Misión Mercal C.A., debido a que permite identificar de manera precisa e inmediata los casos de uso y posibles escenarios que impactaran en la propuesta de arquitectura. Además, se pudo observar que los patrones de Big Data, mostrados en el marco aplicativo, soportan y proporcionan una valiosa columna vertebral para solucionar problemas específicos, los cuales se aplicaron exitosamente 129 junto a la metodología ADD. Cumpliendo con el principio de Ingeniería de Software que impulsa el uso de una Arquitectura inicial con la finalidad de reutilizar componentes y patrones en el desarrollo de software. Cabe destacar que dicho proceso permite generalizar la arquitectura planteada a otros sistemas, como un proceso de migración a Big Data Analítica para la empresa que lo requiera. - Tercer objetivo específico (Seleccionar las herramientas de la Arquitectura) se concluye, que el uso de la metodología Desmet permite satisfacer los requerimientos del sistema para la organización, debido a que con el uso de requerimientos de software y ponderaciones se toman las decisiones más acertada en la elección de las herramientas, dependiendo del caso de estudio y componente a implantar. - Para el último objetivo específico se concluye, que acorde a la implementación de la arquitectura no solo basta diseñarla e instalar cada uno de sus componentes, es necesario realizar pruebas de cada una de las herramientas que la conforman, de manera que estas pudiesen interoperar observando el funcionamiento óptimo de las mismas. En cada uno de los componentes de la arquitectura fue necesario realizar actividades de entonación para lograr satisfacer las necesidades del cliente. Además, debido a que el cliente no está familiarizado con el paradigma Big Data, fue necesario emprender actividades de gestión del cambio. Recomendaciones a nivel técnico:  Es recomendable que la transformación se configure por lotes, debido a que transferir datos muy pesados puede consumir rápidamente los recursos de la máquina virtual de Java, lo que ocasiona que el kernel del sistema finalice el proceso antes por consumir muchos recursos.  Se recomienda que los servidores en los que se despliegue la arquitectura o herramientas, sea de alta capacidad para empresas grandes como la 130 Misión Mercal C.A., mínimo 8 GB de RAM y 2 procesadores con sistema operativo GNU/Linux.  Se recomienda probar las herramientas en ambiente de calidad para adaptarse y solucionar cualquier inconveniente que pueda surgir durante la implementación de arquitecturas Analíticas Big Data.  Se recomienda que la red en la cual se maneja el tráfico de datos sea una red de alta velocidad y en una red virtual separada. Recomendaciones a nivel General:  No en todos los casos es viable la utilización de arquitecturas o plataformas Analíticas de Big Data, por lo que es recomendable hacer un estudio de planificación de capacidad en la organización antes de hacer cualquier propuesta de Big Data.  El implementar una arquitectura Analítica Big Data en una organización no significa que se deba descartar la utilización de sistemas relacionales, en algunos casos será necesario trabajar con ambas tecnologías como lo es, por ejemplo, para la Misión Mercal C.A.  En Venezuela existen muchas organizaciones que manejan gran cantidad de datos y que utilizan un modelo relacional que comienza a tener problemas de renovación de licenciamiento en moneda extranjera, lo cual implica que existe una gran posibilidad de reutilización de la arquitectura planteada a través del presente trabajo.  Además, es recomendable que el personal encargado del área de tecnologías de información se instruya en la utilización y desarrollo de la arquitectura propuesta en este trabajo para poder satisfacer las necesidades del mercado y que las empresas puedan brindar un mejor servicio a sus clientes. 131 Bibliografía Antiñanco, M. J. (2013). Bases de Datos NoSQL: Escalabilidad y alta disponibilidad a través de patrones de diseño. 10-20. Apache. (2016). Hadoop. Obtenido de Apache Hadoop Org: http://hadoop.apache.org/ Apache. (26 de Enero de 2016). Hadoop Yarn. Obtenido de Apache Org: http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html Apache. (2016). Lucene. Obtenido de http://lucene.apache.org/core/ Arias, F. G. (1999). El Proyecto de Investigacion : Guia para su elaboracion. Caracas: Episteme. Blog, T. B. (7 de Noviembre de 2016). The Big Data Blog. Obtenido de Hadoop Ecosystem Overview: http://thebigdatablog.weebly.com/blog/the-hadoop-ecosystem-overview Brewer, E. (2000). Toward Robust Distributed Systems. Obtenido de http://www.cs.berkeley.edu/~brewer/cs262b-2004/PODC-keynote.pdf Buhler, P., Erl, T., & Khattak, W. (2015). Big Data Fundamentals: Concepts, Drivers & Techniques. Chauhan, A. (2012). Blog Microsoft. Obtenido de https://blogs.msdn.microsoft.com/avkashchauhan/2012/02/27/primary-namenode- and-secondary-namenode-configuration-in-apache-hadoop/ Comercio, C. d. (2016). codigo de derecho. Obtenido de https://derechovenezolano.wordpress.com/2012/11/01/el-codigo-de-comercio-el- registro-mercantil-concepto-documentos-sujetos-a-registro-efectos/ Conway, D. (30 de Septiembre de 2010). Drew Conway Venn Diagram. Obtenido de Drew Conway: http://drewconway.com/zia/2013/3/26/the-data-science-venn-diagram Dean, J., & Ghemawat, S. (2003). Research Google Inc. Obtenido de http://research.google.com/archive/mapreduce-osdi04-slides/index-auto-0008.html Española, R. A. (Marzo de 2016). Diccionario de la lengua española. Obtenido de http://dle.rae.es/?id=Vj40asb Evans, E. (12 de 05 de 2009). NOSQL 2009. May 2009. – Blog post of 2009-05-12. Obtenido de http://blog.sym-link.com/2009/05/12/nosql_2009.html Eyssautier de la Mora, M. (2006). Metodología de la investigación: desarrollo de la inteligencia. Cengage Learning Editores. 132 Gutierrez, D. (Junio de 2014). Yarn is All the Rage at Hadoop Summit 2014. Obtenido de Kdnuggets: http://www.kdnuggets.com/2014/06/yarn-all-rage-hadoop-summit.html Hanmer, R. (2013). Pattern-Oriented Software Architecture for Dummies. En R. Hanmer. Wiley. IBM. (2015). IBM. Obtenido de Infografia: http://www.ibmbigdatahub.com/infographic/four- vs-big-data Kitchenham, B. (1996). DESMET: A method for evaluating software engineering methods and tools. Lowell, U. (2013). Computer Science. Obtenido de Umass Lowell: http://www.cs.uml.edu/~jlu1/doc/source/report/MapReduce.html MacCandles, M., Hatcher, E., & Gospodnetic, O. (2010). Lucene in Action. Manning. Magazine, L. V. (10 de noviembre de 2013). Big Data, el tesoro oculto del siglo XXI. Manifesto, A. (2001). Agil Manifesto. Obtenido de http://agilemanifesto.org/ Marco. (8 de Mayo de 2013). Slideshare. Obtenido de Seminario Apache Solr: http://es.slideshare.net/paradigmatecnologico/seminario-apache-solr MERCAL. (2017). MERCAL. Obtenido de http://www.mercal.gob.ve/?p=41 MERCAL. (2017). Organigrama. Obtenido de Estructura Organizativa Mercal: http://www.mercal.gob.ve/wp-content/uploads/2013/06/ORGANIGRAMA2.jpg Microsoft. (2017). Microsoft. Obtenido de https://msdn.microsoft.com/en- us/library/ee658098.aspx Morgan, L. (5 de Abril de 2015). Information Week. Obtenido de Information Week Big Data: http://www.informationweek.com/big-data/big-data-analytics/6-ways-to-master-the- data-driven-enterprise/d/d-id/1320234 Moya, D. d. (2002). El Proyecto Factible: una modalidad de investigación. Caracas: Sapiens. Murthy, A. (15 de Octubre de 2013). Apache Hadoop. Obtenido de Hortonworks: http://hortonworks.com/blog/apache-hadoop-2-is-ga/ NoSQL. (2016). NoSQL Database . Obtenido de NoSQL Database org: http://nosql- database.org/ Pedraz, A. (2004). La revisión bibliográfica. Pritchett, D. (2008). BASE: An Acid Alternative. Obtenido de http://queue.acm.org/detail.cfm?id=1394128 Sabino, C. (1992). EL PROCESO DE INVESTIGACIÓN. Caracas. 133 Sánchez, F. M. (2014). Herramientas para Big Data: Entorno Hadoop. 59. Shvachko, K., Kuang, H., Radia, S., & Chansler, R. (2006). The Hadoop Distributed File System. Soares, S. (2012). Dataversity. Obtenido de http://www.dataversity.net/not-your-type-big- data-matchmaker-on-five-data-types-you-need-to-explore-today/ Strozzi, C. (2010). NoSQL – A relational database management system. Obtenido de http://www.strozzi.it/cgi-bin/CSA/tw7/I/en_US/nosql/Home%20Page The Register. (2006). Obtenido de http://www.theregister.co.uk/2006/08/15/beer_diapers/ Vance, A. (2009). Hadoop, a Free Software Program, Finds Uses Beyond Search. New York: New York Times. Wall, M. (6 de Marzo de 2014). BBC. Obtenido de BBC Mundo: http://www.bbc.com/mundo/noticias/2014/03/140304_big_data_grandes_datos_rg White, T. (2012). Hadoop: The definitive guide. 41-47. White, T. (2012). Hadoop: The Definitive Guide (3rd ed.). O’Reilly Media. Wikipedia. (2015). Motores de busqueda. Obtenido de https://es.wikipedia.org/wiki/Motor_de_b%C3%BAsqueda Wikipedia. (2016). Apache Solr. Obtenido de https://es.wikipedia.org/wiki/Apache_Solr Wikipedia. (2016). Stub Method. Obtenido de https://en.wikipedia.org/wiki/Method_stub Wikipedia. (2016). Wikipedia. Obtenido de https://es.wikipedia.org/wiki/Registro_mercantil winshuttle. (2016). winshuttle.com. Obtenido de http://www.winshuttle.es/big-data-historia- cronologica/ 134 ANEXO 1: Pentaho vs Palo 135 ANEXO 2: Hadoop vs Otros 136 ANEXO 3: Hive vs Pig 137 ANEXO 4: Solr vs Elasticsearch Leyenda Solr Elasticsearch Tip o Descripció n Condició n Pes o Cumplimient o Observacione s Estrategi a Calificació n Ponderada Replicación O-Obligatorio 5 S-Si Cumple 6 5,00 S-Si Cumple 6 5,00 Facetado O-Obligatorio 5 S-Si Cumple 6 5,00 S-Si Cumple 6 5,00 Escalabilidad O-Obligatorio 5 S-Si Cumple 6 5,00 S-Si Cumple 6 5,00 Búsqueda orientada a texto 5 S-Si Cumple 6 5,00 S-Si Cumple ES provee queries complejos de análisis 5 4,17 Búsqueda geo- espacial D-Deseable 4 S-Si Cumple 6 4,00 S-Si Cumple 6 4,00 Visualización S- Suplementari o 4 S-Si Cumple 6 4,00 S-Si Cumple 6 4,00 Debe poseer licencia de código libre o abierto O-Obligatorio 5 S-Si Cumple Licencia de Apache 6 5,00 S-Si Cumple Licencia de Apache 6 5,00 Documentació n O-Obligatorio 5 S-Si Cumple Solr se encuentra muy bien documentado. 6 5,00 S-Si Cumple 5 5,00 Comunidad D-Deseable 4 S-Si Cumple La comunidad es muy amplia y puede realizar cambios. Aporte de varias compañías 6 4,00 S-Si Cumple La comunidad tiene acceso al código, pero solo la compañía puede hacer cambios 5 3,33 Sistemas operativos soportados O-Obligatorio 5 S-Si Cumple GNU/Linux, Unix, Windows 6 5,00 S-Si Cumple GNU/Linux, Unix, Windows 6 5,00 Total 4,7 4,55 R e q . F u n c io n a le s R e q . F u n c io n a le s R e q . E s p e c íf ic o s R e q . E s p e c íf ic o s 138 ANEXO 5: HUE vs Banano Leyenda Hue Banano Tip o Descripció n Condició n Pes o Cumplimient o Observacione s Estrategi a Calificació n Ponderada Integración con Ecosistema Hadoop O- Obligatorio 5 S-Si Cumple 6 5,00 S-Si Cumple Se integra con Solr y Solr al HDFS 4 3,3 Acceso al HDFS O- Obligatorio 5 S-Si Cumple 6 5,00 S-Si Cumple Mediante Solr 4 3,3 Generación de Dashboards y gráficos O- Obligatorio 5 S-Si Cumple 6 5,00 S-Si Cumple 6 5,00 Exportación de reportes y gráficos en diferentes formatos O- Obligatorio 5 S-Si Cumple 6 5,00 S-Si Cumple 6 5,00 Plataforma fácil de desplegar D-Deseable 4 S-Si Cumple Basado en Web 6 4,00 S-Si Cumple Debe integrarse con Solr 4 2,7 Comunidad O- Obligatorio 5 S-Si Cumple 6 5,00 S-Si Cumple 6 5,00 Gratuito y de licencia libre O- Obligatorio 5 S-Si Cumple 6 5,00 S-Si Cumple 6 5,00 Documentación O- Obligatorio 5 S-Si Cumple 6 5,00 S-Si Cumple 5 5,00 Seguridad O- Obligatorio 5 S-Si Cumple 6 5,00 N-No Cumple No posee autenticación 0 0 Total 4,9 3,8 R e q . F u n c io n a le s R e q . F u n c io n a le s R e q . E s p e c íf ic o s R e q . E s p e c íf ic o s 139 ANEXO 6: Diseño de los mecanismos ETL Procesos de Extracción, Transformación y Carga El mecanismo de ETL (Extracción, Transformación y Carga) facilita el movimiento de los datos entre diferentes sistemas aplicando métodos de Extracción, Limpieza, Organización, Transformación y Validación de información, de forma automatizada desde un Origen hacia otra fuente destino. La herramienta utilizada para construir los ETL es Pentaho Data Integration (PDI). Esta herramienta cuenta con una interfaz gráfica que permite construir procesos de Extracción, Limpieza, Organización, Transformación y Validación de la Data. Para este proyecto se diseñaron un conjunto significativo de mecanismos ETL para la carga inicial de datos provenientes de aproximadamente 300 oficinas de la Misión Mercal C.A. 1 Diseño de Mecanismos de ETL para la Carga Inicial Se presentan a continuación las características más relevantes de los mecanismos ETL, diseñados para la ejecución del proceso de Carga Inicial. Representación del proceso de obtención de datos, desde las oficinas de Mercal hacia el estacionamiento distribuido, mediante el uso de los mecanismos ETL. Figura 28 Diseño ETL Misión Mercal 2 Aspectos técnicos de los mecanismos de ETL de Carga Inicial. Para el desarrollo de los ETL de carga inicial, es necesario tomar como punto de partida una tabla de alguno de los esquemas de datos de los sistemas fuentes Logístico, Saf o Siga sobre el cual se procederá a calcular el indicador; procediendo de la siguiente forma: • Se analizan las tablas revisando sus relaciones primarias y secundarias. 140 • Generar una consulta SQL capaz de extraer el contenido de la tabla principal junto con las tablas relacionadas y los campos de importancia. • Se ejecuta un proceso de validación de datos y estructura. • Posteriormente se inserta en el estacionamiento HDFS. Observe el patrón de Jobs usado por los mecanismos ETL, en los próximos párrafos comprenderá el comportamiento y detalle de cada uno de los pasos. Cada job contiene Cinco (5) pasos, los cuales se detallan a continuación: 1. Start: Es el paso que únicamente da inicio el job. 2. Transf_pdi_hadoop: Es el paso encargado de la llamada a la transformación que realizara la carga de datos de las fuentes al estacionamiento HDFS. 2.1 Transformación (Transf_pdi_hadoop): Es la encargada de consultar los datos de relevancia para el cálculo de los indicadores y almacenarlos en el HDFS de Hadoop. Figura 29 Job Carga Inicial 141 Figura 30 Transf_pdi_hadoop 2.1.1 Table Input: Paso en el cual se establece la conexión compartida a los diferentes sistemas fuentes y consulta SQL. ◦ La Conexión a la Base de Datos: para ello se utilizó una conexión compartida para todas las transformaciones. El nombre de la conexión es Oracle_37, donde se indica el host (Dirección IP del Servidor), el nombre de la Base de Datos Oracle, el puerto a utilizar y las credenciales de acceso (Usuario y Contraseña) correspondiente al esquema específico para cada uno. 142 ◦ La Consulta SQL: Es una consulta SQL de tipo SELECT para obtener los datos de la base de datos Oracle que se quieren cargar en el estacionamiento HDFS. Figura 32 Paso de consulta SQL sobre Oracle Figura 31 Paso de conexión Base de datos Oracle 143 2.1.2 Calculator: Paso mediante el cual se realiza una conversión de los campos tipo date obtenidos de Oracle a un formato aceptado por el motor de búsqueda SolR. Figura 33 Calculator 2.1.3 Hadoop file Output: Es el encargado de la conexión con el clúster Hadoop y la carga de un archivo .txt con los datos y campos especificados que se obtuvieron de los pasos previos en el estacionamiento HDFS. Figura 34 Hadoop file Output 144 Figura 35 Campos a Cargar HDFS 3. Load_Hive: Llamada a la ejecución de un job encargado del procesamiento sobre el almacén distribuido Hive. 3.1 Start: Es el paso que únicamente da inicio el job. 3.2 Sql: En este paso se procede a crear una tabla en el almacén de datos Hive que corresponda a la estructura de los datos previamente obtenidos Figura 36 load_hive 145 de los sistemas fuentes en Oracle y almacenados en Hadoop. ◦ La Conexión a la Base de Datos: para ello se utilizó una conexión compartida para todas las transformaciones. El nombre de la conexión es Hive, donde se indica el host (Dirección IP del Servidor), el nombre de la Base de Datos Oracle, el puerto a utilizar y las credenciales de acceso (Usuario y Contraseña) correspondiente al esquema específico para cada uno. Figura 37 Conexión a Hive ◦ La Consulta HQL: Es una consulta muy parecida a SQL de tipo CREATE para crear una estructura acorde a los datos cargados en el estacionamiento HDFS. 146 Figura 38 Consulta HQL 3.3 Hadoop Copy Files: En este paso se realiza la carga de los datos que se encuentran en texto plano en el HDFS a la nueva estructura creada en Hive sobre el mismo estacionamiento especificando origen y destino. Figura 39 Carga de los datos en estructura Hive 4. Load_hive_map: Llamada a la ejecución de un job encargado del procesamiento para la generación de cubos sobre el almacén distribuido Hive. 147 4.1 Start: Es el paso que únicamente da inicio el job. 4.2 Sql: En este paso se procede a crear el cubo para los indicadores asociados sobre el almacén de datos Hive que corresponda a la estructura de los datos previamente obtenidos de la estructura Hive creada. ◦ La Conexión a la Base de Datos: para ello se utilizó una conexión compartida para todas las transformaciones. El nombre de la conexión es Hive, donde se indica el host (Dirección IP del Servidor), el nombre de la Base de Datos Oracle, el puerto a utilizar y las credenciales de acceso (Usuario y Contraseña) correspondiente al esquema específico para cada uno. Figura 40 load_hive_map 148 Figura 41 Conexión a Hive ◦ La Consulta HQL: Es una consulta muy parecida a SQL de tipo CREATE AS para crear una estructura de cubo proporcionado por Hive. Figura 42 Creación de Cubo Hive 149 5. HiveToSolr: En este paso se realiza la llamada a la transformación que se encarga de indexar los cubos Hive en nuevos documentos Json sobre el motor de búsqueda SolR. 5.1 Table Imput: En este paso se establece la conexión a Hive para obtener los datos almacenados en los cubos y la conversión a formato ligero tipo Json de cada registro del mismo. Figura 43 HiveToSolr 150 ◦ La Conexión a la Base de Datos: para ello se utilizó una conexión compartida para todas las transformaciones. El nombre de la conexión es Hive, donde se indica el host (Dirección IP del Servidor), el nombre de la Base de Datos Oracle, el puerto a utilizar y las credenciales de acceso (Usuario y Contraseña) correspondiente al esquema específico para cada uno. Figura 44 Conexión a Hive ◦ La Consulta HQL: Es una consulta muy parecida a SQL de tipo SELECT con el particular uso de una función creada en Hive (UDF), para la conversión de cada registro en formato Json. 151 Figura 45 Consulta HQL y Conversión a Json 5.2 Select Values: En este paso se procede a seleccionar el valor denominado data que representa a cada registro Json generado en el paso anterior. Figura 46 Selección de Valores 5.3 Crear campos faltantes: En este paso se crean los campos necesarios para llevar a cabo la conexión con el servidor solR. 152 Figura 47 Campos Conexión SolR 5.4 REST Client: Paso encargado de indexar cada uno de los registros obtenidos de los pasos previos en el servidor SolR a través del protocolo REST. Figura 48 REST Client 3 Aspectos técnicos de los mecanismos de ETL de Mediación. Para el desarrollo de los ETL de Mediación por cada uno de los indicadores previamente calculados, se hace de igual manera con la única variante de consideración de la fecha de última carga de datos, como fecha base, para el pase de diferencial de registros. 153 ANEXO 7: Casos de uso Nivel 1 Figura 49 Casos de uso nivel 1 Fuente: propia Tabla 13 Funcionalidades casos de uso nivel 1 Fuente: propia Caso de uso Conectarse a Repositorio Actor PDI Flujo Básico Para ejecutar una transformación de carga de datos, es necesario que 154 antes se haya conectado al repositorio de las transformaciones, las cuales están almacenadas en alguna base de datos, como puede ser postgreSQL Pre-Condición Las transformaciones fueron almacenadas en una base de datos relacional. Caso de uso Ejecución MapReduce Actor Hadoop Flujo Básico Una vez que se solicita la ejecución de un trabajo MapReduce, el jobtracker se encarga de asignar cada trabajo de mapeo a los tasktracker más cercanos a los datanodes, luego de que los tasktrackers finalicen su trabajo le notifican al jobtracker, el cual se encargara de asignar la tarea de reducción a un tasktracker en específico y guardar el resultado en la ruta esperada. Pre-Condición Se solicita la ejecución de un trabajo MapReduce y deben estar activos los procesos principales como el namenode, el datanode, el resource manager, el jobtracker, tasktracker. Caso de uso Almacenamiento en HDFS Actor Hadoop Flujo Básico Un proceso o usuario solicita almacenar en HDFS bien sea mediante un comando o mediante una configuración previa. Pre-Condición Debe estar ejecutándose los procesos principales de Hadoop (namenode, datanode, resource manager) Caso de uso Conectarse a HDFS Actor Hive Flujo Básico Para realizar las consultas en Hive, primero debe realizarse una conexión al HDFS la cual fue configurada previamente en el archivo hive- site.xml. Pre-Condición Pre configuración de hive-site.xml para almacenar en HDFS. 155 Caso de uso Conectarse a las herramientas Actor HUE Flujo Básico Mediante HUE se hace una integración con las herramientas principales de generación como son Hadoop, Hive y Solr, las cuales fueron configuradas previamente. Pre-Condición Se configuro el archivo hue.ini para indicar las herramientas a conectarse y los puertos mediante el cual se harán las conexiones Caso de uso Generación de reportes con Solr Actor Mercal Flujo Básico El usuario de Mercal al ingresar a la interfaz web, puede escoger la opción de generar reportes y dashboards en Solr para obtener información de los Registros Almacenados Pre-Condición El usuario ingreso al sistema mediante autenticación y las herramientas están funcionando correctamente. Caso de uso Consultas con Hive Actor Mercal Flujo Básico Una vez que el usuario ingreso a la interfaz de HUE y escoge la opción de hacer consultas con Hive, puede realizar cualquier consulta en HQL mediante un panel de texto de consultas. Pre-Condición El usuario ingreso previamente al sistema y el Hiveserver2 está corriendo. Caso de uso Administrar HDFS Actor Mercal Flujo Básico El usuario puede observar y administrar el HDFS mediante una interfaz que permite hacer búsquedas de archivos. Pre-Condición Los procesos de Hadoop deben estar levantados. Caso de uso Realizar Petición 156 Actor Nginx Flujo Básico El usuario realiza una petición al momento de ingresar a la interfaz Web que será redirigida a alguno de los servidores HUE configurados. Pre-Condición Todos los componentes y procesos de la arquitectura deben de estar iniciados y en correcto funcionamiento. 157 ANEXO 8: Instalación y configuración de Herramientas Instalación de Java ambiente de Desarrollo 1. Ingresar al servidor con el usuario phd2014. $> ssh phd2014@<ip-servidor> 2. Añada y descargue de los repositorios e instale java 8, luego se establece por defecto en el sistema, ejecute los siguientes comandos. $> su - $> echo "deb http://ppa.launchpad.net/webupd8team/java/ubuntu xenial main" | tee /etc/apt/sources.list.d/webupd8team-java.list $> echo "deb-src http://ppa.launchpad.net/webupd8team/java/ubuntu xenial main" | tee -a /etc/apt/sources.list.d/webupd8team-java.list $> apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys EEA14886 $> apt-get update $> apt-get install oracle-java8-installer $> exit 3. Verificar con el siguiente comando que el java se instaló correctamente: $> java –version Figura 50 Evidencia de la instalación de java en ambiente de desarrollo 4. Para termina, establecer java por defecto ejecute: sudo apt-get install oracle-java8-set-default 158 2 Instalación de Hadoop Una vez iniciada la sesión en el servidor, descargue e instale hadoop-2.7.1, descomprima y mueva la carpeta descomprimida al directorio /phd2014/hdfs/. $>wget http://apache.mesi.com.ar/hadoop/common/stable2/hadoop- 2.7.1.tar.gz $> cd /phd2014/hdfs/ $> tar xzvf hadoop-2.7.1.tar.gz $> rm -f hadoop-2.7.1.tar.gz $> mv hadoop-2.7.1/* . $> rm -rf hadoop-2.7.1 Nota: Para esta instalación ip-nodo-1 corresponde al servidor Maestro, ip-nodo- 2 y ip-nodo-3 los nodos esclavos. Configuración de Hadoop A continuación, se especifica la configuración del clúster de Apache Hadoop para la Misión Mercal C.A., el cual cuenta con tres nodos, designando el servidor de <IP-nodo1> como nodo maestro-esclavo y los servidores de <IP-nodo2> y <IP- nodo3> como nodos esclavos. Iniciar sesión en el servidor. Editar el archivo /etc/host en todos los nodos del clúster. Todos los nodos deben tener la misma configuración en el archivo /etc/hosts ya que esto permitirá conocer los demás nodos pertenecientes al clúster <ip-nodo1> master <ip-nodo2> slave1 <ip-nodo3> slave2 Nota: Este archivo tiene la misma configuración para los tres nodos del clúster. Edite los siguientes archivos en el directorio /phd2014/hdfs/etc/hadoop. 159 hadoop-env.sh: Esta configuración debe aplicarse a todos los nodos Hadoop del clúster. En este archivo debemos especificar el directorio de instalación de Java, para ello modifique la propiedad JAVA_HOME. export JAVA_HOME=/usr/lib/jvm/jdk1.8.0_45 core-site.xml: El nodo maestro y los nodos esclavos deben utilizar el mismo valor para la propiedad fs.defaultFS. Todos los nodos esclavos deben apuntar al nodo maestro. Todos los servicios de Hadoop y clientes usan este archivo para localizar el NameNode, ya que este archivo contiene el nombre del sistema de archivos por defecto. El nodo NameNode será el servidor master con <ip- nodo1>. Además, se especifica la propiedad hadoop.tmp.dir, la cual se utiliza como la base para los directorios temporales a nivel local, y también en el HDFS. Agregue las siguientes propiedades dentro de la etiqueta <configuration>al archivo. Este archivo queda igual para todos los nodos. <property> <name>fs.defaultFS</name> <value>hdfs:master:9000</value> </property> <property> <name>hadoop.tmp.dir</name> <value>file:/var/hadoop/data/hdfs/tmp</value> </property> Nota: Este archivo tiene la misma configuración para los tres nodos del clúster. Antes de configurar el archivo hdfs-site.xml debemos crear los directorios para los datos del datanode y el namenode. $> sudo mkdir -p /var/hadoop/data/hdfs/namenode $> sudo mkdir -p /var/hadoop/data/hdfs/datanode $> sudo mkdir -p /var/hadoop/data/hdfs/tmp 160 $> sudo chown -R phd2014:phd2014 /var/hadoop/* Hdfs-site.xml: Esta configuración debe aplicarse al nodo maestro y a los nodos esclavos. En este archivo especificamos el directorio en el sistema de archivos local donde el NameNode almacenará su archivo de metadatos y DataNode almacenará los bloques de datos. Además, se indica el factor de replicación que tendrá la data del HDFS, esto significa que, por cada archivo almacenado en el HDFS, habrá una repetición redundante de ese archivo en algún otro nodo del clúster. Este archivo queda igual para todos los nodos. Agregar al archivo las siguientes propiedades: <property> <name>dfs.replication</name> <value>3</value> </property> <property> <name>dfs.name.dir</name> <value>file:/var/hadoop/data/hdfs/namenode</value> </property> <property> <name>dfs.data.dir</name> <value>file:/var/hadoop/data/hdfs/datanode</value> </property> Nota: Este archivo tiene la misma configuración para los tres nodos del clúster. yarn-site.xml: Esta configuración debe aplicarse al nodo maestro y los nodos esclavos. Este archivo es necesario para que un nodo trabaje como un nodo hilado, porque posee la configuración adecuada que permite dividir las dos principales funcionalidades del JobTracker, la gestión de recursos (ResourceManager) y trabajo de planificación/monitoreo (NodeManager), en demonios separados (hilos). Los Nodos maestro y esclavo deben utilizar el mismo valor para las siguientes propiedades, y deben estar apuntando a nodo maestro solamente. 161 <property> <name>yarn.nodemanager.aux-services</name> <value>mapreduce_shuffle</value> </property> <property> <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name> <value>org.apache.hadoop.mapred.ShuffleHandler</value> </property> <property> <name>yarn.resourcemanager.address</name> <value>master:8050</value> </property> <property> <name>yarn.resourcemanager.resource-tracker.address</name> <value>master:8025</value> </property> <property> <name>yarn.resourcemanager.scheduler.address</name> <value>master:8035</value> </property> Nota: Este archivo tiene la misma configuración para los tres nodos del clúster. mapred-site.xml: Esta configuración debe aplicarse solo al nodo maestro. En este archivo se especifica el host y el puerto donde se ejecuta el JobTracker y el framework para la ejecución de trabajos MapReduce. Este archivo no existe y se debe crear copiando el archivo mapred-site0.xml.template con el nombre de mapred-site.xml. 162 <property> <name>mapreduce.jobtracker.address</name> <value>master:5431</value> </property> <property> <name>mapreduce.framework.name</name> <value>yarn</value> </property> Actualiza el archivo slaves que se encuentra en el directorio /phd2014/hdfs/etc/hadoop solamente para el nodo maestro. Coloque solo el nombre de los servidores que actúan como esclavos en el clúster. Este archivo es utilizado por los scripts de Hadoop para iniciar los servicios apropiados en los nodos maestros y esclavos. master slave1 slave2 Cree el archivo masters en el directorio /phd2014/hdfs/etc/hadoop solamente para el nodo maestro. Coloque solo nombre del servidor que actúa como maestro en el clúster. Este archivo es utilizado por los scripts de Hadoop para iniciar los servicios apropiados en los nodos maestros y esclavos. Master 4. Configuración SSH Hadoop requiere acceso SSH para administrar sus nodos (máquinas remotas) y su máquina local. El próximo paso es generar una clave ssh sin contraseña de inicio de sesión entre el nodo maestro y los nodos esclavos. Ejecute los siguientes comandos sólo en el nodo maestro. 163 $> su – phd2014 $> ssh-keygen -t rsa -P "" $>cat /home/phd2014/.ssh/id_rsa.pub >> /home/phd2014/.ssh/authorized_keys $> chmod 600 authorized_keys $> ssh-copy-id -i ~/.ssh/id_rsa.pub slave1 $> ssh-copy-id -i ~/.ssh/id_rsa.pub slave2 $> ssh master $> ssh slave1 $> ssh slave2 5. Copiar las siguientes líneas en el archivo .bashrc ubicado en el directorio / home/phd2014 Realizar este paso en el nodo maestro y cada nodo esclavo. export HADOOP_HOME=/phd2014/hdfs export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin Nota: Luego de modificar el archivo .bashrc, éste debe ser cargado nuevamente para que tome los cambios realizados, ejecute source ~/.bashrc para volver a cargar el archivo. 6. Formatear el NameNode Antes de iniciar el clúster, se debe formatear el NameNode solo en el nodo maestro. Utilice el siguiente comando para formatear el nodo. Tenga en cuenta que cada vez que formatea el namenode se elimina la data almacenada en el HDFS. $> cd /phd2014/hdfs/bin $> ./hdfs namenode –format 7. Inicie el sistema de archivo desde el nodo maestro. 164 $> cd /phd2014/hdfs/sbin $> ./start-dfs.sh Para validar que el DFS se inició con éxito, ejecute el siguiente comando en el nodo maestro y el nodo esclavo: $> jps La salida de este comando debe enumerar NameNode y SecondaryNameNode iniciado en el nodo maestro y DataNode en todos los nodos esclavos. 8. Inicie el Yarn MapReduce desde el nodo maestro. $> cd /phd2014/hdfs/sbin $> ./start-yarn.sh Para validar que el Yarn Mapreduce se haya iniciado correctamente, ejecute el comando jps nuevamente en los nodos maestro y esclavos. La salida de este comando debe enumerar ResourceManager en el nodo maestro y NodeManager, en todos los nodos esclavos. 9. Valide el inicio exitoso del clúster a través de las Consolas Web: Para ResourceManager: http://<ip-nodo-1>:8088 Para NameNode: http://<ip-nodo-1>:50070 Nota: Para esta instalación ip-nodo-1 corresponde al servidor master. Figura 51 Evidencia de salida del comando. Enumera NameNode y SecondaryNameNode Figura 52 Evidencia de salida del comando. Enumera NameNode y SecondaryNameNode 165 3 Instalación de Apache Hive, Solr y HUE A continuación, se muestra la instalación de las herramientas Hive, Solr y HUE, las cuales permiten la ejecución de consultas sobre la data almacenada en el clúster de Hadoop de una manera eficiente. Dichas instalaciones solo deben realizarse en el nodo Hadoop Maestro para Hive y SolR y HUE en sus respectivos servidores. Preparación del Entorno: Ingresar al servidor y descargar el paquete apache-hive-1.2.1-bin.tar.gz mediante el comando: wget http://www-us.apache.org/dist/hive/hive- 1.2.1/apache-hive-1.2.1-bin.tar.gz, Luego, se crean los directorios /phd2014/hive/ en el servidor master y /phd2014/hue/ en el servidor HUE. $> mkdir /phd2014/hive $> mkdir /phd2014/hue Instalación de Apache Hive Descomprima el archivo apache-hive-1.2.1-bin.tar.gz, ubicado en el directorio de descarga, dentro del directorio /phd2014/hive/.: Edite el archivo /home/phd2014/.bashrc y agregue las siguientes líneas: export HIVE_HOME=/phd2014/hive export PATH=$PATH:$HIVE_HOME/bin Nota: Luego de modificar el archivo .bashrc, éste debe ser cargado nuevamente para que tome los cambios realizados, ejecute source ~/.bashrc para volver a cargar el archivo. 3. Hive ha actualizado la librería Jline2, pero existe una versión anterior de esta librería en el directorio HADOOP_HOME/share/hadoop/yarn/lib la cual debemos eliminar antes de iniciar Hive, para ello ejecute el siguiente comando: 166 rm -r /phd2014/hdfs/share/hadoop/yarn/lib/jline-0.9.94.jar 4. Ejecute hive con el siguiente comando y verifique la instalación: $> /phd2014/bin/./hive Logging initialized using configuration in jar:file:/phd2014/hive/lib/hive-common- 1.2.0.jar!/hive-log4j.properties hive> 5. Para salir de hive ejecute: hive> exit; Instalación de Solr 1. Una vez dentro del servidor Solr descomprimir el archivo solr-6.1.0.zip,, ubicado en la ruta: /phd2014/instaladores/, dentro de /phd2014/solr/. La estructura del directorio debe quedar como se muestra continuación: 2. Edite el archivo /home/phd2014/.bashrc y agregue las siguientes líneas: Figura 53 Evidencia Ejecución Hive Figura 54 Evidencia Ejecución Hive 167 export SOLR_HOME=/phd2014/solr export PATH=$PATH:$SOLR_HOME/bin Nota: Luego de modificar el archivo .bashrc, éste debe ser cargado nuevamente para que tome los cambios realizados, ejecute source ~/.bashrc para volver a cargar el archivo. 3. Ejecute solr con el siguiente comando. $> cd /phd2014/solr/bin $> ./solr start 4. Verifique la instalación: http://<ip-nodo>:8983 Instalación de Hadoop User Experience (HUE) Instalando requisitos previos Ejecutar los siguientes comandos para instalar los paquetes de desarrollo Nota: La instalación debe realizarse en el mismo usuario donde esté instalado Hadoop. Este debe poseer permisos de administrador para poder realizar la instalación. Nota: La instalación debe realizarse en el mismo usuario donde esté instalado Hadoop. Este debe poseer permisos de administrador para poder realizar la instalación. Si el usuario no posee los permisos de administrador, estos pueden ser otorgados desde el usuario root modificando el archivo sudoers con el comando sudo nano etc/sudoers, luego agregamos la línea “<usuario> ALL=(ALL:ALL) ALL” justo debajo de los privilegios de root y guardamos el archivo. sudo apt-get install python2.7-dev make libkrb5-dev libxml2- dev libxslt-dev libsqlite3-dev libssl-dev libldap2-dev python- pip sudo apt-get install python2.7-dev make libkrb5-dev libxml2- dev libxslt-dev libsqlite3-dev libssl-dev libldap2-dev python- pip sudo apt-get install ant gcc g++ libmysqlclient-dev libssl-dev libsasl2-dev libsasl2-modules-gssapi-mit libtidy-0.99-0 make libldap2-dev maven python-dev python-setuptools libgmp3-dev 168 Instalación de Maven Utilizamos este comando para descargarnos la última versión de Maven Tarball Figura 55 Descarga Maven Tarball Descomprimimos el archivo Figura 56 Descomprimir Maven wget http://www-eu.apache.org/dist/maven/maven- 3/3.3.9/binaries/apache-maven-3.3.9-bin.tar.gz wget http://www-eu.apache.org/dist/maven/maven- 3/3.3.9/binaries/apache-maven-3.3.9-bin.tar.gz tar xzvf apache-maven-3.3.9-bin.tar.gz tar xzvf apache-maven-3.3.9-bin.tar.gz 169 Wget https://dl.dropboxusercontent.com/u/730827/hue/releases/3.8.1/hue- 3.8.1.tgz Verificamos el valor de nuestra variable $JAVA_HOME (este valor variará según la versión de Java instalada) Figura 57 Verificando versión Java Agregamos a PATH y verificamos instalación Figura 58 Verificar Instalación Maven Instalando y configurando Hue. Para descargar Hue utilizamos el siguiente comando export PATH=/rutahastaelarchivomaven/apache-maven-3.3.9/bin:$PATH export PATH=/rutahastaelarchivomaven/apache-maven-3.3.9/bin:$PATH mvn -v 170 make apps tar xzvf hue-3.8.1.tgz mv hue-3.8.1 hue Cd Hue Figura 59 Descargando HUE Descomprimimos el archivo y renombramos la carpeta Figura 60 Construyendo HUE Para instalar Hue nos posicionamos en la carpeta Hue e instalamos 171 ./build/env/bin/hue runserver http://127.0.0.1:8000/ Figura 61 Construyendo HUE Así debería finalizar la instalación Figura 62 Finalizando instalación HUE Después de terminar la instalación podemos iniciar Hue posicionándonos otra vez en la carpeta de Hue y ejecutando Figura 63 Iniciando HUE Y luego accedemos desde el navegador web a Hue solicitará que se cree un usuario para poder continuar. Es recomendado que este coincida con el nombre de usuario de GNU/linux donde está instalado Hue y Hadoop. 172 <property> <name>dfs.webhdfs.enabled</name> <value>true</value> </property> cd $HADOOP_HOME/etc/hadoop nano hdfs-site.xml Figura 64 Interfaz Web HUE Enlazar Hue y Hadoop Abrimos y modificamos el archivo hdfs-site.xml de hadoop y agregamos las siguientes líneas 173 <property> <name>hadoop.proxyuser.hue.hosts</name> <value>*</value> </property> <property> <name>hadoop.proxyuser.hue.groups</name> <value>*</value> </property> cd $HADOOP_HOME/etc/hadoop nano core-site.xml Figura 65 Enlazando HUE - Hadoop Abrimos y modificamos el archivo core.site.xml de hadoop y agregamos las siguientes líneas 174 Figura 66 Enlazando HUE – Hadoop 2 Para verificar que Hue y Hadoop fueron enlazados exitosamente iniciar los demonios de hadoop, iniciar hue con el comando ./build/env/bin/hue runserver, acceder a http://127.0.0.1:8000/, y seleccionar la opción Gestionar HFDS que se encuentra en la parte superior derecha de la página principal de Hue. Ahí deberían visualizarse los archivos HDFS. Figura 67 Gestión HDFS con HUE http://127.0.0.1:8000/ 175 Instalando y configuración Nginx. Para lograr la configuración requerida y el correcto balance de peticiones entrantes es necesario la utilización de un balanceador de carga. En esta arquitectura se utilizará un servidor de NGINX. Instalando requisitos previos: Instalar las dependencias necesarias para la compilación: $> sudo apt-get install build-essential libssl-dev libpcre3-dev Descargar la versión del NGINX: $> wget http://nginx.org/download/nginx-1.4.4.tar.gz Descomprima el archivo nginx-1.4.4.tar.gz que acaba de descargar mueva la carpeta generada al directorio /home/<su_usuario>/mercal/ $> tar xzvf nginx-1.4.4.tar.gz $> mv nginx-1.4.4 nginx $> mv nginx /home/<su_usuario>/mercal/ Queda compilarlo e instalarlo, de momento necesitaremos los módulos siguientes para la ejecución de la configuración: --with-http_gzip_static_module –sbin-path=/usr/local/sbin -- with-http_ssl_module --without-mail_pop3_module -- without-mail_imap_module --without-mail_smtp_module -- with-http_stub_status_module --with-http_realip_module Compilar e instalar: 176 $> cd /home/<su_usuario>/mercal/nginx $> ./configure --with-http_gzip_static_module --sbin- path=/usr/local/sbin \ --with-http_ssl_module --without-mail_pop3_module –without- mail_imap_module\ --without-mail_smtp_module --with- http_stub_status_module --with-http_realip_module $> make -j 4 && make install El resultado debe ser similar a: Figura 68 Compilar Nginx Despliegue del Nginx Luego, es necesario descargar el script que permite iniciar, detener, reiniciar y recargar NGINX mediante el comando service, podemos descargarlo utilizand los siguientes comandos: 177 $> wget https://raw.github.com/JasonGiedymin/nginx-init- ubuntu/master/nginx $> sudo mv nginx /etc/init.d/nginx $> sudo chmod +x /etc/init.d/nginx $> sudo chown root:root /etc/init.d/nginx $> update-rc.d nginx defaults Por último inicie el servicio $> service nginx start Verificación del despliegue Para verificar que el NGINX se ejecuta de manera exitosa escriba por consola el siguiente comando: $> service nginx status La salida debe ser similar a esto Figura 69 Status Nginx También puede navegar a través de algún navegador con la dirección ip de su servidor por el puerto 80 y se encontrara la siguiente interfaz: 178 Figura 70 Interfaz Nginx Archivos de configuración utilizados Para configurar el nginx ubíquese a la ruta /usr/local/nginx/sites-available/ y modifique el archivo default upstream lb { # ip_hash; server ip-server1:8243; server ip-server2:8243; server ip-serverN:8243; } server { listen 443; # ssl on; # ssl_certificate /etc/nginx/keys/current/esb/cacert.pem; # ssl_certificate_key /etc/nginx/keys/current/esb/newcakey.pem; # ssl_session_timeout 5m; # ssl_protocols SSLv3 TLSv1; # ssl_ciphers ALL:!ADH:!EXPORT56:RC4+RSA:+HIGH:+MEDIUM:+LOW:+SSLv3: +EXP; # ssl_prefer_server_ciphers on; location / { proxy_pass https://lb; 179 proxy_read_timeout 240s; } } El archivo permite que toda solicitud que se haga utilizando el nombre provisto lb agregado previamente en el archivo /etc/hosts por el puerto seguro https 443, sea redirigida a alguno de los servidores a balancear la carga. Configuración de los aspectos de seguridad Si desea habilitar la seguridad puede descomentar las líneas del archivo que se muestran a continuación: upstream lb { # ip_hash; server ip-server1:8243; server ip-server2:8243; server ip-serverN:8243; } server { listen 443; ssl on; ssl_certificate /usr/local/nginx/keys/current/esb/cacert.pem; ssl_certificate_key /usr/local/nginx/keys/current/esb/newcakey.pem; ssl_session_timeout 5m; ssl_protocols SSLv3 TLSv1; ssl_ciphers ALL:!ADH:!EXPORT56:RC4+RSA:+HIGH:+MEDIUM:+LOW:+SSLv3:+E XP; ssl_prefer_server_ciphers on; location / { proxy_pass https://lb; proxy_read_timeout 240s; 180 } } En ellas debe indicar la ubicación con los archivos con las llaves públicas y privadas. Verificación y ubicación de archivos Log Para verificar el registro de su nginx ubique la ruta /usr/local/nginx/logs Revise los archivos access.log y error.log Comandos utilizados frecuentemente El siguiente comando inicia el NGINX en el sistema. $> service nginx start Para detener el proceso basta con ejecutar el siguiente comando. $> service nginx stop Por último podemos reiniciar el NGINX en caso de existir algún comportamiento no deseado. $> service nginx restart Recomendaciones y mejores prácticas • Utilizar nombres simbólicos en los archivos de configuración y agregar estos a su archivo hosts ubicado en la ruta /etc/ esto evitara volver a modificar los archivos en caso de necesitar cambiar las direcciones de los servidores a los que se le realiza el balanceo de carga. • Antes de comenzar con la instalación ejecute el comando $> netstat -putan 181 • Verifique que el puerto 80 no esté ocupado. Ese puerto es el por defecto del NGINX. • Apache2 utiliza ese puerto por defecto también por lo tanto si este está activo ejecute $> service apache stop • Si su usuario no posee privilegios de administrador, al momento de ejecutar el comando make hágalo como un supe usuario. • Si utiliza la seguridad a través de llaves, guarde esos archivos en una carpeta dentro de la misma ruta /usr/local/nginx. 182 ANEXO 9: Arquitectura Propuesta de Componentes 183 ANEXO 10: Arquitectura Propuesta con Herramientas Seleccionadas 184 ANEXO 11: Indicadores Calculados Ilustración 1 Cheques Emitidos Ilustración 2 Ingresos 185 ANEXO 12: Juicio de Experto Caracas, Enero de 2017 Estimado (a) señor (a): Motiva la presente el solicitar su valiosa colaboración en la revisión del instrumento anexo, el cual tiene como objeto obtener la validación del cuestionario que se aplicará para la fundamentación y desarrollo de la tesis de grado titulada “Implementación de una Arquitectura Analítica BIG DATA basada en herramientas de software libre”. Acudo a usted debido a sus conocimientos y experiencias en la materia, los cuales aportarían una útil y completa información para la culminación exitosa de este trabajo especial de grado. Gracias por su valioso aporte y participación. Atentamente, José Luis Prato Zuñiga. Hever Alfonso Salas Garcia. INSTRUCCIONES A) Lea detenidamente las preguntas antes de responder. B) Este instrumento de validación consta de una primera parte de identificación del experto, seguidamente otra en donde se identifica el título de la investigación, los objetivos, indicadores y alternativas de respuesta del cuestionario objeto de esta validación. Luego se encuentra una sección en la que se pide el juicio de experto con respecto al cuestionario, la cual está formada por siete preguntas, cuyas respuestas son: suficiente, medianamente suficiente e insuficiente, las dos 186 primeras interrogantes, y si o no las restantes, seleccione la opción de su preferencia marcando una equis (x) en el espacio indicado para tal fin. C) Seguido del juicio del experto se solicita una opinión sobre el instrumento diseñado. D) Por último, se pide al experto que analizó el cuestionario una constancia de que realizó dicha tarea. 1. Identificación del Experto: Nombre y Apellido: ______________________________________________ Instituto donde Trabaja: ___________________________________________ Título de Pregrado: ______________________________________________ Título de Postgrado: ____________________________________Institución donde lo obtuvo: ________________________________________________ Año: _________________________________________Trabajos Publicados: _______________________________________________________________ ___________________________________________________________ 2. Título de la Investigación: Implementación de una Arquitectura Analítica BIG DATA basada en herramientas de software libre. 2.1. Objetivos del Estudio: 2.2. Objetivo General: Implementación de una Arquitectura Analítica BIG DATA basada en herramientas de software libre, específicamente para la Misión Mercal C.A. (Mercado de Alimentos).2.3. Objetivos Específicos:  Diseñar la arquitectura Analítica Big Data.  Seleccionar las herramientas asociadas a cada componente de la arquitectura diseñada.  Proponer e implementar la arquitectura con las herramientas seleccionadas. 187 3. Variable que se pretende medir: Desarrollo de una Arquitectura Analítica BIG DATA basada en herramientas de software libre para la Misión Mercal C.A. (Mercado de Alimentos). 3.1. Indicadores: 3.1.1. Seguridad 3.1.2. Capacidad de pruebas 3.1.3. Escalabilidad 3.1.4. Tolerancia a fallos 3.1.5. Análisis de datos 3.1.6. Visualización de datos 4. Alternativas de respuestas: Sí No 5. JUICIOS DEL EXPERTO: 5.1. En líneas generales, considera que los indicadores de la variable están inmersos en su contexto teórico de forma: _____ Suficiente _____ Medianamente suficiente _____ Insuficiente 5.2. Considera que los reactivos del cuestionario miden los indicadores seleccionados para la variable de manera: _____ Suficiente _____ Medianamente Suficiente _____ Insuficiente 5.3. Considera que existe pertinencia entre los objetivos de la investigación. _____ Si _____ No 188 Observaciones: ________________________________________________ _____________________________________________________________ 5.4. Considera que existe pertinencia entre los indicadores y la variable de estudio. _____ Si _____ No Observaciones: ________________________________________________ _____________________________________________________________ 5.5. Considera que existe pertinencia entre los indicadores y los objetivos de la investigación. _____ Si _____ No Observaciones: ________________________________________________ _____________________________________________________________ 5.6. Considera que existe pertinencia entre los indicadores y las dimensiones de la investigación. _____ Si _____ No Observaciones: ________________________________________________ _____________________________________________________________ 5.7. Considera que los reactivos del cuestionario están redactados de manera adecuada. _____ Si _____ No Observaciones: ________________________________________________ _____________________________________________________________ 6. El instrumento diseñado es: 189 _______________________________________________________________ _______________________________________________________________ _______________________________________________________________ _______________________________________________________________ _______________________________________________________________ _______________________________________________________________ _______________________________________________________________ ___________________________________ 7. Constancia de Juicio de experto: Yo, _______________________________, titular de la cédula de identidad No. ____________________ certifico que realicé el juicio del experto al instrumento diseñado por los bachilleres José Luis Prato Zuñiga y Hever Alfonso Salas Garcia en el Trabajo Especial de Grado: “Implementación de una Arquitectura Analítica BIG DATA basada en herramientas de software libre.” 190 Gráficos de los resultados 0 1 2 3 4 5 6 7 8 SUFICIENTE MEDIANAMENTE SUFICIENTE INSUFICIENTE Considera que los indicadores de la variable estan inmersos en su contexto teorico de forma Considera que los indicadores de la variable estan inmersos en su contexto teorico de forma 0 1 2 3 4 5 6 7 8 9 SUFICIENTE MEDIANAMENTE SUFICIENTE INSUFICIENTE Considera que los reactivos del cuestionario miden los indicadores seleccionados para la variable de manera Considera que los reactivos del cuestionario miden los indicadores seleccionados para la variable de manera 191 Considera que existe pertinencia entre los objetivos de la investigación SI NO Considera que existe pertinencia entre los indicadores y la variable de estudio SI NO 192 Considera que existe pertinencia entre los indicadores y los objetivos de la investigación SI NO Considera que existe pertinencia entre los indicadores y las dimensiones de la investigación SI NO 193 Considera que los reactivos del cuestionario están redactados de manera adecuada SI NO